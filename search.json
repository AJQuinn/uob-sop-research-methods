[
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "copilot.html",
    "href": "copilot.html",
    "title": "Using Copilot at University of Birmingham",
    "section": "",
    "text": "Microsoft Copilot is now available to use for University of Birmingham members. See the University documentation pages to get access\nYou are welcome to use Copilot to support your learning of statistics and coding throughout the Research Methods B computer practical sessions. In particular, Copilot can be a useful way to get a second perspective on different technical concepts, to help generate data analysis code or to understand where data analysis code might have gone wrong.\nThroughout the course - we will suggest places where you might benefit from interacting with Copilot to support your learning. Look out for dialogue boxes like this one.\n\n\n\n\n\n\nCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about variables in R. We suggest you try the following prompts to get started.\n\n‚ÄúWrite a simple description of variables in R for a beginner‚Äù\n‚ÄúHow can I choose a name for my variable in R‚Äù\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\n\n\n\n\n\n\nEthical use of Copilot\n\n\n\nTools like Copilot can be a valuable learning aid, however it is critical that you familiarise yourself with the University‚Äôs student and PGR guidance on using GenAI tools ethically for work. This guidance provides you with the risks and limitations of using Generative AI tools in your academic study\nIt is not permitted to use Generative AI tools in assessment unless explicitly stated on the assessment page. It is your responsibility to use these tools in an appropriate way.\n\n\n\n\nHere are some questions with Copilot‚Äôs answers to help you decide how and when to use Copilot. Note that Copilot may respond differently if you ask identical prompts.\n\n\n\nStarting to learn R with the help of Copilot can be a smooth and enjoyable process. Here are some steps and tips to get you started:\n\nBegin by asking Copilot to explain fundamental concepts in R. This will give you a solid foundation to build on. - Example Questions: - ‚ÄúWhat are the basic data types in R?‚Äù - ‚ÄúHow do I create a vector in R?‚Äù\n\nRequest detailed guides for common tasks. This helps you understand the process and logic behind the code. - Example Questions: - ‚ÄúCan you show me how to read a CSV file into R?‚Äù - ‚ÄúHow do I create a simple plot using ggplot2?‚Äù\n\nReinforce your learning by practicing with beginner exercises. This will help you apply what you‚Äôve learned. - Example Questions: - ‚ÄúCan you give me a beginner exercise for data manipulation in R?‚Äù - ‚ÄúWhat are some basic exercises to practice using loops in R?‚Äù\n\nLearn about important R packages that can simplify your work. Copilot can introduce you to these packages and show you how to use them. - Example Questions: - ‚ÄúWhat is the dplyr package and how do I use it?‚Äù - ‚ÄúCan you explain how to use the tidyverse package for data analysis?‚Äù\n\nWhen you encounter errors, ask Copilot for help in understanding and fixing them. This will improve your debugging skills. - Example Questions: - ‚ÄúI‚Äôm getting an error with my R code, can you help me debug it?‚Äù - ‚ÄúWhat does this error message mean in R?‚Äù\n\nAsk about best practices for writing clean and efficient R code. This will help you develop good &gt; coding habits. - Example Questions: - ‚ÄúWhat are some best practices for writing R code?‚Äù - ‚ÄúHow can I make my R scripts more efficient?‚Äù\n\nSeeing examples can be very helpful. Ask for code snippets to understand how to implement different &gt; functions and methods. - Example Questions: - ‚ÄúCan you show me an example of how to use the print function in R?‚Äù - ‚ÄúHow do I perform a linear regression in R?‚Äù\n\nDon‚Äôt hesitate to ask about anything you‚Äôre curious about, even if it seems basic. Copilot is here to help you learn at your own pace. - Example Questions: - ‚ÄúWhat are some good resources for learning R?‚Äù - ‚ÄúHow do I create a histogram in R?‚Äù\n\n\n‚ÄúHow do I extract specific rows from a data frame in R?‚Äù\n‚ÄúWhat are some common data visualization techniques in R?‚Äù\n\nBy leveraging Copilot‚Äôs capabilities, you can get personalized assistance and resources tailored to your learning needs. Remember, learning a new language takes time and practice, so be patient with yourself and keep experimenting. Happy coding! If you have any specific questions or need further guidance, feel free to ask!\n\n\n\n\n\nCode generated by Copilot can be a great starting point, but it‚Äôs important to review and test it thoroughly. Here are a few tips to ensure the code meets your needs:\n\nReview for Accuracy: Check the logic and syntax to make sure it aligns with your requirements.\nTest Thoroughly: Run tests to ensure the code works as expected in different scenarios.\nSecurity Considerations: Be mindful of potential security vulnerabilities and address them as needed.\nCustomization: Adapt and refine the code to better fit your specific use case.\n\nUsing these practices can help you make the most of Copilot‚Äôs assistance while ensuring the quality and reliability of your code. If you have any specific coding questions or need help with a particular piece of code, feel free to ask! üòä",
    "crumbs": [
      "Home",
      "Support",
      "Using Copilot at UoB"
    ]
  },
  {
    "objectID": "copilot.html#copilot-giving-advice-about-how-to-use-copilot",
    "href": "copilot.html#copilot-giving-advice-about-how-to-use-copilot",
    "title": "Using Copilot at University of Birmingham",
    "section": "",
    "text": "Here are some questions with Copilot‚Äôs answers to help you decide how and when to use Copilot. Note that Copilot may respond differently if you ask identical prompts.\n\n\n\nStarting to learn R with the help of Copilot can be a smooth and enjoyable process. Here are some steps and tips to get you started:\n\nBegin by asking Copilot to explain fundamental concepts in R. This will give you a solid foundation to build on. - Example Questions: - ‚ÄúWhat are the basic data types in R?‚Äù - ‚ÄúHow do I create a vector in R?‚Äù\n\nRequest detailed guides for common tasks. This helps you understand the process and logic behind the code. - Example Questions: - ‚ÄúCan you show me how to read a CSV file into R?‚Äù - ‚ÄúHow do I create a simple plot using ggplot2?‚Äù\n\nReinforce your learning by practicing with beginner exercises. This will help you apply what you‚Äôve learned. - Example Questions: - ‚ÄúCan you give me a beginner exercise for data manipulation in R?‚Äù - ‚ÄúWhat are some basic exercises to practice using loops in R?‚Äù\n\nLearn about important R packages that can simplify your work. Copilot can introduce you to these packages and show you how to use them. - Example Questions: - ‚ÄúWhat is the dplyr package and how do I use it?‚Äù - ‚ÄúCan you explain how to use the tidyverse package for data analysis?‚Äù\n\nWhen you encounter errors, ask Copilot for help in understanding and fixing them. This will improve your debugging skills. - Example Questions: - ‚ÄúI‚Äôm getting an error with my R code, can you help me debug it?‚Äù - ‚ÄúWhat does this error message mean in R?‚Äù\n\nAsk about best practices for writing clean and efficient R code. This will help you develop good &gt; coding habits. - Example Questions: - ‚ÄúWhat are some best practices for writing R code?‚Äù - ‚ÄúHow can I make my R scripts more efficient?‚Äù\n\nSeeing examples can be very helpful. Ask for code snippets to understand how to implement different &gt; functions and methods. - Example Questions: - ‚ÄúCan you show me an example of how to use the print function in R?‚Äù - ‚ÄúHow do I perform a linear regression in R?‚Äù\n\nDon‚Äôt hesitate to ask about anything you‚Äôre curious about, even if it seems basic. Copilot is here to help you learn at your own pace. - Example Questions: - ‚ÄúWhat are some good resources for learning R?‚Äù - ‚ÄúHow do I create a histogram in R?‚Äù\n\n\n‚ÄúHow do I extract specific rows from a data frame in R?‚Äù\n‚ÄúWhat are some common data visualization techniques in R?‚Äù\n\nBy leveraging Copilot‚Äôs capabilities, you can get personalized assistance and resources tailored to your learning needs. Remember, learning a new language takes time and practice, so be patient with yourself and keep experimenting. Happy coding! If you have any specific questions or need further guidance, feel free to ask!\n\n\n\n\n\nCode generated by Copilot can be a great starting point, but it‚Äôs important to review and test it thoroughly. Here are a few tips to ensure the code meets your needs:\n\nReview for Accuracy: Check the logic and syntax to make sure it aligns with your requirements.\nTest Thoroughly: Run tests to ensure the code works as expected in different scenarios.\nSecurity Considerations: Be mindful of potential security vulnerabilities and address them as needed.\nCustomization: Adapt and refine the code to better fit your specific use case.\n\nUsing these practices can help you make the most of Copilot‚Äôs assistance while ensuring the quality and reliability of your code. If you have any specific coding questions or need help with a particular piece of code, feel free to ask! üòä",
    "crumbs": [
      "Home",
      "Support",
      "Using Copilot at UoB"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "This week we will explore how to use Jamovi to compute and visualise descriptive statistics from a dataset. This will include some revision on Jamovi from last semester and an introduction on how we can use the R coding language to simplify complex data processing.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nMeasures of central tendency\n\n\n\nMeasures of dispersion and spread\n\n\n\nMeasures of skew and normality\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Rj editor to use R code within Jamovi\n\n\n\nLearn the basics about functions and variables in R\n\n\n\nComputing descriptive statistics using R\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nValidating and working with openly available research data\n\n\n\nCreate a reproducible data analysis script\n\n\n\n\n\n\n\n\n\n\n\nDataset\n\n\n\nNote that this is the same dataset as in week 1\n\n\nWe will be exploring data that was collected as part of the ‚ÄòMany Labs 3‚Äô project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‚ÄòBig Five‚Äô personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs.¬†extravagant/careless)\nAgreeableness (friendly/compassionate vs.¬†critical/rational)\nNeuroticism (sensitive/nervous vs.¬†resilient/confident)\nOpenness to experience (inventive/curious vs.¬†consistent/cautious)\nExtraversion (outgoing/energetic vs.¬†solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.\n\n\n\nLast week, we looked at how we can spend some time understanding a new dataset and compute some descriptive statistics from it.\nNow, we‚Äôre going to learn how to use Jamovi and R to make this process more repeatable and more transparent. This will make it much easier for us to come back to the analysis and understand what steps were taken at a later date.\n\n\n\n\n\n\nOpening Jamovi\n\n\n\nWe‚Äôll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham‚Äôs AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‚ÄòDesktop‚Äô application - not the ‚ÄòCloud‚Äô version.\n\n\n\n\n\nWe‚Äôll be working with the Rj Add-On for Jamovi this week.\n\n\n\n\n\n\nKey step\n\n\n\nBefore continuing, install the Rj add-on for jamovi following the guide for installing Jamovi modules. Once installed you should have a new button on the Ribbon menu at the top of Jamovi‚Ä¶\n\n\n\nClick on this and select ‚ÄòRj Editor‚Äô to open a window in which we can write R code! Click on the small cog icon and make sure that ‚ÄòOutput‚Äô is set to ‚ÄòShow code and output‚Äô.\n\nWe can use the left-hand window, named Rj editor, to write commands, using the R language. These are the straightforward text commands that tell the computer a series of steps that we wanted to carry out. These commands can be edited and rerun as many times as we need, they can even be shared with collaborators and included as part of scientific publication.\nThis is a very powerful tool once we have a written set of instructions to work with rather than individually clicking through instructions. Though it takes a bit longer to write the commands, our analysis is much more transparent and reproducible once it is done.\nThe outputs of the analysis defined in the Rj editor appear on the right-hand side of the Jamovi window as usual. To check this out, let‚Äôs explore using or as a calculator.\n\n\n\n\n\n\nData Skills - R as a calculator\n\n\n\nTry typing a set of sums or equations into the Rj editor. Include one sum on each line and include as many as you like. When you‚Äôre ready, you can tell RJ to run the commands by clicking on the green triangle in the top right corner of the RJ editor. This will automatically compute. All of the sums you to find and print the results out in the right hand side window. see the example below.\nYou can use the following symbols to define your sums.\n\n\n\nSymbol\nOperation\n\n\n\n\n+\nAdd\n\n\n-\nSubtract\n\n\n*\nMultiply\n\n\n/\nDivide\n\n\n\n\n\n\n\n\n\n\nOften, we don‚Äôt just want to see the result of a calculation, we want to store the result to use again later. For this, we want to use variables. For example, here we store the result of a calculator sum in a variable named ‚Äòy‚Äô\ny &lt;- 2 + 2\nIf you run this in Rj, you‚Äôll notice that we no longer get the number 4 printed to the screen. This is as the output is now caught and stored rather than printed out. If we still want to see what is contained in y, we can use another helpful function named print.\ny &lt;- 2 + 2\nprint(y)\nWill now also display the content of y.\n\n\n\n\n\n\nDefinition of a variable\n\n\n\nA variable is an object that provides a labelled space to store information. The &lt;- or = symbols indicate that we‚Äôre assigning some information to a variable (the two symbols behave the same way, you could see either).\nCrucially, the information on the right hand side is assigned to a variable with the name from the left hand side.\n\n\n\n\n\n\n\n\nCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about variables in R. We suggest you try the following prompts to get started.\n\n‚ÄúWrite a simple description of variables in R for a beginner‚Äù\n‚ÄúHow can I choose a name for my variable in R‚Äù\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\nNotice that the one of the equations in my example is a bit different to the others. The final example uses a function called sqrt as a shortcut to compute the square root of the number nine.\nThe sqrt function calculates the square root of a given number. In R code, this looks something like this:\nsqrt(x)\nwith the ‚Äòx‚Äô replaced by any number. For example, we could compute the square root of 36 using the following command.\nsqrt(36)\nAdd this line to your Rj editor and check the result..\nMany functions, take inputs contained in parentheses written after the name of the function. These inputs are sometimes known as the arguments of a function. In the last example, 36 is an argument for the function sqrt.\nSometimes functions take several arguments, and the arguments themselves can have specific names. For example.\nround(12.345, digits = 0)\nwill round the value x to a whole number with zero digits after the decimal point. Where as\nround(12.345, digits = 2)\nwould round the same number to preserve 2 digits after the decimal point. Try running these examples in Rj with a range of different numbers to get a sense of how it works.\n\n\n\n\n\n\nDefinition of a function\n\n\n\nFunctions are used to perform specific, predefined tasks, such as calculations, data transformations, or visualization. You can think of it as a ‚Äúblack box‚Äù that takes in a value, performs some internal calculation, and returns the result.\n\n\n\n\n\n\n\n\nCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about functions in R. We suggest you try the following prompts to get started.\n\n‚ÄúWrite a simple description of functions in R for a beginner‚Äù\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\nA single analysis might have several lines of code that create, store and reuse a several different variables with several different functions. For example:\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat does this R code do? what is the value of x, y and z?\n x &lt;- 10 + 10\n y &lt;- sqrt(16)\n z &lt;- x / y\n\n\nTry copying out and running the code in Rj.\nYou can add some print functions to see the value of the intermediate variables x and y as well. Try adding print(x) on the line immediately after x is defined.\nTalk to your tutor if you‚Äôre getting stuck.\n\n\nThe final answers:\nx is 20 and y is 4, so z = 20 / 4 which is 5.\n\n\n\n\n\n\n\nRj has a special feature that the data loaded into our spreadsheet is always contained in a special variable named data. We can refer to this data variable to view and run analyses on our dataset.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to load the data file into Jamovi and compute the descriptive statistics for the big five personality factors again.\n\n\n\nThe data we have loaded into our Jamovi session can be accessed in Rj with the data variable. Jamovi automatically makes the data available for us in the R environment.\nWe can view and interact with the data here as well. For example, to view the first few rows of the dataset you can run the head function and pass in data as an argument.\nhead(data)\nand to view the last few rows, you can do the same with the tail function.\ntail(data)\nOften, we‚Äôll only want to work with a subset of the columns in a data frame at any given time. We can access individual columns using the dollar operator. For example, this code will access the age column within the loaded dataset.\ndata$age\nLet‚Äôs break this command down into its pieces:\n\n\n\n\n\n\n\nR code\nMeaning\n\n\n\n\ndata\nthis is the name of the whole data array we have loaded in\n\n\n$\nthis is a separator that R uses to help us access subsections of datasets\n\n\nage\nThis is the name of a single column we want to access\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you access the column of Mood scores from the same dataset?\n\n\nYou can use the head function to see the list of all available columns in the dataset. The example above accesses the ‚Äòage‚Äô column, you can tweak this to extract the mood data instead.\n\n\nThe final code should look like this\ndata$Mood\n\n\n\n\n\nWe can also apply functions to these variables. R includes a wide range of standard mathmatical functions that we can use for common computations. This code computes the mean of the age data in our dataset.\nmean(data$age)\nTry running this in Rj. You should get the answer 19.3 - this matches the value we computed using the descriptives analysis last week.\nSimilarly, the following function computes the standard deviation\nsd(data$age)\nIf you run this, you should get the answer 3.808.\n\n\n\nOk, back to statistics! How can we use our new R skills to compute statistics?\nThe trick is to understand that all the analyses in Jamovi are actually carried out by functions in R. Each analysis we can select from the top ribbon in Jamovi corresponds to a function that can be called in the code. Similarly, all of the options that we select in the analysis, dialogue boxes, correspond to arguments for that function. Let‚Äôs take a look in practice.\nJamovi provides a collection of R functions that correspond to each analysis dialogue box in Jamovi. This package of functions is named jmv. Thejmv function that computes descriptive statistics is called:\njmv::descriptives\nHere we first name the jmv package specify the descriptives function within it (this link is made with the two colons).\nWe can compute the descriptive statistics for age by passing the data into the function and specifying that we want to use the age variable. Try running the following:\njmv::descriptives(data, vars='age')\nand you should generate the following result table.\n\n\n\n\n\n\n\nNote!\n\n\n\nYou might notice that we still have the unusual ages in this analysis, we‚Äôll discuss how to remove them in the next section\n\n\nJamovi provide a lot of documentation on the descriptives function. This lists every single option that can be passed into jmv::descriptives. There is an option here for every single checkbox and selector in the main Jamovi dialogue.\nThe following code additionally computes a histogram by adding the hist argument.\njmv::descriptives(data, vars='age', hist=TRUE)\n\nWe can end up specifying a lot of these options. Sometimes it can be a good idea to split the function across multiple lines. This is fine as long as the parentheses are in the right place and there are commas in between each option.\nThis code computes descriptive statistics including the skewness, interquartile range and mode.\njmv::descriptives(data, \n  vars='age', \n  skew=TRUE,\n  iqr=TRUE,\n  mode=TRUE,\n  hist=TRUE\n  )\n\nOf course, we could run these analyses by hand. The main advantage here is that we can automate the selection of critical options and reduce the possiblity of mistakes when communicating analyses within teams. As long as we have this line saved, we can rerun it on new datasets without having to recheck every box by hand.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse the examples above and the JMV documentation on the descriptives function to write a function definition that does the following.\nCompute the core descriptive statistics for the Openness variable, make sure to include the range and a boxplot.\n\n\nTake a look at the ‚Äòrange‚Äô and ‚Äòbox‚Äô options in the documentation page.\nBe careful with the spelling of each argument!\n\n\nThe final code should be:\njmv::descriptives(data, \n  vars='Openness', \n  range=TRUE,\n  box=TRUE\n  )\n\n\n\n\n\n\n\n\n\n\nRemember that last week we very carefully removed several participants who had entered unrealisic or impossible ages into the datasets. This may have been an accident or mistake but either way we should not use this data. Our examples so far have just used all the data.\nR and Rj make it very easy to clean data without the boring, manual work we did last week. R can do the hard work and we can avoid scrolling through very large datasets by hand. Sounds good!\nThe first thing we can do is investigate what range of responses that people actually gave when asked about their age. To do this we can use a new function called unique(). This takes a column of a dataset as an input and returns a list of all the unique responses that it contains.\n\n\n\n\n\n\nKey step\n\n\n\nLoad up an Rj Editor window (or continue in your old one) and run unique() on the age data.\nThe result should look like this:\n\n\n\nMost of these responses are fine but, as we saw last week, there are a lot of strange ones. Most importantly, we can find the extreme values that are very likely errors during data collection (-2 and 149)\nWe need to remove these from our analysis to get the most accurate results.\nTo do this we‚Äôre going to use an R library named dplyr (short for ‚ÄòData Pliers‚Äô, see their documentation here). This is an extremely powerful library that is very widely used across the world.\nTo use dplyr, we first need to load the library into Rj by adding the following line to our code.\nlibrary(dplyr)\nThis tells Rj that we want to use the functions from this library.\nThe key function we need today is filter() - this extracts rows from a dataset based on logical conditions that we specify in the code. We are going to specify two arguments, firstly we will pass in the data and secondly we will specify the condition.\nIn this example we extract only the rows in which participants have an age above 20.\nlibrary(dplyr)\n\nfilter(data, age&gt;20)\n\nNotice that the dataset printed in the right hand window now only has values from the participants who are older than 20.\nWe can even specify multiple options - here we extract rows with participants older than 20 but still 23 or younger.\nlibrary(dplyr)\n\nfilter(data, age&gt;20, age&lt;=23)\nNext, to recreate our data tidying form last week, we need to extract the rows of participants who are older than -2 and younger than 149.\nThese values might be too specific and might not apply to every dataset. We can apply some common sense and suggest some broad, but much more sensible boundaries for a student population. We also save the result into a variable we can reuse later.\ntidy_data &lt;- filter(data, age &gt; 15, age &lt; 60)\nAnd finally, we can combine this with our call to jmv::descriptives to both filter our data to only the rows we want to use and compute the descriptive statistics on the result.\ntidy_data &lt;- filter(data, age &gt; 15, age &lt; 60)\n\njmv::descriptives(tidy_data, vars='age')\n\n\n\n\n\n\n\nKey step\n\n\n\nPractice changing the age conditions to a few different options to see the difference it makes to the descriptives analysis. What happens when you set a condition that results in zero valid participants?\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse the information in this tutorial to compute descriptive statistics for the Openness variable from Introverted participants. In other words - what is the mean Openness score for participants who are more Introverted than Extraverted.\nCompute the core descriptive statistics for the Openness variable, make sure to include the median and a histogram\n\n\nTo extract the data rows for Introverted participants we want to extract rows where the Extraversion score is less than 3.5\n\n\nThe final code should be:\nlibrary(dplyr)\n\ntidy_data &lt;- filter(data, Extraversion &lt; 3.5)\n\njmv::descriptives(tidy_data, vars='Openness')\n\n\n\n\n\n\n\n\n\n\nIt probably seems like a small difference so far. After all, we same results for our analysis using the check, boxes in Jamovi or the code in R. The Jamovi analysis is certainly quicker to run for the first time and it‚Äôs more convenient when exploring your data.\nHowever the R code has several crucial advantages.\n\nThe R code is transparent about what is actually being run to generate the results and by sharing this code, we enable others to repeat our analysis. This makes it really straightforward to run and re-run identical analyses in situations where we need to be really sure that everybody has done the same thing. For example, if we want to be certain that two data sets have been analysed in an identical way then that our code would be the best way to do this.\nR code is the best way to share what we‚Äôve done with the rest of the world. It is common practice for modern academic public it to include the code that was used to generate the results in the paper. This way everybody who wants to check can make sure they fully understand what the authors of the paper have done.\n\nFinally, over the course of the semester we‚Äôll see that R allows us to do much more than what is possible in Jamovi. We can define blocks of R code that can tidy-up and arrange our data, run multiple analyses and generate tables & figures all in one go. Much easier that clicking through all the menus each time we want to rerun an analysis!\n\n\n\n\n\nBlock, Jack. 1995. ‚ÄúA Contrarian View of the Five-Factor Approach to Personality Description.‚Äù Psychological Bulletin 117 (2): 187‚Äì215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. ‚ÄúMany Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.‚Äù Journal of Experimental Social Psychology 67: 68‚Äì82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. ‚ÄúA Very Brief Measure of the Big-Five Personality Domains.‚Äù Journal of Research in Personality 37 (6): 504‚Äì28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#the-dataset",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#the-dataset",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Dataset\n\n\n\nNote that this is the same dataset as in week 1\n\n\nWe will be exploring data that was collected as part of the ‚ÄòMany Labs 3‚Äô project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‚ÄòBig Five‚Äô personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs.¬†extravagant/careless)\nAgreeableness (friendly/compassionate vs.¬†critical/rational)\nNeuroticism (sensitive/nervous vs.¬†resilient/confident)\nOpenness to experience (inventive/curious vs.¬†consistent/cautious)\nExtraversion (outgoing/energetic vs.¬†solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#the-challenge",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#the-challenge",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Last week, we looked at how we can spend some time understanding a new dataset and compute some descriptive statistics from it.\nNow, we‚Äôre going to learn how to use Jamovi and R to make this process more repeatable and more transparent. This will make it much easier for us to come back to the analysis and understand what steps were taken at a later date.\n\n\n\n\n\n\nOpening Jamovi\n\n\n\nWe‚Äôll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham‚Äôs AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‚ÄòDesktop‚Äô application - not the ‚ÄòCloud‚Äô version.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#install-the-rj-add-on-in-jamovi",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#install-the-rj-add-on-in-jamovi",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "We‚Äôll be working with the Rj Add-On for Jamovi this week.\n\n\n\n\n\n\nKey step\n\n\n\nBefore continuing, install the Rj add-on for jamovi following the guide for installing Jamovi modules. Once installed you should have a new button on the Ribbon menu at the top of Jamovi‚Ä¶\n\n\n\nClick on this and select ‚ÄòRj Editor‚Äô to open a window in which we can write R code! Click on the small cog icon and make sure that ‚ÄòOutput‚Äô is set to ‚ÄòShow code and output‚Äô.\n\nWe can use the left-hand window, named Rj editor, to write commands, using the R language. These are the straightforward text commands that tell the computer a series of steps that we wanted to carry out. These commands can be edited and rerun as many times as we need, they can even be shared with collaborators and included as part of scientific publication.\nThis is a very powerful tool once we have a written set of instructions to work with rather than individually clicking through instructions. Though it takes a bit longer to write the commands, our analysis is much more transparent and reproducible once it is done.\nThe outputs of the analysis defined in the Rj editor appear on the right-hand side of the Jamovi window as usual. To check this out, let‚Äôs explore using or as a calculator.\n\n\n\n\n\n\nData Skills - R as a calculator\n\n\n\nTry typing a set of sums or equations into the Rj editor. Include one sum on each line and include as many as you like. When you‚Äôre ready, you can tell RJ to run the commands by clicking on the green triangle in the top right corner of the RJ editor. This will automatically compute. All of the sums you to find and print the results out in the right hand side window. see the example below.\nYou can use the following symbols to define your sums.\n\n\n\nSymbol\nOperation\n\n\n\n\n+\nAdd\n\n\n-\nSubtract\n\n\n*\nMultiply\n\n\n/\nDivide",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#variables-and-functions",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#variables-and-functions",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Often, we don‚Äôt just want to see the result of a calculation, we want to store the result to use again later. For this, we want to use variables. For example, here we store the result of a calculator sum in a variable named ‚Äòy‚Äô\ny &lt;- 2 + 2\nIf you run this in Rj, you‚Äôll notice that we no longer get the number 4 printed to the screen. This is as the output is now caught and stored rather than printed out. If we still want to see what is contained in y, we can use another helpful function named print.\ny &lt;- 2 + 2\nprint(y)\nWill now also display the content of y.\n\n\n\n\n\n\nDefinition of a variable\n\n\n\nA variable is an object that provides a labelled space to store information. The &lt;- or = symbols indicate that we‚Äôre assigning some information to a variable (the two symbols behave the same way, you could see either).\nCrucially, the information on the right hand side is assigned to a variable with the name from the left hand side.\n\n\n\n\n\n\n\n\nCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about variables in R. We suggest you try the following prompts to get started.\n\n‚ÄúWrite a simple description of variables in R for a beginner‚Äù\n‚ÄúHow can I choose a name for my variable in R‚Äù\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\nNotice that the one of the equations in my example is a bit different to the others. The final example uses a function called sqrt as a shortcut to compute the square root of the number nine.\nThe sqrt function calculates the square root of a given number. In R code, this looks something like this:\nsqrt(x)\nwith the ‚Äòx‚Äô replaced by any number. For example, we could compute the square root of 36 using the following command.\nsqrt(36)\nAdd this line to your Rj editor and check the result..\nMany functions, take inputs contained in parentheses written after the name of the function. These inputs are sometimes known as the arguments of a function. In the last example, 36 is an argument for the function sqrt.\nSometimes functions take several arguments, and the arguments themselves can have specific names. For example.\nround(12.345, digits = 0)\nwill round the value x to a whole number with zero digits after the decimal point. Where as\nround(12.345, digits = 2)\nwould round the same number to preserve 2 digits after the decimal point. Try running these examples in Rj with a range of different numbers to get a sense of how it works.\n\n\n\n\n\n\nDefinition of a function\n\n\n\nFunctions are used to perform specific, predefined tasks, such as calculations, data transformations, or visualization. You can think of it as a ‚Äúblack box‚Äù that takes in a value, performs some internal calculation, and returns the result.\n\n\n\n\n\n\n\n\nCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about functions in R. We suggest you try the following prompts to get started.\n\n‚ÄúWrite a simple description of functions in R for a beginner‚Äù\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\nA single analysis might have several lines of code that create, store and reuse a several different variables with several different functions. For example:\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat does this R code do? what is the value of x, y and z?\n x &lt;- 10 + 10\n y &lt;- sqrt(16)\n z &lt;- x / y\n\n\nTry copying out and running the code in Rj.\nYou can add some print functions to see the value of the intermediate variables x and y as well. Try adding print(x) on the line immediately after x is defined.\nTalk to your tutor if you‚Äôre getting stuck.\n\n\nThe final answers:\nx is 20 and y is 4, so z = 20 / 4 which is 5.\n\n\n\n\n\n\n\nRj has a special feature that the data loaded into our spreadsheet is always contained in a special variable named data. We can refer to this data variable to view and run analyses on our dataset.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#accessing-the-data-table-in-rj",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#accessing-the-data-table-in-rj",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Key step\n\n\n\nTake a moment to load the data file into Jamovi and compute the descriptive statistics for the big five personality factors again.\n\n\n\nThe data we have loaded into our Jamovi session can be accessed in Rj with the data variable. Jamovi automatically makes the data available for us in the R environment.\nWe can view and interact with the data here as well. For example, to view the first few rows of the dataset you can run the head function and pass in data as an argument.\nhead(data)\nand to view the last few rows, you can do the same with the tail function.\ntail(data)\nOften, we‚Äôll only want to work with a subset of the columns in a data frame at any given time. We can access individual columns using the dollar operator. For example, this code will access the age column within the loaded dataset.\ndata$age\nLet‚Äôs break this command down into its pieces:\n\n\n\n\n\n\n\nR code\nMeaning\n\n\n\n\ndata\nthis is the name of the whole data array we have loaded in\n\n\n$\nthis is a separator that R uses to help us access subsections of datasets\n\n\nage\nThis is the name of a single column we want to access\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you access the column of Mood scores from the same dataset?\n\n\nYou can use the head function to see the list of all available columns in the dataset. The example above accesses the ‚Äòage‚Äô column, you can tweak this to extract the mood data instead.\n\n\nThe final code should look like this\ndata$Mood\n\n\n\n\n\nWe can also apply functions to these variables. R includes a wide range of standard mathmatical functions that we can use for common computations. This code computes the mean of the age data in our dataset.\nmean(data$age)\nTry running this in Rj. You should get the answer 19.3 - this matches the value we computed using the descriptives analysis last week.\nSimilarly, the following function computes the standard deviation\nsd(data$age)\nIf you run this, you should get the answer 3.808.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#running-desriptive-statistics-using-rj",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#running-desriptive-statistics-using-rj",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Ok, back to statistics! How can we use our new R skills to compute statistics?\nThe trick is to understand that all the analyses in Jamovi are actually carried out by functions in R. Each analysis we can select from the top ribbon in Jamovi corresponds to a function that can be called in the code. Similarly, all of the options that we select in the analysis, dialogue boxes, correspond to arguments for that function. Let‚Äôs take a look in practice.\nJamovi provides a collection of R functions that correspond to each analysis dialogue box in Jamovi. This package of functions is named jmv. Thejmv function that computes descriptive statistics is called:\njmv::descriptives\nHere we first name the jmv package specify the descriptives function within it (this link is made with the two colons).\nWe can compute the descriptive statistics for age by passing the data into the function and specifying that we want to use the age variable. Try running the following:\njmv::descriptives(data, vars='age')\nand you should generate the following result table.\n\n\n\n\n\n\n\nNote!\n\n\n\nYou might notice that we still have the unusual ages in this analysis, we‚Äôll discuss how to remove them in the next section\n\n\nJamovi provide a lot of documentation on the descriptives function. This lists every single option that can be passed into jmv::descriptives. There is an option here for every single checkbox and selector in the main Jamovi dialogue.\nThe following code additionally computes a histogram by adding the hist argument.\njmv::descriptives(data, vars='age', hist=TRUE)\n\nWe can end up specifying a lot of these options. Sometimes it can be a good idea to split the function across multiple lines. This is fine as long as the parentheses are in the right place and there are commas in between each option.\nThis code computes descriptive statistics including the skewness, interquartile range and mode.\njmv::descriptives(data, \n  vars='age', \n  skew=TRUE,\n  iqr=TRUE,\n  mode=TRUE,\n  hist=TRUE\n  )\n\nOf course, we could run these analyses by hand. The main advantage here is that we can automate the selection of critical options and reduce the possiblity of mistakes when communicating analyses within teams. As long as we have this line saved, we can rerun it on new datasets without having to recheck every box by hand.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse the examples above and the JMV documentation on the descriptives function to write a function definition that does the following.\nCompute the core descriptive statistics for the Openness variable, make sure to include the range and a boxplot.\n\n\nTake a look at the ‚Äòrange‚Äô and ‚Äòbox‚Äô options in the documentation page.\nBe careful with the spelling of each argument!\n\n\nThe final code should be:\njmv::descriptives(data, \n  vars='Openness', \n  range=TRUE,\n  box=TRUE\n  )",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#selecting-rows-of-data-using-the-filter-function",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#selecting-rows-of-data-using-the-filter-function",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Remember that last week we very carefully removed several participants who had entered unrealisic or impossible ages into the datasets. This may have been an accident or mistake but either way we should not use this data. Our examples so far have just used all the data.\nR and Rj make it very easy to clean data without the boring, manual work we did last week. R can do the hard work and we can avoid scrolling through very large datasets by hand. Sounds good!\nThe first thing we can do is investigate what range of responses that people actually gave when asked about their age. To do this we can use a new function called unique(). This takes a column of a dataset as an input and returns a list of all the unique responses that it contains.\n\n\n\n\n\n\nKey step\n\n\n\nLoad up an Rj Editor window (or continue in your old one) and run unique() on the age data.\nThe result should look like this:\n\n\n\nMost of these responses are fine but, as we saw last week, there are a lot of strange ones. Most importantly, we can find the extreme values that are very likely errors during data collection (-2 and 149)\nWe need to remove these from our analysis to get the most accurate results.\nTo do this we‚Äôre going to use an R library named dplyr (short for ‚ÄòData Pliers‚Äô, see their documentation here). This is an extremely powerful library that is very widely used across the world.\nTo use dplyr, we first need to load the library into Rj by adding the following line to our code.\nlibrary(dplyr)\nThis tells Rj that we want to use the functions from this library.\nThe key function we need today is filter() - this extracts rows from a dataset based on logical conditions that we specify in the code. We are going to specify two arguments, firstly we will pass in the data and secondly we will specify the condition.\nIn this example we extract only the rows in which participants have an age above 20.\nlibrary(dplyr)\n\nfilter(data, age&gt;20)\n\nNotice that the dataset printed in the right hand window now only has values from the participants who are older than 20.\nWe can even specify multiple options - here we extract rows with participants older than 20 but still 23 or younger.\nlibrary(dplyr)\n\nfilter(data, age&gt;20, age&lt;=23)\nNext, to recreate our data tidying form last week, we need to extract the rows of participants who are older than -2 and younger than 149.\nThese values might be too specific and might not apply to every dataset. We can apply some common sense and suggest some broad, but much more sensible boundaries for a student population. We also save the result into a variable we can reuse later.\ntidy_data &lt;- filter(data, age &gt; 15, age &lt; 60)\nAnd finally, we can combine this with our call to jmv::descriptives to both filter our data to only the rows we want to use and compute the descriptive statistics on the result.\ntidy_data &lt;- filter(data, age &gt; 15, age &lt; 60)\n\njmv::descriptives(tidy_data, vars='age')\n\n\n\n\n\n\n\nKey step\n\n\n\nPractice changing the age conditions to a few different options to see the difference it makes to the descriptives analysis. What happens when you set a condition that results in zero valid participants?\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse the information in this tutorial to compute descriptive statistics for the Openness variable from Introverted participants. In other words - what is the mean Openness score for participants who are more Introverted than Extraverted.\nCompute the core descriptive statistics for the Openness variable, make sure to include the median and a histogram\n\n\nTo extract the data rows for Introverted participants we want to extract rows where the Extraversion score is less than 3.5\n\n\nThe final code should be:\nlibrary(dplyr)\n\ntidy_data &lt;- filter(data, Extraversion &lt; 3.5)\n\njmv::descriptives(tidy_data, vars='Openness')",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#summary",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#summary",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "It probably seems like a small difference so far. After all, we same results for our analysis using the check, boxes in Jamovi or the code in R. The Jamovi analysis is certainly quicker to run for the first time and it‚Äôs more convenient when exploring your data.\nHowever the R code has several crucial advantages.\n\nThe R code is transparent about what is actually being run to generate the results and by sharing this code, we enable others to repeat our analysis. This makes it really straightforward to run and re-run identical analyses in situations where we need to be really sure that everybody has done the same thing. For example, if we want to be certain that two data sets have been analysed in an identical way then that our code would be the best way to do this.\nR code is the best way to share what we‚Äôve done with the rest of the world. It is common practice for modern academic public it to include the code that was used to generate the results in the paper. This way everybody who wants to check can make sure they fully understand what the authors of the paper have done.\n\nFinally, over the course of the semester we‚Äôll see that R allows us to do much more than what is possible in Jamovi. We can define blocks of R code that can tidy-up and arrange our data, run multiple analyses and generate tables & figures all in one go. Much easier that clicking through all the menus each time we want to rerun an analysis!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#references",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#references",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Block, Jack. 1995. ‚ÄúA Contrarian View of the Five-Factor Approach to Personality Description.‚Äù Psychological Bulletin 117 (2): 187‚Äì215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. ‚ÄúMany Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.‚Äù Journal of Experimental Social Psychology 67: 68‚Äì82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. ‚ÄúA Very Brief Measure of the Big-Five Personality Domains.‚Äù Journal of Research in Personality 37 (6): 504‚Äì28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/frequently-asked-questions.html",
    "href": "tutorials/frequently-asked-questions.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "First check the start menu, if you can find both Jamovi and Rstudio then just open both from there and get started. Any programme that is not in the start menu can be opened using AppsAnywhere. This is available in the start menu of every campus computer ‚Äì general information about AppsAnywhere can be found on this IT services page\n\n\n\nThis should be available as an icon on the desktop, or from within the start menu. If not, they you can try the web interface - https://apps.bham.ac.uk/ . The website may ask you to confirm that AppsAnywhere is installed on your device ‚Äì you can click ‚ÄòI have it already‚Äô if using a campus computer.\n\n\n\nOnce you have access to AppsAnywhere, you may have to log-in using your UoB credentials. The homepage of AppsAnywhere contains a search bar and will look something like this:\n\nYou can search for Jamovi and Rstudio in the search bar and click ‚ÄòLaunch‚Äô to run them. If this is the first time you‚Äôre running the app it may take a few minutes to get started. This is as AppsAnywhere is downloading and installing the programme on your computer.\n\n\n\nAppyAnywhere needs to be able to link to your desktop computer to run properly, the validation process ensures that this is working properly. Normally all you need to do is click to start validation and answer ‚ÄòOpen AppsAnywhere Launcher‚Äô when prompted by the browser.",
    "crumbs": [
      "Home",
      "Support",
      "FAQs"
    ]
  },
  {
    "objectID": "tutorials/frequently-asked-questions.html#i-cant-find-jamovirstudio-on-my-campus-computer.",
    "href": "tutorials/frequently-asked-questions.html#i-cant-find-jamovirstudio-on-my-campus-computer.",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "First check the start menu, if you can find both Jamovi and Rstudio then just open both from there and get started. Any programme that is not in the start menu can be opened using AppsAnywhere. This is available in the start menu of every campus computer ‚Äì general information about AppsAnywhere can be found on this IT services page",
    "crumbs": [
      "Home",
      "Support",
      "FAQs"
    ]
  },
  {
    "objectID": "tutorials/frequently-asked-questions.html#i-cant-find-appsanywhere-on-my-campus-computer.",
    "href": "tutorials/frequently-asked-questions.html#i-cant-find-appsanywhere-on-my-campus-computer.",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "This should be available as an icon on the desktop, or from within the start menu. If not, they you can try the web interface - https://apps.bham.ac.uk/ . The website may ask you to confirm that AppsAnywhere is installed on your device ‚Äì you can click ‚ÄòI have it already‚Äô if using a campus computer.",
    "crumbs": [
      "Home",
      "Support",
      "FAQs"
    ]
  },
  {
    "objectID": "tutorials/frequently-asked-questions.html#how-do-i-use-appsanywhere",
    "href": "tutorials/frequently-asked-questions.html#how-do-i-use-appsanywhere",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Once you have access to AppsAnywhere, you may have to log-in using your UoB credentials. The homepage of AppsAnywhere contains a search bar and will look something like this:\n\nYou can search for Jamovi and Rstudio in the search bar and click ‚ÄòLaunch‚Äô to run them. If this is the first time you‚Äôre running the app it may take a few minutes to get started. This is as AppsAnywhere is downloading and installing the programme on your computer.",
    "crumbs": [
      "Home",
      "Support",
      "FAQs"
    ]
  },
  {
    "objectID": "tutorials/frequently-asked-questions.html#appsanywhere-keeps-asking-me-about-validation.",
    "href": "tutorials/frequently-asked-questions.html#appsanywhere-keeps-asking-me-about-validation.",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "AppyAnywhere needs to be able to link to your desktop computer to run properly, the validation process ensures that this is working properly. Normally all you need to do is click to start validation and answer ‚ÄòOpen AppsAnywhere Launcher‚Äô when prompted by the browser.",
    "crumbs": [
      "Home",
      "Support",
      "FAQs"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "This week we will explore how to use the jmv package in R to test hypotheses about a dataset using t-tests and to compute effect sizes corresponding to those tests. We will use those effect sizes to help interpret the sensitivity and power of our experiments.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nCohen‚Äôs d effect size\n\n\n\nPower analysis\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nUse mutate() in R to compute a new variable\n\n\n\nCompute t-tests using the jmv R package\n\n\n\nRun a power analysis in Rj\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nUnderstand and validate code written by someone else\n\n\n\nUse power analysis to recommend a sample size for a replication\n\n\n\n\n\n\n\n\n\n\n\nDataset\n\n\n\nPlease note, this is the same dataset as we used in week 4 - but make sure that you load in the rmb-week-3_lecture-quiz-data_ai-faces-fixed.omv file from this week and NOT the week 4 file. It contains a small fix that we need for this week.\n\n\nWe‚Äôll be working with the dataset we collected after the break during the lecture in Week 3. This is a partial replication of a study published in the journal Psychological Science (Miller et al. 2023). The original paper found the following results summarised in the abstract.\n\nRecent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 (N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces‚Äîa phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 (N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI.\n\nWe ran a quiz that students in the lecture could join on their phones/laptops. There were two parts to the quiz:\nThe faces section included 12 faces that were either photographs or real people or AI generated images of people. The stimuli were taked from the materials released by (Miller et al. 2023) on the Open Science Framework. Responses were either ‚ÄòReal person‚Äô or ‚ÄòAI generated‚Äô.\n\nThe confidence section asked participants to rate their confidence in their ability to do the following three things.\n\nDistinguish real photos of people from AI generated images (Experimental condition)\nDistinguish photos of happy people from photos of sad people (Emotional control condition)\nDistinguish photos of people you used to know in primary school from strangers (Memory control condition)\n\nScores were recorded on a scale from 1 (Completely confidence) to 10 (Not at all confident). The confidence section was repeated before and after the faces section to see if participants confidence changed as a result of doing the AI faces task\n\n\n\nThis week we will explore some factors that influence t-stats and p-values whilst introducing the concept of loops in R. We will finish by using effect sizes to compute a power analysis.\n\n\n\nLast week, we used Jamovi to create several new variables that allowed us to run our analyses. We can do the same in a more transparent and reproducible way using R code.\nWe need to create both our proportion of correct faces across the 12 stimuli in one variable and a grouping variable which indicates whether each participant was confident in their ability to tell the difference between AI faces and real faces.\nWe can do both with the mutate() from the dplyr library, this provides functionality that lets us create, modify, and delete columns within a dataset. Take a look at the official documentation for mutate for more information.\nWhen computing a new varaible with mutate() we three pieces of information.\n\nThe dataset to work on\nThe name of the new variable to be created\nThe definition of how to compute the new variable for existing variables.\n\n\n\nLet‚Äôs start with our grouping variable. Our three pieces of information are\n\nThe dataset to work on - is data, which refers to the original datasheet loaded into Jamovi\nThe name of the new variable - is ConfidentBefore, the same as we used last week\nThe definition - is AIConfidenceBefore&lt;4, again the same as we used last week\n\nWe can combine these into a single line to create the variable. Note that we save the result into a new variable named my_data, we‚Äôll use this from now on to avoid confusion with the original datasheet.\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\nWe can check that this has done what we expected by running head to see the first few rows of the dataset.\nhead(my_data)\nThe result should look like this:\n\nWe can see on in the results on the right hand side that the additional colume ConfidentBefore now appears with TRUE and FALSE values for each participant.\n\n\n\nLet‚Äôs do the same for total proportion of correctly identified faces. We can use the same principle as we used for the grouping variable and use mutate() along with the variable definition and name to create our new column.\nThis definition is pretty long as we have 12 different faces to work with! You can copy the whole line using the copy icon in the right hand side of the code box. Take a moment to look through and understand each part: which part is the Dataset, the Name and the Definition?\nmy_data &lt;- mutate(my_data, PropFacesCorrect = (Face1_Real + Face2_Real + Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face7_Real + Face8_Real + Face9_Real + Face10_Real + Face11_AI + Face12_AI) / 12)\nand validate the overall result using head() to make sure that my_data now has the column we expected.\n\nThe final column of the dataset is now PropFacesCorrect and contains the proportion of face trials that participant got right.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute variables containing the proportion of correct responses for photos of real people and AI generated faces separately?\nStore the results in PropRealCorrectFaces for real faces and PropAICorrectFaces for AI generated faces.\n\n\nWe can use the code we wrote to compute the proportion of correct responses for all faces as a starting point.\nThink about how you could modify this line to compute the result for either real or AI faces on their own? What would you need to change in the code?\n\n\nThe following code will compute the variables\nmy_data &lt;- mutate(my_data, PropRealFacesCorrect = (Face1_Real + Face2_Real + Face7_Real + Face8_Real + Face9_Real + Face10_Real) / 6)\nmy_data &lt;- mutate(my_data, PropAIFacesCorrect = (Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face11_AI + Face12_AI) / 6)\nWe‚Äôve separated out the columns to sum all the real or AI faces together and changed the division to divide the results by 6 rather than 12.\nChecking the results with head() should produce the following outputs with four additional columns!\n\n\n\n\n\n\n\n\n\n\n\nWe have the ingredients for our hypothesis test. Let‚Äôs use R to explore the following hypothesis (Hypothesis 2 from week 4).\n\nConfident people are better at distinguishing AI faces from real faces\n\nWe can compute this test using the following code that calls ttestIS() - this is the function that computes independent samples t-tests. Read more about it on the ttestIS documentation page.\nYou can click the numbers by the definitions at the bottom to highlight the corresponding part of the code.\n1jmv::ttestIS(\n2    formula = PropFacesCorrect ~ ConfidentBefore,\n3    data = my_data,\n4    effectSize = TRUE)\n\n1\n\njmv::ttestIS is the name of the R function that Jamovi uses to compute independent samples t-tests\n\n2\n\nThis tells ttestIS the formula that defines the test we want to run.\n\n3\n\nThis tells ttestIS which dataset we want to analyse\n\n4\n\nThis adds an additional effect size computation to the results\n\n\nMost of this will be familiar from previous weeks, but let‚Äôs think about the formula in a little more detail.\nIn R, the tilde (~) is used in formula definitions to specify the relationship between variables, particularly in statistical modeling and data analysis. Here, the tilde separates the dependent variable (response) from the independent variables (predictors). In our example, PropFacesCorrect is the dependent variable and ConfidentBefore is our independent variable (grouping variable) - so this formula\nPropFacesCorrect ~ ConfidentBefore\nis essentially telling ttestIS() to ‚ÄúCompute the ttest on PropFacesCorrect using ConfidentBefore as our groups‚Äù.\nWe can now run all our code to get the results of the ttest - the code clearly tells the reader how all the relevant variables were computed and what hypothesis test has been run all in one screen.\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nRead through the ttestIS documentation page. How could you change your code to add the following assumption checks\n\nTest for homogeneity of variance\nTest for normal distribution\n\n\n\nThe documentation page contains a list of all the possible information that we can pass as an input to our ttestIS() function call. Each item in the list corresponds to the options available in the Jamovi dialogue box.\nHave a look at the norm and eqv definitions. What would you need to add to the function call to run these additional checks?\n\n\nThe following code will compute all the checks in addition to the t-test.\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE,\n    norm = TRUE,\n    eqv = TRUE)                             \nThe results should look like this:\n\nDo the assumptions for Student‚Äôs t-test hold for this data?\n\n\n\n\n\n\nYour script might be getting quite long at this point! It is always a good idea to keep code neat and tidy where possible so that other people are able to read it, and so that we can read it if we come back to the analysis in the future.\nThere are many ways to keep things organised in a script. Here are two good hints, code comments can organise your script without changing the output and print() statements can help organise your code and the outputs.\n\n\nAny line of R code that starts with a hashtag is called a ‚Äòcomment‚Äô. The writing on this line is there for our information and will not be run by the computer. Adding code comments is a useful way to annotate your code indicating what each line or each section is doing. For example:\n# This is code comment\n\n# Compute which participants were more confident than the median\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\n\nThis will make it easier to understand what the coding is doing in future.\n\n\n\nWe can use the print() function to help organise our code as well. The text within the call to print() will not be executed by the computer but will simply be printed into the output console. This can be useful to break the output of your code into sections and to inlcude additional information about the analysis next to the outputs themselves.\nFor example:\nprint('Hypothesis 1')\nprint('Confident people are better at distinguishing AI faces from real faces')\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)\nThis code will print out information about the hypothesis next to the outputs.\nA full cleaned and commented version of our code might look like this.\n# Compute which participants were more confident than the median\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\n\n# Compute the proportion of all faces each participant got right\nmy_data &lt;- mutate(my_data, PropFacesCorrect = (Face1_Real + Face2_Real + Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face7_Real + Face8_Real + Face9_Real + Face10_Real + Face11_AI + Face12_AI) / 12)\n\n# Compute the proportion of REAL faces each participant got right\nmy_data &lt;- mutate(my_data, PropRealFacesCorrect = (Face1_Real + Face2_Real + Face7_Real + Face8_Real + Face9_Real + Face10_Real) / 6)\n\n# Compute the proportion of AI faces each participant got right\nmy_data &lt;- mutate(my_data, PropAIFacesCorrect = (Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face11_AI + Face12_AI) / 6)\n\n# Hypothesis test\nprint('Hypothesis 1')\nprint('Confident people are better at distinguishing AI faces from real faces')\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)\nand running this code produces the following outputs including the test from our print statements\n\nNice and clear what is happening at each stage! There is no perfect or ‚Äòcorrect‚Äô way to tidy up your code (though some people can get opinionated about this‚Ä¶). Choose a mix of comments and print statements that makes sense to you.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nExtend your code to test a second hypothesis.\n\nConfident participants are more accurate than unconfident participants when identifying photos of real faces.\n\nInclude some code comments and print statements\n\n\nThis will require a second indenpendent samples t-test asking whether the mean proportion correct value for only real faces is different between our confident and not-confident groups.\nUse the previous t-test as a starting point, can you copy this and modify it to do what we need?\n\n\nThe following code will test the new hypothesis with some comments and print statements\n# Hypothesis test\nprint('Hypothesis 2')\nprint('Confident participants are more accurate than unconfident participants when identifying photos of real faces.')\njmv::ttestIS(                                       \n    formula = PropRealFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)                       \nThe output of the whole script should now look like this.\n\nNote how the print statements help to separate the results into interpretable chunks.\n\n\n\n\n\n\nNow we‚Äôre going to explore how the size of the data sample changes the t-values that w\nIntroduce slice_sample Introduce simple loops\n\n\n\n\nImagine that of your friends has been running some analyses on this dataset and would like you to check over their work. They send you the following R code that they have put together.\n# Compute some new variables\nmy_data &lt;- mutate(data, ChangeAIConfidence = AIConfidenceBefore - AIConfidenceAfter)\nmy_data &lt;- mutate(my_data, ChangeEmoConfidence = EmoConfidenceBefore - EmoConfidenceAfter)\nmy_data &lt;- mutate(my_data, ChangeMemoryConfidence = MemoryConfidenceBefore - MemoryConfidenceAfter)\n\n# Run the tests\njmv::ttestOneS(\n    data = my_data,\n    vars = vars(ChangeAIConfidence, ChangeEmoConfidence, ChangeMemoryConfidence),\n    testValue = 0.0,\n    wilcoxon = TRUE,\n    desc = TRUE,\n    norm = TRUE)\nLet‚Äôs think about this code a bit, your friend didn‚Äôt really explain the plan‚Ä¶\n\n\n\n\n\n\n Data Skills - what variables are created?\n\n\n\n\n\nThe code creates three new variables, ChangeAIConfidence, ChangeEmoConfidence and ChangeMemoryConfidence - each is computed by subtracting the confidence values after the face task from the confidence values before the task.\nThese variables represent the change in confidence from completing the task.\n\n\n\n\n\n\n\n\n\n Data Skills - what statistical test is performed?\n\n\n\n\n\nThe code runs a one sample t-test comparing the data mean to 0.0 using the jamovi function jmv::ttestOneS(). The code runs the test on all three variables. We can see that the input vars is passed a list of all three variable names.\n\n\n\n\n\n\n\n\n\n Data Skills - what additional computations are performed?\n\n\n\n\n\nIn addition to the t-tests, the code runs a Wilcoxon Rank test, a Shapiro-Wilk test for normality and a descriptive statistics table.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and run this code before going any further! Take a moment to read through the code and decide what it is doing.\n\n\n\n\n\n\nClick here if you want to check that you have the right outputs.\n\n\n\n\n\nThe results should look like this:\n\n\n\n\n\n\nThey have written the following about the results.\n\nA one sample t-test found that the change in participant confidence in distinguishing AI faces from real faces was significantly reduced after performing the task. The difference in confidence before and after the task (M=-1.1, SD=2.7) was significantly below zero, t(76) = 3.538, p = 0.0007. In contrast there was no change after the task confidence in emotion recognition (M=-.017, SD=1.341), t(76) = -1.012, p = 0.313 and face memory (M=-0.02985, SD=1.044), t(76) = -0.234, p = 0.816.\n\nLet‚Äôs think about whether this is accurate.\n\n\n\n\n\n\n Data Skills - are the reported statistics correct? take your time to read carefully and compare to the outputs\n\n\n\n\n\nNo! there are a number of mistakes throughout this writing. How many of the following mistakes did you spot?\n\nA one sample t-test found that the change in participant confidence in distinguishing AI faces from real faces was significantly reduced after performing the task. The difference in confidence before and after the task (M=-1.1, SD=2.7) was significantly below zero, t(76) = 3.538, p = 0.0007. In contrast there was no change after the task confidence in emotion recognition (M=-.017, SD=1.341), t(76) = -1.012, p = 0.313 and face memory (M=-0.02985, SD=1.044), t(76) = -0.234, p = 0.816.\n\n\n3.538 -&gt; the sign of the t-values on this test is wrong. Take a look a the output table, this should be -3.538.\n76 -&gt; the degrees of freedom for the second and third tests are reported incorrectly (perhaps copied from the first test?). These should be 64 and 66 respectively\n\n\n\n\n\n\n\n\n\n\n Data Skills - was the correct test reported?\n\n\n\n\n\nAgain, no! there are mistakes here as well. The friend has done the right thing by computing the Shapiro-Wilk statistics but appears to have ignored them in the writing.\nShapiro-Wilk is significant for all three variables - we should strongly consider reporting the Wilcoxon Rank Sum test rather than Student‚Äôs t.\nThey could have run this analysis as a paired t-test, but the one sample t-test of the differences is also a valid approach.\n\n\n\nComputing statistics and accurately reporting the results takes a lot of precision! take your time when writing code and results sections - make sure to check and double check your work as you go‚Ä¶\n\n\n\nLet‚Äôs think more about effect sizes - computing effect sizes in Jamovi and R is really easy. Simply click the check box to add ‚ÄòEffect size‚Äô under the ‚ÄòAddtional Statistics‚Äô section of the Jamovi window - or add effectSize = TRUE to the function call in R (we‚Äôve already been doing this above!).\n\n\n\n\n\n\nKey step\n\n\n\nUpdate your friend‚Äôs code to include computation of effect sizes - the results should appear in the t-test results table on the right.\n\n\n\nThere are effect sizes computed for every t-test and its non-parametric alternatives.\n\nCohen‚Äôs d is the parametric effect size corresponding to Student‚Äôs t-test\nRank Biserial Correlation is the non-parametric effect size corresponding to Wilcoxon‚Äôs test.\n\nThough they have methodological differences, these effect sizes can be interpreted in the same way as a measure of the magnitude of an effect.\nRemember that this is different to the t-value which is a measure of evidence against the null hypothesis. The important difference is that we can have strong evidence against the null either as the difference is large or if we have measured a small difference very precisely. The effect size only cares about the size of the difference between conditions.\nNote that Jamovi/R provide a ‚Äòsigned‚Äô effect size indicating the direction of the effect in the same way that a t-statistic does. For the following work we can ignore this sign and focus on the magnitudes only. In other words, we‚Äôll consider an effect size of +0.5 or -0.5 to indicate the same magitude of effect.\nThe creator of many of our classic effect size measures provided a guideline for what might be considered a ‚Äòsmall‚Äô or a ‚Äòlarge‚Äô effect.\n\n\n\nEffect Size\nCohen‚Äôs d\nInterpretation\n\n\n\n\nSmall\n0.2\nSmall effect\n\n\nMedium\n0.5\nMedium effect\n\n\nLarge\n0.8\nLarge effect\n\n\n\nThese can be useful guidelines - through they have been criticised strongly for both the arbitrary nature of the thresholds and for over-simplifying interpretation of effect sizes. We can still use them as an informal indicator to help us quickly interpret our results.\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeAIConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.403 is just below the thresold to be a ‚Äòmedium‚Äô effect (ignoring the sign of the effect size). The effect size for the Rank Biserial correlation is similar.\n\n\n\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeEmoConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.126 is just below the thresold to be a ‚Äòsmall‚Äô effect (ignoring the sign of the effect size). The effect size for the Rank Biserial correlation is larger but still closer to small than medium.\n\n\n\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeMemoryConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.0286 is tiny! the Rank Biserial correlation is even smaller‚Ä¶\n\n\n\n\n\n\nOne core application for effect sizes is that we can use the results to inform our sample size planning for future studies. we can do this with four linked pieces of information\n\nEffect Size this is our effect size, usually Cohen‚Äôs d for analyses in RM-B\nSignificance Thresold - this is the ‚Äòalpha‚Äô level at which we would consider a result significant, usually 5% or 0.05.\nPower - this is probability of correctly detecting an effect if one exists in the data, we usually want 80% power.\nNumber of datapoints (N) - this is our sample size.\n\nThese four pieces of information are linked such that if we have any three of these values we can always compute the fourth.\nThis lets us do some very useful calculations to see how well our dataset is enabling us to answer our hypothesis and to plan for future studies. For examples\n\nIf we know the effect size, significance threshold and number of participants in a study, we can compute the power to tell us the probability that we can correctly detect an effect. This is known as ‚Äòsensitivity‚Äô.\n\nor\n\nIf we want to compute how many participants we‚Äôll need in a future study and we already know the effect size from previous literature, we can specify the power and significance threshold we want and compute the number of participants that we‚Äôll need.\n\nVery useful! We can compute these different values using a Jamovi module known as ‚Äòjpower‚Äô\n\n\n\n\n\n\nKey step - Install the jpower addon in Jamovi\n\n\n\nClick through the ‚ÄòModules‚Äô button on the top right of the Jamovi ribbon. Click to the list of ‚ÄòAvailable‚Äô modules and install ‚Äòjpower‚Äô\n\nClick through to open the jpower menu and select a ‚Äòone-sample t-test‚Äô from the options. You should see the following dialogue box\n\n\n\nLet‚Äôs take a moment to understand the options.\nFirst - notice that we have the four pieces of information listed near the top of the options.\n\n\n\njpower menu\nInterpretation\n\n\n\n\nMinimally interesting effect size (Œ¥)\nEffect size\n\n\nMinimum desired power\nPower\n\n\nN\nNumber of participants\n\n\nŒ± (type 1 error rate)\nSignificance threshold\n\n\n\nBy default, one of these options is grayed out and not modifible. This is as jpower works to compute one value at a time whilst asking you to specify the other three. By default, we‚Äôre computing N as specified by the ‚ÄòCalculate‚Äô drop down menu at the top of the dialogue box. Try changing this to calculate one of the other values and notice that the greyed out box changes each time.\n\n\n\n\n\n\nKey step - Install the jpower addon in Jamovi\n\n\n\nLet‚Äôs run through an example. Say that we want to answer the following question:\n\nWhat is the probability the ChangeAIConfidence has been correctly detected in our previous analysis.\n\nWe should run through the following steps.\n\nSelect that we want to Calcuate ‚ÄòPower‚Äô in the top drop down menu.\nSet the ‚ÄòMinimally interesting effect size (Œ¥)‚Äô to our observed effect size from before. This is 0.429 (ignoring the sign)\nSet the ‚ÄòN‚Äô to 77 - this is the number of participants in the test (degrees of freedom are 76)\nSet ‚ÄòŒ± (type 1 error rate)‚Äô to 0.05 - this is usually the default\n\nThe results should appear automatically.\n\n\n\njpower gives us some really useful explanatory text along with the results. Read this carefully to understand what is going on. Firstly,\n\nThe purpose of a power analysis is to evaluate the sensitivity of a design and test. You have chosen to calculate the sensitivity of the chosen design for detecting the specified effect size.\n\nThis clarifies the type of analyses we have run - we want to compute the sensitivity of our analysis given particular data and effect size. Next the results are summarised\n\nA design with a sample size of 77 can detect effect sizes of Œ¥ ‚â• 0.429 with a probability of at least 0.96, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of Œ±= 0.05.\n\nThis is the critical information. This tells us that the power is 0.96 for our analysis, indicating that we have a 96% chance of correctly detecting an effect size at least as large as the one from our test. Great news!\nFinally, the ‚ÄòPower by Effect size‚Äô table tells us the power we would have for different effect sizes. For example, in this case if we have an effect size of 0.3 - we would have power between 50% and 80%. If the effect size were 0.2 we would have less than 50% power.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat is the sensitivity (power) of the effect size estimated by Cohen‚Äôs d for the ChangeEmoConfidence variable?\n\n\nTake a look at our previous results. Cohen‚Äôd d for ChangeEmoConfidence was -0.126.\n\n\nOur estimated power for the ChangeEmoConfidence is only 19.4%.\n\nA design with a sample size of 77 can detect effect sizes of Œ¥ ‚â• 0.126 with a probability of at least0.194, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of Œ± = 0.05.\n\n\nThis is the practical consequence of effect sizes! The smaller effect for ChangeEmoConfidence is harder to detect than the larger value for ChangeAIConfidence so we have a much lower chance of getting the right answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nIt might be the case that our face recognition really does change participants confidence in recognising emotional faces, and our study simply doesn‚Äôt have enough participants to detect the change with statistical confidence. Perhaps we should consider running a larger study?\nIf we assume that our value for Cohen‚Äôs d is accurate and we assume that there really is an effect to be detected (these are pretty brave assumptions!), how many participants would a future study need to have 80% chance of correctly detecting the effect?\n\n\nWe‚Äôll need to change our analysis to calculate ‚ÄòN‚Äô rather than ‚ÄòPower‚Äô using the drop down menu at the top. Can you specify the power?\n\n\nWe would need at least 497 participants to reject the null hypothesis if the estimated effect size for the change in emotional face recognition before and after our task is accurate! that is a lot‚Ä¶.\n\nWe would need a sample size of 497 to reliably (with probability greater than 0.8) detect an effect size of Œ¥ ‚â• 0.126, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of Œ± = 0.05.\n\n\nThis information suggests that this looking for differences in emotional face confidence is probably not worth pursuing futher. Our experiment wasn‚Äôt perfect but this effect is so small that it is likely to be of little to no practical significance.\n\n\n\n\n\n\n\n\nWe have explored some new methods for creating new variables and running t-tests with effect sizes using R code. The R code we wrote is a really clear way to specify how variables were manipulated and which tests were run. Quite complex analyses with several stages can be clearly expressed this way.\nFinally, we used effect sizes to explore the power and sensitivity of our experiment and explore the possiblity of a follow up study.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#the-dataset",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#the-dataset",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Dataset\n\n\n\nPlease note, this is the same dataset as we used in week 4 - but make sure that you load in the rmb-week-3_lecture-quiz-data_ai-faces-fixed.omv file from this week and NOT the week 4 file. It contains a small fix that we need for this week.\n\n\nWe‚Äôll be working with the dataset we collected after the break during the lecture in Week 3. This is a partial replication of a study published in the journal Psychological Science (Miller et al. 2023). The original paper found the following results summarised in the abstract.\n\nRecent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 (N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces‚Äîa phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 (N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI.\n\nWe ran a quiz that students in the lecture could join on their phones/laptops. There were two parts to the quiz:\nThe faces section included 12 faces that were either photographs or real people or AI generated images of people. The stimuli were taked from the materials released by (Miller et al. 2023) on the Open Science Framework. Responses were either ‚ÄòReal person‚Äô or ‚ÄòAI generated‚Äô.\n\nThe confidence section asked participants to rate their confidence in their ability to do the following three things.\n\nDistinguish real photos of people from AI generated images (Experimental condition)\nDistinguish photos of happy people from photos of sad people (Emotional control condition)\nDistinguish photos of people you used to know in primary school from strangers (Memory control condition)\n\nScores were recorded on a scale from 1 (Completely confidence) to 10 (Not at all confident). The confidence section was repeated before and after the faces section to see if participants confidence changed as a result of doing the AI faces task",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#the-challenge",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#the-challenge",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "This week we will explore some factors that influence t-stats and p-values whilst introducing the concept of loops in R. We will finish by using effect sizes to compute a power analysis.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-new-variables-with-mutate",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-new-variables-with-mutate",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Last week, we used Jamovi to create several new variables that allowed us to run our analyses. We can do the same in a more transparent and reproducible way using R code.\nWe need to create both our proportion of correct faces across the 12 stimuli in one variable and a grouping variable which indicates whether each participant was confident in their ability to tell the difference between AI faces and real faces.\nWe can do both with the mutate() from the dplyr library, this provides functionality that lets us create, modify, and delete columns within a dataset. Take a look at the official documentation for mutate for more information.\nWhen computing a new varaible with mutate() we three pieces of information.\n\nThe dataset to work on\nThe name of the new variable to be created\nThe definition of how to compute the new variable for existing variables.\n\n\n\nLet‚Äôs start with our grouping variable. Our three pieces of information are\n\nThe dataset to work on - is data, which refers to the original datasheet loaded into Jamovi\nThe name of the new variable - is ConfidentBefore, the same as we used last week\nThe definition - is AIConfidenceBefore&lt;4, again the same as we used last week\n\nWe can combine these into a single line to create the variable. Note that we save the result into a new variable named my_data, we‚Äôll use this from now on to avoid confusion with the original datasheet.\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\nWe can check that this has done what we expected by running head to see the first few rows of the dataset.\nhead(my_data)\nThe result should look like this:\n\nWe can see on in the results on the right hand side that the additional colume ConfidentBefore now appears with TRUE and FALSE values for each participant.\n\n\n\nLet‚Äôs do the same for total proportion of correctly identified faces. We can use the same principle as we used for the grouping variable and use mutate() along with the variable definition and name to create our new column.\nThis definition is pretty long as we have 12 different faces to work with! You can copy the whole line using the copy icon in the right hand side of the code box. Take a moment to look through and understand each part: which part is the Dataset, the Name and the Definition?\nmy_data &lt;- mutate(my_data, PropFacesCorrect = (Face1_Real + Face2_Real + Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face7_Real + Face8_Real + Face9_Real + Face10_Real + Face11_AI + Face12_AI) / 12)\nand validate the overall result using head() to make sure that my_data now has the column we expected.\n\nThe final column of the dataset is now PropFacesCorrect and contains the proportion of face trials that participant got right.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute variables containing the proportion of correct responses for photos of real people and AI generated faces separately?\nStore the results in PropRealCorrectFaces for real faces and PropAICorrectFaces for AI generated faces.\n\n\nWe can use the code we wrote to compute the proportion of correct responses for all faces as a starting point.\nThink about how you could modify this line to compute the result for either real or AI faces on their own? What would you need to change in the code?\n\n\nThe following code will compute the variables\nmy_data &lt;- mutate(my_data, PropRealFacesCorrect = (Face1_Real + Face2_Real + Face7_Real + Face8_Real + Face9_Real + Face10_Real) / 6)\nmy_data &lt;- mutate(my_data, PropAIFacesCorrect = (Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face11_AI + Face12_AI) / 6)\nWe‚Äôve separated out the columns to sum all the real or AI faces together and changed the division to divide the results by 6 rather than 12.\nChecking the results with head() should produce the following outputs with four additional columns!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-a-t-test-using-r",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-a-t-test-using-r",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "We have the ingredients for our hypothesis test. Let‚Äôs use R to explore the following hypothesis (Hypothesis 2 from week 4).\n\nConfident people are better at distinguishing AI faces from real faces\n\nWe can compute this test using the following code that calls ttestIS() - this is the function that computes independent samples t-tests. Read more about it on the ttestIS documentation page.\nYou can click the numbers by the definitions at the bottom to highlight the corresponding part of the code.\n1jmv::ttestIS(\n2    formula = PropFacesCorrect ~ ConfidentBefore,\n3    data = my_data,\n4    effectSize = TRUE)\n\n1\n\njmv::ttestIS is the name of the R function that Jamovi uses to compute independent samples t-tests\n\n2\n\nThis tells ttestIS the formula that defines the test we want to run.\n\n3\n\nThis tells ttestIS which dataset we want to analyse\n\n4\n\nThis adds an additional effect size computation to the results\n\n\nMost of this will be familiar from previous weeks, but let‚Äôs think about the formula in a little more detail.\nIn R, the tilde (~) is used in formula definitions to specify the relationship between variables, particularly in statistical modeling and data analysis. Here, the tilde separates the dependent variable (response) from the independent variables (predictors). In our example, PropFacesCorrect is the dependent variable and ConfidentBefore is our independent variable (grouping variable) - so this formula\nPropFacesCorrect ~ ConfidentBefore\nis essentially telling ttestIS() to ‚ÄúCompute the ttest on PropFacesCorrect using ConfidentBefore as our groups‚Äù.\nWe can now run all our code to get the results of the ttest - the code clearly tells the reader how all the relevant variables were computed and what hypothesis test has been run all in one screen.\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nRead through the ttestIS documentation page. How could you change your code to add the following assumption checks\n\nTest for homogeneity of variance\nTest for normal distribution\n\n\n\nThe documentation page contains a list of all the possible information that we can pass as an input to our ttestIS() function call. Each item in the list corresponds to the options available in the Jamovi dialogue box.\nHave a look at the norm and eqv definitions. What would you need to add to the function call to run these additional checks?\n\n\nThe following code will compute all the checks in addition to the t-test.\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE,\n    norm = TRUE,\n    eqv = TRUE)                             \nThe results should look like this:\n\nDo the assumptions for Student‚Äôs t-test hold for this data?\n\n\n\n\n\n\nYour script might be getting quite long at this point! It is always a good idea to keep code neat and tidy where possible so that other people are able to read it, and so that we can read it if we come back to the analysis in the future.\nThere are many ways to keep things organised in a script. Here are two good hints, code comments can organise your script without changing the output and print() statements can help organise your code and the outputs.\n\n\nAny line of R code that starts with a hashtag is called a ‚Äòcomment‚Äô. The writing on this line is there for our information and will not be run by the computer. Adding code comments is a useful way to annotate your code indicating what each line or each section is doing. For example:\n# This is code comment\n\n# Compute which participants were more confident than the median\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\n\nThis will make it easier to understand what the coding is doing in future.\n\n\n\nWe can use the print() function to help organise our code as well. The text within the call to print() will not be executed by the computer but will simply be printed into the output console. This can be useful to break the output of your code into sections and to inlcude additional information about the analysis next to the outputs themselves.\nFor example:\nprint('Hypothesis 1')\nprint('Confident people are better at distinguishing AI faces from real faces')\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)\nThis code will print out information about the hypothesis next to the outputs.\nA full cleaned and commented version of our code might look like this.\n# Compute which participants were more confident than the median\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\n\n# Compute the proportion of all faces each participant got right\nmy_data &lt;- mutate(my_data, PropFacesCorrect = (Face1_Real + Face2_Real + Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face7_Real + Face8_Real + Face9_Real + Face10_Real + Face11_AI + Face12_AI) / 12)\n\n# Compute the proportion of REAL faces each participant got right\nmy_data &lt;- mutate(my_data, PropRealFacesCorrect = (Face1_Real + Face2_Real + Face7_Real + Face8_Real + Face9_Real + Face10_Real) / 6)\n\n# Compute the proportion of AI faces each participant got right\nmy_data &lt;- mutate(my_data, PropAIFacesCorrect = (Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face11_AI + Face12_AI) / 6)\n\n# Hypothesis test\nprint('Hypothesis 1')\nprint('Confident people are better at distinguishing AI faces from real faces')\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)\nand running this code produces the following outputs including the test from our print statements\n\nNice and clear what is happening at each stage! There is no perfect or ‚Äòcorrect‚Äô way to tidy up your code (though some people can get opinionated about this‚Ä¶). Choose a mix of comments and print statements that makes sense to you.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nExtend your code to test a second hypothesis.\n\nConfident participants are more accurate than unconfident participants when identifying photos of real faces.\n\nInclude some code comments and print statements\n\n\nThis will require a second indenpendent samples t-test asking whether the mean proportion correct value for only real faces is different between our confident and not-confident groups.\nUse the previous t-test as a starting point, can you copy this and modify it to do what we need?\n\n\nThe following code will test the new hypothesis with some comments and print statements\n# Hypothesis test\nprint('Hypothesis 2')\nprint('Confident participants are more accurate than unconfident participants when identifying photos of real faces.')\njmv::ttestIS(                                       \n    formula = PropRealFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)                       \nThe output of the whole script should now look like this.\n\nNote how the print statements help to separate the results into interpretable chunks.\n\n\n\n\n\n\nNow we‚Äôre going to explore how the size of the data sample changes the t-values that w\nIntroduce slice_sample Introduce simple loops",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#analysis-from-a-friend",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#analysis-from-a-friend",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Imagine that of your friends has been running some analyses on this dataset and would like you to check over their work. They send you the following R code that they have put together.\n# Compute some new variables\nmy_data &lt;- mutate(data, ChangeAIConfidence = AIConfidenceBefore - AIConfidenceAfter)\nmy_data &lt;- mutate(my_data, ChangeEmoConfidence = EmoConfidenceBefore - EmoConfidenceAfter)\nmy_data &lt;- mutate(my_data, ChangeMemoryConfidence = MemoryConfidenceBefore - MemoryConfidenceAfter)\n\n# Run the tests\njmv::ttestOneS(\n    data = my_data,\n    vars = vars(ChangeAIConfidence, ChangeEmoConfidence, ChangeMemoryConfidence),\n    testValue = 0.0,\n    wilcoxon = TRUE,\n    desc = TRUE,\n    norm = TRUE)\nLet‚Äôs think about this code a bit, your friend didn‚Äôt really explain the plan‚Ä¶\n\n\n\n\n\n\n Data Skills - what variables are created?\n\n\n\n\n\nThe code creates three new variables, ChangeAIConfidence, ChangeEmoConfidence and ChangeMemoryConfidence - each is computed by subtracting the confidence values after the face task from the confidence values before the task.\nThese variables represent the change in confidence from completing the task.\n\n\n\n\n\n\n\n\n\n Data Skills - what statistical test is performed?\n\n\n\n\n\nThe code runs a one sample t-test comparing the data mean to 0.0 using the jamovi function jmv::ttestOneS(). The code runs the test on all three variables. We can see that the input vars is passed a list of all three variable names.\n\n\n\n\n\n\n\n\n\n Data Skills - what additional computations are performed?\n\n\n\n\n\nIn addition to the t-tests, the code runs a Wilcoxon Rank test, a Shapiro-Wilk test for normality and a descriptive statistics table.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and run this code before going any further! Take a moment to read through the code and decide what it is doing.\n\n\n\n\n\n\nClick here if you want to check that you have the right outputs.\n\n\n\n\n\nThe results should look like this:\n\n\n\n\n\n\nThey have written the following about the results.\n\nA one sample t-test found that the change in participant confidence in distinguishing AI faces from real faces was significantly reduced after performing the task. The difference in confidence before and after the task (M=-1.1, SD=2.7) was significantly below zero, t(76) = 3.538, p = 0.0007. In contrast there was no change after the task confidence in emotion recognition (M=-.017, SD=1.341), t(76) = -1.012, p = 0.313 and face memory (M=-0.02985, SD=1.044), t(76) = -0.234, p = 0.816.\n\nLet‚Äôs think about whether this is accurate.\n\n\n\n\n\n\n Data Skills - are the reported statistics correct? take your time to read carefully and compare to the outputs\n\n\n\n\n\nNo! there are a number of mistakes throughout this writing. How many of the following mistakes did you spot?\n\nA one sample t-test found that the change in participant confidence in distinguishing AI faces from real faces was significantly reduced after performing the task. The difference in confidence before and after the task (M=-1.1, SD=2.7) was significantly below zero, t(76) = 3.538, p = 0.0007. In contrast there was no change after the task confidence in emotion recognition (M=-.017, SD=1.341), t(76) = -1.012, p = 0.313 and face memory (M=-0.02985, SD=1.044), t(76) = -0.234, p = 0.816.\n\n\n3.538 -&gt; the sign of the t-values on this test is wrong. Take a look a the output table, this should be -3.538.\n76 -&gt; the degrees of freedom for the second and third tests are reported incorrectly (perhaps copied from the first test?). These should be 64 and 66 respectively\n\n\n\n\n\n\n\n\n\n\n Data Skills - was the correct test reported?\n\n\n\n\n\nAgain, no! there are mistakes here as well. The friend has done the right thing by computing the Shapiro-Wilk statistics but appears to have ignored them in the writing.\nShapiro-Wilk is significant for all three variables - we should strongly consider reporting the Wilcoxon Rank Sum test rather than Student‚Äôs t.\nThey could have run this analysis as a paired t-test, but the one sample t-test of the differences is also a valid approach.\n\n\n\nComputing statistics and accurately reporting the results takes a lot of precision! take your time when writing code and results sections - make sure to check and double check your work as you go‚Ä¶",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-effect-sizes-for-t-tests",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-effect-sizes-for-t-tests",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Let‚Äôs think more about effect sizes - computing effect sizes in Jamovi and R is really easy. Simply click the check box to add ‚ÄòEffect size‚Äô under the ‚ÄòAddtional Statistics‚Äô section of the Jamovi window - or add effectSize = TRUE to the function call in R (we‚Äôve already been doing this above!).\n\n\n\n\n\n\nKey step\n\n\n\nUpdate your friend‚Äôs code to include computation of effect sizes - the results should appear in the t-test results table on the right.\n\n\n\nThere are effect sizes computed for every t-test and its non-parametric alternatives.\n\nCohen‚Äôs d is the parametric effect size corresponding to Student‚Äôs t-test\nRank Biserial Correlation is the non-parametric effect size corresponding to Wilcoxon‚Äôs test.\n\nThough they have methodological differences, these effect sizes can be interpreted in the same way as a measure of the magnitude of an effect.\nRemember that this is different to the t-value which is a measure of evidence against the null hypothesis. The important difference is that we can have strong evidence against the null either as the difference is large or if we have measured a small difference very precisely. The effect size only cares about the size of the difference between conditions.\nNote that Jamovi/R provide a ‚Äòsigned‚Äô effect size indicating the direction of the effect in the same way that a t-statistic does. For the following work we can ignore this sign and focus on the magnitudes only. In other words, we‚Äôll consider an effect size of +0.5 or -0.5 to indicate the same magitude of effect.\nThe creator of many of our classic effect size measures provided a guideline for what might be considered a ‚Äòsmall‚Äô or a ‚Äòlarge‚Äô effect.\n\n\n\nEffect Size\nCohen‚Äôs d\nInterpretation\n\n\n\n\nSmall\n0.2\nSmall effect\n\n\nMedium\n0.5\nMedium effect\n\n\nLarge\n0.8\nLarge effect\n\n\n\nThese can be useful guidelines - through they have been criticised strongly for both the arbitrary nature of the thresholds and for over-simplifying interpretation of effect sizes. We can still use them as an informal indicator to help us quickly interpret our results.\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeAIConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.403 is just below the thresold to be a ‚Äòmedium‚Äô effect (ignoring the sign of the effect size). The effect size for the Rank Biserial correlation is similar.\n\n\n\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeEmoConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.126 is just below the thresold to be a ‚Äòsmall‚Äô effect (ignoring the sign of the effect size). The effect size for the Rank Biserial correlation is larger but still closer to small than medium.\n\n\n\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeMemoryConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.0286 is tiny! the Rank Biserial correlation is even smaller‚Ä¶",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#use-an-effect-size-for-a-sensitivity-power-analysis",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#use-an-effect-size-for-a-sensitivity-power-analysis",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "One core application for effect sizes is that we can use the results to inform our sample size planning for future studies. we can do this with four linked pieces of information\n\nEffect Size this is our effect size, usually Cohen‚Äôs d for analyses in RM-B\nSignificance Thresold - this is the ‚Äòalpha‚Äô level at which we would consider a result significant, usually 5% or 0.05.\nPower - this is probability of correctly detecting an effect if one exists in the data, we usually want 80% power.\nNumber of datapoints (N) - this is our sample size.\n\nThese four pieces of information are linked such that if we have any three of these values we can always compute the fourth.\nThis lets us do some very useful calculations to see how well our dataset is enabling us to answer our hypothesis and to plan for future studies. For examples\n\nIf we know the effect size, significance threshold and number of participants in a study, we can compute the power to tell us the probability that we can correctly detect an effect. This is known as ‚Äòsensitivity‚Äô.\n\nor\n\nIf we want to compute how many participants we‚Äôll need in a future study and we already know the effect size from previous literature, we can specify the power and significance threshold we want and compute the number of participants that we‚Äôll need.\n\nVery useful! We can compute these different values using a Jamovi module known as ‚Äòjpower‚Äô\n\n\n\n\n\n\nKey step - Install the jpower addon in Jamovi\n\n\n\nClick through the ‚ÄòModules‚Äô button on the top right of the Jamovi ribbon. Click to the list of ‚ÄòAvailable‚Äô modules and install ‚Äòjpower‚Äô\n\nClick through to open the jpower menu and select a ‚Äòone-sample t-test‚Äô from the options. You should see the following dialogue box\n\n\n\nLet‚Äôs take a moment to understand the options.\nFirst - notice that we have the four pieces of information listed near the top of the options.\n\n\n\njpower menu\nInterpretation\n\n\n\n\nMinimally interesting effect size (Œ¥)\nEffect size\n\n\nMinimum desired power\nPower\n\n\nN\nNumber of participants\n\n\nŒ± (type 1 error rate)\nSignificance threshold\n\n\n\nBy default, one of these options is grayed out and not modifible. This is as jpower works to compute one value at a time whilst asking you to specify the other three. By default, we‚Äôre computing N as specified by the ‚ÄòCalculate‚Äô drop down menu at the top of the dialogue box. Try changing this to calculate one of the other values and notice that the greyed out box changes each time.\n\n\n\n\n\n\nKey step - Install the jpower addon in Jamovi\n\n\n\nLet‚Äôs run through an example. Say that we want to answer the following question:\n\nWhat is the probability the ChangeAIConfidence has been correctly detected in our previous analysis.\n\nWe should run through the following steps.\n\nSelect that we want to Calcuate ‚ÄòPower‚Äô in the top drop down menu.\nSet the ‚ÄòMinimally interesting effect size (Œ¥)‚Äô to our observed effect size from before. This is 0.429 (ignoring the sign)\nSet the ‚ÄòN‚Äô to 77 - this is the number of participants in the test (degrees of freedom are 76)\nSet ‚ÄòŒ± (type 1 error rate)‚Äô to 0.05 - this is usually the default\n\nThe results should appear automatically.\n\n\n\njpower gives us some really useful explanatory text along with the results. Read this carefully to understand what is going on. Firstly,\n\nThe purpose of a power analysis is to evaluate the sensitivity of a design and test. You have chosen to calculate the sensitivity of the chosen design for detecting the specified effect size.\n\nThis clarifies the type of analyses we have run - we want to compute the sensitivity of our analysis given particular data and effect size. Next the results are summarised\n\nA design with a sample size of 77 can detect effect sizes of Œ¥ ‚â• 0.429 with a probability of at least 0.96, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of Œ±= 0.05.\n\nThis is the critical information. This tells us that the power is 0.96 for our analysis, indicating that we have a 96% chance of correctly detecting an effect size at least as large as the one from our test. Great news!\nFinally, the ‚ÄòPower by Effect size‚Äô table tells us the power we would have for different effect sizes. For example, in this case if we have an effect size of 0.3 - we would have power between 50% and 80%. If the effect size were 0.2 we would have less than 50% power.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat is the sensitivity (power) of the effect size estimated by Cohen‚Äôs d for the ChangeEmoConfidence variable?\n\n\nTake a look at our previous results. Cohen‚Äôd d for ChangeEmoConfidence was -0.126.\n\n\nOur estimated power for the ChangeEmoConfidence is only 19.4%.\n\nA design with a sample size of 77 can detect effect sizes of Œ¥ ‚â• 0.126 with a probability of at least0.194, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of Œ± = 0.05.\n\n\nThis is the practical consequence of effect sizes! The smaller effect for ChangeEmoConfidence is harder to detect than the larger value for ChangeAIConfidence so we have a much lower chance of getting the right answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nIt might be the case that our face recognition really does change participants confidence in recognising emotional faces, and our study simply doesn‚Äôt have enough participants to detect the change with statistical confidence. Perhaps we should consider running a larger study?\nIf we assume that our value for Cohen‚Äôs d is accurate and we assume that there really is an effect to be detected (these are pretty brave assumptions!), how many participants would a future study need to have 80% chance of correctly detecting the effect?\n\n\nWe‚Äôll need to change our analysis to calculate ‚ÄòN‚Äô rather than ‚ÄòPower‚Äô using the drop down menu at the top. Can you specify the power?\n\n\nWe would need at least 497 participants to reject the null hypothesis if the estimated effect size for the change in emotional face recognition before and after our task is accurate! that is a lot‚Ä¶.\n\nWe would need a sample size of 497 to reliably (with probability greater than 0.8) detect an effect size of Œ¥ ‚â• 0.126, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of Œ± = 0.05.\n\n\nThis information suggests that this looking for differences in emotional face confidence is probably not worth pursuing futher. Our experiment wasn‚Äôt perfect but this effect is so small that it is likely to be of little to no practical significance.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#summary",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#summary",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "We have explored some new methods for creating new variables and running t-tests with effect sizes using R code. The R code we wrote is a really clear way to specify how variables were manipulated and which tests were run. Quite complex analyses with several stages can be clearly expressed this way.\nFinally, we used effect sizes to explore the power and sensitivity of our experiment and explore the possiblity of a follow up study.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "This week we will explore how to use Jamovi to compute and visualise descriptive statistics from a dataset. This will include some revision on Jamovi from last semester and an introduction on how we can use the R coding language to simplify complex data processing.\n\n\n\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nMeasures of central tendency (Mean and Median)\n\n\n\nMeasures of dispersion and spread (Standard Deviation and Interquartile Range)\n\n\n\nMeasures of skew and normality\n\n\n\nRecognising different types of distribution\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nExploring and cleaning datasets Jamovi\n\n\n\nComputing descriptive statistics in Jamovi\n\n\n\nComputing descriptive plots in Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with openly available research data\n\n\n\n\n\n\nWe will be exploring data that was collected as part of the ‚ÄòMany Labs 3‚Äô project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‚ÄòBig Five‚Äô personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs.¬†extravagant/careless)\nAgreeableness (friendly/compassionate vs.¬†critical/rational)\nNeuroticism (sensitive/nervous vs.¬†resilient/confident)\nOpenness to experience (inventive/curious vs.¬†consistent/cautious)\nExtraversion (outgoing/energetic vs.¬†solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.\n\n\n\nWe have data from a sample of students in a big data table. This raw data needs to be summarised and visualised so that we can turn this massive block of numbers into something intuitive.\nWe can generate a concise overview of the main characteristics of the data by computing and visualising few key features from the data.\n\n\n\n\n\n\nOpening Jamovi\n\n\n\nWe‚Äôll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham‚Äôs AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‚ÄòDesktop‚Äô application - not the ‚ÄòCloud‚Äô version.\n\n\n\n\n\n\n\n\n\n\n\n\nRevision from RM-A\n\n\n\nThis computer practical will have some revision from RM-A, please check your notes from RM-A for help with loading data and handling variables.\nIf you aren‚Äôt familiar with Jamovi, or would like a refresher - please take a quick look at the following topics in the Jamovi user guide\n\nFirst steps in Jamovi\nAnalyses\nThe spreadsheet\n\nPlease ask your tutor for help in the computer practical sessions if you need any assistance.\n\n\n\n\n\n\n\n\nKey step\n\n\n\nLet‚Äôs start by loading the dataset into Jamovi. You can download the file ML3AllSites_RMB.csv from canvas.\nYou can import csv files directly into Jamovi. There is more information in the Jamovi Textbook\nIt‚Äôs easy to open csv files in Jamovi. From the top left menu (the button with three parallel lines) choose ‚ÄòOpen‚Äô and browse to where you have stored the csv file on your computer. If you‚Äôre on a Mac, it‚Äôll look like the usual Finder window that you use to choose a file; on Windows it looks like an Explorer window. Find the one you want, then click on the ‚ÄòOpen‚Äô button.\n\n\n\nThis is the original dataset provided by the authors of the paper! It is a massive dataset - take a moment to get familiar with it by answering the following questions.\nSome of the questions will require you to run a descriptive statistics analysis on the age variable. Run what ever you need to explore the data and the questions.\n\n\n\n\n\n\n Data Skills - How many participants are included in the dataset?\n\n\n\n\n\nThere are 2402 participants in total - this is the number of rows in the dataset\n\n\n\n\n\n\n\n\n\n Data Skills - Is the dataset complete? do we have all information from everyone?\n\n\n\n\n\nNo, unfortunately some data is missing from some participants. You can see empty cells in quite a few places across the dataset. This means that some analyses may have an N of less than 2402 for analyses using some of these variables.\n\n\n\n\n\n\n\n\n\n Data Skills - Take a look at the configuration of the Age variable. Is it the correct type?\n\n\n\n\n\nJamovi has to make an educated guess at what type of data is included in each column. Looks like it has correctly set the age variable to be ‚ÄòContinuous‚Äô.\n\n\n\n\n\n\n\n\n\n Data Skills - How many participants do not have age information?\n\n\n\n\n\nYou can compute this information from the ‚ÄòExploration -&gt; Descriptives‚Äô menu.\nThe N for the age descriptives is 2402 and there are 0 missing participants. This is a large sample and there is age information for everyone.\n\n\n\n\n\n\n\n\n\n\n Data Skills - What is the average and standard deviation of the age of the participants?\n\n\n\n\n\nYou can compute this information from the ‚ÄòExploration -&gt; Descriptives‚Äô menu.\nThe mean age is 19.3 years, typical for a student population. The standard deviation is 3.81.\nThis means that a participant who is 19.3+3.81 = 23.11 years old would be one standard deviation above the average age of the sample.\n\n\n\n\n\n\n\n\n\n\n Data Skills - Do you think all the information in the age variable is accurate?\n\n\n\n\n\nYou can compute this information from the ‚ÄòExploration -&gt; Descriptives‚Äô menu. Try checking the box to show the 5 most extreme values in the dataset under the ‚ÄòStatistics -&gt; Outliers‚Äô submenu.\nTake a close look at the variable and its descriptive statistics.\nThe youngest age in the dataset is -2 and the oldest age is 149! neither of these can be true and are probably typos (or jokes) made during data collection. Some other values are unusual but not impossible - the second youngest student is 13. We do not know what has happened during data collection to lead to this as this is not our dataset. We should be careful about interpreting the mean whilst these variables are included‚Ä¶\n\n\n\n\nThis process of exploring and understanding your data is a critical first step when working on a new analysis. Real data is rarely in perfect condition - we have to take some time and apply common sense to understand the data and what challenges there might be when analysing it.\nIf we do identify issues, such are large amounts of missing data, incorrect data types or mistakes in data entry, we can normally fix them - but only if they‚Äôve been spotted! If we assume our data is perfect and go straight to the analysis then we could end up including a 149 year old student in our sample‚Ä¶\n\n\n\nIts probably not a good idea to include the data from the participants with impossible ages in our analysis. One simple step we can take is just to remove those rows from our analysis.\n\n\n\n\n\n\nData Skills - removing rows from a dataset\n\n\n\nRemove the two participants with impossible ages, you can use the extreme values table in the ‚ÄòDescriptives‚Äô output to help find the row numbers. You can remove a row in Jamovi by right clicking on the row number (far left of the Data view) and clicking ‚ÄòDelete Row‚Äô.\n\nThis will not change the original data file (unless you export the data back out again..), it will only affect the view of the data loaded into Jamovi. If you make a mistake you can simply load the original file and start again.\nNotice that the bottom of the data viewer tells you how many rows have been deleted in that dataset.\n\n\n\n\n\n\n\n\nKey step\n\n\n\nNow you can recompute the descriptive statistics of your age variable and see what has changed.\n\n\n\nThe mean is pretty much the same - still 19.3 - but the standard deviation is much smaller now. It has reduced from 3.81 to 2.70. That is a close to 25% reduction in the estimate of the variability of this data - big difference. Including outliers like these can have a large impact on our analyses.\n\n\n\n\n\n\n Critical thinking - Are you happy with the data in the age variable now?\n\n\n\n\n\nTough question! take a look at the Minimum value - we still have one participant who is 13 years old. Whilst this is more likely than finding a -2 or 149 year old student, it is still extremely unlikely. It is harder to make a judgement for this participant than the others. The first two participants had impossible ages, whereas a 13 year old student is just very unlikely. Sometimes we have to make a judgement call.\nLet‚Äôs delete row 709 to remove this participant as well.\nThe recomputed descriptive statistics now has an N=2399. Removing the 13 year old hasn‚Äôt changed our mean or standard deviation estimates very much - indicating that it was less important to remove this data point than the first two.\nThe youngest participant is now 16 which is more realistic (though still very impressive for that 16 year old!).\n\n\n\nThis process can be intricate and involves a lot of clicking in Jamovi - it would be easy to make a mistake and not notice that something has gone wrong. We‚Äôll explore some faster and more transparent ways to do this using R code during this course.\n\n\n\n\nOk, let‚Äôs move on to personality factors. As we said at the top, the big five factors are recorded on a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. So someone with an Extraversion score or 6 might be very outgoing where as someone with a score of 1 would be more of an introvert.\n\n\n\n\n\n\nKey step\n\n\n\nCompute the descriptive statistics for the big 5 personality factors.\nTake some time to consider the data quality - use the questions we answered with age as a guide for what to think about.\n\n\n\nNotice that we have a few missing data values in these variables. Jamovi will remove these from any analyses so they shouldn‚Äôt affect our descriptive statistics.\n\n\n\n\n\n\n Critical thinking - Are you happy to use the data in the big 5 personality factors?\n\n\n\n\n\nYes, these look reasonable. The data range matches what we would expect from the rating scales (values between 1 and 7) and there is only a small amount of missing data in each of the five variables.\nWe don‚Äôt need to do any further data cleaning based on these descriptive statistics.\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding 1\n\n\n\nTry your best with this question and don‚Äôt forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nYou have reproduced the descriptive statistics for the big 5 personality factors. Now you can do the same for some of the other factors in Table 2 of section 3.5 in Ebersole et al. (2016).\nSpecifically - try and compute the mean and standard deviations for the following factors (Descriptions of the factors are the the cited section of Ebersole et al. (2016)):\n\nSelf Esteem (see section 3.5.1)\nDaily Mood (see section 3.5.3)\nNeed for cognition (see section 3.5.5)\n\nCan you find variables (columns) that correspond to these factors?\nNote: Open datasets sometimes use shorthand for variable names rather than the names used in the manuscript. This is not ideal but does happen in practice! we should be ready take a little time to explore and understand a new dataset when working with it.\n\n\nIn this case the variables in the dataset to use are:\n\n\n\nIn Manuscript\nIn Jamovi\n\n\n\n\nSelf Esteem\n‚ÄòSelfEsteem‚Äô\n\n\nDaily Mood.\n‚ÄòMood‚Äô\n\n\nNeed for cognition\n‚ÄòNFC‚Äô\n\n\n\nTalk to your tutor if you‚Äôre getting stuck.\n\n\nYou should be able to reproduce the following table. Verify that the computed Mean and Standard Deviations match those published in table 2 of Ebersole et al (2016)\n\nThe final mean values are very close to the values in Ebersole et al. (2016) - but not identical. Looks like the original paper kept in all the data that had missing or unusual age values.\n\n\n\n\n\nNext, let‚Äôs think about the distributions of these variables. Its great to think about summarising a dataset with its mean value, but this doesn‚Äôt tell us about the wider distribution.\n\n\n\n\nNext we‚Äôll have a look at computing histograms from this data. Remember that histograms divide the data range into intervals called bins. The choice of bin width and number of bins can affect the appearance and interpretation of the histogram. Most of the time we can use the automatic bin definitions provided by Jamovi.\nWe can use Histograms to explore a few key factors of the wider data distributions that are hard to see from the descriptive statistics tables.\n\nShape: The shape of the histogram can reveal the distribution pattern of the data, such as normal distribution, skewness, or bimodality.\nOutliers: Histograms can help identify outliers or unusual data points that fall outside the general pattern.\n\nJamovi makes it very simple to compute a histogram. Open the ‚ÄòPlots‚Äô sub-menu of the descriptive statistics tab and select the ‚ÄòHistogram‚Äô checkbox.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a histogram of the participant ages\n\n\n\nThe descriptives tables give us a numerical summary of the data - but this can be hard to build an intuitive understanding of the dataset. A histogram gives us something immediately interpretable on a more ‚Äòhuman‚Äô level. Have a think about the following questions.\n\n\n\n\n\n\n Data Skills - What is the average age of the pariticpants in this dataset?\n\n\n\n\n\nWe can see that the most common ages are clustered around the late teenage years and early 20s. We can‚Äôt tell the exact mean from the histogram alone - we need to compute the descriptives table for this - but histograms are good to get a quick ‚Äòsense‚Äô of the data.\n\n\n\nIf you haven‚Äôt heard about the ‚Äònormal distribution‚Äô in your previous studies, this is a good opportunity to ask Copilot to give you a simple explanation.\n\n\n\n\n\n\n Data Skills - Do the ages look normally distributed?\n\n\n\n\n\nThere is a long tail to this distribution with a very small number of participants in their 30s, 40s and 50s. This is a strong positive skew and likely indicates that the data are not normally distributed.\nThis would be hard to tell from the descriptives table alone.\nOverall, this is a good match for what we would expect from a student population at a large university.\n\n\n\nWe can get an alternative perspective by visualising our data using a ‚ÄòBox Plot‚Äô.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a Box plot of the participant ages\n\n\n\nThe boxplot visualises the median and interquartile range (25th to 75th percentile) of the data, as well as highlighting outliers. This gives us less resolution around the middle of the distribution close to the median - but does give a much clearer view of the long tail in the ages of the student data sample.\n\n\n\n\n\n\n Data Skills - How old is the oldest participant in the dataset??\n\n\n\n\n\nThe most extreme outliers in the dataset are labelled with their row ID in the boxplot. The largest age value is in row 1081 - it looks like this is around 55 in the boxplot.\nWe can check the corresponding row in the data table to get some more precision. This tells us that the oldest participant is exactly 54 years old.\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding 2\n\n\n\nTry your best with this question and don‚Äôt forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nCompute histograms and/or boxplots for the big-5 personality factors and take some notes their distributions. Think about the following questions for each factor:\n\nIs the distribution symmetric?\nIf not, is there a positive or a negative skew?\nAre there any outliers?\n\nWrite some quick notes on your answers - we‚Äôll use these again in the next section\n\n\nIt might be easiest to do this one factor at a time to avoid being swamped with figures!\nCreate the figure for a single factor, make some notes and make the next figure with the next factor. You can consider saving the figure into a notebook or document if you want to make a more controlled comparison.\nIf you aren‚Äôt sure about any of the concepts - this is a good opportunity to ask your tutors in the session - or use Copilot to give you a simple explanation.\n\n\nHere are my notes about each distribution:\n\nOpenness, this does not look like a normal distribution - there is a negative skew in the data.\nConscientiousness, this is definitely not a normal distribution - there is a strong negative skew.\nExtraversion, this is close to being a normal distribution - perhaps a very slight negative skew.\nAgreeableness, this does not look like a normal distribution - there is a negative skew in the data.\nNeuroticism, this does not look like a normal distribution - there is a positive skew in the data.\n\nNone of the factors have obvious outliers.\n\n\n\n\n\n\n\n\nAs a final step, we can see if our intuitions about the distribution of big-5 personality factors match with more formal statistics measures checking for normal distributions and skewness.\n\n\n\n\n\n\nKey step\n\n\n\nCompute statistics Shapero-Wilk‚Äôs test for normality and for ‚ÄòSkewness‚Äô. You can add both of these tests from the ‚ÄòStatistics‚Äô sub-menu of the Descriptives analysis.\n\n\n\n\n\nThe Shapiro-Wilk test evaluates whether a sample comes from a normally distributed population. It returns 2 values:\n\n‚ÄòW‚Äô The W statistic measures how well the data conforms to a normal distribution. It ranges from 0 to 1 where values close to 1 indicate the data is normally distributed.\n‚Äòp‚Äô is a p-value indicating the probability that we could see a value of W at least as large as the one we have measured if the data really were normally distributed. Typically, we would say that the data are not normally distributed if p is less than or equal to 0.05.\n\n\n\n\n\n\n\n Data Skills - Do our notes from the histograms match the Shapiro-Wilk statistics?\n\n\n\n\n\nYes, If we look at the W statistics, then we see that Extraversion has the highest value indicating that it has the most ‚Äònormal‚Äô data distribution whilst the other four factors are similar. This matches what we saw with the histograms.\nThe p-values from the formal hypothesis test indicate that none of the 5 factors have normal distributions. We couldn‚Äôt tell this from our informal evaulation of the histograms and box plots.\n\n\n\n\n\n\nThe skewness metric returns a number close to zero if the data are symmetrical around zero, and a number close to +1 or -1 if the data are skewed. The sign of the skewness value indicates whether there is a positive or a negative skew.\n\n\n\n\n\n\n Data Skills - Do our notes from the histograms match the skewness statistic?\n\n\n\n\n\nYes, its very close. We can see that ‚ÄòExtraversion‚Äô has the least skew with a Skewness metric very close to zero (-0.0714). In contrast, ‚ÄòConscientiousness‚Äô has the most skew. Its value of ‚Äò-0.656‚Äô indicates a very substantial negative skewness in the distribution.\nOpenness and Agreeableness have moderate negative skews, whilst Neuroticism has a moderate positive skew.\n\n\n\n\n\n\n\nIn this session we have gone through the process of exploring a dataset from a published scientific article. We have computed a range of descriptive statistics and used histograms and boxplots to visualise the dataset.\nIt is common for publicly available data to contain little mistakes and issues. We need to go through a process like this with every new dataset and it should be like second nature by the end of this module. Remember to use common sense and practical thinking to identify issues and think about possible solutions.\nRemember this process when collecting your own data. Think carefully about how you can avoid typos and mistakes when collecting a dataset. It can save you and your collaborators a lot of time in future!\n\n\n\n\n\nBlock, Jack. 1995. ‚ÄúA Contrarian View of the Five-Factor Approach to Personality Description.‚Äù Psychological Bulletin 117 (2): 187‚Äì215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. ‚ÄúMany Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.‚Äù Journal of Experimental Social Psychology 67: 68‚Äì82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. ‚ÄúA Very Brief Measure of the Big-Five Personality Domains.‚Äù Journal of Research in Personality 37 (6): 504‚Äì28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#the-dataset",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#the-dataset",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "We will be exploring data that was collected as part of the ‚ÄòMany Labs 3‚Äô project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‚ÄòBig Five‚Äô personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs.¬†extravagant/careless)\nAgreeableness (friendly/compassionate vs.¬†critical/rational)\nNeuroticism (sensitive/nervous vs.¬†resilient/confident)\nOpenness to experience (inventive/curious vs.¬†consistent/cautious)\nExtraversion (outgoing/energetic vs.¬†solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#the-challenge",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#the-challenge",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "We have data from a sample of students in a big data table. This raw data needs to be summarised and visualised so that we can turn this massive block of numbers into something intuitive.\nWe can generate a concise overview of the main characteristics of the data by computing and visualising few key features from the data.\n\n\n\n\n\n\nOpening Jamovi\n\n\n\nWe‚Äôll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham‚Äôs AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‚ÄòDesktop‚Äô application - not the ‚ÄòCloud‚Äô version.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#exploring-a-new-dataset-in-jamovi",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#exploring-a-new-dataset-in-jamovi",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Revision from RM-A\n\n\n\nThis computer practical will have some revision from RM-A, please check your notes from RM-A for help with loading data and handling variables.\nIf you aren‚Äôt familiar with Jamovi, or would like a refresher - please take a quick look at the following topics in the Jamovi user guide\n\nFirst steps in Jamovi\nAnalyses\nThe spreadsheet\n\nPlease ask your tutor for help in the computer practical sessions if you need any assistance.\n\n\n\n\n\n\n\n\nKey step\n\n\n\nLet‚Äôs start by loading the dataset into Jamovi. You can download the file ML3AllSites_RMB.csv from canvas.\nYou can import csv files directly into Jamovi. There is more information in the Jamovi Textbook\nIt‚Äôs easy to open csv files in Jamovi. From the top left menu (the button with three parallel lines) choose ‚ÄòOpen‚Äô and browse to where you have stored the csv file on your computer. If you‚Äôre on a Mac, it‚Äôll look like the usual Finder window that you use to choose a file; on Windows it looks like an Explorer window. Find the one you want, then click on the ‚ÄòOpen‚Äô button.\n\n\n\nThis is the original dataset provided by the authors of the paper! It is a massive dataset - take a moment to get familiar with it by answering the following questions.\nSome of the questions will require you to run a descriptive statistics analysis on the age variable. Run what ever you need to explore the data and the questions.\n\n\n\n\n\n\n Data Skills - How many participants are included in the dataset?\n\n\n\n\n\nThere are 2402 participants in total - this is the number of rows in the dataset\n\n\n\n\n\n\n\n\n\n Data Skills - Is the dataset complete? do we have all information from everyone?\n\n\n\n\n\nNo, unfortunately some data is missing from some participants. You can see empty cells in quite a few places across the dataset. This means that some analyses may have an N of less than 2402 for analyses using some of these variables.\n\n\n\n\n\n\n\n\n\n Data Skills - Take a look at the configuration of the Age variable. Is it the correct type?\n\n\n\n\n\nJamovi has to make an educated guess at what type of data is included in each column. Looks like it has correctly set the age variable to be ‚ÄòContinuous‚Äô.\n\n\n\n\n\n\n\n\n\n Data Skills - How many participants do not have age information?\n\n\n\n\n\nYou can compute this information from the ‚ÄòExploration -&gt; Descriptives‚Äô menu.\nThe N for the age descriptives is 2402 and there are 0 missing participants. This is a large sample and there is age information for everyone.\n\n\n\n\n\n\n\n\n\n\n Data Skills - What is the average and standard deviation of the age of the participants?\n\n\n\n\n\nYou can compute this information from the ‚ÄòExploration -&gt; Descriptives‚Äô menu.\nThe mean age is 19.3 years, typical for a student population. The standard deviation is 3.81.\nThis means that a participant who is 19.3+3.81 = 23.11 years old would be one standard deviation above the average age of the sample.\n\n\n\n\n\n\n\n\n\n\n Data Skills - Do you think all the information in the age variable is accurate?\n\n\n\n\n\nYou can compute this information from the ‚ÄòExploration -&gt; Descriptives‚Äô menu. Try checking the box to show the 5 most extreme values in the dataset under the ‚ÄòStatistics -&gt; Outliers‚Äô submenu.\nTake a close look at the variable and its descriptive statistics.\nThe youngest age in the dataset is -2 and the oldest age is 149! neither of these can be true and are probably typos (or jokes) made during data collection. Some other values are unusual but not impossible - the second youngest student is 13. We do not know what has happened during data collection to lead to this as this is not our dataset. We should be careful about interpreting the mean whilst these variables are included‚Ä¶\n\n\n\n\nThis process of exploring and understanding your data is a critical first step when working on a new analysis. Real data is rarely in perfect condition - we have to take some time and apply common sense to understand the data and what challenges there might be when analysing it.\nIf we do identify issues, such are large amounts of missing data, incorrect data types or mistakes in data entry, we can normally fix them - but only if they‚Äôve been spotted! If we assume our data is perfect and go straight to the analysis then we could end up including a 149 year old student in our sample‚Ä¶",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#data-cleaning",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#data-cleaning",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Its probably not a good idea to include the data from the participants with impossible ages in our analysis. One simple step we can take is just to remove those rows from our analysis.\n\n\n\n\n\n\nData Skills - removing rows from a dataset\n\n\n\nRemove the two participants with impossible ages, you can use the extreme values table in the ‚ÄòDescriptives‚Äô output to help find the row numbers. You can remove a row in Jamovi by right clicking on the row number (far left of the Data view) and clicking ‚ÄòDelete Row‚Äô.\n\nThis will not change the original data file (unless you export the data back out again..), it will only affect the view of the data loaded into Jamovi. If you make a mistake you can simply load the original file and start again.\nNotice that the bottom of the data viewer tells you how many rows have been deleted in that dataset.\n\n\n\n\n\n\n\n\nKey step\n\n\n\nNow you can recompute the descriptive statistics of your age variable and see what has changed.\n\n\n\nThe mean is pretty much the same - still 19.3 - but the standard deviation is much smaller now. It has reduced from 3.81 to 2.70. That is a close to 25% reduction in the estimate of the variability of this data - big difference. Including outliers like these can have a large impact on our analyses.\n\n\n\n\n\n\n Critical thinking - Are you happy with the data in the age variable now?\n\n\n\n\n\nTough question! take a look at the Minimum value - we still have one participant who is 13 years old. Whilst this is more likely than finding a -2 or 149 year old student, it is still extremely unlikely. It is harder to make a judgement for this participant than the others. The first two participants had impossible ages, whereas a 13 year old student is just very unlikely. Sometimes we have to make a judgement call.\nLet‚Äôs delete row 709 to remove this participant as well.\nThe recomputed descriptive statistics now has an N=2399. Removing the 13 year old hasn‚Äôt changed our mean or standard deviation estimates very much - indicating that it was less important to remove this data point than the first two.\nThe youngest participant is now 16 which is more realistic (though still very impressive for that 16 year old!).\n\n\n\nThis process can be intricate and involves a lot of clicking in Jamovi - it would be easy to make a mistake and not notice that something has gone wrong. We‚Äôll explore some faster and more transparent ways to do this using R code during this course.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#descriptives-from-big-5-personality-factors",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#descriptives-from-big-5-personality-factors",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Ok, let‚Äôs move on to personality factors. As we said at the top, the big five factors are recorded on a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. So someone with an Extraversion score or 6 might be very outgoing where as someone with a score of 1 would be more of an introvert.\n\n\n\n\n\n\nKey step\n\n\n\nCompute the descriptive statistics for the big 5 personality factors.\nTake some time to consider the data quality - use the questions we answered with age as a guide for what to think about.\n\n\n\nNotice that we have a few missing data values in these variables. Jamovi will remove these from any analyses so they shouldn‚Äôt affect our descriptive statistics.\n\n\n\n\n\n\n Critical thinking - Are you happy to use the data in the big 5 personality factors?\n\n\n\n\n\nYes, these look reasonable. The data range matches what we would expect from the rating scales (values between 1 and 7) and there is only a small amount of missing data in each of the five variables.\nWe don‚Äôt need to do any further data cleaning based on these descriptive statistics.\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding 1\n\n\n\nTry your best with this question and don‚Äôt forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nYou have reproduced the descriptive statistics for the big 5 personality factors. Now you can do the same for some of the other factors in Table 2 of section 3.5 in Ebersole et al. (2016).\nSpecifically - try and compute the mean and standard deviations for the following factors (Descriptions of the factors are the the cited section of Ebersole et al. (2016)):\n\nSelf Esteem (see section 3.5.1)\nDaily Mood (see section 3.5.3)\nNeed for cognition (see section 3.5.5)\n\nCan you find variables (columns) that correspond to these factors?\nNote: Open datasets sometimes use shorthand for variable names rather than the names used in the manuscript. This is not ideal but does happen in practice! we should be ready take a little time to explore and understand a new dataset when working with it.\n\n\nIn this case the variables in the dataset to use are:\n\n\n\nIn Manuscript\nIn Jamovi\n\n\n\n\nSelf Esteem\n‚ÄòSelfEsteem‚Äô\n\n\nDaily Mood.\n‚ÄòMood‚Äô\n\n\nNeed for cognition\n‚ÄòNFC‚Äô\n\n\n\nTalk to your tutor if you‚Äôre getting stuck.\n\n\nYou should be able to reproduce the following table. Verify that the computed Mean and Standard Deviations match those published in table 2 of Ebersole et al (2016)\n\nThe final mean values are very close to the values in Ebersole et al. (2016) - but not identical. Looks like the original paper kept in all the data that had missing or unusual age values.\n\n\n\n\n\nNext, let‚Äôs think about the distributions of these variables. Its great to think about summarising a dataset with its mean value, but this doesn‚Äôt tell us about the wider distribution.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#visualising-distributions",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#visualising-distributions",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Next we‚Äôll have a look at computing histograms from this data. Remember that histograms divide the data range into intervals called bins. The choice of bin width and number of bins can affect the appearance and interpretation of the histogram. Most of the time we can use the automatic bin definitions provided by Jamovi.\nWe can use Histograms to explore a few key factors of the wider data distributions that are hard to see from the descriptive statistics tables.\n\nShape: The shape of the histogram can reveal the distribution pattern of the data, such as normal distribution, skewness, or bimodality.\nOutliers: Histograms can help identify outliers or unusual data points that fall outside the general pattern.\n\nJamovi makes it very simple to compute a histogram. Open the ‚ÄòPlots‚Äô sub-menu of the descriptive statistics tab and select the ‚ÄòHistogram‚Äô checkbox.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a histogram of the participant ages\n\n\n\nThe descriptives tables give us a numerical summary of the data - but this can be hard to build an intuitive understanding of the dataset. A histogram gives us something immediately interpretable on a more ‚Äòhuman‚Äô level. Have a think about the following questions.\n\n\n\n\n\n\n Data Skills - What is the average age of the pariticpants in this dataset?\n\n\n\n\n\nWe can see that the most common ages are clustered around the late teenage years and early 20s. We can‚Äôt tell the exact mean from the histogram alone - we need to compute the descriptives table for this - but histograms are good to get a quick ‚Äòsense‚Äô of the data.\n\n\n\nIf you haven‚Äôt heard about the ‚Äònormal distribution‚Äô in your previous studies, this is a good opportunity to ask Copilot to give you a simple explanation.\n\n\n\n\n\n\n Data Skills - Do the ages look normally distributed?\n\n\n\n\n\nThere is a long tail to this distribution with a very small number of participants in their 30s, 40s and 50s. This is a strong positive skew and likely indicates that the data are not normally distributed.\nThis would be hard to tell from the descriptives table alone.\nOverall, this is a good match for what we would expect from a student population at a large university.\n\n\n\nWe can get an alternative perspective by visualising our data using a ‚ÄòBox Plot‚Äô.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a Box plot of the participant ages\n\n\n\nThe boxplot visualises the median and interquartile range (25th to 75th percentile) of the data, as well as highlighting outliers. This gives us less resolution around the middle of the distribution close to the median - but does give a much clearer view of the long tail in the ages of the student data sample.\n\n\n\n\n\n\n Data Skills - How old is the oldest participant in the dataset??\n\n\n\n\n\nThe most extreme outliers in the dataset are labelled with their row ID in the boxplot. The largest age value is in row 1081 - it looks like this is around 55 in the boxplot.\nWe can check the corresponding row in the data table to get some more precision. This tells us that the oldest participant is exactly 54 years old.\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding 2\n\n\n\nTry your best with this question and don‚Äôt forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nCompute histograms and/or boxplots for the big-5 personality factors and take some notes their distributions. Think about the following questions for each factor:\n\nIs the distribution symmetric?\nIf not, is there a positive or a negative skew?\nAre there any outliers?\n\nWrite some quick notes on your answers - we‚Äôll use these again in the next section\n\n\nIt might be easiest to do this one factor at a time to avoid being swamped with figures!\nCreate the figure for a single factor, make some notes and make the next figure with the next factor. You can consider saving the figure into a notebook or document if you want to make a more controlled comparison.\nIf you aren‚Äôt sure about any of the concepts - this is a good opportunity to ask your tutors in the session - or use Copilot to give you a simple explanation.\n\n\nHere are my notes about each distribution:\n\nOpenness, this does not look like a normal distribution - there is a negative skew in the data.\nConscientiousness, this is definitely not a normal distribution - there is a strong negative skew.\nExtraversion, this is close to being a normal distribution - perhaps a very slight negative skew.\nAgreeableness, this does not look like a normal distribution - there is a negative skew in the data.\nNeuroticism, this does not look like a normal distribution - there is a positive skew in the data.\n\nNone of the factors have obvious outliers.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#computing-statistics-for-normality-and-skewness.",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#computing-statistics-for-normality-and-skewness.",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "As a final step, we can see if our intuitions about the distribution of big-5 personality factors match with more formal statistics measures checking for normal distributions and skewness.\n\n\n\n\n\n\nKey step\n\n\n\nCompute statistics Shapero-Wilk‚Äôs test for normality and for ‚ÄòSkewness‚Äô. You can add both of these tests from the ‚ÄòStatistics‚Äô sub-menu of the Descriptives analysis.\n\n\n\n\n\nThe Shapiro-Wilk test evaluates whether a sample comes from a normally distributed population. It returns 2 values:\n\n‚ÄòW‚Äô The W statistic measures how well the data conforms to a normal distribution. It ranges from 0 to 1 where values close to 1 indicate the data is normally distributed.\n‚Äòp‚Äô is a p-value indicating the probability that we could see a value of W at least as large as the one we have measured if the data really were normally distributed. Typically, we would say that the data are not normally distributed if p is less than or equal to 0.05.\n\n\n\n\n\n\n\n Data Skills - Do our notes from the histograms match the Shapiro-Wilk statistics?\n\n\n\n\n\nYes, If we look at the W statistics, then we see that Extraversion has the highest value indicating that it has the most ‚Äònormal‚Äô data distribution whilst the other four factors are similar. This matches what we saw with the histograms.\nThe p-values from the formal hypothesis test indicate that none of the 5 factors have normal distributions. We couldn‚Äôt tell this from our informal evaulation of the histograms and box plots.\n\n\n\n\n\n\nThe skewness metric returns a number close to zero if the data are symmetrical around zero, and a number close to +1 or -1 if the data are skewed. The sign of the skewness value indicates whether there is a positive or a negative skew.\n\n\n\n\n\n\n Data Skills - Do our notes from the histograms match the skewness statistic?\n\n\n\n\n\nYes, its very close. We can see that ‚ÄòExtraversion‚Äô has the least skew with a Skewness metric very close to zero (-0.0714). In contrast, ‚ÄòConscientiousness‚Äô has the most skew. Its value of ‚Äò-0.656‚Äô indicates a very substantial negative skewness in the distribution.\nOpenness and Agreeableness have moderate negative skews, whilst Neuroticism has a moderate positive skew.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#summary",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#summary",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "In this session we have gone through the process of exploring a dataset from a published scientific article. We have computed a range of descriptive statistics and used histograms and boxplots to visualise the dataset.\nIt is common for publicly available data to contain little mistakes and issues. We need to go through a process like this with every new dataset and it should be like second nature by the end of this module. Remember to use common sense and practical thinking to identify issues and think about possible solutions.\nRemember this process when collecting your own data. Think carefully about how you can avoid typos and mistakes when collecting a dataset. It can save you and your collaborators a lot of time in future!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#references",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#references",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Block, Jack. 1995. ‚ÄúA Contrarian View of the Five-Factor Approach to Personality Description.‚Äù Psychological Bulletin 117 (2): 187‚Äì215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. ‚ÄúMany Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.‚Äù Journal of Experimental Social Psychology 67: 68‚Äì82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. ‚ÄúA Very Brief Measure of the Big-Five Personality Domains.‚Äù Journal of Research in Personality 37 (6): 504‚Äì28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "This week we will explore how to use Jamovi to test hypotheses about a dataset using t-tests. We will touch on some revision from previous weeks so please do jump back to past computer practicals for revision if required\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nIndependent samples t-tests\n\n\n\nPaired samples t-tests\n\n\n\nAssumptions of parametric tests\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nComputing a new variable from existing data\n\n\n\nComputing checks for normality and homogeneity of variance\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nExploring and understanding new datasets\n\n\n\n\n\nWe‚Äôll be working with the dataset we collected after the break during the lecture in Week 3. This is a partial replication of a study published in the journal Psychological Science (Miller et al. 2023). The original paper found the following results summarised in the abstract.\n\nRecent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 (N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces‚Äîa phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 (N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI.\n\nWe ran a quiz that students in the lecture could join on their phones/laptops. There were two parts to the quiz:\nThe faces section included 12 faces that were either photographs or real people or AI generated images of people. The stimuli were taked from the materials released by (Miller et al. 2023) on the Open Science Framework. Responses were either ‚ÄòReal person‚Äô or ‚ÄòAI generated‚Äô.\n\nThe confidence section asked participants to rate their confidence in their ability to do the following three things.\n\nDistinguish real photos of people from AI generated images (Experimental condition)\nDistinguish photos of happy people from photos of sad people (Emotional control condition)\nDistinguish photos of people you used to know in primary school from strangers (Memory control condition)\n\nScores were recorded on a scale from 1 (Completely confidence) to 10 (Not at all confident). The confidence section was repeated before and after the faces section to see if participants confidence changed as a result of doing the AI faces task\n\n\n\nThis week we will use both one sample and two sample t-tests to explore the following hypotheses.\n\nPeople are able to distinguish AI generated faces from real photos of humans.\nConfident people are better at distinguishing AI faces from real faces.\nPeople‚Äôs confidence in distinguishing AI generated faces will reduce after performing the task, but their confidence about emotion perception and memory will not change.\n\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to think about these hypotheses. Which statistical test is most appropriate for each? Do they call for a one-tailed or a two-tailed test?\n\n\n\n\n\nIt is critical to take some time to understand the data we work with before running critical hypothesis tests. Here we‚Äôll take a look through the dataset to understand what information is present and if we‚Äôre happy to proceed with the analysis. This is similar to what we did in week 1 - you can refer back to the week 1 materials for additional guidance if you need it.\n\n\n\n\n\n\nKey step\n\n\n\nBefore going any further, the data file rmb-week-3_lecture-quiz-data_ai-faces.csv into a new Jamovi session.\n\n\nTake a read through the data columns. We have 26 in total with the following information.\n\n\n\n\n\n\n\nColumn Names\nDescription\n\n\n\n\nFirst Name\nParticipant ID - always ‚ÄòAnonymous‚Äô\n\n\nDataUse\nParticipant response to data re-use question\n\n\nAIConfidenceBefore\nConfidence in distinguishing AI faces from real BEFORE the task : 1 (Completely confident) to 10 (Not at all confident)\n\n\nEmoConfidenceBefore\nConfidence in distinguishing happy from sad faces BEFORE the task (Emotional control) : 1 (Completely confidence) to 10 (Not at all confident)\n\n\nMemoryConfidenceBefore\nConfidence in recognising a face from a long time ago BEFORE the task (Memory control) : 1 (Completely confidence) to 10 (Not at all confident)\n\n\nFace1_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace2_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace3_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace4_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace5_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace6_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace7_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace8_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace9_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace10_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace11_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace12_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nQuiz1\nResponse for revision quiz question\n\n\nQuiz2\nResponse for revision quiz question\n\n\nQuiz3\nResponse for revision quiz question\n\n\nAIConfidenceAfter\nConfidence in distinguishing AI faces from real AFTER the task\n\n\nEmoConfidenceAfter\nConfidence in distinguishing happy from sad faces AFTER the task (Emotional control)\n\n\nMemoryConfidenceAfter\nConfidence in recognising a face from a long time ago AFTER the task (Memory control)\n\n\n\nWork through the following questions, try to get an answer yourself before clicking to see the result. Data exploration is a critical skill that you‚Äôll need whenever looking a new data throughout your degree.\n\n\n\n\n\n\n Data Skills - how many participants took part in the quiz?\n\n\n\n\n\nWe have 124 rows of data observations the spreadsheet.\n\n\n\n\n\n\n\n\n\n Data Skills - did everyone consent to have their data included in this practical?\n\n\n\n\n\nYes, the responses in the DataUse column are always positive - ‚ÄúYes, I‚Äôm happy for my data to be included‚Äù. We removed the data with negative responses before sharing the data here.\n\n\n\n\n\n\n\n\n\n Data Skills - proportion of responsese would we expect to be accurate if participants responded randomly in the face questions.?\n\n\n\n\n\nWe would expect 50% accuracy if participants answered randomly. There were only two response options ‚ÄòReal person‚Äô or ‚ÄòAI generated‚Äô.\n\n\n\nYou can use Descriptive Statistics to answer the following questions.\n\n\n\n\n\n\n Data Skills - which face did participants identify most accurately?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‚ÄòMean‚Äô.\nFace 11 was most accurately identified as AI generated with 91.9% accuracy!\nFace 4 (AI) and Face 1 (Real) were close behind.\n\n\n\n\n\n\n\n\n\n Data Skills - which face did participants identify least accurately?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‚ÄòMean‚Äô.\nFace 9 was least accurately indified as a real human with 31.8% accuracy. Face 5 (AI was second least accurate)\n\n\n\n\n\n\n\n\n\n Data Skills - did we get complete data from all participants in the face task?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‚ÄòMean‚Äô.\nNo, quite a few participants dropped in and out during the task. We‚Äôre missing between 6 (Face 2 and 3) and 21 (Face 12) participants on each question.\n\n\n\n\n\n\n\n\n\n Data Skills - were participants most confident in their AI discrimination, emotion recognition or memory before the face task?\n\n\n\n\n\nCompute desriptive statistics for all AIConfidenceBefore, EmoConfidenceBefore and MemoryConfidenceBefore.\nParticipants were most confident in their ability to distinguish happy from sad faces in the Emotional face control condition with a score of 2.51. Participants were least confident in their ability to distinguish AI faces from real faces with a score of 4.54.\n\n\n\n\n\n\n\n\n\n Data Skills - are the confidence scores before face task normally distributed?\n\n\n\n\n\nCompute desriptive statistics for all AIConfidenceBefore, EmoConfidenceBefore and MemoryConfidenceBefore. Add the ‚ÄòShapiro-Wilk‚Äô statistic to the table.\nIt looks like none of these variabiles are normally distributied‚Ä¶ The W statistics for AIConfidenceBefore is much higher than the other two, but the p-values indicate that all three show a departure from a normal distribution.\nAdd a Histogram to your descriptive plots - we can see that AI confidence looks close to normally distributed but there is a very large skew in both Emotional confidence and Memory confidence - some participants had very low confidence scores in these conditions!\n\n\n\n\n\n\nThe descriptive statistics gave us a good overview of the dataset and we can start working towards testing our hypotheses.\nOne critical piece of information is missing though! we have accuracy for each individual face but not an overall score for each participant. We‚Äôll need to compute this new variable ourselves from the average accuracy of all twelve faces.\n\n\n\n\n\n\nData Skills - computing a variable from other columns\n\n\n\nWe can define our own variables in Jamovi using the ‚ÄòCompute‚Äô function in the ‚ÄòVariables‚Äô or the ‚ÄòData‚Äô tabs. Open a new Transformed variable.\nThis will open a menu with an option to give the new variable an name and description. Name the variable ‚ÄòPropFacesCorrect‚Äô to indicate that it contatins the proportion of faces that the participant responded correctly on. You can add a description if you like though this is optional.\nThe variable is defined within the formula box below the name definitions. We want to compute the average accuracy across all 12 faces so we can add the formula to compute that into the box.\n\nThe formula should add all the columns together and divide the result by 12 (the total number of faces). Make sure that all the additions are grouped by parentheses! otherwise Jamovi will only divide the final value by 12 and add it to the others. This is an example of BODMAS - Brackets, Of, Division/Multiplication, Addition/Subtraction that you might have covered in maths in school. talk to your tutor to make sure that this step makes sense.\nThe formula should look something like this, I‚Äôve removed some faces to simplify the visualisation. You should include them all.\n(Face1_Real + Face2_Real + ... + Face11_AI + Face12_AI) / 12\nOnce this is complete, you should be able to find your new column of values.\n\n\nNow, let‚Äôs take a look at our new variable. Compute some descritive statistics!\n\n\n\n\n\n\n Data Skills - how many participants gave a response to all 12 faces?\n\n\n\n\n\nWe have an N of 86 with 38 participants not responding to one or more of the faces.\n\n\n\n\n\n\n\n\n\n Data Skills - are the average accuracies normally distributed?\n\n\n\n\n\nTake a look at the Shaprio-Wilk statistic and the histogram of PropFacesCorrect, this looks like a normally distributed data variable.\n\n\n\n\n\n\n\n\n\n Data Skills - what proportion correct did the most and least accurate participants get?\n\n\n\n\n\nTake a look at the maxiumum and minumim of hte descriptive statistics, and perhaps add the table of ‚ÄòMost extreme‚Äô data values in the Outiers section.\nTwo individuals managed to get all 12 faces correct! Two individuals were at 25% accurate corresponding to 3 out of 12 faces correct.\n\n\n\n\n\n\nOk, we‚Äôre ready to test the first hypothesis. Use the information you know about the dataset and try to find an answer!\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nTest the following hypothesis:\n\nPeople are able to distinguish AI generated faces from real photos of humans\n\nWhat sort of hypothesis is this and what is the most approprate statistical test?\nCompute the statistics, do the data support the experimental or the null hypothesis?\n\n\nWe could write a statistical version of this hypothesis as something like this\n\nPeople are able to distinguish AI generated faces from real photos of faces at an accuracy greater than chance level.\n\nWhat sort of test do you need to run?\n\n\nWe need to run a one sample t-test that compares the PropCorrectFaces variable to a chance level of 0.5 (corresponding to 50%). The results could be reported as follows\n\nA one sample t-test comparing the group average proportion of correctly identified faces (M = 0.634, SD=0.164) to chance level (proportion correct = 0.5) showed a significant effect, t(85) = 7.52, p&lt;0.001. Participants were on average more accurate than chance at distinguishing AI generated face from photos of real faces.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 1?\n\n\n\n\n\nYes, the statistical test allows us to reject the null hypothesis that participant performance was no different to chance level on this task and accept the experimental hypothesis that participants are able to distinguish AI generated faces from real photos of humans.\n\n\n\n\n\n\nNow the second hypothesis. We don‚Äôt have everything we need to test this hypothesis yet. We‚Äôll need some way to split our participants into two groups - one with high confidence in AI face detection and one with low confidence. Time to compute another variable.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCompute a new variable named ConfidentBefore that separates the groups based on the median AI face detection confidence before the task.\n\n\nYou‚Äôll need the median value for AIConfidenceBefore - you can compute this from descriptive statistics.\nThe computed variable will need some logical condition (using operators like ‚Äò&gt;‚Äô, ‚Äò&lt;‚Äô or ‚Äò==‚Äô) that separates participants with confidence above and below the median.\n\n\nThe median value for AIConfidenceBefore is 4, so our computed variable definition will look like this.\n\nThe values in ConfidentBefore will now be ‚ÄòTrue‚Äô for people with high confidence and ‚ÄòFalse‚Äô for people with low confidence. It doesn‚Äôt matter if you‚Äôve done this the other way around - the tests will still work but the results will be flipped in the other direction (multiplied by -1)\n\n\n\n\n\nWith our new variable, we have what we need to run an independent samples t-test. This is very straightforward following the analyses we‚Äôve run previously in the module.\nOpen the ‚ÄòIndependent Sample t-test‚Äô menu under ‚Äòt-tests‚Äô. To run the analysis, drag PropFacesCorrect across as our dependent variable and our new ConfidentBefore variable as the grouping variable. The results should appear on the right automatically.\nOnce you have computed the core test - add the following options:\n\nDescriptives\nHomogeneity Test\nNormality Test\n\n\nLet‚Äôs think through the results\n\n\n\n\n\n\n Data Skills - are the data normally distributed?\n\n\n\n\n\nThe Shairo-Wilk statistic is not significant, indicating that the data are likely to be normally distributed\n\n\n\n\n\n\n\n\n\n Data Skills - do we have homogeneity of variance?\n\n\n\n\n\nLevene‚Äôs statistic is not significant, indicating that the variance of the two groups is comparable. We can confirm this by looking at the standard deviations in the descriptives table. 0.171 and 0.158 are fairly similar so the test report makes intuitive sense.\n\n\n\n\n\n\n\n\n\n Data Skills - which test should we report? Student‚Äôs, Welch‚Äôs or Mann-Whitney U?\n\n\n\n\n\nBoth assumptions of the standard Student‚Äôs t-test we looked at above are met by the data - we can proceed with Student‚Äôs test.\n\n\n\n\n\n\n\n\n\n Data Skills - were confident partiticpants or unconfident participants more accurate?\n\n\n\n\n\nConfident participants were more accurate! participants in this study are good at judging their ability.\nWe can see this by looking at the means in the descriptive statistics and the descriptives plot if you have run it.\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 2?\n\n\n\n\n\nYes, overall it seems like we can reject the null for hypothesis 2 and accept the experimental hypothesis that more confident participants are better at distinguishing AI faces from real human faces.\n\n\n\n\n\n\nNext we want to explore whether performing the face decision task changes peoples confidences in their abililty to detect AI generated faces. Remember that all participants categorised 12 faces with immediate correct/incorrect feedback and made confidence ratings at the start and end of the task.\nOpen the ‚ÄòPaired Sample t-test‚Äô menu under ‚Äòt-tests‚Äô. To run the analysis, drag AIConfidenceBefore and AIConfidenceAfter across as our pair of dependent variables. This is the format for running paired samples t-tests, the rest of the options should be familiar from our previous analyses.\nOnce you have computed the core test, do the same for the EmoConfidence and MemoryConfidence, and add the following options:\n\nDescriptives\nNormality Test\n\nThe results should appear on the right automatically.\n\nLet‚Äôs think through the results‚Ä¶\n\n\n\n\n\n\n Data Skills - does confidence in distinguishing AI faces from real faces change after completing the face task?\n\n\n\n\n\nYes, the results indicate that there is a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to distinguish AI faces from real faces before (M=4.48, SD=2.20) and after (M=5.66, SD=2.57) the face perception task. t(76) = -3.769, p&lt;0.001. Confidence was significantly reduced after completing the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - does confidence in distinguishing sad faces from happy faces change after completing the face task?\n\n\n\n\n\nNo, the results indicate that there is not a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to distinguish sad faces from happy faces before (M=2.57, SD=1.49) and after (M=2.74, SD=1.37) the face perception task. t(64) = -1.1017, p=0.313. Confidence was unchanged before and after the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - does confidence in recognising a face you haven‚Äôt seen for a long time change after completing the face task?\n\n\n\n\n\nNo, the results indicate that there is not a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to recognise faces of people they haven‚Äôt seen for a long time before (M=3.67, SD=1.89) and after (M=3.70, SD=1.86) the face perception task. t(66) = -0.234, p=0.816. Confidence was unchanged before and after the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - What test should we report for these comparisons? Are the parametric assumptions met?\n\n\n\n\n\nCheck the results of hte normality tests. The Shapiro-Wilk statistic is significant for all three tests! We should consider the non-parametric alternative test - add the ‚ÄòWilcoxon Rank‚Äô test to your analysis.\nWe should report the Wilcoxon Rank test along with the medians for non-parametric data. For example:\n\nA Wilcoxon Rank test showed a significant difference in participants confidence in their ability to distinguish AI faces from real faces before (Median=4) and after (Median=5) the face perception task. W=680, p&lt;0.001. Confidence was significantly reduced after completing the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 3?\n\n\n\n\n\nYes, overall it seems like we can reject the null for hypothesis 3 and accept the experimental hypothesis. Participants were less confident in their ability to detect AI generated faces after completing the task but their confidence in detecting emotions and remembering faces from a long time ago remains unchanged.\n\n\n\n\n\n\nLet‚Äôs use our new skills from this practical to answer one last question.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse a t-test to test this hypothesis:\n\nPeople are more accurate at identifying photos of real people compared to AI generated photos\n\nYou‚Äôll several of the skills from this session to answer the question‚Ä¶ think through what sort of variables you‚Äôll need and what sort of test you‚Äôll need.\n\n\nWe need 2 new variables to answer this question. We have already computed PropCorrectFaces in an earlier section, but we now need to make separate versions of this for AI faces and real faces‚Ä¶\nThe variable transforms will look something like this:\n \nWhat sort of t-test will you need?\n\n\nWe need to compute a paired samples t-test to answer the question as each participant contributes to both the AI face and real face conditions. Compute the test along with some descriptive statistics, we can report the test as follows\n\nA paired samples t-test showed a significant difference in the correct identification of AI faces (M=0.702, SD=0.198) compared to real faces (M=0.566, SD=0.229). t(85) = -4.60, p&lt;0.001. AI faces were identified more accurately than real faces.\n\n\n\n\n\n\n\n\n\n\nWe‚Äôve computed a range of tests to statistically assess our hypotheses today! One experiment can often yield enough data to run a wide range of analyses. It is always a good idea to start with your hypotheses and predictions to break the analysis down into manageable chunks.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#the-dataset",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#the-dataset",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "We‚Äôll be working with the dataset we collected after the break during the lecture in Week 3. This is a partial replication of a study published in the journal Psychological Science (Miller et al. 2023). The original paper found the following results summarised in the abstract.\n\nRecent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 (N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces‚Äîa phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 (N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI.\n\nWe ran a quiz that students in the lecture could join on their phones/laptops. There were two parts to the quiz:\nThe faces section included 12 faces that were either photographs or real people or AI generated images of people. The stimuli were taked from the materials released by (Miller et al. 2023) on the Open Science Framework. Responses were either ‚ÄòReal person‚Äô or ‚ÄòAI generated‚Äô.\n\nThe confidence section asked participants to rate their confidence in their ability to do the following three things.\n\nDistinguish real photos of people from AI generated images (Experimental condition)\nDistinguish photos of happy people from photos of sad people (Emotional control condition)\nDistinguish photos of people you used to know in primary school from strangers (Memory control condition)\n\nScores were recorded on a scale from 1 (Completely confidence) to 10 (Not at all confident). The confidence section was repeated before and after the faces section to see if participants confidence changed as a result of doing the AI faces task",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#the-challenge",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#the-challenge",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "This week we will use both one sample and two sample t-tests to explore the following hypotheses.\n\nPeople are able to distinguish AI generated faces from real photos of humans.\nConfident people are better at distinguishing AI faces from real faces.\nPeople‚Äôs confidence in distinguishing AI generated faces will reduce after performing the task, but their confidence about emotion perception and memory will not change.\n\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to think about these hypotheses. Which statistical test is most appropriate for each? Do they call for a one-tailed or a two-tailed test?",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#exploring-the-data",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#exploring-the-data",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "It is critical to take some time to understand the data we work with before running critical hypothesis tests. Here we‚Äôll take a look through the dataset to understand what information is present and if we‚Äôre happy to proceed with the analysis. This is similar to what we did in week 1 - you can refer back to the week 1 materials for additional guidance if you need it.\n\n\n\n\n\n\nKey step\n\n\n\nBefore going any further, the data file rmb-week-3_lecture-quiz-data_ai-faces.csv into a new Jamovi session.\n\n\nTake a read through the data columns. We have 26 in total with the following information.\n\n\n\n\n\n\n\nColumn Names\nDescription\n\n\n\n\nFirst Name\nParticipant ID - always ‚ÄòAnonymous‚Äô\n\n\nDataUse\nParticipant response to data re-use question\n\n\nAIConfidenceBefore\nConfidence in distinguishing AI faces from real BEFORE the task : 1 (Completely confident) to 10 (Not at all confident)\n\n\nEmoConfidenceBefore\nConfidence in distinguishing happy from sad faces BEFORE the task (Emotional control) : 1 (Completely confidence) to 10 (Not at all confident)\n\n\nMemoryConfidenceBefore\nConfidence in recognising a face from a long time ago BEFORE the task (Memory control) : 1 (Completely confidence) to 10 (Not at all confident)\n\n\nFace1_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace2_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace3_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace4_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace5_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace6_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace7_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace8_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace9_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace10_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace11_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace12_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nQuiz1\nResponse for revision quiz question\n\n\nQuiz2\nResponse for revision quiz question\n\n\nQuiz3\nResponse for revision quiz question\n\n\nAIConfidenceAfter\nConfidence in distinguishing AI faces from real AFTER the task\n\n\nEmoConfidenceAfter\nConfidence in distinguishing happy from sad faces AFTER the task (Emotional control)\n\n\nMemoryConfidenceAfter\nConfidence in recognising a face from a long time ago AFTER the task (Memory control)\n\n\n\nWork through the following questions, try to get an answer yourself before clicking to see the result. Data exploration is a critical skill that you‚Äôll need whenever looking a new data throughout your degree.\n\n\n\n\n\n\n Data Skills - how many participants took part in the quiz?\n\n\n\n\n\nWe have 124 rows of data observations the spreadsheet.\n\n\n\n\n\n\n\n\n\n Data Skills - did everyone consent to have their data included in this practical?\n\n\n\n\n\nYes, the responses in the DataUse column are always positive - ‚ÄúYes, I‚Äôm happy for my data to be included‚Äù. We removed the data with negative responses before sharing the data here.\n\n\n\n\n\n\n\n\n\n Data Skills - proportion of responsese would we expect to be accurate if participants responded randomly in the face questions.?\n\n\n\n\n\nWe would expect 50% accuracy if participants answered randomly. There were only two response options ‚ÄòReal person‚Äô or ‚ÄòAI generated‚Äô.\n\n\n\nYou can use Descriptive Statistics to answer the following questions.\n\n\n\n\n\n\n Data Skills - which face did participants identify most accurately?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‚ÄòMean‚Äô.\nFace 11 was most accurately identified as AI generated with 91.9% accuracy!\nFace 4 (AI) and Face 1 (Real) were close behind.\n\n\n\n\n\n\n\n\n\n Data Skills - which face did participants identify least accurately?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‚ÄòMean‚Äô.\nFace 9 was least accurately indified as a real human with 31.8% accuracy. Face 5 (AI was second least accurate)\n\n\n\n\n\n\n\n\n\n Data Skills - did we get complete data from all participants in the face task?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‚ÄòMean‚Äô.\nNo, quite a few participants dropped in and out during the task. We‚Äôre missing between 6 (Face 2 and 3) and 21 (Face 12) participants on each question.\n\n\n\n\n\n\n\n\n\n Data Skills - were participants most confident in their AI discrimination, emotion recognition or memory before the face task?\n\n\n\n\n\nCompute desriptive statistics for all AIConfidenceBefore, EmoConfidenceBefore and MemoryConfidenceBefore.\nParticipants were most confident in their ability to distinguish happy from sad faces in the Emotional face control condition with a score of 2.51. Participants were least confident in their ability to distinguish AI faces from real faces with a score of 4.54.\n\n\n\n\n\n\n\n\n\n Data Skills - are the confidence scores before face task normally distributed?\n\n\n\n\n\nCompute desriptive statistics for all AIConfidenceBefore, EmoConfidenceBefore and MemoryConfidenceBefore. Add the ‚ÄòShapiro-Wilk‚Äô statistic to the table.\nIt looks like none of these variabiles are normally distributied‚Ä¶ The W statistics for AIConfidenceBefore is much higher than the other two, but the p-values indicate that all three show a departure from a normal distribution.\nAdd a Histogram to your descriptive plots - we can see that AI confidence looks close to normally distributed but there is a very large skew in both Emotional confidence and Memory confidence - some participants had very low confidence scores in these conditions!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#computing-overall-accuracy",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#computing-overall-accuracy",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "The descriptive statistics gave us a good overview of the dataset and we can start working towards testing our hypotheses.\nOne critical piece of information is missing though! we have accuracy for each individual face but not an overall score for each participant. We‚Äôll need to compute this new variable ourselves from the average accuracy of all twelve faces.\n\n\n\n\n\n\nData Skills - computing a variable from other columns\n\n\n\nWe can define our own variables in Jamovi using the ‚ÄòCompute‚Äô function in the ‚ÄòVariables‚Äô or the ‚ÄòData‚Äô tabs. Open a new Transformed variable.\nThis will open a menu with an option to give the new variable an name and description. Name the variable ‚ÄòPropFacesCorrect‚Äô to indicate that it contatins the proportion of faces that the participant responded correctly on. You can add a description if you like though this is optional.\nThe variable is defined within the formula box below the name definitions. We want to compute the average accuracy across all 12 faces so we can add the formula to compute that into the box.\n\nThe formula should add all the columns together and divide the result by 12 (the total number of faces). Make sure that all the additions are grouped by parentheses! otherwise Jamovi will only divide the final value by 12 and add it to the others. This is an example of BODMAS - Brackets, Of, Division/Multiplication, Addition/Subtraction that you might have covered in maths in school. talk to your tutor to make sure that this step makes sense.\nThe formula should look something like this, I‚Äôve removed some faces to simplify the visualisation. You should include them all.\n(Face1_Real + Face2_Real + ... + Face11_AI + Face12_AI) / 12\nOnce this is complete, you should be able to find your new column of values.\n\n\nNow, let‚Äôs take a look at our new variable. Compute some descritive statistics!\n\n\n\n\n\n\n Data Skills - how many participants gave a response to all 12 faces?\n\n\n\n\n\nWe have an N of 86 with 38 participants not responding to one or more of the faces.\n\n\n\n\n\n\n\n\n\n Data Skills - are the average accuracies normally distributed?\n\n\n\n\n\nTake a look at the Shaprio-Wilk statistic and the histogram of PropFacesCorrect, this looks like a normally distributed data variable.\n\n\n\n\n\n\n\n\n\n Data Skills - what proportion correct did the most and least accurate participants get?\n\n\n\n\n\nTake a look at the maxiumum and minumim of hte descriptive statistics, and perhaps add the table of ‚ÄòMost extreme‚Äô data values in the Outiers section.\nTwo individuals managed to get all 12 faces correct! Two individuals were at 25% accurate corresponding to 3 out of 12 faces correct.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-1---people-are-able-to-distinguish-ai-generated-faces-from-real-photos-of-humans",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-1---people-are-able-to-distinguish-ai-generated-faces-from-real-photos-of-humans",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "Ok, we‚Äôre ready to test the first hypothesis. Use the information you know about the dataset and try to find an answer!\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nTest the following hypothesis:\n\nPeople are able to distinguish AI generated faces from real photos of humans\n\nWhat sort of hypothesis is this and what is the most approprate statistical test?\nCompute the statistics, do the data support the experimental or the null hypothesis?\n\n\nWe could write a statistical version of this hypothesis as something like this\n\nPeople are able to distinguish AI generated faces from real photos of faces at an accuracy greater than chance level.\n\nWhat sort of test do you need to run?\n\n\nWe need to run a one sample t-test that compares the PropCorrectFaces variable to a chance level of 0.5 (corresponding to 50%). The results could be reported as follows\n\nA one sample t-test comparing the group average proportion of correctly identified faces (M = 0.634, SD=0.164) to chance level (proportion correct = 0.5) showed a significant effect, t(85) = 7.52, p&lt;0.001. Participants were on average more accurate than chance at distinguishing AI generated face from photos of real faces.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 1?\n\n\n\n\n\nYes, the statistical test allows us to reject the null hypothesis that participant performance was no different to chance level on this task and accept the experimental hypothesis that participants are able to distinguish AI generated faces from real photos of humans.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-2---confident-people-are-better-at-distinguishing-ai-faces-from-real-faces.",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-2---confident-people-are-better-at-distinguishing-ai-faces-from-real-faces.",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "Now the second hypothesis. We don‚Äôt have everything we need to test this hypothesis yet. We‚Äôll need some way to split our participants into two groups - one with high confidence in AI face detection and one with low confidence. Time to compute another variable.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCompute a new variable named ConfidentBefore that separates the groups based on the median AI face detection confidence before the task.\n\n\nYou‚Äôll need the median value for AIConfidenceBefore - you can compute this from descriptive statistics.\nThe computed variable will need some logical condition (using operators like ‚Äò&gt;‚Äô, ‚Äò&lt;‚Äô or ‚Äò==‚Äô) that separates participants with confidence above and below the median.\n\n\nThe median value for AIConfidenceBefore is 4, so our computed variable definition will look like this.\n\nThe values in ConfidentBefore will now be ‚ÄòTrue‚Äô for people with high confidence and ‚ÄòFalse‚Äô for people with low confidence. It doesn‚Äôt matter if you‚Äôve done this the other way around - the tests will still work but the results will be flipped in the other direction (multiplied by -1)\n\n\n\n\n\nWith our new variable, we have what we need to run an independent samples t-test. This is very straightforward following the analyses we‚Äôve run previously in the module.\nOpen the ‚ÄòIndependent Sample t-test‚Äô menu under ‚Äòt-tests‚Äô. To run the analysis, drag PropFacesCorrect across as our dependent variable and our new ConfidentBefore variable as the grouping variable. The results should appear on the right automatically.\nOnce you have computed the core test - add the following options:\n\nDescriptives\nHomogeneity Test\nNormality Test\n\n\nLet‚Äôs think through the results\n\n\n\n\n\n\n Data Skills - are the data normally distributed?\n\n\n\n\n\nThe Shairo-Wilk statistic is not significant, indicating that the data are likely to be normally distributed\n\n\n\n\n\n\n\n\n\n Data Skills - do we have homogeneity of variance?\n\n\n\n\n\nLevene‚Äôs statistic is not significant, indicating that the variance of the two groups is comparable. We can confirm this by looking at the standard deviations in the descriptives table. 0.171 and 0.158 are fairly similar so the test report makes intuitive sense.\n\n\n\n\n\n\n\n\n\n Data Skills - which test should we report? Student‚Äôs, Welch‚Äôs or Mann-Whitney U?\n\n\n\n\n\nBoth assumptions of the standard Student‚Äôs t-test we looked at above are met by the data - we can proceed with Student‚Äôs test.\n\n\n\n\n\n\n\n\n\n Data Skills - were confident partiticpants or unconfident participants more accurate?\n\n\n\n\n\nConfident participants were more accurate! participants in this study are good at judging their ability.\nWe can see this by looking at the means in the descriptive statistics and the descriptives plot if you have run it.\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 2?\n\n\n\n\n\nYes, overall it seems like we can reject the null for hypothesis 2 and accept the experimental hypothesis that more confident participants are better at distinguishing AI faces from real human faces.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-3---peoples-confidence-in-their-ability-to-distinguish-ai-generated-faces-will-reduce-after-performing-the-task",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-3---peoples-confidence-in-their-ability-to-distinguish-ai-generated-faces-will-reduce-after-performing-the-task",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "Next we want to explore whether performing the face decision task changes peoples confidences in their abililty to detect AI generated faces. Remember that all participants categorised 12 faces with immediate correct/incorrect feedback and made confidence ratings at the start and end of the task.\nOpen the ‚ÄòPaired Sample t-test‚Äô menu under ‚Äòt-tests‚Äô. To run the analysis, drag AIConfidenceBefore and AIConfidenceAfter across as our pair of dependent variables. This is the format for running paired samples t-tests, the rest of the options should be familiar from our previous analyses.\nOnce you have computed the core test, do the same for the EmoConfidence and MemoryConfidence, and add the following options:\n\nDescriptives\nNormality Test\n\nThe results should appear on the right automatically.\n\nLet‚Äôs think through the results‚Ä¶\n\n\n\n\n\n\n Data Skills - does confidence in distinguishing AI faces from real faces change after completing the face task?\n\n\n\n\n\nYes, the results indicate that there is a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to distinguish AI faces from real faces before (M=4.48, SD=2.20) and after (M=5.66, SD=2.57) the face perception task. t(76) = -3.769, p&lt;0.001. Confidence was significantly reduced after completing the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - does confidence in distinguishing sad faces from happy faces change after completing the face task?\n\n\n\n\n\nNo, the results indicate that there is not a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to distinguish sad faces from happy faces before (M=2.57, SD=1.49) and after (M=2.74, SD=1.37) the face perception task. t(64) = -1.1017, p=0.313. Confidence was unchanged before and after the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - does confidence in recognising a face you haven‚Äôt seen for a long time change after completing the face task?\n\n\n\n\n\nNo, the results indicate that there is not a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to recognise faces of people they haven‚Äôt seen for a long time before (M=3.67, SD=1.89) and after (M=3.70, SD=1.86) the face perception task. t(66) = -0.234, p=0.816. Confidence was unchanged before and after the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - What test should we report for these comparisons? Are the parametric assumptions met?\n\n\n\n\n\nCheck the results of hte normality tests. The Shapiro-Wilk statistic is significant for all three tests! We should consider the non-parametric alternative test - add the ‚ÄòWilcoxon Rank‚Äô test to your analysis.\nWe should report the Wilcoxon Rank test along with the medians for non-parametric data. For example:\n\nA Wilcoxon Rank test showed a significant difference in participants confidence in their ability to distinguish AI faces from real faces before (Median=4) and after (Median=5) the face perception task. W=680, p&lt;0.001. Confidence was significantly reduced after completing the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 3?\n\n\n\n\n\nYes, overall it seems like we can reject the null for hypothesis 3 and accept the experimental hypothesis. Participants were less confident in their ability to detect AI generated faces after completing the task but their confidence in detecting emotions and remembering faces from a long time ago remains unchanged.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#bonus-hypothesis-people-are-more-accurate-at-identifying-photos-of-real-people-compared-to-ai-generated-photos.",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#bonus-hypothesis-people-are-more-accurate-at-identifying-photos-of-real-people-compared-to-ai-generated-photos.",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "Let‚Äôs use our new skills from this practical to answer one last question.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse a t-test to test this hypothesis:\n\nPeople are more accurate at identifying photos of real people compared to AI generated photos\n\nYou‚Äôll several of the skills from this session to answer the question‚Ä¶ think through what sort of variables you‚Äôll need and what sort of test you‚Äôll need.\n\n\nWe need 2 new variables to answer this question. We have already computed PropCorrectFaces in an earlier section, but we now need to make separate versions of this for AI faces and real faces‚Ä¶\nThe variable transforms will look something like this:\n \nWhat sort of t-test will you need?\n\n\nWe need to compute a paired samples t-test to answer the question as each participant contributes to both the AI face and real face conditions. Compute the test along with some descriptive statistics, we can report the test as follows\n\nA paired samples t-test showed a significant difference in the correct identification of AI faces (M=0.702, SD=0.198) compared to real faces (M=0.566, SD=0.229). t(85) = -4.60, p&lt;0.001. AI faces were identified more accurately than real faces.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#summary",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#summary",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "We‚Äôve computed a range of tests to statistically assess our hypotheses today! One experiment can often yield enough data to run a wide range of analyses. It is always a good idea to start with your hypotheses and predictions to break the analysis down into manageable chunks.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html",
    "href": "tutorials/rmb-week-9_one-way-anova.html",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "This week we will explore how to check the assumptions of hypothesis tests and make sure that we‚Äôre using the right tests. We will introduce the idea of transforming data to squeeze the data into the shape of a normal distribution.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nAssumptions of parametric tests\n\n\n\nLog transforms and Box-Cox transforms\n\n\n\nHistograms\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nCompute data transformations using Jamovi\n\n\n\nRun non-parametric alternative tests in Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nReplicate part of a published analysis\n\n\n\n\n\n\n\n\n\nContent Info\n\n\n\nThis tutorial refers to a video in which participants watched unpleasant events to simulate a traumatic experience in the lab. The content of this video is not disclosed or dicussed in any more detail than this during this tutorial - those who are interested can find more information in methods section the published article. This tutorial does explore the emotional impact that the video has on participants.\n\n\n\n\nFlashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we‚Äôre going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind‚Äôs eye and cause distress. We investigated whether reconsolidation‚Äîthe process during which memories become malleable when recalled‚Äîcan be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group.\n\n\n\nThis week - we will explore two of the four conditions to test the following hypothesis:\n\nPlaying tetris will after a trumatic event will reduce number of subsequent intrusive memories"
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#the-dataset",
    "href": "tutorials/rmb-week-9_one-way-anova.html#the-dataset",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Flashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we‚Äôre going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind‚Äôs eye and cause distress. We investigated whether reconsolidation‚Äîthe process during which memories become malleable when recalled‚Äîcan be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group."
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#the-challenge",
    "href": "tutorials/rmb-week-9_one-way-anova.html#the-challenge",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "This week - we will explore two of the four conditions to test the following hypothesis:\n\nPlaying tetris will after a trumatic event will reduce number of subsequent intrusive memories"
  },
  {
    "objectID": "tutorials/rmb-week-8_tidy-data.html",
    "href": "tutorials/rmb-week-8_tidy-data.html",
    "title": "Week 8 : Tidy data",
    "section": "",
    "text": "Week 8 : Tidy data\nDatasets we want to work with are often difficult to work with. It is common to find interesting and useful datasets that have strange column names, missing data, mistakes in data entry or weird formatting. We can and still should use these datasets but we have to TIDY them up first.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nTidy data principles\n\n\n\nLog transforms and Box-Cox transforms\n\n\n\nHistograms\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nData formatting using dplyr functions.\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with open access data"
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "This week we will explore how to use Jamovi to compute and visualise descriptive statistics from a dataset. This will include some revision on Jamovi from last semester and an introduction on how we can use the R coding language to simplify complex data processing.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nMeasures of central tendency\n\n\n\nMeasures of dispersion and spread\n\n\n\nMeasures of skew and normality\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Rj editor to use R code within Jamovi\n\n\n\nLearn the basics about functions and variables in R\n\n\n\nComputing descriptive statistics using R\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nValidating and working with openly available research data\n\n\n\nCreate a reproducible data analysis script\n\n\n\n\n\nMetaphors are often used to help people understand abstract concepts in terms of perceptual experiences. Though this is often useful, it can bias perception and decision making in unexpected ways. For example, North and South are abstract concepts that have a spatial association (north = up, south = down). Participants in the USA have a biased expectation that richer people will live in the north of a city (Meier et al. 2011). However, these expectations are not uniform across all people and cultures and we should not naively generalise this result to all people. A follow up study comparing this effect in participants from the USA and Hong Kong replicated the original effect for American participants but found the opposite effect in people from Hong Kong (Huang, Tse, and Cho 2014).\nThe Many Labs 2 project (Klein et al. 2018) performed an additional replication of this effect across a wide range of countries. Participants were presented with the following map outline of a city:\n\nand asked to indicate where they though a fictional person would live within the city. Participants were shown one of two descriptions of the person.\n\n‚ÄúDr.¬†Bennett lives in the city. He is a wealthy businessman who has travelled the world. He inherited a significant amount of money from a Great Aunt, and was educated at the best schools growing up. He enjoys fine dining and going to the theater on weekends.‚Äù\n\nOr\n\n‚ÄúMr.¬†Bennett lives in the city. He is unemployed. He was born and raised in the city he now calls home. He struggles to pay the rent each month, and dropped out of high school before graduation. He enjoys a good hot dog and a six pack of beers when he can.‚Äù\n\nThe first description is a caricature of a person with high Socio-Economic Status (SES) whilst the second is a caricature of someone with a low SES. The hypothesis is that people in the United States and Hong Kong have different demographic knowledge that may shape their metaphoric link between positive association and cardinal direction (north vs.¬†south).\n\n\n\nThis week we will use one-sample t-tests to explore whether participants from the UK would expect people with low or high SES to live north or south of the centre of the city.\n\n\n\n\n\n\nA new hypothesis?\n\n\n\nBefore we start, think about what you would predict about participants from the UK. Is there a reason to expect that people in the UK would associate the north of a city with high SES or vice versa?\nMake a note of your answer and the reasons behind it. Note that there isn‚Äôt necessarily a correct answer to this question and we won‚Äôt ask you for it later.\nIt is good practice to be clear about our predictions and expectations before starting a data analysis.\n\n\n\n\n\nLet‚Äôs start by loading the dataset into Jamovi and checking out the data. You can use the file named RMB-2024_computer-practical_huang-2014_country-UK_cond-All.csv.\nThis dataset has several columns of data. The main column of interest is currently named variable. This represents the y-axis position of the persons expectation about where the fictional character would live. A value of zero represents the middle of the city, positive values represent more northern locations and negative values represent more southern locations.\nThe other key variable is named factor. This indicates which group the participant was allocated to. This can be either ‚Äòhigh‚Äô or ‚Äòlow‚Äô depending on whether the participant saw a description of a person with stereotypes of high or low socioeconomic status.\nThis is open-access data and the column names aren‚Äôt very informative. It would be better to rename them to something more considered so that we can avoid mistakes later.\n\n\n\n\n\n\nKey step\n\n\n\nRename the following variables using the editor under the ‚ÄòVariables‚Äô tab.\n\n‚Äòvariable‚Äô should be renamed to ‚ÄòNorthSouth_Position‚Äô, and\n‚Äòfactor‚Äô should be renamed to ‚ÄòSES_Story‚Äô.\n\nDon‚Äôt include any spaces in the new variable names! this can cause an issue later.\n\n\n\nNext, take a look through the data itself. Remember the steps we took in the last couple of weeks. Have a careful scan through the data values and see if you can spot anything that needs to be fixed.\n\n\n\n\n\n\n Data Skills - how many participants are included in the dataset?\n\n\n\n\n\nWe have 136 rows of data observations the spreadsheet.\n\n\n\n\n\n\n\n\n\n Data Skills - what experimental design was used in this experiment?\n\n\n\n\n\nThis is a between-subjects design, though that might not be obvious from the data alone! Checking through the paper methods we can see that each participant was either in the high or low SES condition.\n\n\n\n\n\n\n\n\n\n Data Skills - do we need to tidy up this dataset?\n\n\n\n\n\nNo, in this case the dataset looks very clean. We appear to have data for everyone without any obvious typos or missing information. Looks good to go.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nCompute some descriptive statistics on the NorthSouth column. Make sure that you include the standard error of the mean!\nYou can split the descriptive statistics using a second variable. This\n\n\n\nNext, let‚Äôs think about what these values can tell us.\n\n\n\n\n\n\n Data Skills - do we have an equal number of data points in each condition?\n\n\n\n\n\nNo, we have 75 participants in the High condition and 61 participants in the Low condition.\n\n\n\n\n\n\n\n\n\n Data Skills - which condition has a wider variation in NorthSouth Positions chosen by participants?\n\n\n\n\n\nWe can see this from the standard deviations. The High condition has a standard deviation of 63.2 and the Low condition has a standard deviation of 77.8.\nThe responses in the Low SES story condition were more variable.\n\n\n\n\n\n\n\n\n\n Data Skills - which condition has a more northern mean position?\n\n\n\n\n\nRecall from the introduction that positive values indicate a position in the north of the city. The High condition has a mean value of -39.0 and the Low condition has a mean value of 18.6\nThe responses in the Low SES story condition were more northern.\n\n\n\n\n\n\n\n\n\n Data Skills - can we tell from the descriptives whether this is a significant difference?\n\n\n\n\n\nNo! remember that descriptive statistics only summarise the data as it is. They cannot tell us whether we would expect what we see in that data to generalise beyond the specific data sample.\n\n\n\n\n\n\n\n\n\n Data Skills - are we more confident in our estimate of the mean for the ‚ÄòNorth‚Äô or ‚ÄòSouth‚Äô condition?\n\n\n\n\n\nThe standard error of the mean tells us how well we think our estimated mean from the data sample represents the ‚Äòtrue‚Äô population mean.\nThe standard error of the mean for the ‚ÄòLow‚Äô group is higher (9.96 vs 7.3 for ‚ÄòHigh‚Äô) - indicating that we are less confident that this is a good representation of our population mean. Note that it could still be a good estimate, we‚Äôre just relatively less confident than we are for the ‚ÄòHigh‚Äô condition.\n\n\n\n\n\n\nNext, we‚Äôre going to ask whether the participants in the either the High or Low SES conditions had a preference towards locating our fictional character in the North or South of the city. This is our one-sample t-test. The NorthSouth_Position is our data variable and the centre point of the city is the comparison value.\nFirst, we need to split out the data so that we can run the t-test using either the Low or High SES conditions, not using both mixed together.\nWe can do this in Jamovi using a ‚ÄòFilter‚Äô. Jamovi provides some great documentation on data filters on their website. Take a few minutes and read through the info on ‚ÄòRow Filters‚Äô on this page.\nThe most important thing about filters is that they are reversible. We can apply a filter to run an analysis on a subset of the data without actually changing the dataset itself. This means we could make several different filters and turn them on and off to repeat the same analyses on different sections of the dataset. Convenient!\n\n\n\n\n\n\nKey step\n\n\n\nFollowing the guidance on the Jamovi documentation on data filters, create a filter to select ONLY the participants who heard the low SES story.\nThe result should exclude the first 75 rows from your dataset, you can confirm this by looking at the ticks and crosses in the ‚ÄòFilter‚Äô column, and by inspecting the values in the very bottom of the window.\n\n\n\nNote that your analyses in results windows will automatically update when you apply a filter to the data. With the filter applied, you will only see the descriptive statistics for the ‚ÄòLow‚Äô SES_Story groups.\nIf you click the ‚Äòactive‚Äô toggle in the ‚ÄòFilter‚Äô window you can turn the filter off and your analysis will return to its initial state.\n\n\n\n\n\n\nKey step\n\n\n\nWith your first Filter turned OFF, create a filter to select ONLY the participants who heard the High SES story.\nThe result should now exclude the last 61 rows from your dataset.\n You can turn your filters on and off to see how this changes the data available for the analysis, though turning both filters on will exclude all the data in this case‚Ä¶\n\n\n\n\n\nNow we‚Äôre ready to test our hypothesis! remember that the equation for a one-sample t-test is the following:\n\\[\nt = \\frac{\\text{The sample mean} - \\text{Comparison Value}}{\\text{The standard error of the mean}}\n\\]\nWe‚Äôre looking at the difference between our observed mean and a comparison value as a ratio to the precision to which we have estimated our observed mean.\nOur descriptive statistics output contains the mean and standard error of the mean for both groups. Our comparison value is the centre of the city, indicated by a score of 0 (zero).\nWe all the values required for a one-sample t-test already in the descriptive statistics! For the ‚ÄòLow‚Äô SES_Story condition, we have:\n\nGroup Mean = 18.6\nComparison Value = 0\nStandard error of the mean = 9.96\n\nSo, the one sample t-test value is:\n\\[\nt = \\frac{18.6 - 0}{9.96} = 1.867\n\\]\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute the t-value for the High SES Story group using a calculator or Rj?\n\n\nFirst, get the three values that you‚Äôll need for the computation. These are the comparison value and the mean and standard error of the mean for the High SES group.\n\n\nThe individual values are\n\nGroup Mean = -39.0\nComparison Value = 0\nStandard error of the mean = 7.30\n\nso the t-value is\n\\[\nt = \\frac{-39.0 - 0}{7.30} = -5.342\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nBefore going any further - make sure that your data filter is set to include only the data from the ‚ÄòLow‚Äô SES_Story condition.\n\n\nOf course, we will normally compute the t-test using Jamovi as rather than relying on hand calculations. We can do this by opening the ‚ÄòOne Sample T-Test‚Äô option under the ‚ÄòAnalysis -&gt; T-Tests‚Äô menu on the top ribbon.\nOpen the One Sample T-Test option now, and drag the NorthSouth_Position variable over to the ‚Äòdependent variables‚Äô box. The result should appear on the right hand side. Before we inspect the result, we should check and add a number of important options to our analysis.\n\n\nHypothesis - test value it is critical that we select the correct hypothesis or our test will be meaningless. Our ‚Äòtest value‚Äô, also known as a ‚Äòcomparison value‚Äô, is set to zero by default. This is correct for our analysis but may need setting to something else for other analyses. Take care to check it makes sense each time.\nHypothesis - test tail we can also specify whether we‚Äôre going to perform a one- or two-tailed test. Here we want to specify a two-tailed test in which the mean does not equal the test value.\nAssumption Checks we should add a test to check that our data are normally distributed. Selecting the ‚ÄòNormality test‚Äô box will compute a Shapio-Wilks statistic for you\nAdditional Statistics there are a number of important options here. For now, add the ‚ÄòDescriptive plots‚Äô option to your analysis. This will show us a visualisation of the estimated mean and its confidence intervals.\n\nLet‚Äôs take a look at the outputs, first the t-test table.\n\nThis contains the information we need to report our t-test: the t-value, the degrees of freedom and the p-value.\n\nt(60) = 1.87, p = 0.066\n\nThis t-value should match the number you computed by hand earlier on. Note that the footnote of the table reminds us of the hypothesis that we specified. Make sure that this matches what you expect!\nNext, the normality check:\n\nThis reports the Shapiro-Wilk W statistic and associated p-value. In this case the p-value is not significant which indicates that the data are normally distributed and we‚Äôre happy to go ahead with the parametric t-test.\nNext, the descriptives plot:\n\nThis is a really useful visualisation of our test. We can see the mean of the data (black dot) and the test value (dotted line). We also have the 95% confidence intervals of the estimated mean. We can see that the confidence intervals overlap with the test value - this tells us that it is likely our test will not be significant - according to our confidence intervals, the true population mean might include our test value zero which would indicate no real difference.\n\n\n\n\n\n\nKey step\n\n\n\nChange your data filter to compute the one-sample t-test for the ‚ÄòHigh‚Äô SES_Story condition. The test should update once your filter is on without you having to respecify the test.\n\n\n\n\n\n\n\n\n Data Skills - Is the mean position of participants in the ‚ÄòHigh‚Äô condition significantly different to zero?\n\n\n\n\n\nYes, we do have a significant difference - we could report our test as follows\n\nt(74) = -5.35, p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n Data Skills - Is it valid to use parametric statistics on this data?\n\n\n\n\n\nThe Shapiro Wilk statistic has a p-value of 0.038 which is less than p = 0.05. This suggets that our data are not normally distributed. We should inspect the data in more detail and consider using a non-parametric alternative.\nThe Wilcoxon Rank test is the non-parametric alternative for One Sample t-test. You can add this to the analysis to see if it gives the same result as the parametric test.\n\n\n\n\n\n\n\n\n\n Data Skills - Do the confidence intervals of the estimated mean cross zero?\n\n\n\n\n\nNo - in the ‚ÄòHigh‚Äô condition the confidence intervals of the mean are a long way from zero.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nGo back to your notes from the start of the session. Do the results match your predictions about whether a UK data sample would expect a high or low socio-economic status people to live in the north or south of the city?\n\n\n\n\n\n\nFinally - we should make a reproducible version of our analysis using Rj.\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and add the following line to load the dplyr library.\nlibrary(dplyr)\n\n\nFirstly, we‚Äôll need to replicate our data filter to select only the ‚ÄòHigh‚Äô or ‚ÄòLow‚Äô conditions for our analysis. We covered this in detail last week - if you need a refresher, take a look at the information on selecting rows of data using the filter function from week 2\nLet‚Äôs build a filter to select only the ‚ÄòHigh‚Äô condition.\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\nNext - turn on Jamovi‚Äôs syntax mode to display the R code associated with our analyses. This should create some R code at the top of each results block.\n\nWe can right click on this code to copy it before pasting it into the Rj window. Our code should then look like this:\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\n\njmv::ttestOneS(\n    data = data,\n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\n\n\n\n\n\n\nWarning\n\n\n\nSome people have encountered an issue with running this code block - if your R session stalls whilst running the code above, please try again whilst removing plots=TRUE.\nApologies - we‚Äôre working on a fix for this.\n\n\nThis command runs the function ttestOneS which computes the One Sample t-test. Jamovi has automatically detected that we want to add the normality test and the descriptive plots and included these options in the function call (see norm=TRUE and plots=TRUE).\nThere is a keyword argument in R for every option in Jamovi - take a look at the jmv library documentation for ttestOneS to see a full list.\nBefore running the code we need to make one adjustment. The function runs on the full dataset data by default but we want to use our new filtered data high_ses instead. Update the function to take in the filtered data as an input.\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\n\njmv::ttestOneS(\n    data = high_ses,\n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\nNow running the code should output result tables that match our previous analysis. Importantly the form of data preprocessing (the filtering) and the test itself are all clearly specified in the code. Making it easier for others to understand what analysis we have run.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you add a code block to compute the one-sample t-test for only the ‚ÄòLow‚Äô SES_Story condition\n\n\nRemember that you‚Äôll need to write a new filter line to create a copy of the dataset with only the ‚ÄòLow‚Äô values for SES_Story. You can save this into a variable and pass it in as the data argument to ttestOneS\n\n\nYour final code should look like this:\nlow_ses &lt;- filter(data, SES_Story == 'Low')\n\njmv::ttestOneS(\n    data = low_ses, \n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\n\n\n\n\n\nYou can keep adding R code to your script to run multiple analyses at once, this way we can run the t-test for both conditions with out having to go back and forth in Jamovi toggling the data filters on and off.\nIt takes a little more setup but the R code has some big advantages once you get used to it!\n\n\n\nWe‚Äôve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#the-dataset",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#the-dataset",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Metaphors are often used to help people understand abstract concepts in terms of perceptual experiences. Though this is often useful, it can bias perception and decision making in unexpected ways. For example, North and South are abstract concepts that have a spatial association (north = up, south = down). Participants in the USA have a biased expectation that richer people will live in the north of a city (Meier et al. 2011). However, these expectations are not uniform across all people and cultures and we should not naively generalise this result to all people. A follow up study comparing this effect in participants from the USA and Hong Kong replicated the original effect for American participants but found the opposite effect in people from Hong Kong (Huang, Tse, and Cho 2014).\nThe Many Labs 2 project (Klein et al. 2018) performed an additional replication of this effect across a wide range of countries. Participants were presented with the following map outline of a city:\n\nand asked to indicate where they though a fictional person would live within the city. Participants were shown one of two descriptions of the person.\n\n‚ÄúDr.¬†Bennett lives in the city. He is a wealthy businessman who has travelled the world. He inherited a significant amount of money from a Great Aunt, and was educated at the best schools growing up. He enjoys fine dining and going to the theater on weekends.‚Äù\n\nOr\n\n‚ÄúMr.¬†Bennett lives in the city. He is unemployed. He was born and raised in the city he now calls home. He struggles to pay the rent each month, and dropped out of high school before graduation. He enjoys a good hot dog and a six pack of beers when he can.‚Äù\n\nThe first description is a caricature of a person with high Socio-Economic Status (SES) whilst the second is a caricature of someone with a low SES. The hypothesis is that people in the United States and Hong Kong have different demographic knowledge that may shape their metaphoric link between positive association and cardinal direction (north vs.¬†south).",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#the-challenge",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#the-challenge",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "This week we will use one-sample t-tests to explore whether participants from the UK would expect people with low or high SES to live north or south of the centre of the city.\n\n\n\n\n\n\nA new hypothesis?\n\n\n\nBefore we start, think about what you would predict about participants from the UK. Is there a reason to expect that people in the UK would associate the north of a city with high SES or vice versa?\nMake a note of your answer and the reasons behind it. Note that there isn‚Äôt necessarily a correct answer to this question and we won‚Äôt ask you for it later.\nIt is good practice to be clear about our predictions and expectations before starting a data analysis.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#getting-started-with-descriptive-statistics",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#getting-started-with-descriptive-statistics",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Let‚Äôs start by loading the dataset into Jamovi and checking out the data. You can use the file named RMB-2024_computer-practical_huang-2014_country-UK_cond-All.csv.\nThis dataset has several columns of data. The main column of interest is currently named variable. This represents the y-axis position of the persons expectation about where the fictional character would live. A value of zero represents the middle of the city, positive values represent more northern locations and negative values represent more southern locations.\nThe other key variable is named factor. This indicates which group the participant was allocated to. This can be either ‚Äòhigh‚Äô or ‚Äòlow‚Äô depending on whether the participant saw a description of a person with stereotypes of high or low socioeconomic status.\nThis is open-access data and the column names aren‚Äôt very informative. It would be better to rename them to something more considered so that we can avoid mistakes later.\n\n\n\n\n\n\nKey step\n\n\n\nRename the following variables using the editor under the ‚ÄòVariables‚Äô tab.\n\n‚Äòvariable‚Äô should be renamed to ‚ÄòNorthSouth_Position‚Äô, and\n‚Äòfactor‚Äô should be renamed to ‚ÄòSES_Story‚Äô.\n\nDon‚Äôt include any spaces in the new variable names! this can cause an issue later.\n\n\n\nNext, take a look through the data itself. Remember the steps we took in the last couple of weeks. Have a careful scan through the data values and see if you can spot anything that needs to be fixed.\n\n\n\n\n\n\n Data Skills - how many participants are included in the dataset?\n\n\n\n\n\nWe have 136 rows of data observations the spreadsheet.\n\n\n\n\n\n\n\n\n\n Data Skills - what experimental design was used in this experiment?\n\n\n\n\n\nThis is a between-subjects design, though that might not be obvious from the data alone! Checking through the paper methods we can see that each participant was either in the high or low SES condition.\n\n\n\n\n\n\n\n\n\n Data Skills - do we need to tidy up this dataset?\n\n\n\n\n\nNo, in this case the dataset looks very clean. We appear to have data for everyone without any obvious typos or missing information. Looks good to go.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nCompute some descriptive statistics on the NorthSouth column. Make sure that you include the standard error of the mean!\nYou can split the descriptive statistics using a second variable. This\n\n\n\nNext, let‚Äôs think about what these values can tell us.\n\n\n\n\n\n\n Data Skills - do we have an equal number of data points in each condition?\n\n\n\n\n\nNo, we have 75 participants in the High condition and 61 participants in the Low condition.\n\n\n\n\n\n\n\n\n\n Data Skills - which condition has a wider variation in NorthSouth Positions chosen by participants?\n\n\n\n\n\nWe can see this from the standard deviations. The High condition has a standard deviation of 63.2 and the Low condition has a standard deviation of 77.8.\nThe responses in the Low SES story condition were more variable.\n\n\n\n\n\n\n\n\n\n Data Skills - which condition has a more northern mean position?\n\n\n\n\n\nRecall from the introduction that positive values indicate a position in the north of the city. The High condition has a mean value of -39.0 and the Low condition has a mean value of 18.6\nThe responses in the Low SES story condition were more northern.\n\n\n\n\n\n\n\n\n\n Data Skills - can we tell from the descriptives whether this is a significant difference?\n\n\n\n\n\nNo! remember that descriptive statistics only summarise the data as it is. They cannot tell us whether we would expect what we see in that data to generalise beyond the specific data sample.\n\n\n\n\n\n\n\n\n\n Data Skills - are we more confident in our estimate of the mean for the ‚ÄòNorth‚Äô or ‚ÄòSouth‚Äô condition?\n\n\n\n\n\nThe standard error of the mean tells us how well we think our estimated mean from the data sample represents the ‚Äòtrue‚Äô population mean.\nThe standard error of the mean for the ‚ÄòLow‚Äô group is higher (9.96 vs 7.3 for ‚ÄòHigh‚Äô) - indicating that we are less confident that this is a good representation of our population mean. Note that it could still be a good estimate, we‚Äôre just relatively less confident than we are for the ‚ÄòHigh‚Äô condition.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#data-filters-in-jamovi",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#data-filters-in-jamovi",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Next, we‚Äôre going to ask whether the participants in the either the High or Low SES conditions had a preference towards locating our fictional character in the North or South of the city. This is our one-sample t-test. The NorthSouth_Position is our data variable and the centre point of the city is the comparison value.\nFirst, we need to split out the data so that we can run the t-test using either the Low or High SES conditions, not using both mixed together.\nWe can do this in Jamovi using a ‚ÄòFilter‚Äô. Jamovi provides some great documentation on data filters on their website. Take a few minutes and read through the info on ‚ÄòRow Filters‚Äô on this page.\nThe most important thing about filters is that they are reversible. We can apply a filter to run an analysis on a subset of the data without actually changing the dataset itself. This means we could make several different filters and turn them on and off to repeat the same analyses on different sections of the dataset. Convenient!\n\n\n\n\n\n\nKey step\n\n\n\nFollowing the guidance on the Jamovi documentation on data filters, create a filter to select ONLY the participants who heard the low SES story.\nThe result should exclude the first 75 rows from your dataset, you can confirm this by looking at the ticks and crosses in the ‚ÄòFilter‚Äô column, and by inspecting the values in the very bottom of the window.\n\n\n\nNote that your analyses in results windows will automatically update when you apply a filter to the data. With the filter applied, you will only see the descriptive statistics for the ‚ÄòLow‚Äô SES_Story groups.\nIf you click the ‚Äòactive‚Äô toggle in the ‚ÄòFilter‚Äô window you can turn the filter off and your analysis will return to its initial state.\n\n\n\n\n\n\nKey step\n\n\n\nWith your first Filter turned OFF, create a filter to select ONLY the participants who heard the High SES story.\nThe result should now exclude the last 61 rows from your dataset.\n You can turn your filters on and off to see how this changes the data available for the analysis, though turning both filters on will exclude all the data in this case‚Ä¶",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#one-sample-t-tests",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#one-sample-t-tests",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Now we‚Äôre ready to test our hypothesis! remember that the equation for a one-sample t-test is the following:\n\\[\nt = \\frac{\\text{The sample mean} - \\text{Comparison Value}}{\\text{The standard error of the mean}}\n\\]\nWe‚Äôre looking at the difference between our observed mean and a comparison value as a ratio to the precision to which we have estimated our observed mean.\nOur descriptive statistics output contains the mean and standard error of the mean for both groups. Our comparison value is the centre of the city, indicated by a score of 0 (zero).\nWe all the values required for a one-sample t-test already in the descriptive statistics! For the ‚ÄòLow‚Äô SES_Story condition, we have:\n\nGroup Mean = 18.6\nComparison Value = 0\nStandard error of the mean = 9.96\n\nSo, the one sample t-test value is:\n\\[\nt = \\frac{18.6 - 0}{9.96} = 1.867\n\\]\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute the t-value for the High SES Story group using a calculator or Rj?\n\n\nFirst, get the three values that you‚Äôll need for the computation. These are the comparison value and the mean and standard error of the mean for the High SES group.\n\n\nThe individual values are\n\nGroup Mean = -39.0\nComparison Value = 0\nStandard error of the mean = 7.30\n\nso the t-value is\n\\[\nt = \\frac{-39.0 - 0}{7.30} = -5.342\n\\]",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#one-sample-t-tests-in-jamovi",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#one-sample-t-tests-in-jamovi",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Key step\n\n\n\nBefore going any further - make sure that your data filter is set to include only the data from the ‚ÄòLow‚Äô SES_Story condition.\n\n\nOf course, we will normally compute the t-test using Jamovi as rather than relying on hand calculations. We can do this by opening the ‚ÄòOne Sample T-Test‚Äô option under the ‚ÄòAnalysis -&gt; T-Tests‚Äô menu on the top ribbon.\nOpen the One Sample T-Test option now, and drag the NorthSouth_Position variable over to the ‚Äòdependent variables‚Äô box. The result should appear on the right hand side. Before we inspect the result, we should check and add a number of important options to our analysis.\n\n\nHypothesis - test value it is critical that we select the correct hypothesis or our test will be meaningless. Our ‚Äòtest value‚Äô, also known as a ‚Äòcomparison value‚Äô, is set to zero by default. This is correct for our analysis but may need setting to something else for other analyses. Take care to check it makes sense each time.\nHypothesis - test tail we can also specify whether we‚Äôre going to perform a one- or two-tailed test. Here we want to specify a two-tailed test in which the mean does not equal the test value.\nAssumption Checks we should add a test to check that our data are normally distributed. Selecting the ‚ÄòNormality test‚Äô box will compute a Shapio-Wilks statistic for you\nAdditional Statistics there are a number of important options here. For now, add the ‚ÄòDescriptive plots‚Äô option to your analysis. This will show us a visualisation of the estimated mean and its confidence intervals.\n\nLet‚Äôs take a look at the outputs, first the t-test table.\n\nThis contains the information we need to report our t-test: the t-value, the degrees of freedom and the p-value.\n\nt(60) = 1.87, p = 0.066\n\nThis t-value should match the number you computed by hand earlier on. Note that the footnote of the table reminds us of the hypothesis that we specified. Make sure that this matches what you expect!\nNext, the normality check:\n\nThis reports the Shapiro-Wilk W statistic and associated p-value. In this case the p-value is not significant which indicates that the data are normally distributed and we‚Äôre happy to go ahead with the parametric t-test.\nNext, the descriptives plot:\n\nThis is a really useful visualisation of our test. We can see the mean of the data (black dot) and the test value (dotted line). We also have the 95% confidence intervals of the estimated mean. We can see that the confidence intervals overlap with the test value - this tells us that it is likely our test will not be significant - according to our confidence intervals, the true population mean might include our test value zero which would indicate no real difference.\n\n\n\n\n\n\nKey step\n\n\n\nChange your data filter to compute the one-sample t-test for the ‚ÄòHigh‚Äô SES_Story condition. The test should update once your filter is on without you having to respecify the test.\n\n\n\n\n\n\n\n\n Data Skills - Is the mean position of participants in the ‚ÄòHigh‚Äô condition significantly different to zero?\n\n\n\n\n\nYes, we do have a significant difference - we could report our test as follows\n\nt(74) = -5.35, p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n Data Skills - Is it valid to use parametric statistics on this data?\n\n\n\n\n\nThe Shapiro Wilk statistic has a p-value of 0.038 which is less than p = 0.05. This suggets that our data are not normally distributed. We should inspect the data in more detail and consider using a non-parametric alternative.\nThe Wilcoxon Rank test is the non-parametric alternative for One Sample t-test. You can add this to the analysis to see if it gives the same result as the parametric test.\n\n\n\n\n\n\n\n\n\n Data Skills - Do the confidence intervals of the estimated mean cross zero?\n\n\n\n\n\nNo - in the ‚ÄòHigh‚Äô condition the confidence intervals of the mean are a long way from zero.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nGo back to your notes from the start of the session. Do the results match your predictions about whether a UK data sample would expect a high or low socio-economic status people to live in the north or south of the city?",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#reproducible-one-sample-t-tests",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#reproducible-one-sample-t-tests",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Finally - we should make a reproducible version of our analysis using Rj.\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and add the following line to load the dplyr library.\nlibrary(dplyr)\n\n\nFirstly, we‚Äôll need to replicate our data filter to select only the ‚ÄòHigh‚Äô or ‚ÄòLow‚Äô conditions for our analysis. We covered this in detail last week - if you need a refresher, take a look at the information on selecting rows of data using the filter function from week 2\nLet‚Äôs build a filter to select only the ‚ÄòHigh‚Äô condition.\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\nNext - turn on Jamovi‚Äôs syntax mode to display the R code associated with our analyses. This should create some R code at the top of each results block.\n\nWe can right click on this code to copy it before pasting it into the Rj window. Our code should then look like this:\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\n\njmv::ttestOneS(\n    data = data,\n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\n\n\n\n\n\n\nWarning\n\n\n\nSome people have encountered an issue with running this code block - if your R session stalls whilst running the code above, please try again whilst removing plots=TRUE.\nApologies - we‚Äôre working on a fix for this.\n\n\nThis command runs the function ttestOneS which computes the One Sample t-test. Jamovi has automatically detected that we want to add the normality test and the descriptive plots and included these options in the function call (see norm=TRUE and plots=TRUE).\nThere is a keyword argument in R for every option in Jamovi - take a look at the jmv library documentation for ttestOneS to see a full list.\nBefore running the code we need to make one adjustment. The function runs on the full dataset data by default but we want to use our new filtered data high_ses instead. Update the function to take in the filtered data as an input.\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\n\njmv::ttestOneS(\n    data = high_ses,\n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\nNow running the code should output result tables that match our previous analysis. Importantly the form of data preprocessing (the filtering) and the test itself are all clearly specified in the code. Making it easier for others to understand what analysis we have run.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you add a code block to compute the one-sample t-test for only the ‚ÄòLow‚Äô SES_Story condition\n\n\nRemember that you‚Äôll need to write a new filter line to create a copy of the dataset with only the ‚ÄòLow‚Äô values for SES_Story. You can save this into a variable and pass it in as the data argument to ttestOneS\n\n\nYour final code should look like this:\nlow_ses &lt;- filter(data, SES_Story == 'Low')\n\njmv::ttestOneS(\n    data = low_ses, \n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\n\n\n\n\n\nYou can keep adding R code to your script to run multiple analyses at once, this way we can run the t-test for both conditions with out having to go back and forth in Jamovi toggling the data filters on and off.\nIt takes a little more setup but the R code has some big advantages once you get used to it!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#summary",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#summary",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "We‚Äôve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html",
    "href": "tutorials/rmb_week-7_non-parametrics.html",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "This week we will explore how to check the assumptions of hypothesis tests and make sure that we‚Äôre using the right tests. We will introduce the idea of transforming data to squeeze the data into the shape of a normal distribution.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nAssumptions of parametric tests\n\n\n\nLog transforms and Box-Cox transforms\n\n\n\nHistograms\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nCompute data transformations using Jamovi\n\n\n\nRun non-parametric alternative tests in Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nReplicate part of a published analysis\n\n\n\n\n\n\n\n\n\nContent Info\n\n\n\nThis tutorial refers to a video in which participants watched unpleasant events to simulate a traumatic experience in the lab. The content of this video is not disclosed or dicussed in any more detail than this during this tutorial - those who are interested can find more information in methods section the published article. This tutorial does explore the emotional impact that the video has on participants.\n\n\n\n\nFlashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we‚Äôre going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind‚Äôs eye and cause distress. We investigated whether reconsolidation‚Äîthe process during which memories become malleable when recalled‚Äîcan be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group.\n\n\n\nThis week - we will explore two of the four conditions to test the following hypothesis:\n\nPlaying tetris will after a trumatic event will reduce number of subsequent intrusive memories\n\nAlong the way, we‚Äôll take a detailed tour of our options when working with non-normally distributed data.\nThe theme this week is PRECISION! we need high precision when interpreting and analysing results, the exercises this week are designed to give you practice in paying close attention to the details of your data‚Ä¶\n\n\n\nLet‚Äôs start by getting familiar with the dataset. Load the file into Jamovi and take a look at the ‚ÄòVariables‚Äô tab.\n\nThe creators of this dataset have helpfully included annotations for many of the variables they‚Äôve measured. This is extremely useful for us and definitely worth a few moments for you to do the same when working with your own data.\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to read through the variable descriptions. You‚Äôll need to refer back to this list throughout the session to find relevant variables.\n\n\nNext - take a look at the data sheet and the Condition variable. These relate to the different conditions in the experiment - all participants watched the film containing traumatic events and the four conditions describe what participants did after watching the film on day 1 of the experiment.\n\n\n\n\n\n\n\nCondition\nDescription\n\n\n\n\nNo-Task Control\nParticipants sit quietly\n\n\nReactivation+Tetris\nParticipants rewatch the film and then play Tetris\n\n\nTetris Only\nParticipants play Tetris\n\n\nReactivation Only\nParticipants rewatch the film\n\n\n\nParticipants were asked to keep an diary of any intrusive thoughts that they had prior to the experiment (day zero) and during the week after the experiment (days one to seven).\n\n\n\n\n\n\n Data Skills - which variable tracks the diary of intrusive memories?\n\n\n\n\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary contains the number of intrusive memories the participant recorded in the week after watching the film.\nDay_Zero_Image_Based_Intrusions_in_Intrusion_Diary is very similar - it contains the number of intrusive memories the participant recorded in the time prior to watching the film. This is a good measure to include so we can rule out differences in the baseline number of intrusive memories between our groups.\n\n\n\n\n\n\n\n\n\n Data Skills - what information do we have about participant‚Äôs performance when playing tetris?\n\n\n\n\n\nTetris_Total_Score contains the cumulative score of all tetris games the participant played - participants were allowed to restart their game if they finished before the time was up.\nWe also have Self_Rated_Tetris_Performance which measures how participants rated the difficulty of the tetris task and Tetris_Demand_Rating which measures how participants thought that playing tetris would affect their intrusive memories.\n\n\n\n\n\n\nLet‚Äôs start by taking a look at how participants in the conditions that played Tetris scored during their game time. Compute descriptive statistics on the Tetris_Total_Score variable and take a moment to get a sense of what it is telling you.\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the descriptive statistics of Tetris_Total_Score to answer the following questions in the spaces provided.\nThe average Tetris score was .\nThe most successful Tetris player scored  points.\nThe Tetris scores are positively skewednot skewednegatively skewed with a skewness of .\nThe Shapiro-Wilk statistic has a value of W =  and the p-value indicates that the scores are significantly non-normally distributednormally distributed\nWe should use parametricnon-parametric statistical approaches when analysing Tetris_Total_Score.\n\n\n\n\n\n\n\n\nLet‚Äôs use this data to test a hypothesis.\n\nPerformance whilst playing Tetris will be different at different times of day.\n\nWe can test this with an independent samples t-test as we‚Äôve done in previous weeks. Set up the t-test and include the following:\n\nWelchs‚Äô t-test\nMann-Whitney U\nEffect Size\nHomogeneity test\nNormality test\n\n\nLet‚Äôs unpack the results.\n\n\n\n\n\n\n Data Skills - does the Normality assumption check confirm what we saw in the descriptives?\n\n\n\n\n\nYes, we get the same picture but with a little less detail. Shapiro-Wilk‚Äôs test is significant indicating a non-normal distribution, but we still need the descriptive stats to understand HOW the distribution is different from a normal distribution.\nImportantly, this confirms that we should report the non-parametric alternative test - the Mann-Whitney U.\nYou could add the descriptives to the t-test page - but notice that this only includes a handful of metrics that are specifically related to t-tests (mean, standard deviation etc). We need the full list on the ‚ÄòDescriptives‚Äô tab to investigate distributions in detail.\n\n\n\n\n\n\n\n\n\n Data Skills - what does Levene‚Äôs test indicate and does this matter for us?\n\n\n\n\n\nLevene‚Äôs test explores homogeneity of variance, the result is not significant indicating that we are safe to assume homogeneity of variance between our groups.\nHowever, the data are not normally distributed - non-parametric tests don‚Äôt have an assumption of homogeneity of variance at all so, for this analysis, Levene‚Äôs test isn‚Äôt really relevant.\n\n\n\n\n\n\n\n\n\n Data Skills - what does the Mann-Whitney U test indicate about our hypothesis?\n\n\n\n\n\nThe Mann-Whitney U test is not significant - it seems like tetris scores are similar for participants who played in the morning and those who played in the afternoon.\n\n\n\n\n\n\nOn to our main hypothesis. (James et al. 2015) were most interested in the combination of the reactivation task and the tetris intervention and their effect on intrusive memories about the film. Let‚Äôs focus on only the two conditions we can compare with a t-test.\n\n\n\n\n\n\nKey step\n\n\n\nSet up a Jamovi data filter to focus our analysis on only the two conditions ‚ÄúNo-Task Control‚Äù and ‚ÄúReactivation+Tetris‚Äù conditions. Take a look at week 3 section 4 to revise data filters if you need to.\nYou can use this code to select both conditions together:\nCondition == \"No-Task Control\" or Condition == \"Reactivation+Tetris\"\nFinally, compute an independent samples t-test to test the following:\n\nPerforming a Reactivation and Tetris task changes the number of intrusive memories in the week after watching a traumatic film.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThere are  participants in each group.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p &lt; \nLevene‚Äôs test for homogeneity of variance indicated that we cancan not assume homogeneity of variance between the No-Task and Reactivation+Tetris groups, F(1, 32) = , p = .\nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in the week after the film between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .\n\n\n\n\n\n\n\n\n\n\n\nOur hypothesis test gives statistical support to the idea that playing tetris reduces intrusive memories - but could there be other explanations? This experiment is likely not perfect and it is possible that some bad luck in the data collection could have impacted the results. In this section we‚Äôll validate a few additional factors in the experimenta..\n\n\n(James et al. 2015) collected data variable to act as a control condition and rule out one possible confound in the experiment. Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary indicates the number of intrusive memories each participant had in the time before the experiment.\nThere isn‚Äôt a huge number of participants in each condition, it is possible that one group of participants was having more or less intrusive memories to begin with - irrespective of the experimental procedure.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a t-test to test the following:\n\nThere are no differences in the number of instrusive memories between the two experimental group BEFORE the experiment started.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThe participant in the ‚ÄòNo-Task Control‚Äô group had mean of  intrusive memories prior to the experiment starting.\nThe participant in the ‚ÄòReactivation+Tetris‚Äô group had mean of  intrusive memories prior to the experiment starting.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p = \nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in day before between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .\n\n\n\n\n\n\n\n\n\n\n\nAnother important factor to check was that the film actually changed how the participants felt. If the film didn‚Äôt do anything to impact the participants emotions then it is possible that we don‚Äôt actually understand the source of the difference we saw in the main hypothesis.\nThe film is the main experimental manipulation that so it is critical to check that it worked as we expected.\n\n\n\n\n\n\nKey step\n\n\n\nCompute paired samples t-test‚Äôs to check the following:\n\nWatching the traumatic film will change participants emotional state.\n\nCheck ALL of participants reported sadness, hopelessness, fear, horror and anxiousness.\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nTrue or False? The data variables for these factors are Normally Distributed according to Shaprio-Wilk statistics.\n\nSadness TRUEFALSE\nHopeless TRUEFALSE\nFear TRUEFALSE\nHorror TRUEFALSE\nAxious TRUEFALSE\n\nHow did the following factors change or not after watching the film?\n\nSadness increasedstayed the samedecreased after watching the film\nHopelessless increasedstayed the samedecreased after watching the film\nFear increasedstayed the samedecreased after watching the film\nHorror increasedstayed the samedecreased after watching the film\nAxiousness increasedstayed the samedecreased after watching the film\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - did the film manipulation work?\n\n\n\n\n\nYes, all factors apart from Hopelessless showed a statistically significant increase after watching the film. This indicates that the film had a strongly impact on participants negative emotions. It seems like the experimental manipulation worked.\n\n\n\n\n\n\n\nThere are times when we might not want to use a non-parametric test, for example if we specifically want to interpret the mean of the data rather than the median or if we‚Äôre running many similar tests and we don‚Äôt want to be switching back and forth between parametric and non-parametric tests.\nWe have another option when working with non-normal data. This is to apply a reversible transformation to the data observations in a variable to change the distribution to one that is closer to being normally distributed. When this process works, we are able to run the parametric test on the transformed data rather thans swapping to the non-parametric alternative.\nThis is valid when the transformation preserves the relationships in the dataset, that is it maintains the relative order of the observations. If data point A is larger than B before transformation then it should still be larger after the transformation. It is fine if the transformation changes the numbers themselves - we just have to be careful when making an interpretation.\nWe‚Äôll think about two data transformations in this section\n\n\n\n\n\n\n\nTransform\nExplanation\n\n\n\n\nLog Transform\nThe logarithm function, typically base 10 (log), is applied to each data point.\n\n\nBox-Cox Transform\nSimilar to the log-transform but with some parameters are tuned to work optimally for the specific dataset\n\n\n\nThe main difference is that the Box-Cox transform is more adaptive - whilst it the log-transform is applied in the same way to every dataset, the Box-Cox trasnform can be slightly different so that it works best for the data being analysed each time.\nWe can easily transform variables using Jamovi and have done something very similar when computing variables in previous weeks.\n\n\n\n\n\n\nData Skills - transforming a variable.\n\n\n\nYou can read about transforming variables in detail on the Jamovi documentation page on transforming variables. There are a few different steps but they are all straightforward.\nFirst, head to the ‚ÄòData‚Äô view and right click on the variable you want to transform, in this case we want Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary\n\nThis will open a dialogue box where you cna define the new variable. Give your new variable a descriptive name and make sure that the ‚ÄòSource variable‚Äô is the one that you expect. Note that you might see a ‚ÄòCircular definition‚Äô warning until you rename your variable.\n\nNext we need to specify the transformation that we want to use. Click into the ‚Äòusing transform‚Äô drop-down box and select ‚ÄúCreate New Transform‚Äù.\nHere you can define the transformation that you want to apply. You can define a single transform and apply it to multiple variables. Give the transformation a descriptive name and specify that you want the transfrom to apply the LOG10() function to the $source variable. ($source is a way to tell Jamovi to use ‚Äòwhichever variable is passed into the transform‚Äô rather than defining the transform on only 1 variable here.)\n\nYou can click on the Fx logo to select functions from a list if you prefer, there are lots of pre-defined options.\n\nOnce you‚Äôre happy with the transformation, you can close the transformation definition dialogue and specify that we want to use the ‚ÄòLogTransform‚Äô that we just defined to transfrom Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary to make our new variable.\n\nThere are a lot of steps here - but each is fairly straightforward. Ask your tutor if you get stuck.\n\n\nNow that we have our new variable we can explore what it has done. Let‚Äôs compare the descriptive statistics for the original and the transformed variable.\n\n \n\n\n\n\n\n\n Data Skills - has the log transform changed mean of the data?\n\n\n\n\n\nYes, the log transform has changed the mean by a long way. The original data shows that the average participant had 3.5 intrusive memories but that number has changed to 1.28 in the transformed data.\nThis does not mean that the number has really reduced! remember that the transformation is changing the absolute values of the observations but should be preserving the relative relationships between the data points.\nThis is fine, but we must not report the log-transformed values as the actual means. The participants still had an average of 3.5 intrusive memories whether we transformed the data or not.\n\n\n\n\n\n\n\n\n\n Data Skills - has the log transform changed the skewness and kurtosis of the data?\n\n\n\n\n\nYes, it has had a big impact. Both the skewness and kurtosis values are much closer to zero in the transformed data compared to the original. Both indicates that the transformed data are more normally distributed.\n\n\n\n\n\n\n\n\n\n Data Skills - is there a statistically significant difference from a normal distribution in the transformed data?\n\n\n\n\n\nNo, whereas the Shapiro-Wilk statistic showed a significant departure from a normal distribution for the original data (W = 0.8, p &lt; 0.001) it shows that the transformed data is now indistinguishable from a normal distribution (W = 0.963, p = 0.257)\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCompute a new data transform that applies a Box-Cox transform to Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary.\n\n\nFollow the instructions above, the Box-Cox transform is one of the predefined transformation options.\n\n\n\nYour transformed variable should have the following descriptive statistics. See Days_One_to_Seven_BoxCox below.\n\n\n\n\n\n\nFinally - let‚Äôs see if we can use our transformed variables to compute a parametric t-test. Repeat the independent samples t-test from earlier but include all three variables in the ‚ÄòDependent Variables‚Äô box. Make sure to include\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary - original data\nDays_One_to_Seven_Transformed - Log transformed data\nDays_One_to_Seven_BoxCox - BoxCox transformed data\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nWith the original data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . We can proceed with Student‚Äôs t-testMann-Whitney U test.\nUsing the log-transformed data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . Levene‚Äôs test found that the we cancan not assume that the groups have homogeneity of variance, F(1, 34) = , p = . We can proceed with Student‚Äôs t-testWelch‚Äôs t-testMann-Whitney U test.\nUsing the BoxCox-transformed data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . Levene‚Äôs test found that the we cancan not assume that the groups have homogeneity of variance, F(1, 34) = , p = . We can proceed with Student‚Äôs t-testWelch‚Äôs t-testMann-Whitney U test.\n\n\n\n\n\n\n\n\n\n\n\nWe have explored an interesting dataset in some detail and worked with serveral different analysis approaches. Specifically, we have several options when working on checking the assumptions of our analyses. We have to be careful when computing and reporting hypothesis tests that we‚Äôre using the correct tests in a way that is supported by the data. Sometimes we can use a transform to continuous using parametric tests when the assumptions are not supported but this will not always work.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#the-dataset",
    "href": "tutorials/rmb_week-7_non-parametrics.html#the-dataset",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Flashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we‚Äôre going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind‚Äôs eye and cause distress. We investigated whether reconsolidation‚Äîthe process during which memories become malleable when recalled‚Äîcan be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#the-challenge",
    "href": "tutorials/rmb_week-7_non-parametrics.html#the-challenge",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "This week - we will explore two of the four conditions to test the following hypothesis:\n\nPlaying tetris will after a trumatic event will reduce number of subsequent intrusive memories\n\nAlong the way, we‚Äôll take a detailed tour of our options when working with non-normally distributed data.\nThe theme this week is PRECISION! we need high precision when interpreting and analysing results, the exercises this week are designed to give you practice in paying close attention to the details of your data‚Ä¶",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#dataset-description",
    "href": "tutorials/rmb_week-7_non-parametrics.html#dataset-description",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Let‚Äôs start by getting familiar with the dataset. Load the file into Jamovi and take a look at the ‚ÄòVariables‚Äô tab.\n\nThe creators of this dataset have helpfully included annotations for many of the variables they‚Äôve measured. This is extremely useful for us and definitely worth a few moments for you to do the same when working with your own data.\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to read through the variable descriptions. You‚Äôll need to refer back to this list throughout the session to find relevant variables.\n\n\nNext - take a look at the data sheet and the Condition variable. These relate to the different conditions in the experiment - all participants watched the film containing traumatic events and the four conditions describe what participants did after watching the film on day 1 of the experiment.\n\n\n\n\n\n\n\nCondition\nDescription\n\n\n\n\nNo-Task Control\nParticipants sit quietly\n\n\nReactivation+Tetris\nParticipants rewatch the film and then play Tetris\n\n\nTetris Only\nParticipants play Tetris\n\n\nReactivation Only\nParticipants rewatch the film\n\n\n\nParticipants were asked to keep an diary of any intrusive thoughts that they had prior to the experiment (day zero) and during the week after the experiment (days one to seven).\n\n\n\n\n\n\n Data Skills - which variable tracks the diary of intrusive memories?\n\n\n\n\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary contains the number of intrusive memories the participant recorded in the week after watching the film.\nDay_Zero_Image_Based_Intrusions_in_Intrusion_Diary is very similar - it contains the number of intrusive memories the participant recorded in the time prior to watching the film. This is a good measure to include so we can rule out differences in the baseline number of intrusive memories between our groups.\n\n\n\n\n\n\n\n\n\n Data Skills - what information do we have about participant‚Äôs performance when playing tetris?\n\n\n\n\n\nTetris_Total_Score contains the cumulative score of all tetris games the participant played - participants were allowed to restart their game if they finished before the time was up.\nWe also have Self_Rated_Tetris_Performance which measures how participants rated the difficulty of the tetris task and Tetris_Demand_Rating which measures how participants thought that playing tetris would affect their intrusive memories.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#is-participant-performance-on-tetris-normally-distributed",
    "href": "tutorials/rmb_week-7_non-parametrics.html#is-participant-performance-on-tetris-normally-distributed",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Let‚Äôs start by taking a look at how participants in the conditions that played Tetris scored during their game time. Compute descriptive statistics on the Tetris_Total_Score variable and take a moment to get a sense of what it is telling you.\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the descriptive statistics of Tetris_Total_Score to answer the following questions in the spaces provided.\nThe average Tetris score was .\nThe most successful Tetris player scored  points.\nThe Tetris scores are positively skewednot skewednegatively skewed with a skewness of .\nThe Shapiro-Wilk statistic has a value of W =  and the p-value indicates that the scores are significantly non-normally distributednormally distributed\nWe should use parametricnon-parametric statistical approaches when analysing Tetris_Total_Score.\n\n\n\n\n\n\n\n\nLet‚Äôs use this data to test a hypothesis.\n\nPerformance whilst playing Tetris will be different at different times of day.\n\nWe can test this with an independent samples t-test as we‚Äôve done in previous weeks. Set up the t-test and include the following:\n\nWelchs‚Äô t-test\nMann-Whitney U\nEffect Size\nHomogeneity test\nNormality test\n\n\nLet‚Äôs unpack the results.\n\n\n\n\n\n\n Data Skills - does the Normality assumption check confirm what we saw in the descriptives?\n\n\n\n\n\nYes, we get the same picture but with a little less detail. Shapiro-Wilk‚Äôs test is significant indicating a non-normal distribution, but we still need the descriptive stats to understand HOW the distribution is different from a normal distribution.\nImportantly, this confirms that we should report the non-parametric alternative test - the Mann-Whitney U.\nYou could add the descriptives to the t-test page - but notice that this only includes a handful of metrics that are specifically related to t-tests (mean, standard deviation etc). We need the full list on the ‚ÄòDescriptives‚Äô tab to investigate distributions in detail.\n\n\n\n\n\n\n\n\n\n Data Skills - what does Levene‚Äôs test indicate and does this matter for us?\n\n\n\n\n\nLevene‚Äôs test explores homogeneity of variance, the result is not significant indicating that we are safe to assume homogeneity of variance between our groups.\nHowever, the data are not normally distributed - non-parametric tests don‚Äôt have an assumption of homogeneity of variance at all so, for this analysis, Levene‚Äôs test isn‚Äôt really relevant.\n\n\n\n\n\n\n\n\n\n Data Skills - what does the Mann-Whitney U test indicate about our hypothesis?\n\n\n\n\n\nThe Mann-Whitney U test is not significant - it seems like tetris scores are similar for participants who played in the morning and those who played in the afternoon.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#is-tetris-effective-at-reducing-intrusive-memories",
    "href": "tutorials/rmb_week-7_non-parametrics.html#is-tetris-effective-at-reducing-intrusive-memories",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "On to our main hypothesis. (James et al. 2015) were most interested in the combination of the reactivation task and the tetris intervention and their effect on intrusive memories about the film. Let‚Äôs focus on only the two conditions we can compare with a t-test.\n\n\n\n\n\n\nKey step\n\n\n\nSet up a Jamovi data filter to focus our analysis on only the two conditions ‚ÄúNo-Task Control‚Äù and ‚ÄúReactivation+Tetris‚Äù conditions. Take a look at week 3 section 4 to revise data filters if you need to.\nYou can use this code to select both conditions together:\nCondition == \"No-Task Control\" or Condition == \"Reactivation+Tetris\"\nFinally, compute an independent samples t-test to test the following:\n\nPerforming a Reactivation and Tetris task changes the number of intrusive memories in the week after watching a traumatic film.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThere are  participants in each group.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p &lt; \nLevene‚Äôs test for homogeneity of variance indicated that we cancan not assume homogeneity of variance between the No-Task and Reactivation+Tetris groups, F(1, 32) = , p = .\nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in the week after the film between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#control-analyses-testing-for-potential-confounds.",
    "href": "tutorials/rmb_week-7_non-parametrics.html#control-analyses-testing-for-potential-confounds.",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Our hypothesis test gives statistical support to the idea that playing tetris reduces intrusive memories - but could there be other explanations? This experiment is likely not perfect and it is possible that some bad luck in the data collection could have impacted the results. In this section we‚Äôll validate a few additional factors in the experimenta..\n\n\n(James et al. 2015) collected data variable to act as a control condition and rule out one possible confound in the experiment. Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary indicates the number of intrusive memories each participant had in the time before the experiment.\nThere isn‚Äôt a huge number of participants in each condition, it is possible that one group of participants was having more or less intrusive memories to begin with - irrespective of the experimental procedure.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a t-test to test the following:\n\nThere are no differences in the number of instrusive memories between the two experimental group BEFORE the experiment started.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThe participant in the ‚ÄòNo-Task Control‚Äô group had mean of  intrusive memories prior to the experiment starting.\nThe participant in the ‚ÄòReactivation+Tetris‚Äô group had mean of  intrusive memories prior to the experiment starting.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p = \nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in day before between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .\n\n\n\n\n\n\n\n\n\n\n\nAnother important factor to check was that the film actually changed how the participants felt. If the film didn‚Äôt do anything to impact the participants emotions then it is possible that we don‚Äôt actually understand the source of the difference we saw in the main hypothesis.\nThe film is the main experimental manipulation that so it is critical to check that it worked as we expected.\n\n\n\n\n\n\nKey step\n\n\n\nCompute paired samples t-test‚Äôs to check the following:\n\nWatching the traumatic film will change participants emotional state.\n\nCheck ALL of participants reported sadness, hopelessness, fear, horror and anxiousness.\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nTrue or False? The data variables for these factors are Normally Distributed according to Shaprio-Wilk statistics.\n\nSadness TRUEFALSE\nHopeless TRUEFALSE\nFear TRUEFALSE\nHorror TRUEFALSE\nAxious TRUEFALSE\n\nHow did the following factors change or not after watching the film?\n\nSadness increasedstayed the samedecreased after watching the film\nHopelessless increasedstayed the samedecreased after watching the film\nFear increasedstayed the samedecreased after watching the film\nHorror increasedstayed the samedecreased after watching the film\nAxiousness increasedstayed the samedecreased after watching the film\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - did the film manipulation work?\n\n\n\n\n\nYes, all factors apart from Hopelessless showed a statistically significant increase after watching the film. This indicates that the film had a strongly impact on participants negative emotions. It seems like the experimental manipulation worked.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#transforming-non-nomrally-distributed-data",
    "href": "tutorials/rmb_week-7_non-parametrics.html#transforming-non-nomrally-distributed-data",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "There are times when we might not want to use a non-parametric test, for example if we specifically want to interpret the mean of the data rather than the median or if we‚Äôre running many similar tests and we don‚Äôt want to be switching back and forth between parametric and non-parametric tests.\nWe have another option when working with non-normal data. This is to apply a reversible transformation to the data observations in a variable to change the distribution to one that is closer to being normally distributed. When this process works, we are able to run the parametric test on the transformed data rather thans swapping to the non-parametric alternative.\nThis is valid when the transformation preserves the relationships in the dataset, that is it maintains the relative order of the observations. If data point A is larger than B before transformation then it should still be larger after the transformation. It is fine if the transformation changes the numbers themselves - we just have to be careful when making an interpretation.\nWe‚Äôll think about two data transformations in this section\n\n\n\n\n\n\n\nTransform\nExplanation\n\n\n\n\nLog Transform\nThe logarithm function, typically base 10 (log), is applied to each data point.\n\n\nBox-Cox Transform\nSimilar to the log-transform but with some parameters are tuned to work optimally for the specific dataset\n\n\n\nThe main difference is that the Box-Cox transform is more adaptive - whilst it the log-transform is applied in the same way to every dataset, the Box-Cox trasnform can be slightly different so that it works best for the data being analysed each time.\nWe can easily transform variables using Jamovi and have done something very similar when computing variables in previous weeks.\n\n\n\n\n\n\nData Skills - transforming a variable.\n\n\n\nYou can read about transforming variables in detail on the Jamovi documentation page on transforming variables. There are a few different steps but they are all straightforward.\nFirst, head to the ‚ÄòData‚Äô view and right click on the variable you want to transform, in this case we want Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary\n\nThis will open a dialogue box where you cna define the new variable. Give your new variable a descriptive name and make sure that the ‚ÄòSource variable‚Äô is the one that you expect. Note that you might see a ‚ÄòCircular definition‚Äô warning until you rename your variable.\n\nNext we need to specify the transformation that we want to use. Click into the ‚Äòusing transform‚Äô drop-down box and select ‚ÄúCreate New Transform‚Äù.\nHere you can define the transformation that you want to apply. You can define a single transform and apply it to multiple variables. Give the transformation a descriptive name and specify that you want the transfrom to apply the LOG10() function to the $source variable. ($source is a way to tell Jamovi to use ‚Äòwhichever variable is passed into the transform‚Äô rather than defining the transform on only 1 variable here.)\n\nYou can click on the Fx logo to select functions from a list if you prefer, there are lots of pre-defined options.\n\nOnce you‚Äôre happy with the transformation, you can close the transformation definition dialogue and specify that we want to use the ‚ÄòLogTransform‚Äô that we just defined to transfrom Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary to make our new variable.\n\nThere are a lot of steps here - but each is fairly straightforward. Ask your tutor if you get stuck.\n\n\nNow that we have our new variable we can explore what it has done. Let‚Äôs compare the descriptive statistics for the original and the transformed variable.\n\n \n\n\n\n\n\n\n Data Skills - has the log transform changed mean of the data?\n\n\n\n\n\nYes, the log transform has changed the mean by a long way. The original data shows that the average participant had 3.5 intrusive memories but that number has changed to 1.28 in the transformed data.\nThis does not mean that the number has really reduced! remember that the transformation is changing the absolute values of the observations but should be preserving the relative relationships between the data points.\nThis is fine, but we must not report the log-transformed values as the actual means. The participants still had an average of 3.5 intrusive memories whether we transformed the data or not.\n\n\n\n\n\n\n\n\n\n Data Skills - has the log transform changed the skewness and kurtosis of the data?\n\n\n\n\n\nYes, it has had a big impact. Both the skewness and kurtosis values are much closer to zero in the transformed data compared to the original. Both indicates that the transformed data are more normally distributed.\n\n\n\n\n\n\n\n\n\n Data Skills - is there a statistically significant difference from a normal distribution in the transformed data?\n\n\n\n\n\nNo, whereas the Shapiro-Wilk statistic showed a significant departure from a normal distribution for the original data (W = 0.8, p &lt; 0.001) it shows that the transformed data is now indistinguishable from a normal distribution (W = 0.963, p = 0.257)\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCompute a new data transform that applies a Box-Cox transform to Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary.\n\n\nFollow the instructions above, the Box-Cox transform is one of the predefined transformation options.\n\n\n\nYour transformed variable should have the following descriptive statistics. See Days_One_to_Seven_BoxCox below.\n\n\n\n\n\n\nFinally - let‚Äôs see if we can use our transformed variables to compute a parametric t-test. Repeat the independent samples t-test from earlier but include all three variables in the ‚ÄòDependent Variables‚Äô box. Make sure to include\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary - original data\nDays_One_to_Seven_Transformed - Log transformed data\nDays_One_to_Seven_BoxCox - BoxCox transformed data\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nWith the original data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . We can proceed with Student‚Äôs t-testMann-Whitney U test.\nUsing the log-transformed data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . Levene‚Äôs test found that the we cancan not assume that the groups have homogeneity of variance, F(1, 34) = , p = . We can proceed with Student‚Äôs t-testWelch‚Äôs t-testMann-Whitney U test.\nUsing the BoxCox-transformed data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . Levene‚Äôs test found that the we cancan not assume that the groups have homogeneity of variance, F(1, 34) = , p = . We can proceed with Student‚Äôs t-testWelch‚Äôs t-testMann-Whitney U test.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#summary",
    "href": "tutorials/rmb_week-7_non-parametrics.html#summary",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "We have explored an interesting dataset in some detail and worked with serveral different analysis approaches. Specifically, we have several options when working on checking the assumptions of our analyses. We have to be careful when computing and reporting hypothesis tests that we‚Äôre using the correct tests in a way that is supported by the data. Sometimes we can use a transform to continuous using parametric tests when the assumptions are not supported but this will not always work.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis is a book created from markdown and executable code.\nSee (knuth84?) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methods Computer Practicals",
    "section": "",
    "text": "Week 1 : Descriptive Statistics, Data Processing and R\n\ntoc: true toc-depth: 2"
  },
  {
    "objectID": "index.html#research-methods-b",
    "href": "index.html#research-methods-b",
    "title": "Research Methods Computer Practicals",
    "section": "",
    "text": "Week 1 : Descriptive Statistics, Data Processing and R\n\ntoc: true toc-depth: 2"
  }
]