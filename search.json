[
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "copilot.html",
    "href": "copilot.html",
    "title": "Using Copilot at University of Birmingham",
    "section": "",
    "text": "Microsoft Copilot is now available to use for University of Birmingham members. See the University documentation pages to get access\nYou are welcome to use Copilot to support your learning of statistics and coding throughout the Research Methods B computer practical sessions. In particular, Copilot can be a useful way to get a second perspective on different technical concepts, to help generate data analysis code or to understand where data analysis code might have gone wrong.\nThroughout the course - we will suggest places where you might benefit from interacting with Copilot to support your learning. Look out for dialogue boxes like this one.\n\n\n\n\n\n\nCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about variables in R. We suggest you try the following prompts to get started.\n\n“Write a simple description of variables in R for a beginner”\n“How can I choose a name for my variable in R”\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\n\n\n\n\n\n\nEthical use of Copilot\n\n\n\nTools like Copilot can be a valuable learning aid, however it is critical that you familiarise yourself with the University’s student and PGR guidance on using GenAI tools ethically for work. This guidance provides you with the risks and limitations of using Generative AI tools in your academic study\nIt is not permitted to use Generative AI tools in assessment unless explicitly stated on the assessment page. It is your responsibility to use these tools in an appropriate way.\n\n\n\n\nHere are some questions with Copilot’s answers to help you decide how and when to use Copilot. Note that Copilot may respond differently if you ask identical prompts.\n\n\n\nStarting to learn R with the help of Copilot can be a smooth and enjoyable process. Here are some steps and tips to get you started:\n\nBegin by asking Copilot to explain fundamental concepts in R. This will give you a solid foundation to build on. - Example Questions: - “What are the basic data types in R?” - “How do I create a vector in R?”\n\nRequest detailed guides for common tasks. This helps you understand the process and logic behind the code. - Example Questions: - “Can you show me how to read a CSV file into R?” - “How do I create a simple plot using ggplot2?”\n\nReinforce your learning by practicing with beginner exercises. This will help you apply what you’ve learned. - Example Questions: - “Can you give me a beginner exercise for data manipulation in R?” - “What are some basic exercises to practice using loops in R?”\n\nLearn about important R packages that can simplify your work. Copilot can introduce you to these packages and show you how to use them. - Example Questions: - “What is the dplyr package and how do I use it?” - “Can you explain how to use the tidyverse package for data analysis?”\n\nWhen you encounter errors, ask Copilot for help in understanding and fixing them. This will improve your debugging skills. - Example Questions: - “I’m getting an error with my R code, can you help me debug it?” - “What does this error message mean in R?”\n\nAsk about best practices for writing clean and efficient R code. This will help you develop good &gt; coding habits. - Example Questions: - “What are some best practices for writing R code?” - “How can I make my R scripts more efficient?”\n\nSeeing examples can be very helpful. Ask for code snippets to understand how to implement different &gt; functions and methods. - Example Questions: - “Can you show me an example of how to use the print function in R?” - “How do I perform a linear regression in R?”\n\nDon’t hesitate to ask about anything you’re curious about, even if it seems basic. Copilot is here to help you learn at your own pace. - Example Questions: - “What are some good resources for learning R?” - “How do I create a histogram in R?”\n\n\n“How do I extract specific rows from a data frame in R?”\n“What are some common data visualization techniques in R?”\n\nBy leveraging Copilot’s capabilities, you can get personalized assistance and resources tailored to your learning needs. Remember, learning a new language takes time and practice, so be patient with yourself and keep experimenting. Happy coding! If you have any specific questions or need further guidance, feel free to ask!\n\n\n\n\n\nCode generated by Copilot can be a great starting point, but it’s important to review and test it thoroughly. Here are a few tips to ensure the code meets your needs:\n\nReview for Accuracy: Check the logic and syntax to make sure it aligns with your requirements.\nTest Thoroughly: Run tests to ensure the code works as expected in different scenarios.\nSecurity Considerations: Be mindful of potential security vulnerabilities and address them as needed.\nCustomization: Adapt and refine the code to better fit your specific use case.\n\nUsing these practices can help you make the most of Copilot’s assistance while ensuring the quality and reliability of your code. If you have any specific coding questions or need help with a particular piece of code, feel free to ask! 😊",
    "crumbs": [
      "Home",
      "Support",
      "Using Copilot at UoB"
    ]
  },
  {
    "objectID": "copilot.html#copilot-giving-advice-about-how-to-use-copilot",
    "href": "copilot.html#copilot-giving-advice-about-how-to-use-copilot",
    "title": "Using Copilot at University of Birmingham",
    "section": "",
    "text": "Here are some questions with Copilot’s answers to help you decide how and when to use Copilot. Note that Copilot may respond differently if you ask identical prompts.\n\n\n\nStarting to learn R with the help of Copilot can be a smooth and enjoyable process. Here are some steps and tips to get you started:\n\nBegin by asking Copilot to explain fundamental concepts in R. This will give you a solid foundation to build on. - Example Questions: - “What are the basic data types in R?” - “How do I create a vector in R?”\n\nRequest detailed guides for common tasks. This helps you understand the process and logic behind the code. - Example Questions: - “Can you show me how to read a CSV file into R?” - “How do I create a simple plot using ggplot2?”\n\nReinforce your learning by practicing with beginner exercises. This will help you apply what you’ve learned. - Example Questions: - “Can you give me a beginner exercise for data manipulation in R?” - “What are some basic exercises to practice using loops in R?”\n\nLearn about important R packages that can simplify your work. Copilot can introduce you to these packages and show you how to use them. - Example Questions: - “What is the dplyr package and how do I use it?” - “Can you explain how to use the tidyverse package for data analysis?”\n\nWhen you encounter errors, ask Copilot for help in understanding and fixing them. This will improve your debugging skills. - Example Questions: - “I’m getting an error with my R code, can you help me debug it?” - “What does this error message mean in R?”\n\nAsk about best practices for writing clean and efficient R code. This will help you develop good &gt; coding habits. - Example Questions: - “What are some best practices for writing R code?” - “How can I make my R scripts more efficient?”\n\nSeeing examples can be very helpful. Ask for code snippets to understand how to implement different &gt; functions and methods. - Example Questions: - “Can you show me an example of how to use the print function in R?” - “How do I perform a linear regression in R?”\n\nDon’t hesitate to ask about anything you’re curious about, even if it seems basic. Copilot is here to help you learn at your own pace. - Example Questions: - “What are some good resources for learning R?” - “How do I create a histogram in R?”\n\n\n“How do I extract specific rows from a data frame in R?”\n“What are some common data visualization techniques in R?”\n\nBy leveraging Copilot’s capabilities, you can get personalized assistance and resources tailored to your learning needs. Remember, learning a new language takes time and practice, so be patient with yourself and keep experimenting. Happy coding! If you have any specific questions or need further guidance, feel free to ask!\n\n\n\n\n\nCode generated by Copilot can be a great starting point, but it’s important to review and test it thoroughly. Here are a few tips to ensure the code meets your needs:\n\nReview for Accuracy: Check the logic and syntax to make sure it aligns with your requirements.\nTest Thoroughly: Run tests to ensure the code works as expected in different scenarios.\nSecurity Considerations: Be mindful of potential security vulnerabilities and address them as needed.\nCustomization: Adapt and refine the code to better fit your specific use case.\n\nUsing these practices can help you make the most of Copilot’s assistance while ensuring the quality and reliability of your code. If you have any specific coding questions or need help with a particular piece of code, feel free to ask! 😊",
    "crumbs": [
      "Home",
      "Support",
      "Using Copilot at UoB"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "This week we will explore how to use Jamovi to compute and visualise descriptive statistics from a dataset. This will include some revision on Jamovi from last semester and an introduction on how we can use the R coding language to simplify complex data processing.\n\n\n\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nMeasures of central tendency (Mean and Median)\n\n\n\nMeasures of dispersion and spread (Standard Deviation and Interquartile Range)\n\n\n\nMeasures of skew and normality\n\n\n\nRecognising different types of distribution\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nExploring and cleaning datasets Jamovi\n\n\n\nComputing descriptive statistics in Jamovi\n\n\n\nComputing descriptive plots in Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with openly available research data\n\n\n\n\n\n\nWe will be exploring data that was collected as part of the ‘Many Labs 3’ project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‘Big Five’ personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs. extravagant/careless)\nAgreeableness (friendly/compassionate vs. critical/rational)\nNeuroticism (sensitive/nervous vs. resilient/confident)\nOpenness to experience (inventive/curious vs. consistent/cautious)\nExtraversion (outgoing/energetic vs. solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.\n\n\n\nWe have data from a sample of students in a big data table. This raw data needs to be summarised and visualised so that we can turn this massive block of numbers into something intuitive.\nWe can generate a concise overview of the main characteristics of the data by computing and visualising few key features from the data.\n\n\n\n\n\n\nOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.\n\n\n\n\n\n\n\n\n\n\n\n\nRevision from RM-A\n\n\n\nThis computer practical will have some revision from RM-A, please check your notes from RM-A for help with loading data and handling variables.\nIf you aren’t familiar with Jamovi, or would like a refresher - please take a quick look at the following topics in the Jamovi user guide\n\nFirst steps in Jamovi\nAnalyses\nThe spreadsheet\n\nPlease ask your tutor for help in the computer practical sessions if you need any assistance.\n\n\n\n\n\n\n\n\nKey step\n\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file ML3AllSites_RMB.csv from canvas.\nYou can import csv files directly into Jamovi. There is more information in the Jamovi Textbook\nIt’s easy to open csv files in Jamovi. From the top left menu (the button with three parallel lines) choose ‘Open’ and browse to where you have stored the csv file on your computer. If you’re on a Mac, it’ll look like the usual Finder window that you use to choose a file; on Windows it looks like an Explorer window. Find the one you want, then click on the ‘Open’ button.\n\n\n\nThis is the original dataset provided by the authors of the paper! It is a massive dataset - take a moment to get familiar with it by answering the following questions.\nSome of the questions will require you to run a descriptive statistics analysis on the age variable. Run what ever you need to explore the data and the questions.\n\n\n\n\n\n\n Data Skills - How many participants are included in the dataset?\n\n\n\n\n\nThere are 2402 participants in total - this is the number of rows in the dataset\n\n\n\n\n\n\n\n\n\n Data Skills - Is the dataset complete? do we have all information from everyone?\n\n\n\n\n\nNo, unfortunately some data is missing from some participants. You can see empty cells in quite a few places across the dataset. This means that some analyses may have an N of less than 2402 for analyses using some of these variables.\n\n\n\n\n\n\n\n\n\n Data Skills - Take a look at the configuration of the Age variable. Is it the correct type?\n\n\n\n\n\nJamovi has to make an educated guess at what type of data is included in each column. Looks like it has correctly set the age variable to be ‘Continuous’.\n\n\n\n\n\n\n\n\n\n Data Skills - How many participants do not have age information?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe N for the age descriptives is 2402 and there are 0 missing participants. This is a large sample and there is age information for everyone.\n\n\n\n\n\n\n\n\n\n\n Data Skills - What is the average and standard deviation of the age of the participants?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe mean age is 19.3 years, typical for a student population. The standard deviation is 3.81.\nThis means that a participant who is 19.3+3.81 = 23.11 years old would be one standard deviation above the average age of the sample.\n\n\n\n\n\n\n\n\n\n\n Data Skills - Do you think all the information in the age variable is accurate?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu. Try checking the box to show the 5 most extreme values in the dataset under the ‘Statistics -&gt; Outliers’ submenu.\nTake a close look at the variable and its descriptive statistics.\nThe youngest age in the dataset is -2 and the oldest age is 149! neither of these can be true and are probably typos (or jokes) made during data collection. Some other values are unusual but not impossible - the second youngest student is 13. We do not know what has happened during data collection to lead to this as this is not our dataset. We should be careful about interpreting the mean whilst these variables are included…\n\n\n\n\nThis process of exploring and understanding your data is a critical first step when working on a new analysis. Real data is rarely in perfect condition - we have to take some time and apply common sense to understand the data and what challenges there might be when analysing it.\nIf we do identify issues, such are large amounts of missing data, incorrect data types or mistakes in data entry, we can normally fix them - but only if they’ve been spotted! If we assume our data is perfect and go straight to the analysis then we could end up including a 149 year old student in our sample…\n\n\n\nIts probably not a good idea to include the data from the participants with impossible ages in our analysis. One simple step we can take is just to remove those rows from our analysis.\n\n\n\n\n\n\nData Skills - removing rows from a dataset\n\n\n\nRemove the two participants with impossible ages, you can use the extreme values table in the ‘Descriptives’ output to help find the row numbers. You can remove a row in Jamovi by right clicking on the row number (far left of the Data view) and clicking ‘Delete Row’.\n\nThis will not change the original data file (unless you export the data back out again..), it will only affect the view of the data loaded into Jamovi. If you make a mistake you can simply load the original file and start again.\nNotice that the bottom of the data viewer tells you how many rows have been deleted in that dataset.\n\n\n\n\n\n\n\n\nKey step\n\n\n\nNow you can recompute the descriptive statistics of your age variable and see what has changed.\n\n\n\nThe mean is pretty much the same - still 19.3 - but the standard deviation is much smaller now. It has reduced from 3.81 to 2.70. That is a close to 25% reduction in the estimate of the variability of this data - big difference. Including outliers like these can have a large impact on our analyses.\n\n\n\n\n\n\n Critical thinking - Are you happy with the data in the age variable now?\n\n\n\n\n\nTough question! take a look at the Minimum value - we still have one participant who is 13 years old. Whilst this is more likely than finding a -2 or 149 year old student, it is still extremely unlikely. It is harder to make a judgement for this participant than the others. The first two participants had impossible ages, whereas a 13 year old student is just very unlikely. Sometimes we have to make a judgement call.\nLet’s delete row 709 to remove this participant as well.\nThe recomputed descriptive statistics now has an N=2399. Removing the 13 year old hasn’t changed our mean or standard deviation estimates very much - indicating that it was less important to remove this data point than the first two.\nThe youngest participant is now 16 which is more realistic (though still very impressive for that 16 year old!).\n\n\n\nThis process can be intricate and involves a lot of clicking in Jamovi - it would be easy to make a mistake and not notice that something has gone wrong. We’ll explore some faster and more transparent ways to do this using R code during this course.\n\n\n\n\nOk, let’s move on to personality factors. As we said at the top, the big five factors are recorded on a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. So someone with an Extraversion score or 6 might be very outgoing where as someone with a score of 1 would be more of an introvert.\n\n\n\n\n\n\nKey step\n\n\n\nCompute the descriptive statistics for the big 5 personality factors.\nTake some time to consider the data quality - use the questions we answered with age as a guide for what to think about.\n\n\n\nNotice that we have a few missing data values in these variables. Jamovi will remove these from any analyses so they shouldn’t affect our descriptive statistics.\n\n\n\n\n\n\n Critical thinking - Are you happy to use the data in the big 5 personality factors?\n\n\n\n\n\nYes, these look reasonable. The data range matches what we would expect from the rating scales (values between 1 and 7) and there is only a small amount of missing data in each of the five variables.\nWe don’t need to do any further data cleaning based on these descriptive statistics.\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding 1\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nYou have reproduced the descriptive statistics for the big 5 personality factors. Now you can do the same for some of the other factors in Table 2 of section 3.5 in Ebersole et al. (2016).\nSpecifically - try and compute the mean and standard deviations for the following factors (Descriptions of the factors are the the cited section of Ebersole et al. (2016)):\n\nSelf Esteem (see section 3.5.1)\nDaily Mood (see section 3.5.3)\nNeed for cognition (see section 3.5.5)\n\nCan you find variables (columns) that correspond to these factors?\nNote: Open datasets sometimes use shorthand for variable names rather than the names used in the manuscript. This is not ideal but does happen in practice! we should be ready take a little time to explore and understand a new dataset when working with it.\n\n\nIn this case the variables in the dataset to use are:\n\n\n\nIn Manuscript\nIn Jamovi\n\n\n\n\nSelf Esteem\n‘SelfEsteem’\n\n\nDaily Mood.\n‘Mood’\n\n\nNeed for cognition\n‘NFC’\n\n\n\nTalk to your tutor if you’re getting stuck.\n\n\nYou should be able to reproduce the following table. Verify that the computed Mean and Standard Deviations match those published in table 2 of Ebersole et al (2016)\n\nThe final mean values are very close to the values in Ebersole et al. (2016) - but not identical. Looks like the original paper kept in all the data that had missing or unusual age values.\n\n\n\n\n\nNext, let’s think about the distributions of these variables. Its great to think about summarising a dataset with its mean value, but this doesn’t tell us about the wider distribution.\n\n\n\n\nNext we’ll have a look at computing histograms from this data. Remember that histograms divide the data range into intervals called bins. The choice of bin width and number of bins can affect the appearance and interpretation of the histogram. Most of the time we can use the automatic bin definitions provided by Jamovi.\nWe can use Histograms to explore a few key factors of the wider data distributions that are hard to see from the descriptive statistics tables.\n\nShape: The shape of the histogram can reveal the distribution pattern of the data, such as normal distribution, skewness, or bimodality.\nOutliers: Histograms can help identify outliers or unusual data points that fall outside the general pattern.\n\nJamovi makes it very simple to compute a histogram. Open the ‘Plots’ sub-menu of the descriptive statistics tab and select the ‘Histogram’ checkbox.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a histogram of the participant ages\n\n\n\nThe descriptives tables give us a numerical summary of the data - but this can be hard to build an intuitive understanding of the dataset. A histogram gives us something immediately interpretable on a more ‘human’ level. Have a think about the following questions.\n\n\n\n\n\n\n Data Skills - What is the average age of the pariticpants in this dataset?\n\n\n\n\n\nWe can see that the most common ages are clustered around the late teenage years and early 20s. We can’t tell the exact mean from the histogram alone - we need to compute the descriptives table for this - but histograms are good to get a quick ‘sense’ of the data.\n\n\n\nIf you haven’t heard about the ‘normal distribution’ in your previous studies, this is a good opportunity to ask Copilot to give you a simple explanation.\n\n\n\n\n\n\n Data Skills - Do the ages look normally distributed?\n\n\n\n\n\nThere is a long tail to this distribution with a very small number of participants in their 30s, 40s and 50s. This is a strong positive skew and likely indicates that the data are not normally distributed.\nThis would be hard to tell from the descriptives table alone.\nOverall, this is a good match for what we would expect from a student population at a large university.\n\n\n\nWe can get an alternative perspective by visualising our data using a ‘Box Plot’.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a Box plot of the participant ages\n\n\n\nThe boxplot visualises the median and interquartile range (25th to 75th percentile) of the data, as well as highlighting outliers. This gives us less resolution around the middle of the distribution close to the median - but does give a much clearer view of the long tail in the ages of the student data sample.\n\n\n\n\n\n\n Data Skills - How old is the oldest participant in the dataset??\n\n\n\n\n\nThe most extreme outliers in the dataset are labelled with their row ID in the boxplot. The largest age value is in row 1081 - it looks like this is around 55 in the boxplot.\nWe can check the corresponding row in the data table to get some more precision. This tells us that the oldest participant is exactly 54 years old.\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding 2\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nCompute histograms and/or boxplots for the big-5 personality factors and take some notes their distributions. Think about the following questions for each factor:\n\nIs the distribution symmetric?\nIf not, is there a positive or a negative skew?\nAre there any outliers?\n\nWrite some quick notes on your answers - we’ll use these again in the next section\n\n\nIt might be easiest to do this one factor at a time to avoid being swamped with figures!\nCreate the figure for a single factor, make some notes and make the next figure with the next factor. You can consider saving the figure into a notebook or document if you want to make a more controlled comparison.\nIf you aren’t sure about any of the concepts - this is a good opportunity to ask your tutors in the session - or use Copilot to give you a simple explanation.\n\n\nHere are my notes about each distribution:\n\nOpenness, this does not look like a normal distribution - there is a negative skew in the data.\nConscientiousness, this is definitely not a normal distribution - there is a strong negative skew.\nExtraversion, this is close to being a normal distribution - perhaps a very slight negative skew.\nAgreeableness, this does not look like a normal distribution - there is a negative skew in the data.\nNeuroticism, this does not look like a normal distribution - there is a positive skew in the data.\n\nNone of the factors have obvious outliers.\n\n\n\n\n\n\n\n\nAs a final step, we can see if our intuitions about the distribution of big-5 personality factors match with more formal statistics measures checking for normal distributions and skewness.\n\n\n\n\n\n\nKey step\n\n\n\nCompute statistics Shapero-Wilk’s test for normality and for ‘Skewness’. You can add both of these tests from the ‘Statistics’ sub-menu of the Descriptives analysis.\n\n\n\n\n\nThe Shapiro-Wilk test evaluates whether a sample comes from a normally distributed population. It returns 2 values:\n\n‘W’ The W statistic measures how well the data conforms to a normal distribution. It ranges from 0 to 1 where values close to 1 indicate the data is normally distributed.\n‘p’ is a p-value indicating the probability that we could see a value of W at least as large as the one we have measured if the data really were normally distributed. Typically, we would say that the data are not normally distributed if p is less than or equal to 0.05.\n\n\n\n\n\n\n\n Data Skills - Do our notes from the histograms match the Shapiro-Wilk statistics?\n\n\n\n\n\nYes, If we look at the W statistics, then we see that Extraversion has the highest value indicating that it has the most ‘normal’ data distribution whilst the other four factors are similar. This matches what we saw with the histograms.\nThe p-values from the formal hypothesis test indicate that none of the 5 factors have normal distributions. We couldn’t tell this from our informal evaulation of the histograms and box plots.\n\n\n\n\n\n\nThe skewness metric returns a number close to zero if the data are symmetrical around zero, and a number close to +1 or -1 if the data are skewed. The sign of the skewness value indicates whether there is a positive or a negative skew.\n\n\n\n\n\n\n Data Skills - Do our notes from the histograms match the skewness statistic?\n\n\n\n\n\nYes, its very close. We can see that ‘Extraversion’ has the least skew with a Skewness metric very close to zero (-0.0714). In contrast, ‘Conscientiousness’ has the most skew. Its value of ‘-0.656’ indicates a very substantial negative skewness in the distribution.\nOpenness and Agreeableness have moderate negative skews, whilst Neuroticism has a moderate positive skew.\n\n\n\n\n\n\n\nIn this session we have gone through the process of exploring a dataset from a published scientific article. We have computed a range of descriptive statistics and used histograms and boxplots to visualise the dataset.\nIt is common for publicly available data to contain little mistakes and issues. We need to go through a process like this with every new dataset and it should be like second nature by the end of this module. Remember to use common sense and practical thinking to identify issues and think about possible solutions.\nRemember this process when collecting your own data. Think carefully about how you can avoid typos and mistakes when collecting a dataset. It can save you and your collaborators a lot of time in future!\n\n\n\n\n\nBlock, Jack. 1995. “A Contrarian View of the Five-Factor Approach to Personality Description.” Psychological Bulletin 117 (2): 187–215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. “Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.” Journal of Experimental Social Psychology 67: 68–82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. “A Very Brief Measure of the Big-Five Personality Domains.” Journal of Research in Personality 37 (6): 504–28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#the-dataset",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#the-dataset",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "We will be exploring data that was collected as part of the ‘Many Labs 3’ project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‘Big Five’ personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs. extravagant/careless)\nAgreeableness (friendly/compassionate vs. critical/rational)\nNeuroticism (sensitive/nervous vs. resilient/confident)\nOpenness to experience (inventive/curious vs. consistent/cautious)\nExtraversion (outgoing/energetic vs. solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#the-challenge",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#the-challenge",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "We have data from a sample of students in a big data table. This raw data needs to be summarised and visualised so that we can turn this massive block of numbers into something intuitive.\nWe can generate a concise overview of the main characteristics of the data by computing and visualising few key features from the data.\n\n\n\n\n\n\nOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#exploring-a-new-dataset-in-jamovi",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#exploring-a-new-dataset-in-jamovi",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Revision from RM-A\n\n\n\nThis computer practical will have some revision from RM-A, please check your notes from RM-A for help with loading data and handling variables.\nIf you aren’t familiar with Jamovi, or would like a refresher - please take a quick look at the following topics in the Jamovi user guide\n\nFirst steps in Jamovi\nAnalyses\nThe spreadsheet\n\nPlease ask your tutor for help in the computer practical sessions if you need any assistance.\n\n\n\n\n\n\n\n\nKey step\n\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file ML3AllSites_RMB.csv from canvas.\nYou can import csv files directly into Jamovi. There is more information in the Jamovi Textbook\nIt’s easy to open csv files in Jamovi. From the top left menu (the button with three parallel lines) choose ‘Open’ and browse to where you have stored the csv file on your computer. If you’re on a Mac, it’ll look like the usual Finder window that you use to choose a file; on Windows it looks like an Explorer window. Find the one you want, then click on the ‘Open’ button.\n\n\n\nThis is the original dataset provided by the authors of the paper! It is a massive dataset - take a moment to get familiar with it by answering the following questions.\nSome of the questions will require you to run a descriptive statistics analysis on the age variable. Run what ever you need to explore the data and the questions.\n\n\n\n\n\n\n Data Skills - How many participants are included in the dataset?\n\n\n\n\n\nThere are 2402 participants in total - this is the number of rows in the dataset\n\n\n\n\n\n\n\n\n\n Data Skills - Is the dataset complete? do we have all information from everyone?\n\n\n\n\n\nNo, unfortunately some data is missing from some participants. You can see empty cells in quite a few places across the dataset. This means that some analyses may have an N of less than 2402 for analyses using some of these variables.\n\n\n\n\n\n\n\n\n\n Data Skills - Take a look at the configuration of the Age variable. Is it the correct type?\n\n\n\n\n\nJamovi has to make an educated guess at what type of data is included in each column. Looks like it has correctly set the age variable to be ‘Continuous’.\n\n\n\n\n\n\n\n\n\n Data Skills - How many participants do not have age information?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe N for the age descriptives is 2402 and there are 0 missing participants. This is a large sample and there is age information for everyone.\n\n\n\n\n\n\n\n\n\n\n Data Skills - What is the average and standard deviation of the age of the participants?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe mean age is 19.3 years, typical for a student population. The standard deviation is 3.81.\nThis means that a participant who is 19.3+3.81 = 23.11 years old would be one standard deviation above the average age of the sample.\n\n\n\n\n\n\n\n\n\n\n Data Skills - Do you think all the information in the age variable is accurate?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu. Try checking the box to show the 5 most extreme values in the dataset under the ‘Statistics -&gt; Outliers’ submenu.\nTake a close look at the variable and its descriptive statistics.\nThe youngest age in the dataset is -2 and the oldest age is 149! neither of these can be true and are probably typos (or jokes) made during data collection. Some other values are unusual but not impossible - the second youngest student is 13. We do not know what has happened during data collection to lead to this as this is not our dataset. We should be careful about interpreting the mean whilst these variables are included…\n\n\n\n\nThis process of exploring and understanding your data is a critical first step when working on a new analysis. Real data is rarely in perfect condition - we have to take some time and apply common sense to understand the data and what challenges there might be when analysing it.\nIf we do identify issues, such are large amounts of missing data, incorrect data types or mistakes in data entry, we can normally fix them - but only if they’ve been spotted! If we assume our data is perfect and go straight to the analysis then we could end up including a 149 year old student in our sample…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#data-cleaning",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#data-cleaning",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Its probably not a good idea to include the data from the participants with impossible ages in our analysis. One simple step we can take is just to remove those rows from our analysis.\n\n\n\n\n\n\nData Skills - removing rows from a dataset\n\n\n\nRemove the two participants with impossible ages, you can use the extreme values table in the ‘Descriptives’ output to help find the row numbers. You can remove a row in Jamovi by right clicking on the row number (far left of the Data view) and clicking ‘Delete Row’.\n\nThis will not change the original data file (unless you export the data back out again..), it will only affect the view of the data loaded into Jamovi. If you make a mistake you can simply load the original file and start again.\nNotice that the bottom of the data viewer tells you how many rows have been deleted in that dataset.\n\n\n\n\n\n\n\n\nKey step\n\n\n\nNow you can recompute the descriptive statistics of your age variable and see what has changed.\n\n\n\nThe mean is pretty much the same - still 19.3 - but the standard deviation is much smaller now. It has reduced from 3.81 to 2.70. That is a close to 25% reduction in the estimate of the variability of this data - big difference. Including outliers like these can have a large impact on our analyses.\n\n\n\n\n\n\n Critical thinking - Are you happy with the data in the age variable now?\n\n\n\n\n\nTough question! take a look at the Minimum value - we still have one participant who is 13 years old. Whilst this is more likely than finding a -2 or 149 year old student, it is still extremely unlikely. It is harder to make a judgement for this participant than the others. The first two participants had impossible ages, whereas a 13 year old student is just very unlikely. Sometimes we have to make a judgement call.\nLet’s delete row 709 to remove this participant as well.\nThe recomputed descriptive statistics now has an N=2399. Removing the 13 year old hasn’t changed our mean or standard deviation estimates very much - indicating that it was less important to remove this data point than the first two.\nThe youngest participant is now 16 which is more realistic (though still very impressive for that 16 year old!).\n\n\n\nThis process can be intricate and involves a lot of clicking in Jamovi - it would be easy to make a mistake and not notice that something has gone wrong. We’ll explore some faster and more transparent ways to do this using R code during this course.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#descriptives-from-big-5-personality-factors",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#descriptives-from-big-5-personality-factors",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Ok, let’s move on to personality factors. As we said at the top, the big five factors are recorded on a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. So someone with an Extraversion score or 6 might be very outgoing where as someone with a score of 1 would be more of an introvert.\n\n\n\n\n\n\nKey step\n\n\n\nCompute the descriptive statistics for the big 5 personality factors.\nTake some time to consider the data quality - use the questions we answered with age as a guide for what to think about.\n\n\n\nNotice that we have a few missing data values in these variables. Jamovi will remove these from any analyses so they shouldn’t affect our descriptive statistics.\n\n\n\n\n\n\n Critical thinking - Are you happy to use the data in the big 5 personality factors?\n\n\n\n\n\nYes, these look reasonable. The data range matches what we would expect from the rating scales (values between 1 and 7) and there is only a small amount of missing data in each of the five variables.\nWe don’t need to do any further data cleaning based on these descriptive statistics.\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding 1\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nYou have reproduced the descriptive statistics for the big 5 personality factors. Now you can do the same for some of the other factors in Table 2 of section 3.5 in Ebersole et al. (2016).\nSpecifically - try and compute the mean and standard deviations for the following factors (Descriptions of the factors are the the cited section of Ebersole et al. (2016)):\n\nSelf Esteem (see section 3.5.1)\nDaily Mood (see section 3.5.3)\nNeed for cognition (see section 3.5.5)\n\nCan you find variables (columns) that correspond to these factors?\nNote: Open datasets sometimes use shorthand for variable names rather than the names used in the manuscript. This is not ideal but does happen in practice! we should be ready take a little time to explore and understand a new dataset when working with it.\n\n\nIn this case the variables in the dataset to use are:\n\n\n\nIn Manuscript\nIn Jamovi\n\n\n\n\nSelf Esteem\n‘SelfEsteem’\n\n\nDaily Mood.\n‘Mood’\n\n\nNeed for cognition\n‘NFC’\n\n\n\nTalk to your tutor if you’re getting stuck.\n\n\nYou should be able to reproduce the following table. Verify that the computed Mean and Standard Deviations match those published in table 2 of Ebersole et al (2016)\n\nThe final mean values are very close to the values in Ebersole et al. (2016) - but not identical. Looks like the original paper kept in all the data that had missing or unusual age values.\n\n\n\n\n\nNext, let’s think about the distributions of these variables. Its great to think about summarising a dataset with its mean value, but this doesn’t tell us about the wider distribution.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#visualising-distributions",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#visualising-distributions",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Next we’ll have a look at computing histograms from this data. Remember that histograms divide the data range into intervals called bins. The choice of bin width and number of bins can affect the appearance and interpretation of the histogram. Most of the time we can use the automatic bin definitions provided by Jamovi.\nWe can use Histograms to explore a few key factors of the wider data distributions that are hard to see from the descriptive statistics tables.\n\nShape: The shape of the histogram can reveal the distribution pattern of the data, such as normal distribution, skewness, or bimodality.\nOutliers: Histograms can help identify outliers or unusual data points that fall outside the general pattern.\n\nJamovi makes it very simple to compute a histogram. Open the ‘Plots’ sub-menu of the descriptive statistics tab and select the ‘Histogram’ checkbox.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a histogram of the participant ages\n\n\n\nThe descriptives tables give us a numerical summary of the data - but this can be hard to build an intuitive understanding of the dataset. A histogram gives us something immediately interpretable on a more ‘human’ level. Have a think about the following questions.\n\n\n\n\n\n\n Data Skills - What is the average age of the pariticpants in this dataset?\n\n\n\n\n\nWe can see that the most common ages are clustered around the late teenage years and early 20s. We can’t tell the exact mean from the histogram alone - we need to compute the descriptives table for this - but histograms are good to get a quick ‘sense’ of the data.\n\n\n\nIf you haven’t heard about the ‘normal distribution’ in your previous studies, this is a good opportunity to ask Copilot to give you a simple explanation.\n\n\n\n\n\n\n Data Skills - Do the ages look normally distributed?\n\n\n\n\n\nThere is a long tail to this distribution with a very small number of participants in their 30s, 40s and 50s. This is a strong positive skew and likely indicates that the data are not normally distributed.\nThis would be hard to tell from the descriptives table alone.\nOverall, this is a good match for what we would expect from a student population at a large university.\n\n\n\nWe can get an alternative perspective by visualising our data using a ‘Box Plot’.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a Box plot of the participant ages\n\n\n\nThe boxplot visualises the median and interquartile range (25th to 75th percentile) of the data, as well as highlighting outliers. This gives us less resolution around the middle of the distribution close to the median - but does give a much clearer view of the long tail in the ages of the student data sample.\n\n\n\n\n\n\n Data Skills - How old is the oldest participant in the dataset??\n\n\n\n\n\nThe most extreme outliers in the dataset are labelled with their row ID in the boxplot. The largest age value is in row 1081 - it looks like this is around 55 in the boxplot.\nWe can check the corresponding row in the data table to get some more precision. This tells us that the oldest participant is exactly 54 years old.\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding 2\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nCompute histograms and/or boxplots for the big-5 personality factors and take some notes their distributions. Think about the following questions for each factor:\n\nIs the distribution symmetric?\nIf not, is there a positive or a negative skew?\nAre there any outliers?\n\nWrite some quick notes on your answers - we’ll use these again in the next section\n\n\nIt might be easiest to do this one factor at a time to avoid being swamped with figures!\nCreate the figure for a single factor, make some notes and make the next figure with the next factor. You can consider saving the figure into a notebook or document if you want to make a more controlled comparison.\nIf you aren’t sure about any of the concepts - this is a good opportunity to ask your tutors in the session - or use Copilot to give you a simple explanation.\n\n\nHere are my notes about each distribution:\n\nOpenness, this does not look like a normal distribution - there is a negative skew in the data.\nConscientiousness, this is definitely not a normal distribution - there is a strong negative skew.\nExtraversion, this is close to being a normal distribution - perhaps a very slight negative skew.\nAgreeableness, this does not look like a normal distribution - there is a negative skew in the data.\nNeuroticism, this does not look like a normal distribution - there is a positive skew in the data.\n\nNone of the factors have obvious outliers.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#computing-statistics-for-normality-and-skewness.",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#computing-statistics-for-normality-and-skewness.",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "As a final step, we can see if our intuitions about the distribution of big-5 personality factors match with more formal statistics measures checking for normal distributions and skewness.\n\n\n\n\n\n\nKey step\n\n\n\nCompute statistics Shapero-Wilk’s test for normality and for ‘Skewness’. You can add both of these tests from the ‘Statistics’ sub-menu of the Descriptives analysis.\n\n\n\n\n\nThe Shapiro-Wilk test evaluates whether a sample comes from a normally distributed population. It returns 2 values:\n\n‘W’ The W statistic measures how well the data conforms to a normal distribution. It ranges from 0 to 1 where values close to 1 indicate the data is normally distributed.\n‘p’ is a p-value indicating the probability that we could see a value of W at least as large as the one we have measured if the data really were normally distributed. Typically, we would say that the data are not normally distributed if p is less than or equal to 0.05.\n\n\n\n\n\n\n\n Data Skills - Do our notes from the histograms match the Shapiro-Wilk statistics?\n\n\n\n\n\nYes, If we look at the W statistics, then we see that Extraversion has the highest value indicating that it has the most ‘normal’ data distribution whilst the other four factors are similar. This matches what we saw with the histograms.\nThe p-values from the formal hypothesis test indicate that none of the 5 factors have normal distributions. We couldn’t tell this from our informal evaulation of the histograms and box plots.\n\n\n\n\n\n\nThe skewness metric returns a number close to zero if the data are symmetrical around zero, and a number close to +1 or -1 if the data are skewed. The sign of the skewness value indicates whether there is a positive or a negative skew.\n\n\n\n\n\n\n Data Skills - Do our notes from the histograms match the skewness statistic?\n\n\n\n\n\nYes, its very close. We can see that ‘Extraversion’ has the least skew with a Skewness metric very close to zero (-0.0714). In contrast, ‘Conscientiousness’ has the most skew. Its value of ‘-0.656’ indicates a very substantial negative skewness in the distribution.\nOpenness and Agreeableness have moderate negative skews, whilst Neuroticism has a moderate positive skew.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#summary",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#summary",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "In this session we have gone through the process of exploring a dataset from a published scientific article. We have computed a range of descriptive statistics and used histograms and boxplots to visualise the dataset.\nIt is common for publicly available data to contain little mistakes and issues. We need to go through a process like this with every new dataset and it should be like second nature by the end of this module. Remember to use common sense and practical thinking to identify issues and think about possible solutions.\nRemember this process when collecting your own data. Think carefully about how you can avoid typos and mistakes when collecting a dataset. It can save you and your collaborators a lot of time in future!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#references",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#references",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Block, Jack. 1995. “A Contrarian View of the Five-Factor Approach to Personality Description.” Psychological Bulletin 117 (2): 187–215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. “Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.” Journal of Experimental Social Psychology 67: 68–82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. “A Very Brief Measure of the Big-Five Personality Domains.” Journal of Research in Personality 37 (6): 504–28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "The sampling distribution is central to how we can move from t-values to p-values - yet it is one of the trickier parts of this course. The lectures and pre-lecture materials frequently deal with computer simulations to show the properties of the tests we’re using, so this week you will write your own simulation to help understand the sampling distribution!\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nSampling Distributions\n\n\n\nt-values\n\n\n\np-values\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nComputing a one sample t-test in R\n\n\n\nSimulating data in R\n\n\n\nWriting loops in R\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nCreating shareable code to demonstrate a statistical concept\n\n\n\n\n\nTLDR: There is no data to download! we’re going to make the data ourselves.\nThe dataset this week is very simple - we’re going to create it ourselves using R. At this point, we know a lot about the types of questions that we can ask with t-tests and we know a lot about the data processing around it. We can check our understanding of different aspects of these statistical tests by simulating data samples that have known properties (eg mean and standard deviation) and checking if the outcome of the t-test is what we would expect.\nThis way we can numerically validate the tests we’re running and take a look under the hood and see what sort of processes Jamovi and R are using to compute the tests for us.\n\n\n\n\n\n\nContent Info\n\n\n\nThere is a lot of code in this session - don’t be afraid to ask for help from your tutors!\n\n\n\n\n\nToday, we’ll first introduce some concepts and functions in R to simulate a data sample and compute our own one sample t-test. With that in hand, we will set up code to run thousands of tests in one go to help us to better understand sampling distributions.\n\n\n\nThe first thing we’ll need is to compute a t value for a one sample t-test given a dataset. We can work from this definition you’ll remember from the lectures.\n\\[\nt = \\frac{\\text{The sample mean} - \\text{Comparison Value}}{\\text{The standard error of the mean}}\n\\]\nand\n\\[\n\\text{The standard error of the mean} = \\frac{\\text{The sample standard deviation}}{\\sqrt{n}}\n\\]\nLet’s think about what we’ll need to do to compute our t-test. Reading through the definitions, we’ll need some basic arithmatic (+, - and /) need to use the following R functions that we have worked with in previous weeks.\n\n\n\n\n\n\n\nFunction\nDefinition\n\n\n\n\nmean()\nCompute the average of the numbers in a data variable\n\n\nsd()\nCompute the standard deviation of the numbers in a data variable\n\n\nsqrt()\nCompute the square root of a number\n\n\n\nWith these components, we can compute the t-value for\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nMatch the components of the one sample t-test to the R code that computes them. We can assume that our data is in a variable named sample and the number of data points is in a variable named n. The comparison value that we want to compare our mean to is in a variable named comparison_value.\n\n\n\n\n\n\n\nAssumption\nDefinition\n\n\n\n\nsample_mean =\nsample_standard_deviation / sqrt(n)mean(sample)(sample_mean - comparison_value) / sample_standard_errorsd(sample)\n\n\nsample_standard_deviation =\nsample_standard_deviation / sqrt(n)mean(sample)(sample_mean - comparison_value) / sample_standard_errorsd(sample)\n\n\nsample_standard_error =\nsample_standard_deviation / sqrt(n)mean(sample)(sample_mean - comparison_value) / sample_standard_errorsd(sample)\n\n\nt_value =\nsample_standard_deviation / sqrt(n)mean(sample)(sample_mean - comparison_value) / sample_standard_errorsd(sample)\n\n\n\n\n\nCarefully compare the options to the equations for a t-test and R function definitions above.\n\n\n\n\n\nNow we have our ingredients, let’s run a t-test!\n\n\n\n\n\n\nKey step\n\n\n\nOpen Jamovi, install the Rj add-on and open a new Rj window before we go any further\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nTake this code template and copy it across to an Rj window. Can you complete the code to compute a one-sample t-test to quantify how different the mean of sample is from the comparison_value of zero.\nCopy this code into your Rj window (you can overwrite the previous code if you like), and fill in the blanks indicated by __YOUR_CODE_HERE__ (you should delete __YOUR_CODE_HERE__ and replace it with your answer).\n\n# Number of participants\nn = 25\n\n# Generate a random sample\nsample = rnorm(n, mean=0, sd=1)\n\n# Calculate the sample mean and standard deviation\nsample_mean = __YOUR_CODE_HERE__\nsample_sd = __YOUR_CODE_HERE__\n\n# Calculate the sample error of the mean \nsample_standard_error = __YOUR_CODE_HERE__\n\n# Define comparison value\ncomparison_value = 0\n\n# Compute t-value\nt_value = __YOUR_CODE_HERE__\n\nprint(t_value)\n\n\nAll the answers are in previous sections, pay particular attention to the code examples in the previous ‘Check your understanding’ exercise.\nChat with one of your tutors if you get stuck.\n\n\nThe final code should look like this\n\n# Number of participants\nn = 25\n\n# Generate a random sample\nsample = rnorm(n, mean=0, sd=1)\n\n# Calculate the sample mean and standard deviation\nsample_mean = mean(sample)\nsample_sd = sd(sample)\n\n# Calculate the sample error of the mean \nsample_standard_error = sample_sd / sqrt(n)\n\n# Define comparison value\ncomparison_value = 0\n\n# Compute t-value\nt_value = (sample_mean - comparison_value) / sample_standard_error\n\nprint(t_value)\nTry running the code to make sure that it prints out a t-value. If you get some red text, it means something has gone wrong. Check your answers and chat with one of the tutors.\n\n\n\n\n\nSo, now we have some working code to compute our own t-values. This is going to be very useful in this session.\nWe have checked that the code runs, but it would be good to make sure that we’re all actually getting the same answers from the code. This is difficult at the moment as our random number generator will be producing different numbers every time (as it should!). This is normally a good thing but in a teaching setting it can make things difficult.\nWe can solve this issue by setting a “Random Seed” in our code. A random seed is a starting point for generating a sequence of random numbers. Think of it as the initial value that sets the stage for randomness. When you set a random seed, you ensure that the sequence of random numbers generated can be generated the same way each time. This is really critical for Reproducibility and Consistency when checking code..\nIn R, you can set a random seed using the set.seed() function with a number that defines the ‘initial conditions’ for the random numbers that we will generate. For example, we could use\nset.seed(123)\nTo tell R to generate random numbers using a specific set of initial conditions. We’ll get the same sequence of numbers each time we generate a data sample. This means that if we all add the same random seed to the code, we should get the same t-value every time.\nLet’s try adding the random seed to the code.\n\nWe should all get the value -0.176 as the answer. Now we can be certain that we’re all getting the right answers.\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestions\n\n\nAdapt your code to compute the t-value for the following conditions. Use a random seed of 123 throughout.\n\nA data sample of 100 participants with mean of zero and standard deviation of one with a comparison value of zero: t = \nA data sample of 100 participants with mean of three and standard deviation of one with a comparison value of zero: t = \nA data sample of 10 participants with mean of three and standard deviation of one with a comparison value of two: t = \nA data sample of 10 participants with mean of three and standard deviation of three with a comparison value of two: t = \n\n\n\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nAfter completing the exercise, reset your code to the following conditions\n\nn = 20\nrnorm(n, mean=0, sd=1)\ncomparison_value = 0\n\n\n\n\n\n\nNote that we’re simulating data with a mean of zero and using a comparison_value of zero as well. This is our Null Model - the t-values we get from this code are ones that might appear if there is no true effect in the data.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 0, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, main = \"Sampling Distribution of t-values\", xlim=c(-4, 4),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n    \n\nAfter this exercise, you can now compute the null model for a one sample t-test yourself. With this code you can run an analysis to explore how likely it is to observe different t-values when we assume the null hypothesis is true.\nThink about the following questions and tweak your code to find the answers.\n\n\n\n\n\n\n Data Skills - how does the sampling distribution change when we have large samples? Use n = 250 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 25:\n    # Number of participants\n    n = 250\n\nRunning this a few times, you should see that the sampling distribution gets narrower compared to what we saw for n = 20. We commonly see values between -2 and 2 but almost never see values of -3 or +3.\n\n\n\n\n\n\n\n\n\n Data Skills - how does the sampling distribution change when we have very small samples? Use n = 4 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 5:\n    # Number of participants\n    n = 4\n\nRunning this a few times, you should see that the sampling distribution gets broader than for smaller samples. We see more values close to 4 and -4 than we did with a dataset of 20 or 250.\n\n\n\nIn all of these analyses we are absolutely assuming that the null hypothesis of no difference is true. With this assumptions, our sampling distribution shows that we get more extreme t-values (around +/- 4) by chance when working with smaller data samples. By contrast for larger data samples these same values occur extremely rarely.\nWhen data are perfectly normally distributed (parametric assumptions are met), Jamovi and R can compute these null distributions for us using a shortcut without simulating lots of data samples but the result is the same.\n\n\n\nWe introduced the idea of a null model in the lectures. This is a model of what the outcome of our statistics would look like if the data have a random and unbiased structure. The null model is closely linked to the null hypothesis - it describes the probability to observing a particular outcome from our statistics under the assumption that the null hypothesis is true.\nThe sampling distributions we have computed above are exactly null models. We have\n\nDefined population parameters in which we know for sure that the mean is zero\nSampled data observations from that population\nComputed the one sample t-test between the data sample and zero\nStored the t-value for every single resampling\nVisualised the distribution of t-values with a histrogram\n\nThe critical point here is that the null distribution is very stable. We can make confident predictions about what the null distribution of t-values will look like when there is no effect. Let’s try some examples:\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nHow does the distribution of t-values change when we make these changes? Importantly, we know that there is no effect (no difference between the population mean and the comparison value) in all of these examples.\nRemember that we are using random sampling so run each example a few times to get a sense of the result.\nHow does the t-value distribution change when:\n\n…we increase the standard deviation of the population parameters? The mean increasesthe variance increasesit stays pretty much the samethe variance decreasesthe mean decreases\n…we decrease the standard deviation of the population parameters? The mean increasesthe variance increasesit stays pretty much the samethe variance decreasesthe mean decreases\n…we change the mean and comparison value (keeping both values the same)? The mean increasesthe variance increasesit stays pretty much the samethe variance decreasesthe mean decreases\n\n\n\nRead through your code carefully to find the component that you should change.\nTalk with your tutor if you’re stuck.\n(We’ll look at changing the means more in a later section…)\n\n\n\n\n\nSo, the null model for our t-tests is the sampling distribution of t-values when we know there is no effect. Critically this null module is super stable! We can make a lot of changes to the population parameters and precise values being compared and the distribution remains largely the same. We saw in the previous section that sample size can change the tails of the distribution a bit, but most of the rest doesn’t make a big difference.\nThis is a critical point that enables us to complete our statistical assessment. Now that we can be really confident about the probability of observing a particular t-value under the assumption that there is no effect - we’re ready to compute a p-value for our hypothesis test.\n\n\n\nRemember the definition of the p-value from the lectures, this is a bit fiddely but will hopefully make some more sense now that you’re more familiar with the null model. Take a look at the lecture materials from week 5 if you need a refresher.\n\nThe p-value tells us the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct.\n\nWe now have all the ingredients to compute this value. We need\n\nA null model distribution telling us how likely we are to observe particular t-values when we assume the null hypothesis is correct\nThe t-value from a particular data sample\nA statistical significance threshold\n\nTo conduct a hypothesis test, we need to compare an individual t-value from a test to the null distribution of t-value that we have just computed. Remember that the null distrbution is very stable - if we know the sample size then we can use the same null distribution for any t-test!\nLet’s put this into practice.\n\n\n\n\n\n\nKey step\n\n\n\nBefore going any further, reset your code to the following conditions\n\nn = 20\nrnorm(n, mean=0, sd=1)\ncomparison_value = 0\n\n\n\nWe can visualise where one single t-value is on a histogram by using a convenient R function named abline() which can be used to plot straight lines on a figure. We can use abline() as follows:\nabline(v = 1, col = \"red\", lwd = 2)\nThis has specified three input arguments.\n\n\n\n\n\n\n\nInput\nDescription\n\n\n\n\nv\nA value at which to plot a Vertical line\n\n\ncol\nOptional input to choose a colour\n\n\nlwd\nOptional input to specify the width of the line lwd=1 is a thin line and lwd=5 would be very thick.\n\n\n\nWe can add this to the bottom of our code to see where a t-value of 1 (defined by v=1 in the function) would appear on the null distribution. Try changing the value of v a few times to see the difference.\n\n\n\n\n\n\n\nJamovi Issue\n\n\n\nIt is likely that Jamovi will present two plots when you run this code rather than one. If this happens you will have one plot for the histogram and a second for the same histogram with the vertical line (example in the image above).\nI’m not sure why this happens but it is a mostly harmless quirk of Rj - please ignore the first plot!\n\n\nThe p-value describes the probabilty of observing a t-value as least as large as the one from our test. This is equivalent to ‘cutting’ the null distribution at the specific t-value and calculating how likely it is to see a result that is more extreme than the point of the cut.\nFor example, from the lecture notes, here we have a parametric null distribution of t-values (parametric meaning that it wasn’t computed from simulations) that is cut at a position that means we have a 5% chance of observing a value at least as extreme as the cut.\n\nWe can do the same to compute the probability of observing a value to the left or to the right of the vertical line we’ve added to our plot. We can use some simple maths to compute exactly this value\n# Compute the percentile of the value 1\nobserved_value = 1\npercentile = sum(t_values &gt;= observed_value) / length(t_values)\nprint(percentile)\nThis code does the following steps:\n\nsum(t_values &gt;= observed_t_value) counts how many values in the data are larger than or equal to the specified value.\nlength(t_values) gives the total number of values in the data.\nDividing the count by the total number of values converts the result into a decimal indicating the proportion of values in t_values that are larger than or equal to observed_t_value.\n\nThe final value of percentile is then our p-value, which exactly matches the definition from the lecture.\n\nThe p-value tells us the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct.\n\nLet’s put all of this together in our code.\nset.seed(123)\n\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 2, sd = 4)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 2\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\nobserved_t_value = 2.3\npercentile &lt;- sum(t_values &gt;= observed_t_value) / length(t_values)\n\nprint('My t-value')\nprint(observed_t_value)\nprint('My p-value')\nprint(percentile)\n\n\n# Plot the sampling distribution of t-values\nhist(t_values, breaks=24, main = \"Sampling Distribution of t-values\", xlim=c(-10, 10),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n     \nabline(v = observed_t_value, col = \"red\", lwd=5)\nHere, we’ve added some code to specify a t-value from some data that we need to compute a p-value for (observed_t_value = 2.3), we compute and print the associated p-value and create the histogram with a red line indicating the position of observed_t_value.\nThe result should look like this (ignoring this odd double plot…)\n\nIf you have set.seed(123) at the top of your script you should have exactly the same values that I do.\nThe results are telling us that we have a p-value of 0.019. This is less than our conventional threshold of 0.05 so we would consider a t-value of 2.3 to be statistically significant.\nOur code is only checking the right sided tail of the null distribution. This is ok as we can assume that the null distribution of t-values is symmetrical. This is a key point that means we can simplify some things.\n\nThis is a One-Tailed test, as we are only considering one side of the distribution in our p-value calculation. A two-tailed test is simple though, simply divide the p-value by 2 as the lower tail should have an identical shape. For example if our code says that a result has a one-tailed p-value of 0.08 then the two-tailed p-value is 0.04.\nOur code is only checking the high end of the distribution (values to the right of the red line). p-value for a negative t-value is simple to compute though. It is just one with the computed p-value subtracted from it. eg if we have a t-value of -1, our code will say that 0.837 (or 83.7%) of the null values are larger - we can simply subtract it from one to get the correct p-value 1 - 0.837 = 0.163.\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHintSolution\n\n\nWhat is the p-value for the following values of observed_t_value? Make sure you have set.seed(123) set at the top of your script or the values will differ from mine!\n\n\n\n\n\n\n\n\nobserved_t_value\nTail of test\np-value\n\n\n\n\n1\nOne-Tailed\n\n\n\n3\nOne-Tailed\n\n\n\n2.5\nTwo-Tailed\n\n\n\n-1.4\nOne-Tailed\n\n\n\n-0.5\nTwo-Tailed\n\n\n\n\n\n\nCarefully read the key points from the bullet points above about converting values. Talk with your tutor if you get stuck.\n\n\n\n\n\n\n\n\n\n\n\nobserved_t_value\nTail of test\np-value\nExplanation\n\n\n\n\n1\nOne-Tailed\n0.151\nTake the result directly from the script\n\n\n3\nOne-Tailed\n0.001\nTake the result directly from the script\n\n\n2.5\nTwo-Tailed\n0.01 / 2 = 0.005\nDivide the result from the script by 2.\n\n\n-1.4\nOne-Tailed\n1 - 0.915 = 0.085\nSubtract the result from 1\n\n\n-0.5\nTwo-Tailed\n(1 - 0.685) / 2 = 0.1575\nSubtract the result from 1 AND then divide by 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to feel good about yourself if you have made it this far. These are not straightforward concepts and take time to get used to. Seriously well done!\nIf you’re here but still finding it tough, don’t worry - speak with one of your workshop tutors and they can help to explain things to you.\n\n\n\n\n\nFinally, let’s consider why we test against the null distribution of no effect. This is a weird concept but is fairly straightforward to see why we have to do it based onfrom the code we have written so far.\nWe can absolutely compute a sampling distribution for a case where there is a real difference between the mean and the comparison value. For example, this code simulates data with a mean of 1 and compares that to a comparison value of 0. Note that the rnorm() function inputs have changed.\nset.seed(123)\n\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 1, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\nobserved_t_value = 2.3\npercentile &lt;- sum(t_values &gt;= observed_t_value) / length(t_values)\n\nprint('My t-value')\nprint(observed_t_value)\nprint('My p-value')\nprint(percentile)\n\n# Plot the sampling distribution of t-values\nhist(t_values, breaks=24, main = \"Sampling Distribution of t-values\", xlim=c(-10, 10),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n     \nabline(v = observed_t_value, col = \"red\", lwd=5)\nThe result is a sampling distribution which is not centred around mean.\n\nThis distribution tells us that with 20 participants, a mean of 1 and a standard deviation of 1 - we are mostly likely to get a t-value of 5 but that t-value could be as small as 2 or as large as 9 for extreme data samples.\nThis is very useful, but the issue that is we have had to assume what the real effect is. We get radically different sampling distributions if we change that effect around. Take a look at these examples.\n\n\n\n\n\n\n\n\n\n\n\nThe distributions move around so much that the percentile value of the same t-value would be completely incomparable. A t-value of 5 is hugely unlikey in some cases and very frequent in others. If we have to assume that there is a specific effect the result would be extremely senstive to exactly what that assumption is. In contrast there is only one distribution to work with if we assume there is no effect.\nThere are an infinite number of possible sampling distributions when there is a real effect! It isn’t possible to know which distribution we should be comparing with without making some massive assumptions about the exact size, variability and direction of effect that we expect to see. This simply isn’t possible in practice.\nIn contrast, the null case is very closely controlled and easier to work with. The interpretation is trickier but rejecting the null hypothesis is a tractable problem that we can do a good job of solving.\n\n\n\n\nIt is worth repeating - well done for completing this session. It can be hard work developing the code to explore sampling distributions, t-values and p-values but it is a fantastic way to gain a more intuitive understanding of how and why we do things the way we do.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#the-dataset",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#the-dataset",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "TLDR: There is no data to download! we’re going to make the data ourselves.\nThe dataset this week is very simple - we’re going to create it ourselves using R. At this point, we know a lot about the types of questions that we can ask with t-tests and we know a lot about the data processing around it. We can check our understanding of different aspects of these statistical tests by simulating data samples that have known properties (eg mean and standard deviation) and checking if the outcome of the t-test is what we would expect.\nThis way we can numerically validate the tests we’re running and take a look under the hood and see what sort of processes Jamovi and R are using to compute the tests for us.\n\n\n\n\n\n\nContent Info\n\n\n\nThere is a lot of code in this session - don’t be afraid to ask for help from your tutors!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#the-challenge",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#the-challenge",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "Today, we’ll first introduce some concepts and functions in R to simulate a data sample and compute our own one sample t-test. With that in hand, we will set up code to run thousands of tests in one go to help us to better understand sampling distributions.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#code-for-a-one-sample-t-test",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#code-for-a-one-sample-t-test",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "The first thing we’ll need is to compute a t value for a one sample t-test given a dataset. We can work from this definition you’ll remember from the lectures.\n\\[\nt = \\frac{\\text{The sample mean} - \\text{Comparison Value}}{\\text{The standard error of the mean}}\n\\]\nand\n\\[\n\\text{The standard error of the mean} = \\frac{\\text{The sample standard deviation}}{\\sqrt{n}}\n\\]\nLet’s think about what we’ll need to do to compute our t-test. Reading through the definitions, we’ll need some basic arithmatic (+, - and /) need to use the following R functions that we have worked with in previous weeks.\n\n\n\n\n\n\n\nFunction\nDefinition\n\n\n\n\nmean()\nCompute the average of the numbers in a data variable\n\n\nsd()\nCompute the standard deviation of the numbers in a data variable\n\n\nsqrt()\nCompute the square root of a number\n\n\n\nWith these components, we can compute the t-value for\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nMatch the components of the one sample t-test to the R code that computes them. We can assume that our data is in a variable named sample and the number of data points is in a variable named n. The comparison value that we want to compare our mean to is in a variable named comparison_value.\n\n\n\n\n\n\n\nAssumption\nDefinition\n\n\n\n\nsample_mean =\nsample_standard_deviation / sqrt(n)mean(sample)(sample_mean - comparison_value) / sample_standard_errorsd(sample)\n\n\nsample_standard_deviation =\nsample_standard_deviation / sqrt(n)mean(sample)(sample_mean - comparison_value) / sample_standard_errorsd(sample)\n\n\nsample_standard_error =\nsample_standard_deviation / sqrt(n)mean(sample)(sample_mean - comparison_value) / sample_standard_errorsd(sample)\n\n\nt_value =\nsample_standard_deviation / sqrt(n)mean(sample)(sample_mean - comparison_value) / sample_standard_errorsd(sample)\n\n\n\n\n\nCarefully compare the options to the equations for a t-test and R function definitions above.\n\n\n\n\n\nNow we have our ingredients, let’s run a t-test!\n\n\n\n\n\n\nKey step\n\n\n\nOpen Jamovi, install the Rj add-on and open a new Rj window before we go any further\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nTake this code template and copy it across to an Rj window. Can you complete the code to compute a one-sample t-test to quantify how different the mean of sample is from the comparison_value of zero.\nCopy this code into your Rj window (you can overwrite the previous code if you like), and fill in the blanks indicated by __YOUR_CODE_HERE__ (you should delete __YOUR_CODE_HERE__ and replace it with your answer).\n\n# Number of participants\nn = 25\n\n# Generate a random sample\nsample = rnorm(n, mean=0, sd=1)\n\n# Calculate the sample mean and standard deviation\nsample_mean = __YOUR_CODE_HERE__\nsample_sd = __YOUR_CODE_HERE__\n\n# Calculate the sample error of the mean \nsample_standard_error = __YOUR_CODE_HERE__\n\n# Define comparison value\ncomparison_value = 0\n\n# Compute t-value\nt_value = __YOUR_CODE_HERE__\n\nprint(t_value)\n\n\nAll the answers are in previous sections, pay particular attention to the code examples in the previous ‘Check your understanding’ exercise.\nChat with one of your tutors if you get stuck.\n\n\nThe final code should look like this\n\n# Number of participants\nn = 25\n\n# Generate a random sample\nsample = rnorm(n, mean=0, sd=1)\n\n# Calculate the sample mean and standard deviation\nsample_mean = mean(sample)\nsample_sd = sd(sample)\n\n# Calculate the sample error of the mean \nsample_standard_error = sample_sd / sqrt(n)\n\n# Define comparison value\ncomparison_value = 0\n\n# Compute t-value\nt_value = (sample_mean - comparison_value) / sample_standard_error\n\nprint(t_value)\nTry running the code to make sure that it prints out a t-value. If you get some red text, it means something has gone wrong. Check your answers and chat with one of the tutors.\n\n\n\n\n\nSo, now we have some working code to compute our own t-values. This is going to be very useful in this session.\nWe have checked that the code runs, but it would be good to make sure that we’re all actually getting the same answers from the code. This is difficult at the moment as our random number generator will be producing different numbers every time (as it should!). This is normally a good thing but in a teaching setting it can make things difficult.\nWe can solve this issue by setting a “Random Seed” in our code. A random seed is a starting point for generating a sequence of random numbers. Think of it as the initial value that sets the stage for randomness. When you set a random seed, you ensure that the sequence of random numbers generated can be generated the same way each time. This is really critical for Reproducibility and Consistency when checking code..\nIn R, you can set a random seed using the set.seed() function with a number that defines the ‘initial conditions’ for the random numbers that we will generate. For example, we could use\nset.seed(123)\nTo tell R to generate random numbers using a specific set of initial conditions. We’ll get the same sequence of numbers each time we generate a data sample. This means that if we all add the same random seed to the code, we should get the same t-value every time.\nLet’s try adding the random seed to the code.\n\nWe should all get the value -0.176 as the answer. Now we can be certain that we’re all getting the right answers.\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestions\n\n\nAdapt your code to compute the t-value for the following conditions. Use a random seed of 123 throughout.\n\nA data sample of 100 participants with mean of zero and standard deviation of one with a comparison value of zero: t = \nA data sample of 100 participants with mean of three and standard deviation of one with a comparison value of zero: t = \nA data sample of 10 participants with mean of three and standard deviation of one with a comparison value of two: t = \nA data sample of 10 participants with mean of three and standard deviation of three with a comparison value of two: t = \n\n\n\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nAfter completing the exercise, reset your code to the following conditions\n\nn = 20\nrnorm(n, mean=0, sd=1)\ncomparison_value = 0",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#sampling-distributions-for-one-sample-t-tests",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#sampling-distributions-for-one-sample-t-tests",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "Note that we’re simulating data with a mean of zero and using a comparison_value of zero as well. This is our Null Model - the t-values we get from this code are ones that might appear if there is no true effect in the data.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 0, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, main = \"Sampling Distribution of t-values\", xlim=c(-4, 4),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n    \n\nAfter this exercise, you can now compute the null model for a one sample t-test yourself. With this code you can run an analysis to explore how likely it is to observe different t-values when we assume the null hypothesis is true.\nThink about the following questions and tweak your code to find the answers.\n\n\n\n\n\n\n Data Skills - how does the sampling distribution change when we have large samples? Use n = 250 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 25:\n    # Number of participants\n    n = 250\n\nRunning this a few times, you should see that the sampling distribution gets narrower compared to what we saw for n = 20. We commonly see values between -2 and 2 but almost never see values of -3 or +3.\n\n\n\n\n\n\n\n\n\n Data Skills - how does the sampling distribution change when we have very small samples? Use n = 4 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 5:\n    # Number of participants\n    n = 4\n\nRunning this a few times, you should see that the sampling distribution gets broader than for smaller samples. We see more values close to 4 and -4 than we did with a dataset of 20 or 250.\n\n\n\nIn all of these analyses we are absolutely assuming that the null hypothesis of no difference is true. With this assumptions, our sampling distribution shows that we get more extreme t-values (around +/- 4) by chance when working with smaller data samples. By contrast for larger data samples these same values occur extremely rarely.\nWhen data are perfectly normally distributed (parametric assumptions are met), Jamovi and R can compute these null distributions for us using a shortcut without simulating lots of data samples but the result is the same.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#the-null-model",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#the-null-model",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "We introduced the idea of a null model in the lectures. This is a model of what the outcome of our statistics would look like if the data have a random and unbiased structure. The null model is closely linked to the null hypothesis - it describes the probability to observing a particular outcome from our statistics under the assumption that the null hypothesis is true.\nThe sampling distributions we have computed above are exactly null models. We have\n\nDefined population parameters in which we know for sure that the mean is zero\nSampled data observations from that population\nComputed the one sample t-test between the data sample and zero\nStored the t-value for every single resampling\nVisualised the distribution of t-values with a histrogram\n\nThe critical point here is that the null distribution is very stable. We can make confident predictions about what the null distribution of t-values will look like when there is no effect. Let’s try some examples:\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nHow does the distribution of t-values change when we make these changes? Importantly, we know that there is no effect (no difference between the population mean and the comparison value) in all of these examples.\nRemember that we are using random sampling so run each example a few times to get a sense of the result.\nHow does the t-value distribution change when:\n\n…we increase the standard deviation of the population parameters? The mean increasesthe variance increasesit stays pretty much the samethe variance decreasesthe mean decreases\n…we decrease the standard deviation of the population parameters? The mean increasesthe variance increasesit stays pretty much the samethe variance decreasesthe mean decreases\n…we change the mean and comparison value (keeping both values the same)? The mean increasesthe variance increasesit stays pretty much the samethe variance decreasesthe mean decreases\n\n\n\nRead through your code carefully to find the component that you should change.\nTalk with your tutor if you’re stuck.\n(We’ll look at changing the means more in a later section…)\n\n\n\n\n\nSo, the null model for our t-tests is the sampling distribution of t-values when we know there is no effect. Critically this null module is super stable! We can make a lot of changes to the population parameters and precise values being compared and the distribution remains largely the same. We saw in the previous section that sample size can change the tails of the distribution a bit, but most of the rest doesn’t make a big difference.\nThis is a critical point that enables us to complete our statistical assessment. Now that we can be really confident about the probability of observing a particular t-value under the assumption that there is no effect - we’re ready to compute a p-value for our hypothesis test.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#computing-a-p-value",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#computing-a-p-value",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "Remember the definition of the p-value from the lectures, this is a bit fiddely but will hopefully make some more sense now that you’re more familiar with the null model. Take a look at the lecture materials from week 5 if you need a refresher.\n\nThe p-value tells us the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct.\n\nWe now have all the ingredients to compute this value. We need\n\nA null model distribution telling us how likely we are to observe particular t-values when we assume the null hypothesis is correct\nThe t-value from a particular data sample\nA statistical significance threshold\n\nTo conduct a hypothesis test, we need to compare an individual t-value from a test to the null distribution of t-value that we have just computed. Remember that the null distrbution is very stable - if we know the sample size then we can use the same null distribution for any t-test!\nLet’s put this into practice.\n\n\n\n\n\n\nKey step\n\n\n\nBefore going any further, reset your code to the following conditions\n\nn = 20\nrnorm(n, mean=0, sd=1)\ncomparison_value = 0\n\n\n\nWe can visualise where one single t-value is on a histogram by using a convenient R function named abline() which can be used to plot straight lines on a figure. We can use abline() as follows:\nabline(v = 1, col = \"red\", lwd = 2)\nThis has specified three input arguments.\n\n\n\n\n\n\n\nInput\nDescription\n\n\n\n\nv\nA value at which to plot a Vertical line\n\n\ncol\nOptional input to choose a colour\n\n\nlwd\nOptional input to specify the width of the line lwd=1 is a thin line and lwd=5 would be very thick.\n\n\n\nWe can add this to the bottom of our code to see where a t-value of 1 (defined by v=1 in the function) would appear on the null distribution. Try changing the value of v a few times to see the difference.\n\n\n\n\n\n\n\nJamovi Issue\n\n\n\nIt is likely that Jamovi will present two plots when you run this code rather than one. If this happens you will have one plot for the histogram and a second for the same histogram with the vertical line (example in the image above).\nI’m not sure why this happens but it is a mostly harmless quirk of Rj - please ignore the first plot!\n\n\nThe p-value describes the probabilty of observing a t-value as least as large as the one from our test. This is equivalent to ‘cutting’ the null distribution at the specific t-value and calculating how likely it is to see a result that is more extreme than the point of the cut.\nFor example, from the lecture notes, here we have a parametric null distribution of t-values (parametric meaning that it wasn’t computed from simulations) that is cut at a position that means we have a 5% chance of observing a value at least as extreme as the cut.\n\nWe can do the same to compute the probability of observing a value to the left or to the right of the vertical line we’ve added to our plot. We can use some simple maths to compute exactly this value\n# Compute the percentile of the value 1\nobserved_value = 1\npercentile = sum(t_values &gt;= observed_value) / length(t_values)\nprint(percentile)\nThis code does the following steps:\n\nsum(t_values &gt;= observed_t_value) counts how many values in the data are larger than or equal to the specified value.\nlength(t_values) gives the total number of values in the data.\nDividing the count by the total number of values converts the result into a decimal indicating the proportion of values in t_values that are larger than or equal to observed_t_value.\n\nThe final value of percentile is then our p-value, which exactly matches the definition from the lecture.\n\nThe p-value tells us the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct.\n\nLet’s put all of this together in our code.\nset.seed(123)\n\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 2, sd = 4)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 2\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\nobserved_t_value = 2.3\npercentile &lt;- sum(t_values &gt;= observed_t_value) / length(t_values)\n\nprint('My t-value')\nprint(observed_t_value)\nprint('My p-value')\nprint(percentile)\n\n\n# Plot the sampling distribution of t-values\nhist(t_values, breaks=24, main = \"Sampling Distribution of t-values\", xlim=c(-10, 10),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n     \nabline(v = observed_t_value, col = \"red\", lwd=5)\nHere, we’ve added some code to specify a t-value from some data that we need to compute a p-value for (observed_t_value = 2.3), we compute and print the associated p-value and create the histogram with a red line indicating the position of observed_t_value.\nThe result should look like this (ignoring this odd double plot…)\n\nIf you have set.seed(123) at the top of your script you should have exactly the same values that I do.\nThe results are telling us that we have a p-value of 0.019. This is less than our conventional threshold of 0.05 so we would consider a t-value of 2.3 to be statistically significant.\nOur code is only checking the right sided tail of the null distribution. This is ok as we can assume that the null distribution of t-values is symmetrical. This is a key point that means we can simplify some things.\n\nThis is a One-Tailed test, as we are only considering one side of the distribution in our p-value calculation. A two-tailed test is simple though, simply divide the p-value by 2 as the lower tail should have an identical shape. For example if our code says that a result has a one-tailed p-value of 0.08 then the two-tailed p-value is 0.04.\nOur code is only checking the high end of the distribution (values to the right of the red line). p-value for a negative t-value is simple to compute though. It is just one with the computed p-value subtracted from it. eg if we have a t-value of -1, our code will say that 0.837 (or 83.7%) of the null values are larger - we can simply subtract it from one to get the correct p-value 1 - 0.837 = 0.163.\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHintSolution\n\n\nWhat is the p-value for the following values of observed_t_value? Make sure you have set.seed(123) set at the top of your script or the values will differ from mine!\n\n\n\n\n\n\n\n\nobserved_t_value\nTail of test\np-value\n\n\n\n\n1\nOne-Tailed\n\n\n\n3\nOne-Tailed\n\n\n\n2.5\nTwo-Tailed\n\n\n\n-1.4\nOne-Tailed\n\n\n\n-0.5\nTwo-Tailed\n\n\n\n\n\n\nCarefully read the key points from the bullet points above about converting values. Talk with your tutor if you get stuck.\n\n\n\n\n\n\n\n\n\n\n\nobserved_t_value\nTail of test\np-value\nExplanation\n\n\n\n\n1\nOne-Tailed\n0.151\nTake the result directly from the script\n\n\n3\nOne-Tailed\n0.001\nTake the result directly from the script\n\n\n2.5\nTwo-Tailed\n0.01 / 2 = 0.005\nDivide the result from the script by 2.\n\n\n-1.4\nOne-Tailed\n1 - 0.915 = 0.085\nSubtract the result from 1\n\n\n-0.5\nTwo-Tailed\n(1 - 0.685) / 2 = 0.1575\nSubtract the result from 1 AND then divide by 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to feel good about yourself if you have made it this far. These are not straightforward concepts and take time to get used to. Seriously well done!\nIf you’re here but still finding it tough, don’t worry - speak with one of your workshop tutors and they can help to explain things to you.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#optional-why-do-we-test-the-null-hypothesis",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#optional-why-do-we-test-the-null-hypothesis",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "Finally, let’s consider why we test against the null distribution of no effect. This is a weird concept but is fairly straightforward to see why we have to do it based onfrom the code we have written so far.\nWe can absolutely compute a sampling distribution for a case where there is a real difference between the mean and the comparison value. For example, this code simulates data with a mean of 1 and compares that to a comparison value of 0. Note that the rnorm() function inputs have changed.\nset.seed(123)\n\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 1, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\nobserved_t_value = 2.3\npercentile &lt;- sum(t_values &gt;= observed_t_value) / length(t_values)\n\nprint('My t-value')\nprint(observed_t_value)\nprint('My p-value')\nprint(percentile)\n\n# Plot the sampling distribution of t-values\nhist(t_values, breaks=24, main = \"Sampling Distribution of t-values\", xlim=c(-10, 10),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n     \nabline(v = observed_t_value, col = \"red\", lwd=5)\nThe result is a sampling distribution which is not centred around mean.\n\nThis distribution tells us that with 20 participants, a mean of 1 and a standard deviation of 1 - we are mostly likely to get a t-value of 5 but that t-value could be as small as 2 or as large as 9 for extreme data samples.\nThis is very useful, but the issue that is we have had to assume what the real effect is. We get radically different sampling distributions if we change that effect around. Take a look at these examples.\n\n\n\n\n\n\n\n\n\n\n\nThe distributions move around so much that the percentile value of the same t-value would be completely incomparable. A t-value of 5 is hugely unlikey in some cases and very frequent in others. If we have to assume that there is a specific effect the result would be extremely senstive to exactly what that assumption is. In contrast there is only one distribution to work with if we assume there is no effect.\nThere are an infinite number of possible sampling distributions when there is a real effect! It isn’t possible to know which distribution we should be comparing with without making some massive assumptions about the exact size, variability and direction of effect that we expect to see. This simply isn’t possible in practice.\nIn contrast, the null case is very closely controlled and easier to work with. The interpretation is trickier but rejecting the null hypothesis is a tractable problem that we can do a good job of solving.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#summary",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#summary",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "It is worth repeating - well done for completing this session. It can be hard work developing the code to explore sampling distributions, t-values and p-values but it is a fantastic way to gain a more intuitive understanding of how and why we do things the way we do.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "This week we will explore how to use Jamovi to compute and visualise descriptive statistics from a dataset. This will include some revision on Jamovi from last semester and an introduction on how we can use the R coding language to simplify complex data processing.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nMeasures of central tendency\n\n\n\nMeasures of dispersion and spread\n\n\n\nMeasures of skew and normality\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Rj editor to use R code within Jamovi\n\n\n\nLearn the basics about functions and variables in R\n\n\n\nComputing descriptive statistics using R\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nValidating and working with openly available research data\n\n\n\nCreate a reproducible data analysis script\n\n\n\n\n\nMetaphors are often used to help people understand abstract concepts in terms of perceptual experiences. Though this is often useful, it can bias perception and decision making in unexpected ways. For example, North and South are abstract concepts that have a spatial association (north = up, south = down). Participants in the USA have a biased expectation that richer people will live in the north of a city (Meier et al. 2011). However, these expectations are not uniform across all people and cultures and we should not naively generalise this result to all people. A follow up study comparing this effect in participants from the USA and Hong Kong replicated the original effect for American participants but found the opposite effect in people from Hong Kong (Huang, Tse, and Cho 2014).\nThe Many Labs 2 project (Klein et al. 2018) performed an additional replication of this effect across a wide range of countries. Participants were presented with the following map outline of a city:\n\nand asked to indicate where they though a fictional person would live within the city. Participants were shown one of two descriptions of the person.\n\n“Dr. Bennett lives in the city. He is a wealthy businessman who has travelled the world. He inherited a significant amount of money from a Great Aunt, and was educated at the best schools growing up. He enjoys fine dining and going to the theater on weekends.”\n\nOr\n\n“Mr. Bennett lives in the city. He is unemployed. He was born and raised in the city he now calls home. He struggles to pay the rent each month, and dropped out of high school before graduation. He enjoys a good hot dog and a six pack of beers when he can.”\n\nThe first description is a caricature of a person with high Socio-Economic Status (SES) whilst the second is a caricature of someone with a low SES. The hypothesis is that people in the United States and Hong Kong have different demographic knowledge that may shape their metaphoric link between positive association and cardinal direction (north vs. south).\n\n\n\nThis week we will use one-sample t-tests to explore whether participants from the UK would expect people with low or high SES to live north or south of the centre of the city.\n\n\n\n\n\n\nA new hypothesis?\n\n\n\nBefore we start, think about what you would predict about participants from the UK. Is there a reason to expect that people in the UK would associate the north of a city with high SES or vice versa?\nMake a note of your answer and the reasons behind it. Note that there isn’t necessarily a correct answer to this question and we won’t ask you for it later.\nIt is good practice to be clear about our predictions and expectations before starting a data analysis.\n\n\n\n\n\nLet’s start by loading the dataset into Jamovi and checking out the data. You can use the file named RMB-2024_computer-practical_huang-2014_country-UK_cond-All.csv.\nThis dataset has several columns of data. The main column of interest is currently named variable. This represents the y-axis position of the persons expectation about where the fictional character would live. A value of zero represents the middle of the city, positive values represent more northern locations and negative values represent more southern locations.\nThe other key variable is named factor. This indicates which group the participant was allocated to. This can be either ‘high’ or ‘low’ depending on whether the participant saw a description of a person with stereotypes of high or low socioeconomic status.\nThis is open-access data and the column names aren’t very informative. It would be better to rename them to something more considered so that we can avoid mistakes later.\n\n\n\n\n\n\nKey step\n\n\n\nRename the following variables using the editor under the ‘Variables’ tab.\n\n‘variable’ should be renamed to ‘NorthSouth_Position’, and\n‘factor’ should be renamed to ‘SES_Story’.\n\nDon’t include any spaces in the new variable names! this can cause an issue later.\n\n\n\nNext, take a look through the data itself. Remember the steps we took in the last couple of weeks. Have a careful scan through the data values and see if you can spot anything that needs to be fixed.\n\n\n\n\n\n\n Data Skills - how many participants are included in the dataset?\n\n\n\n\n\nWe have 136 rows of data observations the spreadsheet.\n\n\n\n\n\n\n\n\n\n Data Skills - what experimental design was used in this experiment?\n\n\n\n\n\nThis is a between-subjects design, though that might not be obvious from the data alone! Checking through the paper methods we can see that each participant was either in the high or low SES condition.\n\n\n\n\n\n\n\n\n\n Data Skills - do we need to tidy up this dataset?\n\n\n\n\n\nNo, in this case the dataset looks very clean. We appear to have data for everyone without any obvious typos or missing information. Looks good to go.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nCompute some descriptive statistics on the NorthSouth column. Make sure that you include the standard error of the mean!\nYou can split the descriptive statistics using a second variable. This\n\n\n\nNext, let’s think about what these values can tell us.\n\n\n\n\n\n\n Data Skills - do we have an equal number of data points in each condition?\n\n\n\n\n\nNo, we have 75 participants in the High condition and 61 participants in the Low condition.\n\n\n\n\n\n\n\n\n\n Data Skills - which condition has a wider variation in NorthSouth Positions chosen by participants?\n\n\n\n\n\nWe can see this from the standard deviations. The High condition has a standard deviation of 63.2 and the Low condition has a standard deviation of 77.8.\nThe responses in the Low SES story condition were more variable.\n\n\n\n\n\n\n\n\n\n Data Skills - which condition has a more northern mean position?\n\n\n\n\n\nRecall from the introduction that positive values indicate a position in the north of the city. The High condition has a mean value of -39.0 and the Low condition has a mean value of 18.6\nThe responses in the Low SES story condition were more northern.\n\n\n\n\n\n\n\n\n\n Data Skills - can we tell from the descriptives whether this is a significant difference?\n\n\n\n\n\nNo! remember that descriptive statistics only summarise the data as it is. They cannot tell us whether we would expect what we see in that data to generalise beyond the specific data sample.\n\n\n\n\n\n\n\n\n\n Data Skills - are we more confident in our estimate of the mean for the ‘North’ or ‘South’ condition?\n\n\n\n\n\nThe standard error of the mean tells us how well we think our estimated mean from the data sample represents the ‘true’ population mean.\nThe standard error of the mean for the ‘Low’ group is higher (9.96 vs 7.3 for ‘High’) - indicating that we are less confident that this is a good representation of our population mean. Note that it could still be a good estimate, we’re just relatively less confident than we are for the ‘High’ condition.\n\n\n\n\n\n\nNext, we’re going to ask whether the participants in the either the High or Low SES conditions had a preference towards locating our fictional character in the North or South of the city. This is our one-sample t-test. The NorthSouth_Position is our data variable and the centre point of the city is the comparison value.\nFirst, we need to split out the data so that we can run the t-test using either the Low or High SES conditions, not using both mixed together.\nWe can do this in Jamovi using a ‘Filter’. Jamovi provides some great documentation on data filters on their website. Take a few minutes and read through the info on ‘Row Filters’ on this page.\nThe most important thing about filters is that they are reversible. We can apply a filter to run an analysis on a subset of the data without actually changing the dataset itself. This means we could make several different filters and turn them on and off to repeat the same analyses on different sections of the dataset. Convenient!\n\n\n\n\n\n\nKey step\n\n\n\nFollowing the guidance on the Jamovi documentation on data filters, create a filter to select ONLY the participants who heard the low SES story.\nThe result should exclude the first 75 rows from your dataset, you can confirm this by looking at the ticks and crosses in the ‘Filter’ column, and by inspecting the values in the very bottom of the window.\n\n\n\nNote that your analyses in results windows will automatically update when you apply a filter to the data. With the filter applied, you will only see the descriptive statistics for the ‘Low’ SES_Story groups.\nIf you click the ‘active’ toggle in the ‘Filter’ window you can turn the filter off and your analysis will return to its initial state.\n\n\n\n\n\n\nKey step\n\n\n\nWith your first Filter turned OFF, create a filter to select ONLY the participants who heard the High SES story.\nThe result should now exclude the last 61 rows from your dataset.\n You can turn your filters on and off to see how this changes the data available for the analysis, though turning both filters on will exclude all the data in this case…\n\n\n\n\n\nNow we’re ready to test our hypothesis! remember that the equation for a one-sample t-test is the following:\n\\[\nt = \\frac{\\text{The sample mean} - \\text{Comparison Value}}{\\text{The standard error of the mean}}\n\\]\nWe’re looking at the difference between our observed mean and a comparison value as a ratio to the precision to which we have estimated our observed mean.\nOur descriptive statistics output contains the mean and standard error of the mean for both groups. Our comparison value is the centre of the city, indicated by a score of 0 (zero).\nWe all the values required for a one-sample t-test already in the descriptive statistics! For the ‘Low’ SES_Story condition, we have:\n\nGroup Mean = 18.6\nComparison Value = 0\nStandard error of the mean = 9.96\n\nSo, the one sample t-test value is:\n\\[\nt = \\frac{18.6 - 0}{9.96} = 1.867\n\\]\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute the t-value for the High SES Story group using a calculator or Rj?\n\n\nFirst, get the three values that you’ll need for the computation. These are the comparison value and the mean and standard error of the mean for the High SES group.\n\n\nThe individual values are\n\nGroup Mean = -39.0\nComparison Value = 0\nStandard error of the mean = 7.30\n\nso the t-value is\n\\[\nt = \\frac{-39.0 - 0}{7.30} = -5.342\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nBefore going any further - make sure that your data filter is set to include only the data from the ‘Low’ SES_Story condition.\n\n\nOf course, we will normally compute the t-test using Jamovi as rather than relying on hand calculations. We can do this by opening the ‘One Sample T-Test’ option under the ‘Analysis -&gt; T-Tests’ menu on the top ribbon.\nOpen the One Sample T-Test option now, and drag the NorthSouth_Position variable over to the ‘dependent variables’ box. The result should appear on the right hand side. Before we inspect the result, we should check and add a number of important options to our analysis.\n\n\nHypothesis - test value it is critical that we select the correct hypothesis or our test will be meaningless. Our ‘test value’, also known as a ‘comparison value’, is set to zero by default. This is correct for our analysis but may need setting to something else for other analyses. Take care to check it makes sense each time.\nHypothesis - test tail we can also specify whether we’re going to perform a one- or two-tailed test. Here we want to specify a two-tailed test in which the mean does not equal the test value.\nAssumption Checks we should add a test to check that our data are normally distributed. Selecting the ‘Normality test’ box will compute a Shapio-Wilks statistic for you\nAdditional Statistics there are a number of important options here. For now, add the ‘Descriptive plots’ option to your analysis. This will show us a visualisation of the estimated mean and its confidence intervals.\n\nLet’s take a look at the outputs, first the t-test table.\n\nThis contains the information we need to report our t-test: the t-value, the degrees of freedom and the p-value.\n\nt(60) = 1.87, p = 0.066\n\nThis t-value should match the number you computed by hand earlier on. Note that the footnote of the table reminds us of the hypothesis that we specified. Make sure that this matches what you expect!\nNext, the normality check:\n\nThis reports the Shapiro-Wilk W statistic and associated p-value. In this case the p-value is not significant which indicates that the data are normally distributed and we’re happy to go ahead with the parametric t-test.\nNext, the descriptives plot:\n\nThis is a really useful visualisation of our test. We can see the mean of the data (black dot) and the test value (dotted line). We also have the 95% confidence intervals of the estimated mean. We can see that the confidence intervals overlap with the test value - this tells us that it is likely our test will not be significant - according to our confidence intervals, the true population mean might include our test value zero which would indicate no real difference.\n\n\n\n\n\n\nKey step\n\n\n\nChange your data filter to compute the one-sample t-test for the ‘High’ SES_Story condition. The test should update once your filter is on without you having to respecify the test.\n\n\n\n\n\n\n\n\n Data Skills - Is the mean position of participants in the ‘High’ condition significantly different to zero?\n\n\n\n\n\nYes, we do have a significant difference - we could report our test as follows\n\nt(74) = -5.35, p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n Data Skills - Is it valid to use parametric statistics on this data?\n\n\n\n\n\nThe Shapiro Wilk statistic has a p-value of 0.038 which is less than p = 0.05. This suggets that our data are not normally distributed. We should inspect the data in more detail and consider using a non-parametric alternative.\nThe Wilcoxon Rank test is the non-parametric alternative for One Sample t-test. You can add this to the analysis to see if it gives the same result as the parametric test.\n\n\n\n\n\n\n\n\n\n Data Skills - Do the confidence intervals of the estimated mean cross zero?\n\n\n\n\n\nNo - in the ‘High’ condition the confidence intervals of the mean are a long way from zero.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nGo back to your notes from the start of the session. Do the results match your predictions about whether a UK data sample would expect a high or low socio-economic status people to live in the north or south of the city?\n\n\n\n\n\n\nFinally - we should make a reproducible version of our analysis using Rj.\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and add the following line to load the dplyr library.\nlibrary(dplyr)\n\n\nFirstly, we’ll need to replicate our data filter to select only the ‘High’ or ‘Low’ conditions for our analysis. We covered this in detail last week - if you need a refresher, take a look at the information on selecting rows of data using the filter function from week 2\nLet’s build a filter to select only the ‘High’ condition.\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\nNext - turn on Jamovi’s syntax mode to display the R code associated with our analyses. This should create some R code at the top of each results block.\n\nWe can right click on this code to copy it before pasting it into the Rj window. Our code should then look like this:\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\n\njmv::ttestOneS(\n    data = data,\n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\n\n\n\n\n\n\nWarning\n\n\n\nSome people have encountered an issue with running this code block - if your R session stalls whilst running the code above, please try again whilst removing plots=TRUE.\nApologies - we’re working on a fix for this.\n\n\nThis command runs the function ttestOneS which computes the One Sample t-test. Jamovi has automatically detected that we want to add the normality test and the descriptive plots and included these options in the function call (see norm=TRUE and plots=TRUE).\nThere is a keyword argument in R for every option in Jamovi - take a look at the jmv library documentation for ttestOneS to see a full list.\nBefore running the code we need to make one adjustment. The function runs on the full dataset data by default but we want to use our new filtered data high_ses instead. Update the function to take in the filtered data as an input.\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\n\njmv::ttestOneS(\n    data = high_ses,\n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\nNow running the code should output result tables that match our previous analysis. Importantly the form of data preprocessing (the filtering) and the test itself are all clearly specified in the code. Making it easier for others to understand what analysis we have run.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you add a code block to compute the one-sample t-test for only the ‘Low’ SES_Story condition\n\n\nRemember that you’ll need to write a new filter line to create a copy of the dataset with only the ‘Low’ values for SES_Story. You can save this into a variable and pass it in as the data argument to ttestOneS\n\n\nYour final code should look like this:\nlow_ses &lt;- filter(data, SES_Story == 'Low')\n\njmv::ttestOneS(\n    data = low_ses, \n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\n\n\n\n\n\nYou can keep adding R code to your script to run multiple analyses at once, this way we can run the t-test for both conditions with out having to go back and forth in Jamovi toggling the data filters on and off.\nIt takes a little more setup but the R code has some big advantages once you get used to it!\n\n\n\nWe’ve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#the-dataset",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#the-dataset",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Metaphors are often used to help people understand abstract concepts in terms of perceptual experiences. Though this is often useful, it can bias perception and decision making in unexpected ways. For example, North and South are abstract concepts that have a spatial association (north = up, south = down). Participants in the USA have a biased expectation that richer people will live in the north of a city (Meier et al. 2011). However, these expectations are not uniform across all people and cultures and we should not naively generalise this result to all people. A follow up study comparing this effect in participants from the USA and Hong Kong replicated the original effect for American participants but found the opposite effect in people from Hong Kong (Huang, Tse, and Cho 2014).\nThe Many Labs 2 project (Klein et al. 2018) performed an additional replication of this effect across a wide range of countries. Participants were presented with the following map outline of a city:\n\nand asked to indicate where they though a fictional person would live within the city. Participants were shown one of two descriptions of the person.\n\n“Dr. Bennett lives in the city. He is a wealthy businessman who has travelled the world. He inherited a significant amount of money from a Great Aunt, and was educated at the best schools growing up. He enjoys fine dining and going to the theater on weekends.”\n\nOr\n\n“Mr. Bennett lives in the city. He is unemployed. He was born and raised in the city he now calls home. He struggles to pay the rent each month, and dropped out of high school before graduation. He enjoys a good hot dog and a six pack of beers when he can.”\n\nThe first description is a caricature of a person with high Socio-Economic Status (SES) whilst the second is a caricature of someone with a low SES. The hypothesis is that people in the United States and Hong Kong have different demographic knowledge that may shape their metaphoric link between positive association and cardinal direction (north vs. south).",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#the-challenge",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#the-challenge",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "This week we will use one-sample t-tests to explore whether participants from the UK would expect people with low or high SES to live north or south of the centre of the city.\n\n\n\n\n\n\nA new hypothesis?\n\n\n\nBefore we start, think about what you would predict about participants from the UK. Is there a reason to expect that people in the UK would associate the north of a city with high SES or vice versa?\nMake a note of your answer and the reasons behind it. Note that there isn’t necessarily a correct answer to this question and we won’t ask you for it later.\nIt is good practice to be clear about our predictions and expectations before starting a data analysis.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#getting-started-with-descriptive-statistics",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#getting-started-with-descriptive-statistics",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Let’s start by loading the dataset into Jamovi and checking out the data. You can use the file named RMB-2024_computer-practical_huang-2014_country-UK_cond-All.csv.\nThis dataset has several columns of data. The main column of interest is currently named variable. This represents the y-axis position of the persons expectation about where the fictional character would live. A value of zero represents the middle of the city, positive values represent more northern locations and negative values represent more southern locations.\nThe other key variable is named factor. This indicates which group the participant was allocated to. This can be either ‘high’ or ‘low’ depending on whether the participant saw a description of a person with stereotypes of high or low socioeconomic status.\nThis is open-access data and the column names aren’t very informative. It would be better to rename them to something more considered so that we can avoid mistakes later.\n\n\n\n\n\n\nKey step\n\n\n\nRename the following variables using the editor under the ‘Variables’ tab.\n\n‘variable’ should be renamed to ‘NorthSouth_Position’, and\n‘factor’ should be renamed to ‘SES_Story’.\n\nDon’t include any spaces in the new variable names! this can cause an issue later.\n\n\n\nNext, take a look through the data itself. Remember the steps we took in the last couple of weeks. Have a careful scan through the data values and see if you can spot anything that needs to be fixed.\n\n\n\n\n\n\n Data Skills - how many participants are included in the dataset?\n\n\n\n\n\nWe have 136 rows of data observations the spreadsheet.\n\n\n\n\n\n\n\n\n\n Data Skills - what experimental design was used in this experiment?\n\n\n\n\n\nThis is a between-subjects design, though that might not be obvious from the data alone! Checking through the paper methods we can see that each participant was either in the high or low SES condition.\n\n\n\n\n\n\n\n\n\n Data Skills - do we need to tidy up this dataset?\n\n\n\n\n\nNo, in this case the dataset looks very clean. We appear to have data for everyone without any obvious typos or missing information. Looks good to go.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nCompute some descriptive statistics on the NorthSouth column. Make sure that you include the standard error of the mean!\nYou can split the descriptive statistics using a second variable. This\n\n\n\nNext, let’s think about what these values can tell us.\n\n\n\n\n\n\n Data Skills - do we have an equal number of data points in each condition?\n\n\n\n\n\nNo, we have 75 participants in the High condition and 61 participants in the Low condition.\n\n\n\n\n\n\n\n\n\n Data Skills - which condition has a wider variation in NorthSouth Positions chosen by participants?\n\n\n\n\n\nWe can see this from the standard deviations. The High condition has a standard deviation of 63.2 and the Low condition has a standard deviation of 77.8.\nThe responses in the Low SES story condition were more variable.\n\n\n\n\n\n\n\n\n\n Data Skills - which condition has a more northern mean position?\n\n\n\n\n\nRecall from the introduction that positive values indicate a position in the north of the city. The High condition has a mean value of -39.0 and the Low condition has a mean value of 18.6\nThe responses in the Low SES story condition were more northern.\n\n\n\n\n\n\n\n\n\n Data Skills - can we tell from the descriptives whether this is a significant difference?\n\n\n\n\n\nNo! remember that descriptive statistics only summarise the data as it is. They cannot tell us whether we would expect what we see in that data to generalise beyond the specific data sample.\n\n\n\n\n\n\n\n\n\n Data Skills - are we more confident in our estimate of the mean for the ‘North’ or ‘South’ condition?\n\n\n\n\n\nThe standard error of the mean tells us how well we think our estimated mean from the data sample represents the ‘true’ population mean.\nThe standard error of the mean for the ‘Low’ group is higher (9.96 vs 7.3 for ‘High’) - indicating that we are less confident that this is a good representation of our population mean. Note that it could still be a good estimate, we’re just relatively less confident than we are for the ‘High’ condition.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#data-filters-in-jamovi",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#data-filters-in-jamovi",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Next, we’re going to ask whether the participants in the either the High or Low SES conditions had a preference towards locating our fictional character in the North or South of the city. This is our one-sample t-test. The NorthSouth_Position is our data variable and the centre point of the city is the comparison value.\nFirst, we need to split out the data so that we can run the t-test using either the Low or High SES conditions, not using both mixed together.\nWe can do this in Jamovi using a ‘Filter’. Jamovi provides some great documentation on data filters on their website. Take a few minutes and read through the info on ‘Row Filters’ on this page.\nThe most important thing about filters is that they are reversible. We can apply a filter to run an analysis on a subset of the data without actually changing the dataset itself. This means we could make several different filters and turn them on and off to repeat the same analyses on different sections of the dataset. Convenient!\n\n\n\n\n\n\nKey step\n\n\n\nFollowing the guidance on the Jamovi documentation on data filters, create a filter to select ONLY the participants who heard the low SES story.\nThe result should exclude the first 75 rows from your dataset, you can confirm this by looking at the ticks and crosses in the ‘Filter’ column, and by inspecting the values in the very bottom of the window.\n\n\n\nNote that your analyses in results windows will automatically update when you apply a filter to the data. With the filter applied, you will only see the descriptive statistics for the ‘Low’ SES_Story groups.\nIf you click the ‘active’ toggle in the ‘Filter’ window you can turn the filter off and your analysis will return to its initial state.\n\n\n\n\n\n\nKey step\n\n\n\nWith your first Filter turned OFF, create a filter to select ONLY the participants who heard the High SES story.\nThe result should now exclude the last 61 rows from your dataset.\n You can turn your filters on and off to see how this changes the data available for the analysis, though turning both filters on will exclude all the data in this case…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#one-sample-t-tests",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#one-sample-t-tests",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Now we’re ready to test our hypothesis! remember that the equation for a one-sample t-test is the following:\n\\[\nt = \\frac{\\text{The sample mean} - \\text{Comparison Value}}{\\text{The standard error of the mean}}\n\\]\nWe’re looking at the difference between our observed mean and a comparison value as a ratio to the precision to which we have estimated our observed mean.\nOur descriptive statistics output contains the mean and standard error of the mean for both groups. Our comparison value is the centre of the city, indicated by a score of 0 (zero).\nWe all the values required for a one-sample t-test already in the descriptive statistics! For the ‘Low’ SES_Story condition, we have:\n\nGroup Mean = 18.6\nComparison Value = 0\nStandard error of the mean = 9.96\n\nSo, the one sample t-test value is:\n\\[\nt = \\frac{18.6 - 0}{9.96} = 1.867\n\\]\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute the t-value for the High SES Story group using a calculator or Rj?\n\n\nFirst, get the three values that you’ll need for the computation. These are the comparison value and the mean and standard error of the mean for the High SES group.\n\n\nThe individual values are\n\nGroup Mean = -39.0\nComparison Value = 0\nStandard error of the mean = 7.30\n\nso the t-value is\n\\[\nt = \\frac{-39.0 - 0}{7.30} = -5.342\n\\]",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#one-sample-t-tests-in-jamovi",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#one-sample-t-tests-in-jamovi",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Key step\n\n\n\nBefore going any further - make sure that your data filter is set to include only the data from the ‘Low’ SES_Story condition.\n\n\nOf course, we will normally compute the t-test using Jamovi as rather than relying on hand calculations. We can do this by opening the ‘One Sample T-Test’ option under the ‘Analysis -&gt; T-Tests’ menu on the top ribbon.\nOpen the One Sample T-Test option now, and drag the NorthSouth_Position variable over to the ‘dependent variables’ box. The result should appear on the right hand side. Before we inspect the result, we should check and add a number of important options to our analysis.\n\n\nHypothesis - test value it is critical that we select the correct hypothesis or our test will be meaningless. Our ‘test value’, also known as a ‘comparison value’, is set to zero by default. This is correct for our analysis but may need setting to something else for other analyses. Take care to check it makes sense each time.\nHypothesis - test tail we can also specify whether we’re going to perform a one- or two-tailed test. Here we want to specify a two-tailed test in which the mean does not equal the test value.\nAssumption Checks we should add a test to check that our data are normally distributed. Selecting the ‘Normality test’ box will compute a Shapio-Wilks statistic for you\nAdditional Statistics there are a number of important options here. For now, add the ‘Descriptive plots’ option to your analysis. This will show us a visualisation of the estimated mean and its confidence intervals.\n\nLet’s take a look at the outputs, first the t-test table.\n\nThis contains the information we need to report our t-test: the t-value, the degrees of freedom and the p-value.\n\nt(60) = 1.87, p = 0.066\n\nThis t-value should match the number you computed by hand earlier on. Note that the footnote of the table reminds us of the hypothesis that we specified. Make sure that this matches what you expect!\nNext, the normality check:\n\nThis reports the Shapiro-Wilk W statistic and associated p-value. In this case the p-value is not significant which indicates that the data are normally distributed and we’re happy to go ahead with the parametric t-test.\nNext, the descriptives plot:\n\nThis is a really useful visualisation of our test. We can see the mean of the data (black dot) and the test value (dotted line). We also have the 95% confidence intervals of the estimated mean. We can see that the confidence intervals overlap with the test value - this tells us that it is likely our test will not be significant - according to our confidence intervals, the true population mean might include our test value zero which would indicate no real difference.\n\n\n\n\n\n\nKey step\n\n\n\nChange your data filter to compute the one-sample t-test for the ‘High’ SES_Story condition. The test should update once your filter is on without you having to respecify the test.\n\n\n\n\n\n\n\n\n Data Skills - Is the mean position of participants in the ‘High’ condition significantly different to zero?\n\n\n\n\n\nYes, we do have a significant difference - we could report our test as follows\n\nt(74) = -5.35, p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n Data Skills - Is it valid to use parametric statistics on this data?\n\n\n\n\n\nThe Shapiro Wilk statistic has a p-value of 0.038 which is less than p = 0.05. This suggets that our data are not normally distributed. We should inspect the data in more detail and consider using a non-parametric alternative.\nThe Wilcoxon Rank test is the non-parametric alternative for One Sample t-test. You can add this to the analysis to see if it gives the same result as the parametric test.\n\n\n\n\n\n\n\n\n\n Data Skills - Do the confidence intervals of the estimated mean cross zero?\n\n\n\n\n\nNo - in the ‘High’ condition the confidence intervals of the mean are a long way from zero.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nGo back to your notes from the start of the session. Do the results match your predictions about whether a UK data sample would expect a high or low socio-economic status people to live in the north or south of the city?",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#reproducible-one-sample-t-tests",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#reproducible-one-sample-t-tests",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Finally - we should make a reproducible version of our analysis using Rj.\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and add the following line to load the dplyr library.\nlibrary(dplyr)\n\n\nFirstly, we’ll need to replicate our data filter to select only the ‘High’ or ‘Low’ conditions for our analysis. We covered this in detail last week - if you need a refresher, take a look at the information on selecting rows of data using the filter function from week 2\nLet’s build a filter to select only the ‘High’ condition.\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\nNext - turn on Jamovi’s syntax mode to display the R code associated with our analyses. This should create some R code at the top of each results block.\n\nWe can right click on this code to copy it before pasting it into the Rj window. Our code should then look like this:\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\n\njmv::ttestOneS(\n    data = data,\n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\n\n\n\n\n\n\nWarning\n\n\n\nSome people have encountered an issue with running this code block - if your R session stalls whilst running the code above, please try again whilst removing plots=TRUE.\nApologies - we’re working on a fix for this.\n\n\nThis command runs the function ttestOneS which computes the One Sample t-test. Jamovi has automatically detected that we want to add the normality test and the descriptive plots and included these options in the function call (see norm=TRUE and plots=TRUE).\nThere is a keyword argument in R for every option in Jamovi - take a look at the jmv library documentation for ttestOneS to see a full list.\nBefore running the code we need to make one adjustment. The function runs on the full dataset data by default but we want to use our new filtered data high_ses instead. Update the function to take in the filtered data as an input.\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\n\njmv::ttestOneS(\n    data = high_ses,\n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\nNow running the code should output result tables that match our previous analysis. Importantly the form of data preprocessing (the filtering) and the test itself are all clearly specified in the code. Making it easier for others to understand what analysis we have run.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you add a code block to compute the one-sample t-test for only the ‘Low’ SES_Story condition\n\n\nRemember that you’ll need to write a new filter line to create a copy of the dataset with only the ‘Low’ values for SES_Story. You can save this into a variable and pass it in as the data argument to ttestOneS\n\n\nYour final code should look like this:\nlow_ses &lt;- filter(data, SES_Story == 'Low')\n\njmv::ttestOneS(\n    data = low_ses, \n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\n\n\n\n\n\nYou can keep adding R code to your script to run multiple analyses at once, this way we can run the t-test for both conditions with out having to go back and forth in Jamovi toggling the data filters on and off.\nIt takes a little more setup but the R code has some big advantages once you get used to it!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#summary",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#summary",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "We’ve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "This week we will explore how to use the jmv package in R to test hypotheses about a dataset using t-tests and to compute effect sizes corresponding to those tests. We will use those effect sizes to help interpret the sensitivity and power of our experiments.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nCohen’s d effect size\n\n\n\nPower analysis\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nUse mutate() in R to compute a new variable\n\n\n\nCompute t-tests using the jmv R package\n\n\n\nRun a power analysis in Rj\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nUnderstand and validate code written by someone else\n\n\n\nUse power analysis to recommend a sample size for a replication\n\n\n\n\n\n\n\n\n\n\n\nDataset\n\n\n\nPlease note, this is the same dataset as we used in week 4 - but make sure that you load in the rmb-week-3_lecture-quiz-data_ai-faces-fixed.omv file from this week and NOT the week 4 file. It contains a small fix that we need for this week.\n\n\nWe’ll be working with the dataset we collected after the break during the lecture in Week 3. This is a partial replication of a study published in the journal Psychological Science (Miller et al. 2023). The original paper found the following results summarised in the abstract.\n\nRecent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 (N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces—a phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 (N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI.\n\nWe ran a quiz that students in the lecture could join on their phones/laptops. There were two parts to the quiz:\nThe faces section included 12 faces that were either photographs or real people or AI generated images of people. The stimuli were taked from the materials released by (Miller et al. 2023) on the Open Science Framework. Responses were either ‘Real person’ or ‘AI generated’.\n\nThe confidence section asked participants to rate their confidence in their ability to do the following three things.\n\nDistinguish real photos of people from AI generated images (Experimental condition)\nDistinguish photos of happy people from photos of sad people (Emotional control condition)\nDistinguish photos of people you used to know in primary school from strangers (Memory control condition)\n\nScores were recorded on a scale from 1 (Completely confidence) to 10 (Not at all confident). The confidence section was repeated before and after the faces section to see if participants confidence changed as a result of doing the AI faces task\n\n\n\nThis week we will explore some factors that influence t-stats and p-values whilst introducing the concept of loops in R. We will finish by using effect sizes to compute a power analysis.\n\n\n\nLast week, we used Jamovi to create several new variables that allowed us to run our analyses. We can do the same in a more transparent and reproducible way using R code.\nWe need to create both our proportion of correct faces across the 12 stimuli in one variable and a grouping variable which indicates whether each participant was confident in their ability to tell the difference between AI faces and real faces.\nWe can do both with the mutate() from the dplyr library, this provides functionality that lets us create, modify, and delete columns within a dataset. Take a look at the official documentation for mutate for more information.\nWhen computing a new varaible with mutate() we three pieces of information.\n\nThe dataset to work on\nThe name of the new variable to be created\nThe definition of how to compute the new variable for existing variables.\n\n\n\nLet’s start with our grouping variable. Our three pieces of information are\n\nThe dataset to work on - is data, which refers to the original datasheet loaded into Jamovi\nThe name of the new variable - is ConfidentBefore, the same as we used last week\nThe definition - is AIConfidenceBefore&lt;4, again the same as we used last week\n\nWe can combine these into a single line to create the variable. Note that we save the result into a new variable named my_data, we’ll use this from now on to avoid confusion with the original datasheet.\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\nWe can check that this has done what we expected by running head to see the first few rows of the dataset.\nhead(my_data)\nThe result should look like this:\n\nWe can see on in the results on the right hand side that the additional colume ConfidentBefore now appears with TRUE and FALSE values for each participant.\n\n\n\nLet’s do the same for total proportion of correctly identified faces. We can use the same principle as we used for the grouping variable and use mutate() along with the variable definition and name to create our new column.\nThis definition is pretty long as we have 12 different faces to work with! You can copy the whole line using the copy icon in the right hand side of the code box. Take a moment to look through and understand each part: which part is the Dataset, the Name and the Definition?\nmy_data &lt;- mutate(my_data, PropFacesCorrect = (Face1_Real + Face2_Real + Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face7_Real + Face8_Real + Face9_Real + Face10_Real + Face11_AI + Face12_AI) / 12)\nand validate the overall result using head() to make sure that my_data now has the column we expected.\n\nThe final column of the dataset is now PropFacesCorrect and contains the proportion of face trials that participant got right.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute variables containing the proportion of correct responses for photos of real people and AI generated faces separately?\nStore the results in PropRealCorrectFaces for real faces and PropAICorrectFaces for AI generated faces.\n\n\nWe can use the code we wrote to compute the proportion of correct responses for all faces as a starting point.\nThink about how you could modify this line to compute the result for either real or AI faces on their own? What would you need to change in the code?\n\n\nThe following code will compute the variables\nmy_data &lt;- mutate(my_data, PropRealFacesCorrect = (Face1_Real + Face2_Real + Face7_Real + Face8_Real + Face9_Real + Face10_Real) / 6)\nmy_data &lt;- mutate(my_data, PropAIFacesCorrect = (Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face11_AI + Face12_AI) / 6)\nWe’ve separated out the columns to sum all the real or AI faces together and changed the division to divide the results by 6 rather than 12.\nChecking the results with head() should produce the following outputs with four additional columns!\n\n\n\n\n\n\n\n\n\n\n\nWe have the ingredients for our hypothesis test. Let’s use R to explore the following hypothesis (Hypothesis 2 from week 4).\n\nConfident people are better at distinguishing AI faces from real faces\n\nWe can compute this test using the following code that calls ttestIS() - this is the function that computes independent samples t-tests. Read more about it on the ttestIS documentation page.\nYou can click the numbers by the definitions at the bottom to highlight the corresponding part of the code.\n1jmv::ttestIS(\n2    formula = PropFacesCorrect ~ ConfidentBefore,\n3    data = my_data,\n4    effectSize = TRUE)\n\n1\n\njmv::ttestIS is the name of the R function that Jamovi uses to compute independent samples t-tests\n\n2\n\nThis tells ttestIS the formula that defines the test we want to run.\n\n3\n\nThis tells ttestIS which dataset we want to analyse\n\n4\n\nThis adds an additional effect size computation to the results\n\n\nMost of this will be familiar from previous weeks, but let’s think about the formula in a little more detail.\nIn R, the tilde (~) is used in formula definitions to specify the relationship between variables, particularly in statistical modeling and data analysis. Here, the tilde separates the dependent variable (response) from the independent variables (predictors). In our example, PropFacesCorrect is the dependent variable and ConfidentBefore is our independent variable (grouping variable) - so this formula\nPropFacesCorrect ~ ConfidentBefore\nis essentially telling ttestIS() to “Compute the ttest on PropFacesCorrect using ConfidentBefore as our groups”.\nWe can now run all our code to get the results of the ttest - the code clearly tells the reader how all the relevant variables were computed and what hypothesis test has been run all in one screen.\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nRead through the ttestIS documentation page. How could you change your code to add the following assumption checks\n\nTest for homogeneity of variance\nTest for normal distribution\n\n\n\nThe documentation page contains a list of all the possible information that we can pass as an input to our ttestIS() function call. Each item in the list corresponds to the options available in the Jamovi dialogue box.\nHave a look at the norm and eqv definitions. What would you need to add to the function call to run these additional checks?\n\n\nThe following code will compute all the checks in addition to the t-test.\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE,\n    norm = TRUE,\n    eqv = TRUE)                             \nThe results should look like this:\n\nDo the assumptions for Student’s t-test hold for this data?\n\n\n\n\n\n\nYour script might be getting quite long at this point! It is always a good idea to keep code neat and tidy where possible so that other people are able to read it, and so that we can read it if we come back to the analysis in the future.\nThere are many ways to keep things organised in a script. Here are two good hints, code comments can organise your script without changing the output and print() statements can help organise your code and the outputs.\n\n\nAny line of R code that starts with a hashtag is called a ‘comment’. The writing on this line is there for our information and will not be run by the computer. Adding code comments is a useful way to annotate your code indicating what each line or each section is doing. For example:\n# This is code comment\n\n# Compute which participants were more confident than the median\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\n\nThis will make it easier to understand what the coding is doing in future.\n\n\n\nWe can use the print() function to help organise our code as well. The text within the call to print() will not be executed by the computer but will simply be printed into the output console. This can be useful to break the output of your code into sections and to inlcude additional information about the analysis next to the outputs themselves.\nFor example:\nprint('Hypothesis 1')\nprint('Confident people are better at distinguishing AI faces from real faces')\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)\nThis code will print out information about the hypothesis next to the outputs.\nA full cleaned and commented version of our code might look like this.\n# Compute which participants were more confident than the median\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\n\n# Compute the proportion of all faces each participant got right\nmy_data &lt;- mutate(my_data, PropFacesCorrect = (Face1_Real + Face2_Real + Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face7_Real + Face8_Real + Face9_Real + Face10_Real + Face11_AI + Face12_AI) / 12)\n\n# Compute the proportion of REAL faces each participant got right\nmy_data &lt;- mutate(my_data, PropRealFacesCorrect = (Face1_Real + Face2_Real + Face7_Real + Face8_Real + Face9_Real + Face10_Real) / 6)\n\n# Compute the proportion of AI faces each participant got right\nmy_data &lt;- mutate(my_data, PropAIFacesCorrect = (Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face11_AI + Face12_AI) / 6)\n\n# Hypothesis test\nprint('Hypothesis 1')\nprint('Confident people are better at distinguishing AI faces from real faces')\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)\nand running this code produces the following outputs including the test from our print statements\n\nNice and clear what is happening at each stage! There is no perfect or ‘correct’ way to tidy up your code (though some people can get opinionated about this…). Choose a mix of comments and print statements that makes sense to you.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nExtend your code to test a second hypothesis.\n\nConfident participants are more accurate than unconfident participants when identifying photos of real faces.\n\nInclude some code comments and print statements\n\n\nThis will require a second indenpendent samples t-test asking whether the mean proportion correct value for only real faces is different between our confident and not-confident groups.\nUse the previous t-test as a starting point, can you copy this and modify it to do what we need?\n\n\nThe following code will test the new hypothesis with some comments and print statements\n# Hypothesis test\nprint('Hypothesis 2')\nprint('Confident participants are more accurate than unconfident participants when identifying photos of real faces.')\njmv::ttestIS(                                       \n    formula = PropRealFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)                       \nThe output of the whole script should now look like this.\n\nNote how the print statements help to separate the results into interpretable chunks.\n\n\n\n\n\n\nNow we’re going to explore how the size of the data sample changes the t-values that w\nIntroduce slice_sample Introduce simple loops\n\n\n\n\nImagine that of your friends has been running some analyses on this dataset and would like you to check over their work. They send you the following R code that they have put together.\n# Compute some new variables\nmy_data &lt;- mutate(data, ChangeAIConfidence = AIConfidenceBefore - AIConfidenceAfter)\nmy_data &lt;- mutate(my_data, ChangeEmoConfidence = EmoConfidenceBefore - EmoConfidenceAfter)\nmy_data &lt;- mutate(my_data, ChangeMemoryConfidence = MemoryConfidenceBefore - MemoryConfidenceAfter)\n\n# Run the tests\njmv::ttestOneS(\n    data = my_data,\n    vars = vars(ChangeAIConfidence, ChangeEmoConfidence, ChangeMemoryConfidence),\n    testValue = 0.0,\n    wilcoxon = TRUE,\n    desc = TRUE,\n    norm = TRUE)\nLet’s think about this code a bit, your friend didn’t really explain the plan…\n\n\n\n\n\n\n Data Skills - what variables are created?\n\n\n\n\n\nThe code creates three new variables, ChangeAIConfidence, ChangeEmoConfidence and ChangeMemoryConfidence - each is computed by subtracting the confidence values after the face task from the confidence values before the task.\nThese variables represent the change in confidence from completing the task.\n\n\n\n\n\n\n\n\n\n Data Skills - what statistical test is performed?\n\n\n\n\n\nThe code runs a one sample t-test comparing the data mean to 0.0 using the jamovi function jmv::ttestOneS(). The code runs the test on all three variables. We can see that the input vars is passed a list of all three variable names.\n\n\n\n\n\n\n\n\n\n Data Skills - what additional computations are performed?\n\n\n\n\n\nIn addition to the t-tests, the code runs a Wilcoxon Rank test, a Shapiro-Wilk test for normality and a descriptive statistics table.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and run this code before going any further! Take a moment to read through the code and decide what it is doing.\n\n\n\n\n\n\nClick here if you want to check that you have the right outputs.\n\n\n\n\n\nThe results should look like this:\n\n\n\n\n\n\nThey have written the following about the results.\n\nA one sample t-test found that the change in participant confidence in distinguishing AI faces from real faces was significantly reduced after performing the task. The difference in confidence before and after the task (M=-1.1, SD=2.7) was significantly below zero, t(76) = 3.538, p = 0.0007. In contrast there was no change after the task confidence in emotion recognition (M=-.017, SD=1.341), t(76) = -1.012, p = 0.313 and face memory (M=-0.02985, SD=1.044), t(76) = -0.234, p = 0.816.\n\nLet’s think about whether this is accurate.\n\n\n\n\n\n\n Data Skills - are the reported statistics correct? take your time to read carefully and compare to the outputs\n\n\n\n\n\nNo! there are a number of mistakes throughout this writing. How many of the following mistakes did you spot?\n\nA one sample t-test found that the change in participant confidence in distinguishing AI faces from real faces was significantly reduced after performing the task. The difference in confidence before and after the task (M=-1.1, SD=2.7) was significantly below zero, t(76) = 3.538, p = 0.0007. In contrast there was no change after the task confidence in emotion recognition (M=-.017, SD=1.341), t(76) = -1.012, p = 0.313 and face memory (M=-0.02985, SD=1.044), t(76) = -0.234, p = 0.816.\n\n\n3.538 -&gt; the sign of the t-values on this test is wrong. Take a look a the output table, this should be -3.538.\n76 -&gt; the degrees of freedom for the second and third tests are reported incorrectly (perhaps copied from the first test?). These should be 64 and 66 respectively\n\n\n\n\n\n\n\n\n\n\n Data Skills - was the correct test reported?\n\n\n\n\n\nAgain, no! there are mistakes here as well. The friend has done the right thing by computing the Shapiro-Wilk statistics but appears to have ignored them in the writing.\nShapiro-Wilk is significant for all three variables - we should strongly consider reporting the Wilcoxon Rank Sum test rather than Student’s t.\nThey could have run this analysis as a paired t-test, but the one sample t-test of the differences is also a valid approach.\n\n\n\nComputing statistics and accurately reporting the results takes a lot of precision! take your time when writing code and results sections - make sure to check and double check your work as you go…\n\n\n\nLet’s think more about effect sizes - computing effect sizes in Jamovi and R is really easy. Simply click the check box to add ‘Effect size’ under the ‘Addtional Statistics’ section of the Jamovi window - or add effectSize = TRUE to the function call in R (we’ve already been doing this above!).\n\n\n\n\n\n\nKey step\n\n\n\nUpdate your friend’s code to include computation of effect sizes - the results should appear in the t-test results table on the right.\n\n\n\nThere are effect sizes computed for every t-test and its non-parametric alternatives.\n\nCohen’s d is the parametric effect size corresponding to Student’s t-test\nRank Biserial Correlation is the non-parametric effect size corresponding to Wilcoxon’s test.\n\nThough they have methodological differences, these effect sizes can be interpreted in the same way as a measure of the magnitude of an effect.\nRemember that this is different to the t-value which is a measure of evidence against the null hypothesis. The important difference is that we can have strong evidence against the null either as the difference is large or if we have measured a small difference very precisely. The effect size only cares about the size of the difference between conditions.\nNote that Jamovi/R provide a ‘signed’ effect size indicating the direction of the effect in the same way that a t-statistic does. For the following work we can ignore this sign and focus on the magnitudes only. In other words, we’ll consider an effect size of +0.5 or -0.5 to indicate the same magitude of effect.\nThe creator of many of our classic effect size measures provided a guideline for what might be considered a ‘small’ or a ‘large’ effect.\n\n\n\nEffect Size\nCohen’s d\nInterpretation\n\n\n\n\nSmall\n0.2\nSmall effect\n\n\nMedium\n0.5\nMedium effect\n\n\nLarge\n0.8\nLarge effect\n\n\n\nThese can be useful guidelines - through they have been criticised strongly for both the arbitrary nature of the thresholds and for over-simplifying interpretation of effect sizes. We can still use them as an informal indicator to help us quickly interpret our results.\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeAIConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.403 is just below the thresold to be a ‘medium’ effect (ignoring the sign of the effect size). The effect size for the Rank Biserial correlation is similar.\n\n\n\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeEmoConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.126 is just below the thresold to be a ‘small’ effect (ignoring the sign of the effect size). The effect size for the Rank Biserial correlation is larger but still closer to small than medium.\n\n\n\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeMemoryConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.0286 is tiny! the Rank Biserial correlation is even smaller…\n\n\n\n\n\n\nOne core application for effect sizes is that we can use the results to inform our sample size planning for future studies. we can do this with four linked pieces of information\n\nEffect Size this is our effect size, usually Cohen’s d for analyses in RM-B\nSignificance Thresold - this is the ‘alpha’ level at which we would consider a result significant, usually 5% or 0.05.\nPower - this is probability of correctly detecting an effect if one exists in the data, we usually want 80% power.\nNumber of datapoints (N) - this is our sample size.\n\nThese four pieces of information are linked such that if we have any three of these values we can always compute the fourth.\nThis lets us do some very useful calculations to see how well our dataset is enabling us to answer our hypothesis and to plan for future studies. For examples\n\nIf we know the effect size, significance threshold and number of participants in a study, we can compute the power to tell us the probability that we can correctly detect an effect. This is known as ‘sensitivity’.\n\nor\n\nIf we want to compute how many participants we’ll need in a future study and we already know the effect size from previous literature, we can specify the power and significance threshold we want and compute the number of participants that we’ll need.\n\nVery useful! We can compute these different values using a Jamovi module known as ‘jpower’\n\n\n\n\n\n\nKey step - Install the jpower addon in Jamovi\n\n\n\nClick through the ‘Modules’ button on the top right of the Jamovi ribbon. Click to the list of ‘Available’ modules and install ‘jpower’\n\nClick through to open the jpower menu and select a ‘one-sample t-test’ from the options. You should see the following dialogue box\n\n\n\nLet’s take a moment to understand the options.\nFirst - notice that we have the four pieces of information listed near the top of the options.\n\n\n\njpower menu\nInterpretation\n\n\n\n\nMinimally interesting effect size (δ)\nEffect size\n\n\nMinimum desired power\nPower\n\n\nN\nNumber of participants\n\n\nα (type 1 error rate)\nSignificance threshold\n\n\n\nBy default, one of these options is grayed out and not modifible. This is as jpower works to compute one value at a time whilst asking you to specify the other three. By default, we’re computing N as specified by the ‘Calculate’ drop down menu at the top of the dialogue box. Try changing this to calculate one of the other values and notice that the greyed out box changes each time.\n\n\n\n\n\n\nKey step - Install the jpower addon in Jamovi\n\n\n\nLet’s run through an example. Say that we want to answer the following question:\n\nWhat is the probability the ChangeAIConfidence has been correctly detected in our previous analysis.\n\nWe should run through the following steps.\n\nSelect that we want to Calcuate ‘Power’ in the top drop down menu.\nSet the ‘Minimally interesting effect size (δ)’ to our observed effect size from before. This is 0.429 (ignoring the sign)\nSet the ‘N’ to 77 - this is the number of participants in the test (degrees of freedom are 76)\nSet ‘α (type 1 error rate)’ to 0.05 - this is usually the default\n\nThe results should appear automatically.\n\n\n\njpower gives us some really useful explanatory text along with the results. Read this carefully to understand what is going on. Firstly,\n\nThe purpose of a power analysis is to evaluate the sensitivity of a design and test. You have chosen to calculate the sensitivity of the chosen design for detecting the specified effect size.\n\nThis clarifies the type of analyses we have run - we want to compute the sensitivity of our analysis given particular data and effect size. Next the results are summarised\n\nA design with a sample size of 77 can detect effect sizes of δ ≥ 0.429 with a probability of at least 0.96, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α= 0.05.\n\nThis is the critical information. This tells us that the power is 0.96 for our analysis, indicating that we have a 96% chance of correctly detecting an effect size at least as large as the one from our test. Great news!\nFinally, the ‘Power by Effect size’ table tells us the power we would have for different effect sizes. For example, in this case if we have an effect size of 0.3 - we would have power between 50% and 80%. If the effect size were 0.2 we would have less than 50% power.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat is the sensitivity (power) of the effect size estimated by Cohen’s d for the ChangeEmoConfidence variable?\n\n\nTake a look at our previous results. Cohen’d d for ChangeEmoConfidence was -0.126.\n\n\nOur estimated power for the ChangeEmoConfidence is only 19.4%.\n\nA design with a sample size of 77 can detect effect sizes of δ ≥ 0.126 with a probability of at least0.194, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α = 0.05.\n\n\nThis is the practical consequence of effect sizes! The smaller effect for ChangeEmoConfidence is harder to detect than the larger value for ChangeAIConfidence so we have a much lower chance of getting the right answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nIt might be the case that our face recognition really does change participants confidence in recognising emotional faces, and our study simply doesn’t have enough participants to detect the change with statistical confidence. Perhaps we should consider running a larger study?\nIf we assume that our value for Cohen’s d is accurate and we assume that there really is an effect to be detected (these are pretty brave assumptions!), how many participants would a future study need to have 80% chance of correctly detecting the effect?\n\n\nWe’ll need to change our analysis to calculate ‘N’ rather than ‘Power’ using the drop down menu at the top. Can you specify the power?\n\n\nWe would need at least 497 participants to reject the null hypothesis if the estimated effect size for the change in emotional face recognition before and after our task is accurate! that is a lot….\n\nWe would need a sample size of 497 to reliably (with probability greater than 0.8) detect an effect size of δ ≥ 0.126, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α = 0.05.\n\n\nThis information suggests that this looking for differences in emotional face confidence is probably not worth pursuing futher. Our experiment wasn’t perfect but this effect is so small that it is likely to be of little to no practical significance.\n\n\n\n\n\n\n\n\nWe have explored some new methods for creating new variables and running t-tests with effect sizes using R code. The R code we wrote is a really clear way to specify how variables were manipulated and which tests were run. Quite complex analyses with several stages can be clearly expressed this way.\nFinally, we used effect sizes to explore the power and sensitivity of our experiment and explore the possiblity of a follow up study.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#the-dataset",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#the-dataset",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Dataset\n\n\n\nPlease note, this is the same dataset as we used in week 4 - but make sure that you load in the rmb-week-3_lecture-quiz-data_ai-faces-fixed.omv file from this week and NOT the week 4 file. It contains a small fix that we need for this week.\n\n\nWe’ll be working with the dataset we collected after the break during the lecture in Week 3. This is a partial replication of a study published in the journal Psychological Science (Miller et al. 2023). The original paper found the following results summarised in the abstract.\n\nRecent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 (N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces—a phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 (N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI.\n\nWe ran a quiz that students in the lecture could join on their phones/laptops. There were two parts to the quiz:\nThe faces section included 12 faces that were either photographs or real people or AI generated images of people. The stimuli were taked from the materials released by (Miller et al. 2023) on the Open Science Framework. Responses were either ‘Real person’ or ‘AI generated’.\n\nThe confidence section asked participants to rate their confidence in their ability to do the following three things.\n\nDistinguish real photos of people from AI generated images (Experimental condition)\nDistinguish photos of happy people from photos of sad people (Emotional control condition)\nDistinguish photos of people you used to know in primary school from strangers (Memory control condition)\n\nScores were recorded on a scale from 1 (Completely confidence) to 10 (Not at all confident). The confidence section was repeated before and after the faces section to see if participants confidence changed as a result of doing the AI faces task",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#the-challenge",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#the-challenge",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "This week we will explore some factors that influence t-stats and p-values whilst introducing the concept of loops in R. We will finish by using effect sizes to compute a power analysis.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-new-variables-with-mutate",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-new-variables-with-mutate",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Last week, we used Jamovi to create several new variables that allowed us to run our analyses. We can do the same in a more transparent and reproducible way using R code.\nWe need to create both our proportion of correct faces across the 12 stimuli in one variable and a grouping variable which indicates whether each participant was confident in their ability to tell the difference between AI faces and real faces.\nWe can do both with the mutate() from the dplyr library, this provides functionality that lets us create, modify, and delete columns within a dataset. Take a look at the official documentation for mutate for more information.\nWhen computing a new varaible with mutate() we three pieces of information.\n\nThe dataset to work on\nThe name of the new variable to be created\nThe definition of how to compute the new variable for existing variables.\n\n\n\nLet’s start with our grouping variable. Our three pieces of information are\n\nThe dataset to work on - is data, which refers to the original datasheet loaded into Jamovi\nThe name of the new variable - is ConfidentBefore, the same as we used last week\nThe definition - is AIConfidenceBefore&lt;4, again the same as we used last week\n\nWe can combine these into a single line to create the variable. Note that we save the result into a new variable named my_data, we’ll use this from now on to avoid confusion with the original datasheet.\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\nWe can check that this has done what we expected by running head to see the first few rows of the dataset.\nhead(my_data)\nThe result should look like this:\n\nWe can see on in the results on the right hand side that the additional colume ConfidentBefore now appears with TRUE and FALSE values for each participant.\n\n\n\nLet’s do the same for total proportion of correctly identified faces. We can use the same principle as we used for the grouping variable and use mutate() along with the variable definition and name to create our new column.\nThis definition is pretty long as we have 12 different faces to work with! You can copy the whole line using the copy icon in the right hand side of the code box. Take a moment to look through and understand each part: which part is the Dataset, the Name and the Definition?\nmy_data &lt;- mutate(my_data, PropFacesCorrect = (Face1_Real + Face2_Real + Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face7_Real + Face8_Real + Face9_Real + Face10_Real + Face11_AI + Face12_AI) / 12)\nand validate the overall result using head() to make sure that my_data now has the column we expected.\n\nThe final column of the dataset is now PropFacesCorrect and contains the proportion of face trials that participant got right.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute variables containing the proportion of correct responses for photos of real people and AI generated faces separately?\nStore the results in PropRealCorrectFaces for real faces and PropAICorrectFaces for AI generated faces.\n\n\nWe can use the code we wrote to compute the proportion of correct responses for all faces as a starting point.\nThink about how you could modify this line to compute the result for either real or AI faces on their own? What would you need to change in the code?\n\n\nThe following code will compute the variables\nmy_data &lt;- mutate(my_data, PropRealFacesCorrect = (Face1_Real + Face2_Real + Face7_Real + Face8_Real + Face9_Real + Face10_Real) / 6)\nmy_data &lt;- mutate(my_data, PropAIFacesCorrect = (Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face11_AI + Face12_AI) / 6)\nWe’ve separated out the columns to sum all the real or AI faces together and changed the division to divide the results by 6 rather than 12.\nChecking the results with head() should produce the following outputs with four additional columns!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-a-t-test-using-r",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-a-t-test-using-r",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "We have the ingredients for our hypothesis test. Let’s use R to explore the following hypothesis (Hypothesis 2 from week 4).\n\nConfident people are better at distinguishing AI faces from real faces\n\nWe can compute this test using the following code that calls ttestIS() - this is the function that computes independent samples t-tests. Read more about it on the ttestIS documentation page.\nYou can click the numbers by the definitions at the bottom to highlight the corresponding part of the code.\n1jmv::ttestIS(\n2    formula = PropFacesCorrect ~ ConfidentBefore,\n3    data = my_data,\n4    effectSize = TRUE)\n\n1\n\njmv::ttestIS is the name of the R function that Jamovi uses to compute independent samples t-tests\n\n2\n\nThis tells ttestIS the formula that defines the test we want to run.\n\n3\n\nThis tells ttestIS which dataset we want to analyse\n\n4\n\nThis adds an additional effect size computation to the results\n\n\nMost of this will be familiar from previous weeks, but let’s think about the formula in a little more detail.\nIn R, the tilde (~) is used in formula definitions to specify the relationship between variables, particularly in statistical modeling and data analysis. Here, the tilde separates the dependent variable (response) from the independent variables (predictors). In our example, PropFacesCorrect is the dependent variable and ConfidentBefore is our independent variable (grouping variable) - so this formula\nPropFacesCorrect ~ ConfidentBefore\nis essentially telling ttestIS() to “Compute the ttest on PropFacesCorrect using ConfidentBefore as our groups”.\nWe can now run all our code to get the results of the ttest - the code clearly tells the reader how all the relevant variables were computed and what hypothesis test has been run all in one screen.\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nRead through the ttestIS documentation page. How could you change your code to add the following assumption checks\n\nTest for homogeneity of variance\nTest for normal distribution\n\n\n\nThe documentation page contains a list of all the possible information that we can pass as an input to our ttestIS() function call. Each item in the list corresponds to the options available in the Jamovi dialogue box.\nHave a look at the norm and eqv definitions. What would you need to add to the function call to run these additional checks?\n\n\nThe following code will compute all the checks in addition to the t-test.\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE,\n    norm = TRUE,\n    eqv = TRUE)                             \nThe results should look like this:\n\nDo the assumptions for Student’s t-test hold for this data?\n\n\n\n\n\n\nYour script might be getting quite long at this point! It is always a good idea to keep code neat and tidy where possible so that other people are able to read it, and so that we can read it if we come back to the analysis in the future.\nThere are many ways to keep things organised in a script. Here are two good hints, code comments can organise your script without changing the output and print() statements can help organise your code and the outputs.\n\n\nAny line of R code that starts with a hashtag is called a ‘comment’. The writing on this line is there for our information and will not be run by the computer. Adding code comments is a useful way to annotate your code indicating what each line or each section is doing. For example:\n# This is code comment\n\n# Compute which participants were more confident than the median\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\n\nThis will make it easier to understand what the coding is doing in future.\n\n\n\nWe can use the print() function to help organise our code as well. The text within the call to print() will not be executed by the computer but will simply be printed into the output console. This can be useful to break the output of your code into sections and to inlcude additional information about the analysis next to the outputs themselves.\nFor example:\nprint('Hypothesis 1')\nprint('Confident people are better at distinguishing AI faces from real faces')\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)\nThis code will print out information about the hypothesis next to the outputs.\nA full cleaned and commented version of our code might look like this.\n# Compute which participants were more confident than the median\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\n\n# Compute the proportion of all faces each participant got right\nmy_data &lt;- mutate(my_data, PropFacesCorrect = (Face1_Real + Face2_Real + Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face7_Real + Face8_Real + Face9_Real + Face10_Real + Face11_AI + Face12_AI) / 12)\n\n# Compute the proportion of REAL faces each participant got right\nmy_data &lt;- mutate(my_data, PropRealFacesCorrect = (Face1_Real + Face2_Real + Face7_Real + Face8_Real + Face9_Real + Face10_Real) / 6)\n\n# Compute the proportion of AI faces each participant got right\nmy_data &lt;- mutate(my_data, PropAIFacesCorrect = (Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face11_AI + Face12_AI) / 6)\n\n# Hypothesis test\nprint('Hypothesis 1')\nprint('Confident people are better at distinguishing AI faces from real faces')\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)\nand running this code produces the following outputs including the test from our print statements\n\nNice and clear what is happening at each stage! There is no perfect or ‘correct’ way to tidy up your code (though some people can get opinionated about this…). Choose a mix of comments and print statements that makes sense to you.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nExtend your code to test a second hypothesis.\n\nConfident participants are more accurate than unconfident participants when identifying photos of real faces.\n\nInclude some code comments and print statements\n\n\nThis will require a second indenpendent samples t-test asking whether the mean proportion correct value for only real faces is different between our confident and not-confident groups.\nUse the previous t-test as a starting point, can you copy this and modify it to do what we need?\n\n\nThe following code will test the new hypothesis with some comments and print statements\n# Hypothesis test\nprint('Hypothesis 2')\nprint('Confident participants are more accurate than unconfident participants when identifying photos of real faces.')\njmv::ttestIS(                                       \n    formula = PropRealFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)                       \nThe output of the whole script should now look like this.\n\nNote how the print statements help to separate the results into interpretable chunks.\n\n\n\n\n\n\nNow we’re going to explore how the size of the data sample changes the t-values that w\nIntroduce slice_sample Introduce simple loops",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#analysis-from-a-friend",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#analysis-from-a-friend",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Imagine that of your friends has been running some analyses on this dataset and would like you to check over their work. They send you the following R code that they have put together.\n# Compute some new variables\nmy_data &lt;- mutate(data, ChangeAIConfidence = AIConfidenceBefore - AIConfidenceAfter)\nmy_data &lt;- mutate(my_data, ChangeEmoConfidence = EmoConfidenceBefore - EmoConfidenceAfter)\nmy_data &lt;- mutate(my_data, ChangeMemoryConfidence = MemoryConfidenceBefore - MemoryConfidenceAfter)\n\n# Run the tests\njmv::ttestOneS(\n    data = my_data,\n    vars = vars(ChangeAIConfidence, ChangeEmoConfidence, ChangeMemoryConfidence),\n    testValue = 0.0,\n    wilcoxon = TRUE,\n    desc = TRUE,\n    norm = TRUE)\nLet’s think about this code a bit, your friend didn’t really explain the plan…\n\n\n\n\n\n\n Data Skills - what variables are created?\n\n\n\n\n\nThe code creates three new variables, ChangeAIConfidence, ChangeEmoConfidence and ChangeMemoryConfidence - each is computed by subtracting the confidence values after the face task from the confidence values before the task.\nThese variables represent the change in confidence from completing the task.\n\n\n\n\n\n\n\n\n\n Data Skills - what statistical test is performed?\n\n\n\n\n\nThe code runs a one sample t-test comparing the data mean to 0.0 using the jamovi function jmv::ttestOneS(). The code runs the test on all three variables. We can see that the input vars is passed a list of all three variable names.\n\n\n\n\n\n\n\n\n\n Data Skills - what additional computations are performed?\n\n\n\n\n\nIn addition to the t-tests, the code runs a Wilcoxon Rank test, a Shapiro-Wilk test for normality and a descriptive statistics table.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and run this code before going any further! Take a moment to read through the code and decide what it is doing.\n\n\n\n\n\n\nClick here if you want to check that you have the right outputs.\n\n\n\n\n\nThe results should look like this:\n\n\n\n\n\n\nThey have written the following about the results.\n\nA one sample t-test found that the change in participant confidence in distinguishing AI faces from real faces was significantly reduced after performing the task. The difference in confidence before and after the task (M=-1.1, SD=2.7) was significantly below zero, t(76) = 3.538, p = 0.0007. In contrast there was no change after the task confidence in emotion recognition (M=-.017, SD=1.341), t(76) = -1.012, p = 0.313 and face memory (M=-0.02985, SD=1.044), t(76) = -0.234, p = 0.816.\n\nLet’s think about whether this is accurate.\n\n\n\n\n\n\n Data Skills - are the reported statistics correct? take your time to read carefully and compare to the outputs\n\n\n\n\n\nNo! there are a number of mistakes throughout this writing. How many of the following mistakes did you spot?\n\nA one sample t-test found that the change in participant confidence in distinguishing AI faces from real faces was significantly reduced after performing the task. The difference in confidence before and after the task (M=-1.1, SD=2.7) was significantly below zero, t(76) = 3.538, p = 0.0007. In contrast there was no change after the task confidence in emotion recognition (M=-.017, SD=1.341), t(76) = -1.012, p = 0.313 and face memory (M=-0.02985, SD=1.044), t(76) = -0.234, p = 0.816.\n\n\n3.538 -&gt; the sign of the t-values on this test is wrong. Take a look a the output table, this should be -3.538.\n76 -&gt; the degrees of freedom for the second and third tests are reported incorrectly (perhaps copied from the first test?). These should be 64 and 66 respectively\n\n\n\n\n\n\n\n\n\n\n Data Skills - was the correct test reported?\n\n\n\n\n\nAgain, no! there are mistakes here as well. The friend has done the right thing by computing the Shapiro-Wilk statistics but appears to have ignored them in the writing.\nShapiro-Wilk is significant for all three variables - we should strongly consider reporting the Wilcoxon Rank Sum test rather than Student’s t.\nThey could have run this analysis as a paired t-test, but the one sample t-test of the differences is also a valid approach.\n\n\n\nComputing statistics and accurately reporting the results takes a lot of precision! take your time when writing code and results sections - make sure to check and double check your work as you go…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-effect-sizes-for-t-tests",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-effect-sizes-for-t-tests",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Let’s think more about effect sizes - computing effect sizes in Jamovi and R is really easy. Simply click the check box to add ‘Effect size’ under the ‘Addtional Statistics’ section of the Jamovi window - or add effectSize = TRUE to the function call in R (we’ve already been doing this above!).\n\n\n\n\n\n\nKey step\n\n\n\nUpdate your friend’s code to include computation of effect sizes - the results should appear in the t-test results table on the right.\n\n\n\nThere are effect sizes computed for every t-test and its non-parametric alternatives.\n\nCohen’s d is the parametric effect size corresponding to Student’s t-test\nRank Biserial Correlation is the non-parametric effect size corresponding to Wilcoxon’s test.\n\nThough they have methodological differences, these effect sizes can be interpreted in the same way as a measure of the magnitude of an effect.\nRemember that this is different to the t-value which is a measure of evidence against the null hypothesis. The important difference is that we can have strong evidence against the null either as the difference is large or if we have measured a small difference very precisely. The effect size only cares about the size of the difference between conditions.\nNote that Jamovi/R provide a ‘signed’ effect size indicating the direction of the effect in the same way that a t-statistic does. For the following work we can ignore this sign and focus on the magnitudes only. In other words, we’ll consider an effect size of +0.5 or -0.5 to indicate the same magitude of effect.\nThe creator of many of our classic effect size measures provided a guideline for what might be considered a ‘small’ or a ‘large’ effect.\n\n\n\nEffect Size\nCohen’s d\nInterpretation\n\n\n\n\nSmall\n0.2\nSmall effect\n\n\nMedium\n0.5\nMedium effect\n\n\nLarge\n0.8\nLarge effect\n\n\n\nThese can be useful guidelines - through they have been criticised strongly for both the arbitrary nature of the thresholds and for over-simplifying interpretation of effect sizes. We can still use them as an informal indicator to help us quickly interpret our results.\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeAIConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.403 is just below the thresold to be a ‘medium’ effect (ignoring the sign of the effect size). The effect size for the Rank Biserial correlation is similar.\n\n\n\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeEmoConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.126 is just below the thresold to be a ‘small’ effect (ignoring the sign of the effect size). The effect size for the Rank Biserial correlation is larger but still closer to small than medium.\n\n\n\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeMemoryConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.0286 is tiny! the Rank Biserial correlation is even smaller…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#use-an-effect-size-for-a-sensitivity-power-analysis",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#use-an-effect-size-for-a-sensitivity-power-analysis",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "One core application for effect sizes is that we can use the results to inform our sample size planning for future studies. we can do this with four linked pieces of information\n\nEffect Size this is our effect size, usually Cohen’s d for analyses in RM-B\nSignificance Thresold - this is the ‘alpha’ level at which we would consider a result significant, usually 5% or 0.05.\nPower - this is probability of correctly detecting an effect if one exists in the data, we usually want 80% power.\nNumber of datapoints (N) - this is our sample size.\n\nThese four pieces of information are linked such that if we have any three of these values we can always compute the fourth.\nThis lets us do some very useful calculations to see how well our dataset is enabling us to answer our hypothesis and to plan for future studies. For examples\n\nIf we know the effect size, significance threshold and number of participants in a study, we can compute the power to tell us the probability that we can correctly detect an effect. This is known as ‘sensitivity’.\n\nor\n\nIf we want to compute how many participants we’ll need in a future study and we already know the effect size from previous literature, we can specify the power and significance threshold we want and compute the number of participants that we’ll need.\n\nVery useful! We can compute these different values using a Jamovi module known as ‘jpower’\n\n\n\n\n\n\nKey step - Install the jpower addon in Jamovi\n\n\n\nClick through the ‘Modules’ button on the top right of the Jamovi ribbon. Click to the list of ‘Available’ modules and install ‘jpower’\n\nClick through to open the jpower menu and select a ‘one-sample t-test’ from the options. You should see the following dialogue box\n\n\n\nLet’s take a moment to understand the options.\nFirst - notice that we have the four pieces of information listed near the top of the options.\n\n\n\njpower menu\nInterpretation\n\n\n\n\nMinimally interesting effect size (δ)\nEffect size\n\n\nMinimum desired power\nPower\n\n\nN\nNumber of participants\n\n\nα (type 1 error rate)\nSignificance threshold\n\n\n\nBy default, one of these options is grayed out and not modifible. This is as jpower works to compute one value at a time whilst asking you to specify the other three. By default, we’re computing N as specified by the ‘Calculate’ drop down menu at the top of the dialogue box. Try changing this to calculate one of the other values and notice that the greyed out box changes each time.\n\n\n\n\n\n\nKey step - Install the jpower addon in Jamovi\n\n\n\nLet’s run through an example. Say that we want to answer the following question:\n\nWhat is the probability the ChangeAIConfidence has been correctly detected in our previous analysis.\n\nWe should run through the following steps.\n\nSelect that we want to Calcuate ‘Power’ in the top drop down menu.\nSet the ‘Minimally interesting effect size (δ)’ to our observed effect size from before. This is 0.429 (ignoring the sign)\nSet the ‘N’ to 77 - this is the number of participants in the test (degrees of freedom are 76)\nSet ‘α (type 1 error rate)’ to 0.05 - this is usually the default\n\nThe results should appear automatically.\n\n\n\njpower gives us some really useful explanatory text along with the results. Read this carefully to understand what is going on. Firstly,\n\nThe purpose of a power analysis is to evaluate the sensitivity of a design and test. You have chosen to calculate the sensitivity of the chosen design for detecting the specified effect size.\n\nThis clarifies the type of analyses we have run - we want to compute the sensitivity of our analysis given particular data and effect size. Next the results are summarised\n\nA design with a sample size of 77 can detect effect sizes of δ ≥ 0.429 with a probability of at least 0.96, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α= 0.05.\n\nThis is the critical information. This tells us that the power is 0.96 for our analysis, indicating that we have a 96% chance of correctly detecting an effect size at least as large as the one from our test. Great news!\nFinally, the ‘Power by Effect size’ table tells us the power we would have for different effect sizes. For example, in this case if we have an effect size of 0.3 - we would have power between 50% and 80%. If the effect size were 0.2 we would have less than 50% power.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat is the sensitivity (power) of the effect size estimated by Cohen’s d for the ChangeEmoConfidence variable?\n\n\nTake a look at our previous results. Cohen’d d for ChangeEmoConfidence was -0.126.\n\n\nOur estimated power for the ChangeEmoConfidence is only 19.4%.\n\nA design with a sample size of 77 can detect effect sizes of δ ≥ 0.126 with a probability of at least0.194, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α = 0.05.\n\n\nThis is the practical consequence of effect sizes! The smaller effect for ChangeEmoConfidence is harder to detect than the larger value for ChangeAIConfidence so we have a much lower chance of getting the right answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nIt might be the case that our face recognition really does change participants confidence in recognising emotional faces, and our study simply doesn’t have enough participants to detect the change with statistical confidence. Perhaps we should consider running a larger study?\nIf we assume that our value for Cohen’s d is accurate and we assume that there really is an effect to be detected (these are pretty brave assumptions!), how many participants would a future study need to have 80% chance of correctly detecting the effect?\n\n\nWe’ll need to change our analysis to calculate ‘N’ rather than ‘Power’ using the drop down menu at the top. Can you specify the power?\n\n\nWe would need at least 497 participants to reject the null hypothesis if the estimated effect size for the change in emotional face recognition before and after our task is accurate! that is a lot….\n\nWe would need a sample size of 497 to reliably (with probability greater than 0.8) detect an effect size of δ ≥ 0.126, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α = 0.05.\n\n\nThis information suggests that this looking for differences in emotional face confidence is probably not worth pursuing futher. Our experiment wasn’t perfect but this effect is so small that it is likely to be of little to no practical significance.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#summary",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#summary",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "We have explored some new methods for creating new variables and running t-tests with effect sizes using R code. The R code we wrote is a really clear way to specify how variables were manipulated and which tests were run. Quite complex analyses with several stages can be clearly expressed this way.\nFinally, we used effect sizes to explore the power and sensitivity of our experiment and explore the possiblity of a follow up study.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rma_week-8_introduction-to-jamovi.html",
    "href": "tutorials/rma_week-8_introduction-to-jamovi.html",
    "title": "Week 8 : Introduction to Jamovi",
    "section": "",
    "text": "This week you will be introduced to Jamovi. You will use Jamovi throughout your degree for statistical analysis, so it is very important to get familiar with it!\nJAMOVI has inbuilt functions which can be used to investigate and manipulate data, and to perform majority of statistical tests used in psychological research. Each task you perform in JAMOVI will generate an output which will support your research decisions, such as choosing a statistical test or deciding whether to reject a hypothesis.\nJAMOVI will already be on the computers on Campus but if you want to use it at home too you will need to download and install it on to your personal laptop or desktop PC. The software is free and we recommend that you install it at home because you will need to use it a lot over the next three years, including for many of your assignments. As you can see on the JAMOVI website, there are versions for WindowsOS, MacOS and ChromeOS: https://www.jamovi.org/download.html\nYou can find instructions for how to install JAMOVI on your personal laptop or desktop PC here: https://docs.jamovi.org/_pages/um_1_installation.html\nThere is also a Cloud version here: https://www.jamovi.org/cloud.html\nYou will also be able to find a lot of information about how to use JAMOVI on their website and all of this is free to access. You should explore their website but this page could be particularly useful because it is a comprehensive User Guide: https://www.jamovi.org/user-manual.html\n\n\n\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nMeasures of central tendency\n\n\n\nMeasures of dispersion and spread\n\n\n\nMeasures of skew and normality\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Jamovi editor\n\n\n\nComputing descriptive statistics in Jamovi\n\n\n\nPlot basic graphs in Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with openly available research data\n\n\n\n\n\n\n\n\n\nToday’s session\n\n\n\nToday you will just be familiarising yourself with the Jamovi interface and its layout / basic functions. Next week we will move on to using Jamovi for statistical analysis of data – so make sure you are comfortable with the layout and using the basic functions this week!\n\n\n\n\n\nWe will be exploring the dataset employees data that you can find on the CANVAS page under Week 8 of the computer labs folder of Research Methods A\n\n\n\nWe have a dataset of employees in a big data table. This raw data needs to be summarised and visualised so that we can turn this massive block of numbers into something intuitive.\nWe can generate a concise overview of the main characteristics of the data by computing and visualising a few key features from the data.\n\n\n\n\n\n\nOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.\n\n\n\n\n\nThe instructions for installing and licensing JAMOVI on your personal computer are available above. To access JAMOVI once installed or when using Campus computers you will be able to find it in the start menu at the bottom left corner of your desktop (sometimes this will be in another location on your personal laptop or personal desktop PC – you know your device best!)\nOnce open, you will see the following screen. Note that if you are working with a later version of JAMOVI, the screen may look a little different e.g. in version ‘2.3.28 solid’ there is a tab called “Variables” before “Data” at the top of the screen. As researchers we work with statistical software which updates occasionally. Most of the changes are often cosmetic but sometimes options and functions move or are improved. You should be prepared to familiarise yourself with the changes each time they happen and JAMOVI update their user guides regularly to account for updates.\n\n\n\n\n\n\nData Skills - Jamovi interface\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file Employees data.xlsx from canvas.\nIn Jamovi, click the 3 horizontal lines in the top left corner, click file, and click open. Find your data file and click to open.\n\nClick on the data tab and then on set-up to see the variable information\n\nTry clicking on different column names to change the information you are presented with.\n\nTry changing the labels for gender from 0 and 1 to something meaningful.\n\n\n\nWhen you start to do things in JAMOVI another screen will also come up to the right of the data view. This is the Output screen which keeps track of what you are doing and all of your results. When you have undertaken some analysis you can click on the results from this screen and cut and paste them into your reports.\n\n\n\n\n\n\n\n\n\nKey step - Saving data in Jamovi\n\n\n\nIn JAMOVI Click on the 3 horizontal lines in the top left corner to File &gt; Save as.\nGive the file an appropriate name and click on ‘Browse’ to decide where you want to save it – then just click on ‘Save’\n\n\n\n\n\n\nNow it is time for you to explore the data. If you are unsure about any of these, please talk to the staff in your computer practical session.\n\n\n\n\n\n\n Data Skills - What do each row and column represent?\n\n\n\n\n\nThe rows represent participants and the columns represent the different variables\n\n\n\n\n\n\n\n\n\n Data Skills - How many variables are there in this data set?\n\n\n\n\n\nThere are 8 variables in this data set\n\n\n\n\n\n\n\n\n\n Data Skills - What are the variables called?\n\n\n\n\n\nThe variable are called gender, bdate, educ, jobcat, salary, salbegin, prevexp, minority\n\n\n\n\n\n\n\n\n\n Data Skills - How many levels are there to each variable?\n\n\n\n\n\nGender: 2 Educ: Jobcat: 3 Minority: 2\nOnly the variables that are categorical have levels as these are the only variables that represent groups.\n\n\n\n\n\n\n\n\n\n Ethical Consideration - How could you ask a question about gender identity in a more inclusive way?\n\n\n\n\n\nYou could ask participants to state their gender without putting constraints on this. You could ask what gender participants identify as\n\n\n\n\n\n\n\n\n\n Data Skills - What type of variables (ordinal, scale (continuous), nominal) are there in the dataset?\n\n\n\n\n\nGender: Nominal\nBdate: ID\nEduc: Nominal\nJobcat: Nominal\nSalary: Scale (continuous)\nSalbegin: Scale (continuous)\nPrevexp: Scale (continuous)\nMinority: Nominal\n\n\n\n\n\n\n\n\n\n Data Skills - Your variables are not always imported as the correct variable type. In this case Bdate has been imported as ID which isn’t one of our variable types. What should you change this to?\n\n\n\n\n\nNominal\nPlease change this on your dataset.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen conducting statistical analysis for your research, you need to ensure that your variables are set to the right type. Otherwise, your analysis may not work correctly.\n\n\n\n\n\n\n\n\n Data Skills - What does the variable ‘Minority’ mean? Is it useful? Is there any information in the dataset to tell us what the numbers mean?\n\n\n\n\n\nThe variable gives us information about whether the participant is part of a minority group. It isn’t very useful without labels. There is no information in the dataset that would allow us to discern what the numbers mean either.\n\n\n\nPlease add labels to your ‘Minority’ variable where 0 = Not a minority and 1 = Minority\nFor the next few exercises you need to run some short analysis to get some descriptive statistics. Go to Analyse -&gt; Exploration -&gt; Descriptive statistics. Put Education and Job Category into the right-hand box. See the results appear in the right-hand window.\n\nNow try putting gender in the ‘split by’ box and see what happens to the output\n\nAdd labels to the variable JobCat where 1 = Manual, 2 = Office and 3 = Corporate. Go to Analyse -&gt; Exploration -&gt; Descriptive statistics. Put Salary into the right-hand box and Press OK. Look at the output\n\n\n\n\n\n\n\n Data Skills - What is the mean salary?\n\n\n\n\n\n34420\n\n\n\nNow do the same again but split by JobCat.\n\n\n\n\n\n\n\n Data Skills - What are the mean salaries for each group?\n\n\n\n\n\nGroup 1: 27839 Group 2: 30939 Group 3: 63978\n\n\n\nOn the same screen click ‘plot’ and then tick the box for histogram and bar plot. You’ll see the graphs appear in the screen to the right. Have a play around with the different options and see what you can produce.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 8 - Introduction to Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rma_week-8_introduction-to-jamovi.html#the-dataset",
    "href": "tutorials/rma_week-8_introduction-to-jamovi.html#the-dataset",
    "title": "Week 8 : Introduction to Jamovi",
    "section": "",
    "text": "We will be exploring the dataset employees data that you can find on the CANVAS page under Week 8 of the computer labs folder of Research Methods A",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 8 - Introduction to Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rma_week-8_introduction-to-jamovi.html#the-challenge",
    "href": "tutorials/rma_week-8_introduction-to-jamovi.html#the-challenge",
    "title": "Week 8 : Introduction to Jamovi",
    "section": "",
    "text": "We have a dataset of employees in a big data table. This raw data needs to be summarised and visualised so that we can turn this massive block of numbers into something intuitive.\nWe can generate a concise overview of the main characteristics of the data by computing and visualising a few key features from the data.\n\n\n\n\n\n\nOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 8 - Introduction to Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rma_week-8_introduction-to-jamovi.html#jamovi-basics",
    "href": "tutorials/rma_week-8_introduction-to-jamovi.html#jamovi-basics",
    "title": "Week 8 : Introduction to Jamovi",
    "section": "",
    "text": "The instructions for installing and licensing JAMOVI on your personal computer are available above. To access JAMOVI once installed or when using Campus computers you will be able to find it in the start menu at the bottom left corner of your desktop (sometimes this will be in another location on your personal laptop or personal desktop PC – you know your device best!)\nOnce open, you will see the following screen. Note that if you are working with a later version of JAMOVI, the screen may look a little different e.g. in version ‘2.3.28 solid’ there is a tab called “Variables” before “Data” at the top of the screen. As researchers we work with statistical software which updates occasionally. Most of the changes are often cosmetic but sometimes options and functions move or are improved. You should be prepared to familiarise yourself with the changes each time they happen and JAMOVI update their user guides regularly to account for updates.\n\n\n\n\n\n\nData Skills - Jamovi interface",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 8 - Introduction to Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rma_week-8_introduction-to-jamovi.html#how-to-open-an-excel-file-in-jamovi",
    "href": "tutorials/rma_week-8_introduction-to-jamovi.html#how-to-open-an-excel-file-in-jamovi",
    "title": "Week 8 : Introduction to Jamovi",
    "section": "",
    "text": "Key step\n\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file Employees data.xlsx from canvas.\nIn Jamovi, click the 3 horizontal lines in the top left corner, click file, and click open. Find your data file and click to open.\n\nClick on the data tab and then on set-up to see the variable information\n\nTry clicking on different column names to change the information you are presented with.\n\nTry changing the labels for gender from 0 and 1 to something meaningful.\n\n\n\nWhen you start to do things in JAMOVI another screen will also come up to the right of the data view. This is the Output screen which keeps track of what you are doing and all of your results. When you have undertaken some analysis you can click on the results from this screen and cut and paste them into your reports.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 8 - Introduction to Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rma_week-8_introduction-to-jamovi.html#how-to-save-your-work-in-jamovi",
    "href": "tutorials/rma_week-8_introduction-to-jamovi.html#how-to-save-your-work-in-jamovi",
    "title": "Week 8 : Introduction to Jamovi",
    "section": "",
    "text": "Key step - Saving data in Jamovi\n\n\n\nIn JAMOVI Click on the 3 horizontal lines in the top left corner to File &gt; Save as.\nGive the file an appropriate name and click on ‘Browse’ to decide where you want to save it – then just click on ‘Save’",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 8 - Introduction to Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rma_week-8_introduction-to-jamovi.html#exercises",
    "href": "tutorials/rma_week-8_introduction-to-jamovi.html#exercises",
    "title": "Week 8 : Introduction to Jamovi",
    "section": "",
    "text": "Now it is time for you to explore the data. If you are unsure about any of these, please talk to the staff in your computer practical session.\n\n\n\n\n\n\n Data Skills - What do each row and column represent?\n\n\n\n\n\nThe rows represent participants and the columns represent the different variables\n\n\n\n\n\n\n\n\n\n Data Skills - How many variables are there in this data set?\n\n\n\n\n\nThere are 8 variables in this data set\n\n\n\n\n\n\n\n\n\n Data Skills - What are the variables called?\n\n\n\n\n\nThe variable are called gender, bdate, educ, jobcat, salary, salbegin, prevexp, minority\n\n\n\n\n\n\n\n\n\n Data Skills - How many levels are there to each variable?\n\n\n\n\n\nGender: 2 Educ: Jobcat: 3 Minority: 2\nOnly the variables that are categorical have levels as these are the only variables that represent groups.\n\n\n\n\n\n\n\n\n\n Ethical Consideration - How could you ask a question about gender identity in a more inclusive way?\n\n\n\n\n\nYou could ask participants to state their gender without putting constraints on this. You could ask what gender participants identify as\n\n\n\n\n\n\n\n\n\n Data Skills - What type of variables (ordinal, scale (continuous), nominal) are there in the dataset?\n\n\n\n\n\nGender: Nominal\nBdate: ID\nEduc: Nominal\nJobcat: Nominal\nSalary: Scale (continuous)\nSalbegin: Scale (continuous)\nPrevexp: Scale (continuous)\nMinority: Nominal\n\n\n\n\n\n\n\n\n\n Data Skills - Your variables are not always imported as the correct variable type. In this case Bdate has been imported as ID which isn’t one of our variable types. What should you change this to?\n\n\n\n\n\nNominal\nPlease change this on your dataset.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen conducting statistical analysis for your research, you need to ensure that your variables are set to the right type. Otherwise, your analysis may not work correctly.\n\n\n\n\n\n\n\n\n Data Skills - What does the variable ‘Minority’ mean? Is it useful? Is there any information in the dataset to tell us what the numbers mean?\n\n\n\n\n\nThe variable gives us information about whether the participant is part of a minority group. It isn’t very useful without labels. There is no information in the dataset that would allow us to discern what the numbers mean either.\n\n\n\nPlease add labels to your ‘Minority’ variable where 0 = Not a minority and 1 = Minority\nFor the next few exercises you need to run some short analysis to get some descriptive statistics. Go to Analyse -&gt; Exploration -&gt; Descriptive statistics. Put Education and Job Category into the right-hand box. See the results appear in the right-hand window.\n\nNow try putting gender in the ‘split by’ box and see what happens to the output\n\nAdd labels to the variable JobCat where 1 = Manual, 2 = Office and 3 = Corporate. Go to Analyse -&gt; Exploration -&gt; Descriptive statistics. Put Salary into the right-hand box and Press OK. Look at the output\n\n\n\n\n\n\n\n Data Skills - What is the mean salary?\n\n\n\n\n\n34420\n\n\n\nNow do the same again but split by JobCat.\n\n\n\n\n\n\n\n Data Skills - What are the mean salaries for each group?\n\n\n\n\n\nGroup 1: 27839 Group 2: 30939 Group 3: 63978\n\n\n\nOn the same screen click ‘plot’ and then tick the box for histogram and bar plot. You’ll see the graphs appear in the screen to the right. Have a play around with the different options and see what you can produce.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 8 - Introduction to Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html",
    "href": "tutorials/rmb-week-11_putting-it-together.html",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "This week we will explore some data replicating a famous but contentious experiment. We will combine your skills from across the course to explore different aspects of the study.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nRevision of t-tests\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nRevision of reproducible analyses\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with published scientific data\n\n\n\n\n\n\n\n\n\n\n\nEthics\n\n\n\n\n\nQuestionable research practices\n\n\n\nConstructive criticism\n\n\n\n\n\nWe will be working with a replication of a vary famous study showing that our body posture can have far reaching impacts on our attitudes and even our physiology! amazing…\nFrom the abstract of the original paper:\nCarney, D. R., Cuddy, A. J. C., & Yap, A. J. (2010). Power Posing: Brief Nonverbal Displays Affect Neuroendocrine Levels and Risk Tolerance. Psychological Science, 21(10), 1363-1368. https://doi.org/10.1177/0956797610383437\n\nHumans and other animals express power through open, expansive postures, and they express powerlessness through closed, contractive postures. But can these postures actually cause power? The results of this study confirmed our prediction that posing in high-power nonverbal displays (as opposed to low-power nonverbal displays) would cause neuroendocrine and behavioral changes for both male and female participants: High-power posers experienced elevations in testosterone, decreases in cortisol, and increased feelings of power and tolerance for risk; low-power posers exhibited the opposite pattern. In short, posing in displays of power caused advantaged and adaptive psychological, physiological, and behavioral changes, and these findings suggest that embodiment extends beyond mere thinking and feeling, to physiology and subsequent behavioral choices. That a person can, by assuming two simple 1-min poses, embody power and instantly become more powerful has real-world, actionable implications. (Carney, Cuddy, and Yap 2010)\n\nThis is a compelling result! Perhaps not so suprising that power poses would change our mood - but to cause a change hormones is amazing.\nThe authors quickly became famous with this result. The second author Amy Cuddy gave high profile talks (TED talk with 74 million views: Video & transcript), wrote books and appeared widely in the media as a result of this work.\n\n\n\nAmy Cuddy at Pop Tech 2011 - Photo by Erik Hersman.\n\n\nThese results attracted several replications attempts and the following debate became so intense that it was attracting attention in the mainstream media - including articles summarising the story in the New York Times magazine (Dominus 2017) and the British Psychological Society (Loncar 2021). It even has it’s own Wikipedia page (Wikipedia 2025) and lead to some very awkward photos of british politicians (Wheeler 2018).\nWe’ll be focusing on one of these replications that tried to faithfully repeat the original methodology (Ranehill et al. 2015) with a large, independent data sample.\n\n\n\nWe will look at how the replication might have been planned, analyse the data and make a shareable version of the analysis.\nWe will also use this story to think about some ethical issues around good scientific practices and how we can encourage collaborative and constructive scientific debate.\n\n\n\nLet’s understand the original experiment in a bit more detail. Read through the introduction and methods section of (Carney, Cuddy, and Yap 2010) and answer the following questions.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestion\n\n\nRead the introduction (you can focus on the hypotheses) and methods section of (Carney, Cuddy, and Yap 2010) and answer the following questions.\n\n\nThe authors predict that high-power poses will lead to increasesdecreases in testosterone.\nThe authors predict that high-power poses will lead to increasesdecreases in the stress hormone cortisol.\nThe authors predict that high-power poses will lead to increasesdecreases in subjective feelings of power.\nThe authors predict that high-power poses will lead to increasesdecreases in tolerance to risk.\n\n\n\nWhat is the design of this study? Between subjectsWithin subjects.\nHow were participants assigned to groups? In order of recruitmentRandomlyBased on their preference.\nWhat is the independent variable (or grouping factor)? GamblingCortisolPoseTestosteroneelectrocardiography?\nWhich of these is NOT a dependent variable of interest? GamblingCortisolTestosteroneelectrocardiography?\nAre participants told the true purpose of the study? YesNo\nDid the experimentors know which condition the participant they were testing was in? YesNo\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nRead the results section of (Carney, Cuddy, and Yap 2010). Identify the reported statistics for the high-power/low-power pose groups for the following effects: Testosterone, Cortisol and Feelings of Power.\nEnter the reported effect sizes (r values) in the table below and use this online tool to convert them to Cohen’d D values.\nFinally - use jPower in Jamovi to estimate the sample size you would need to have 90% power to detect this effect again with an type 1 error rate of 0.05. We covered this in Week 5 Section 7\n\n\n\n\n\n\n\n\n\nHypothesis Test\nReported Effect Size (r)\nConverted Effect Size (Cohen’s d)\nSample size per group for 90% power\n\n\n\n\nTestosterone\n\n\n\n\n\nCortisol\n\n\n\n\n\nFeelings of power\n\n\n\n\n\n\n\n\n(Carney, Cuddy, and Yap 2010) report the following in their results.\n\nAs hypothesized, high-power poses caused an increase in testosterone compared with low-power poses, which caused a decrease in testosterone, F(1, 39) = 4.29, p &lt; .05; r = .34 (Fig. 3). Also as hypothesized, high-power poses caused a decrease in cortisol compared with low-power poses, which caused an increase in cortisol, F(1, 38) = 7.45, p &lt; .02; r = .43 Finally, high-power posers reported feeling significantly more “powerful” and “in charge” (M = 2.57, SD = 0.81) than low-power posers did (M = 1.83, SD = 0.81), F(1, 41) = 9.53, p &lt; .01; r = .44.\n\n(Carney, Cuddy, and Yap 2010) reported ANOVAs with r values as effect sizes - but remember that we can convert the effect sizes to Cohen’s d using the online tool.\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Test\nReported Effect Size (r)\nConverted Effect Size (Cohen’s d)\nSample size per group for 90% power\n\n\n\n\nTestosterone\n0.34\n0.723\n42\n\n\nCortisol\n0.43\n0.953\n25\n\n\nFeelings of power\n0.44\n0.980\n23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - The replication study has a planned sample size of 100 - is this enough to have a 90% chance of detecting the effect if is is there?\n\n\n\n\n\nYes, this should be plenty. The smallest reported effect in the original paper is for Testosterone, which suggests that two groups of 42 participants should be sufficient.\n\n\n\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nMake sure you have downloaded the dataset and loaded it into Jamovi before starting this section.\nTake a moment to have a look through the dataset paying particular attention to the ‘Variables’ tab.\n\n\nNext, we’re going to look at the dataset from (Ranehill et al. 2015) from the replication study. The research team were very careful to closely replicate the original methods.\nThere were a small number of changes in the replication compared to the original study:\n\nWhereas participants in Carney et al.’s study held each position for 1 min, we extended this time to 3 min.\nSecond, Carney et al. manually configured participants’ positions before leaving the room, whereas participants in our study received instructions on a computer. As in Carney et al.’s study, participants were recorded while posing; the recordings verified that participants complied with task instructions. Because the instructions were given via computer, the experimenter was blind to experimental condition, which negated potential experimenter effects.\nThird, the filler task in our study involved constructing words from letters and spaces; in Carney et al.’s study, the task was to form impressions of faces.\nFinally, we did not use deception, but informed participants briefly in the consent form that the study investigated whether physical position influences hormone levels and behavior (the instructions and consent form are in the Supplemental Material).\n\nThese are reasonable changes that aim to improve the orignal design (blinded experimentors, longer pose duration) without making substantial changes to its purpose.\nLet’s take a look at the data.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nIn the original study, (Carney, Cuddy, and Yap 2010) reported that:\n\nHigh-power posers reported feeling significantly more “powerful” and “in charge” (M = 2.57, SD = 0.81) than low-power posers did (M = 1.83, SD = 0.81), F(1, 41) = 9.53, p &lt; .01; r = .44.\n\nCan you replicate this analysis using the powerful and hp_condition (1 is ‘high power pose’ and 0 is ‘low power pose’) variables in the replication dataset from (Ranehill et al. 2015)? (Carney, Cuddy, and Yap 2010) used an ANOVA but you should use a t-test.\nUse the results to fill in the gaps in the reporting below.\n\nThe results replicatedo not replicate the original finding about subjective feeling. High-power posers (M = , SD = ) reported feeling less powerful thanjust as powerful asmore powerful than low-power posers did (M = , SD = ), t() = , p = ; Cohen’s d = .\n\n\n\n\n\n\n\nHigh-power posers reported feeling significantly more “powerful” (M = 2.22, SD = 0.753) than low-power posers did (M = 1.978, SD = 0.663), t(193) = -2.40, p = 0.017; Cohen’s d = -0.344.\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - Compare your results above to those reported in the replication paper - are there any differences?\n\n\n\n\n\nNo! oddly they do not… (Ranehill et al. 2015) report the following:\n\nUsing two-tailed t tests, we replicated Carney et al.’s finding that participants in the high-power condition self-reported, on average, higher feelings of power than did participants in the low-power condition (mean difference = 0.245, 95% confidence interval, or CI = [0.044, 0.446]), t(193) = 2.399, p = .017, Cohen’s d = 0.344. This suggests that the power-posing manipulation worked.\n\nMost of the reported values are the same, but but notice that the sign of the t-value is different!\nWe can confirm that this is a weird variable issue by checking hte descriptive statistics. The direction of our effect is definitely the same as what is reported in the paper.\n\n\n\n\n\n\n\n\n\nKey step - open after answering the previous question\n\n\n\n\n\nSomething strange has happened - it seems like (Ranehill et al. 2015) must have computed the t-test the other way around compared to our analysis. They have a positive t-value and ours is negative.\nRemember that the direction of a t-test (ie whether the t-statistics is positive or negative) doesn’t effect the p-values but does change the interpretation of the result a little.\nThis is because the order of subtraction makes a difference to the answer - if we do 2 + 3 or 3 + 2 we get the same answer however 2 - 3 and 3 - 2 are not the same.\nThe means of our groups are 1.98 for low power and 2.22 for high power poses. (Ranehill et al. 2015) report a positive t-value of 2.399 - so they must have run the equivalent of high power minus low power (2.22 - 1.98) and have a positive t-value indicating that the mean for high power is larger.\nIn contrast, our negative t-value indicates we have run low power minus high power (1.98 - 2.22) and have a negative t-value indicating that the mean for low power is smaller.\nThe order of the conditions in the t-test is controlled from the data varaible view. Double click on hp_condition in the ‘Variables’ tab, or on the column header of the ‘Data’ tab. We should need a ‘Nominal’ variable with two ‘Levels’ - 0 and 1. The conditions in the t-test are sorted in the order of this ‘Levels’ list.\nYou can reorder the levels to put 1 first and 0 second - which should update your t-test live to match (Ranehill et al. 2015)’s result.\n\nWe could make this even easier to remember by changing the names of the Levels to something more memorable. This is often a good idea when data has used an abstract numerical coding. Statistics is about precision and anything we can do to make the interpretation easier is a good thing.\n\nYour adapted result should match the following:\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you replicate the core findings from (Carney, Cuddy, and Yap 2010) using the data from (Ranehill et al. 2015)?\nMake sure you have completed the ‘Key step’ above before starting this exercise!\nRun t-tests to analyse the effect of high vs low power posing on the following variables: powerful, testosterone_diff, cortisol_diff and num_risky_gain_pc as dependent variables.\nRead through the results of the original paper (Carney, Cuddy, and Yap 2010) - does the second study confirm the findings of the original?\n\n\n\n\n\n\n\n\n\n\nVariable\nt-value\np-value\nEffect size\nReplicates (Carney, Cuddy, and Yap 2010)?\n\n\n\n\nFeelings of power\n\n\n\nYesNo\n\n\nRisk Taking (Gain)\n\n\n\nYesNo\n\n\nTestosterone\n\n\n\nYesNo\n\n\nCortisol\n\n\n\nYesNo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nt-value\np-value\nEffect size\nReplicates (Carney, Cuddy, and Yap 2010)?\n\n\n\n\nFeelings of power\n2.40\n0.017\n0.344\nYes\n\n\nRisk Taking (Gain)\n-1.24\n0.215\n-0.176\nNo\n\n\nTestosterone\n-1.40\n0.162\n-0.200\nNo\n\n\nCortisol\n-1.10\n0.272\n-0.157\nNo\n\n\n\n\n\n\n\n\n\n\n\nWe have the results of the replication! but before going any further we should be sure that the assumptions of our test are met.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCheck the assumptions of the high-power vs low-power t-tests for the following variables: powerful, testosterone_diff, cortisol_diff and num_risky_gain_pc.\nAre the assumptions of Normality and Homogeneity of Variance confirmed or violated?\n\n\n\n\n\n\n\n\n\n\nVariable\nNormality\nHomogeneity of Variance\nPreferred Test\n\n\n\n\n\nPowerful\nConfirmedViolated\nConfirmedViolated\nStudent’s tWelch’s tMann-Whitney U\n\n\n\nTestosterone\nConfirmedViolated\nConfirmedViolated\nStudent’s tWelch’s tMann-Whitney U\n\n\n\nCortisol\nConfirmedViolated\nConfirmedViolated\nStudent’s tWelch’s tMann-Whitney U\n\n\n\nRisk Taking (Gain)\nConfirmedViolated\nConfirmedViolated\nStudent’s tWelch’s tMann-Whitney U\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nNormality\nHomogeneity of Variance\nPreferred Test\n\n\n\n\n\nPowerful\nViolated\nViolated\nMann-Whitney U\n\n\n\nTestosterone\nViolated\nConfirmed\nMann-Whitney U\n\n\n\nCortisol\nViolated\nConfirmed\nMann-Whitney U\n\n\n\nRisk Taking (Gain)\nViolated\nConfirmed\nMann-Whitney U\n\n\n\n\n\n\n\n\n\n(Ranehill et al. 2015) do something unusual with their assumption checking. The don’t explicitly check whether the assumptions are met or not, rather they report both parametric and non-parametric statistics for all their tests (Non-parametric tests are reported in supplemental materials) and state that the conclusions would be the same irrespective of which test is would have been chosen (please do not do this in your reports!).\n\n\n\nIt can be challenging to interpret the results of a null replication. If both the original study and replication show the same finding then things are much more straightforward….\nTo interpret a failed replication we have to consider two possibilities:\n\nThe original study was a false positive and the replication is a correct rejection.\nThe original study was a true positive and the replication is a false negative.\n\nOne of our experiments is not accurate.\nIt is very very difficult to separate these options with 100% confidence… Often a second replication is the way to converge on an answer, and perhaps a third and fourth would be better still.\nOne thing we do know is that if the effect size from the original study were accurate, then the replication should have had enough participants to see it again.\nThe same is not true the other way around. Take a look at the figures below:\n\n\n\n\n\nOriginal and replication effect size with 95% confidence intervals for that effect size. Figures by Data Colada: https://datacolada.org/37#identifier_0_910\n\n\n\nThe replication confidence interval is much smaller than the original and even crosses zero for all three key variables. This strongly indicates a very tiny or zero effect.\nThe replication effect is so small that, if it is accurate, then the original study with its smaller sample would only have a 5.9% chance of detecting it. (Analysis by Data Colada).\nSo which study do we trust? Ultimately that your decision as an individual scientist. Consider what you know about both studies and evaulate which you find more compelling.\nIn this case, there is a substantial consensus in the field that the original effects on hormones was likely a false positive, though the increase in the subjective feelings of power after a power pose replicates well. The replications have several advantages including larger sample sizes, blinded and objective experimentors, and longer pose durations.\nThe original authors have engaged in the debate and made several points in defense of their work. The original study was famous by the time the replication was carried out - perhaps the participants had heard of the original study. Similarly, the replication study participants were told what was being measured in advance which might have effected their feelings during hte study. Finally, the original study was in hte USA and hte replication in Switzerland - perhaps there is a cultural component to power posing?\n\n\n\nThe original paper attracted huge scientific and media interest\n\n61 news outlets\n33 blogs\n2 policy sources\n110 X users\n1 peer review site\n11 Facebook pages\n8 Wikipedia pages\n1 Redditor\n11 YouTube creators\n1 Bluesky user\n\ndata from Altmetric, see here for full summary.\nThis was a mix of constructive debate, harsh criticism and genuine interest. Unfortunately a section of this debate became aggressive and personal towards Amy Cuddy to the extent that it had a serious impact on her life, health and career.\n\nIn addition to her fame and influence, Cuddy’s critics seemed obsessed with profits from her book and her fees for speaking engagements. Dollar amounts frequently emerged in conversations where academics discussed the research, and recently, one of her most vocal critics said her speaking engagement fees made him feel “ill.” One Ivy League professor tweeted, “You can listen to her speak about her irreplicable results for $25-$40,000.” In reality, her book and speaking topics have little to do with the power posing studies.\nMany of the negative comments were personal in nature and aimed at Cuddy directly. “They were comparing me to Harvey Weinstein,” Cuddy says, recalling a post to the PsychMAP Facebook group which the site owners were eventually persuaded to remove. As if that’s not bad enough, Cuddy also received death threats via direct messages on social media. “I became an untouchable in my field,” she adds, describing how colleagues were shunned for merely friending her on Facebook.\nAlthough Cuddy says Harvard wanted her to stay, she left her full-time position, because she no longer felt safe doing her work (she remains a faculty member at Harvard Business School in Executive Education). “My dean wanted me to stay, but I later told him, I couldn’t stay, because I couldn’t endure this abuse. It was relentless,” says Cuddy. Her experiences inspired her new book (still a work in progress) entitled, Bullies, Bystanders and Bravehearts aimed at helping others who find themselves facing similar experiences.\nExerpts from (Elsesser 2024).\n\n\nThis has led to a follow-up discussion […] consisting of two general camps: those who think Cuddy is being treated unfairly, and those who believe that complaints about bullying or harassment are a distraction from the most serious problem here, which is the elevation of shoddy research to a place of such profitable prominence.\nExcept from (Singal 2017)\n\nThere is perhaps a tension here between our ethical obligation to act with integrity in research and our obligation for act with respect (BPS, n.d.). Or perhaps some people are just bullies.\nHave a read through these sources and take a moment to do a quick web search for yourself.\nShare what you think about this situation on the padlet board below. what do you think of the situation surrounding this debate? how can we do better in future?\n\nTaking and receiving criticsm is a huge part of progressive constructive science but can be challenging to do well.\n\n\n\nReplication is the gold standard for scientific progress but it is rarely as straightforward as we’d like it to be!\nA wide range of issues can arise - including statistical approaches, experimental design and sometimes interpersonal conflict. We have to navigate all of these points with high ethical standards for the scientific community to progress.\nThank you for your time this semester - I hope you have enjoyed these computer practical sessions!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html#the-dataset",
    "href": "tutorials/rmb-week-11_putting-it-together.html#the-dataset",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "We will be working with a replication of a vary famous study showing that our body posture can have far reaching impacts on our attitudes and even our physiology! amazing…\nFrom the abstract of the original paper:\nCarney, D. R., Cuddy, A. J. C., & Yap, A. J. (2010). Power Posing: Brief Nonverbal Displays Affect Neuroendocrine Levels and Risk Tolerance. Psychological Science, 21(10), 1363-1368. https://doi.org/10.1177/0956797610383437\n\nHumans and other animals express power through open, expansive postures, and they express powerlessness through closed, contractive postures. But can these postures actually cause power? The results of this study confirmed our prediction that posing in high-power nonverbal displays (as opposed to low-power nonverbal displays) would cause neuroendocrine and behavioral changes for both male and female participants: High-power posers experienced elevations in testosterone, decreases in cortisol, and increased feelings of power and tolerance for risk; low-power posers exhibited the opposite pattern. In short, posing in displays of power caused advantaged and adaptive psychological, physiological, and behavioral changes, and these findings suggest that embodiment extends beyond mere thinking and feeling, to physiology and subsequent behavioral choices. That a person can, by assuming two simple 1-min poses, embody power and instantly become more powerful has real-world, actionable implications. (Carney, Cuddy, and Yap 2010)\n\nThis is a compelling result! Perhaps not so suprising that power poses would change our mood - but to cause a change hormones is amazing.\nThe authors quickly became famous with this result. The second author Amy Cuddy gave high profile talks (TED talk with 74 million views: Video & transcript), wrote books and appeared widely in the media as a result of this work.\n\n\n\nAmy Cuddy at Pop Tech 2011 - Photo by Erik Hersman.\n\n\nThese results attracted several replications attempts and the following debate became so intense that it was attracting attention in the mainstream media - including articles summarising the story in the New York Times magazine (Dominus 2017) and the British Psychological Society (Loncar 2021). It even has it’s own Wikipedia page (Wikipedia 2025) and lead to some very awkward photos of british politicians (Wheeler 2018).\nWe’ll be focusing on one of these replications that tried to faithfully repeat the original methodology (Ranehill et al. 2015) with a large, independent data sample.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html#the-challenge.",
    "href": "tutorials/rmb-week-11_putting-it-together.html#the-challenge.",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "We will look at how the replication might have been planned, analyse the data and make a shareable version of the analysis.\nWe will also use this story to think about some ethical issues around good scientific practices and how we can encourage collaborative and constructive scientific debate.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html#understanding-power-posing-and-planning-a-replication.",
    "href": "tutorials/rmb-week-11_putting-it-together.html#understanding-power-posing-and-planning-a-replication.",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "Let’s understand the original experiment in a bit more detail. Read through the introduction and methods section of (Carney, Cuddy, and Yap 2010) and answer the following questions.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestion\n\n\nRead the introduction (you can focus on the hypotheses) and methods section of (Carney, Cuddy, and Yap 2010) and answer the following questions.\n\n\nThe authors predict that high-power poses will lead to increasesdecreases in testosterone.\nThe authors predict that high-power poses will lead to increasesdecreases in the stress hormone cortisol.\nThe authors predict that high-power poses will lead to increasesdecreases in subjective feelings of power.\nThe authors predict that high-power poses will lead to increasesdecreases in tolerance to risk.\n\n\n\nWhat is the design of this study? Between subjectsWithin subjects.\nHow were participants assigned to groups? In order of recruitmentRandomlyBased on their preference.\nWhat is the independent variable (or grouping factor)? GamblingCortisolPoseTestosteroneelectrocardiography?\nWhich of these is NOT a dependent variable of interest? GamblingCortisolTestosteroneelectrocardiography?\nAre participants told the true purpose of the study? YesNo\nDid the experimentors know which condition the participant they were testing was in? YesNo\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nRead the results section of (Carney, Cuddy, and Yap 2010). Identify the reported statistics for the high-power/low-power pose groups for the following effects: Testosterone, Cortisol and Feelings of Power.\nEnter the reported effect sizes (r values) in the table below and use this online tool to convert them to Cohen’d D values.\nFinally - use jPower in Jamovi to estimate the sample size you would need to have 90% power to detect this effect again with an type 1 error rate of 0.05. We covered this in Week 5 Section 7\n\n\n\n\n\n\n\n\n\nHypothesis Test\nReported Effect Size (r)\nConverted Effect Size (Cohen’s d)\nSample size per group for 90% power\n\n\n\n\nTestosterone\n\n\n\n\n\nCortisol\n\n\n\n\n\nFeelings of power\n\n\n\n\n\n\n\n\n(Carney, Cuddy, and Yap 2010) report the following in their results.\n\nAs hypothesized, high-power poses caused an increase in testosterone compared with low-power poses, which caused a decrease in testosterone, F(1, 39) = 4.29, p &lt; .05; r = .34 (Fig. 3). Also as hypothesized, high-power poses caused a decrease in cortisol compared with low-power poses, which caused an increase in cortisol, F(1, 38) = 7.45, p &lt; .02; r = .43 Finally, high-power posers reported feeling significantly more “powerful” and “in charge” (M = 2.57, SD = 0.81) than low-power posers did (M = 1.83, SD = 0.81), F(1, 41) = 9.53, p &lt; .01; r = .44.\n\n(Carney, Cuddy, and Yap 2010) reported ANOVAs with r values as effect sizes - but remember that we can convert the effect sizes to Cohen’s d using the online tool.\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Test\nReported Effect Size (r)\nConverted Effect Size (Cohen’s d)\nSample size per group for 90% power\n\n\n\n\nTestosterone\n0.34\n0.723\n42\n\n\nCortisol\n0.43\n0.953\n25\n\n\nFeelings of power\n0.44\n0.980\n23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - The replication study has a planned sample size of 100 - is this enough to have a 90% chance of detecting the effect if is is there?\n\n\n\n\n\nYes, this should be plenty. The smallest reported effect in the original paper is for Testosterone, which suggests that two groups of 42 participants should be sufficient.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html#analysing-the-replication-data.",
    "href": "tutorials/rmb-week-11_putting-it-together.html#analysing-the-replication-data.",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "Key step\n\n\n\nMake sure you have downloaded the dataset and loaded it into Jamovi before starting this section.\nTake a moment to have a look through the dataset paying particular attention to the ‘Variables’ tab.\n\n\nNext, we’re going to look at the dataset from (Ranehill et al. 2015) from the replication study. The research team were very careful to closely replicate the original methods.\nThere were a small number of changes in the replication compared to the original study:\n\nWhereas participants in Carney et al.’s study held each position for 1 min, we extended this time to 3 min.\nSecond, Carney et al. manually configured participants’ positions before leaving the room, whereas participants in our study received instructions on a computer. As in Carney et al.’s study, participants were recorded while posing; the recordings verified that participants complied with task instructions. Because the instructions were given via computer, the experimenter was blind to experimental condition, which negated potential experimenter effects.\nThird, the filler task in our study involved constructing words from letters and spaces; in Carney et al.’s study, the task was to form impressions of faces.\nFinally, we did not use deception, but informed participants briefly in the consent form that the study investigated whether physical position influences hormone levels and behavior (the instructions and consent form are in the Supplemental Material).\n\nThese are reasonable changes that aim to improve the orignal design (blinded experimentors, longer pose duration) without making substantial changes to its purpose.\nLet’s take a look at the data.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nIn the original study, (Carney, Cuddy, and Yap 2010) reported that:\n\nHigh-power posers reported feeling significantly more “powerful” and “in charge” (M = 2.57, SD = 0.81) than low-power posers did (M = 1.83, SD = 0.81), F(1, 41) = 9.53, p &lt; .01; r = .44.\n\nCan you replicate this analysis using the powerful and hp_condition (1 is ‘high power pose’ and 0 is ‘low power pose’) variables in the replication dataset from (Ranehill et al. 2015)? (Carney, Cuddy, and Yap 2010) used an ANOVA but you should use a t-test.\nUse the results to fill in the gaps in the reporting below.\n\nThe results replicatedo not replicate the original finding about subjective feeling. High-power posers (M = , SD = ) reported feeling less powerful thanjust as powerful asmore powerful than low-power posers did (M = , SD = ), t() = , p = ; Cohen’s d = .\n\n\n\n\n\n\n\nHigh-power posers reported feeling significantly more “powerful” (M = 2.22, SD = 0.753) than low-power posers did (M = 1.978, SD = 0.663), t(193) = -2.40, p = 0.017; Cohen’s d = -0.344.\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - Compare your results above to those reported in the replication paper - are there any differences?\n\n\n\n\n\nNo! oddly they do not… (Ranehill et al. 2015) report the following:\n\nUsing two-tailed t tests, we replicated Carney et al.’s finding that participants in the high-power condition self-reported, on average, higher feelings of power than did participants in the low-power condition (mean difference = 0.245, 95% confidence interval, or CI = [0.044, 0.446]), t(193) = 2.399, p = .017, Cohen’s d = 0.344. This suggests that the power-posing manipulation worked.\n\nMost of the reported values are the same, but but notice that the sign of the t-value is different!\nWe can confirm that this is a weird variable issue by checking hte descriptive statistics. The direction of our effect is definitely the same as what is reported in the paper.\n\n\n\n\n\n\n\n\n\nKey step - open after answering the previous question\n\n\n\n\n\nSomething strange has happened - it seems like (Ranehill et al. 2015) must have computed the t-test the other way around compared to our analysis. They have a positive t-value and ours is negative.\nRemember that the direction of a t-test (ie whether the t-statistics is positive or negative) doesn’t effect the p-values but does change the interpretation of the result a little.\nThis is because the order of subtraction makes a difference to the answer - if we do 2 + 3 or 3 + 2 we get the same answer however 2 - 3 and 3 - 2 are not the same.\nThe means of our groups are 1.98 for low power and 2.22 for high power poses. (Ranehill et al. 2015) report a positive t-value of 2.399 - so they must have run the equivalent of high power minus low power (2.22 - 1.98) and have a positive t-value indicating that the mean for high power is larger.\nIn contrast, our negative t-value indicates we have run low power minus high power (1.98 - 2.22) and have a negative t-value indicating that the mean for low power is smaller.\nThe order of the conditions in the t-test is controlled from the data varaible view. Double click on hp_condition in the ‘Variables’ tab, or on the column header of the ‘Data’ tab. We should need a ‘Nominal’ variable with two ‘Levels’ - 0 and 1. The conditions in the t-test are sorted in the order of this ‘Levels’ list.\nYou can reorder the levels to put 1 first and 0 second - which should update your t-test live to match (Ranehill et al. 2015)’s result.\n\nWe could make this even easier to remember by changing the names of the Levels to something more memorable. This is often a good idea when data has used an abstract numerical coding. Statistics is about precision and anything we can do to make the interpretation easier is a good thing.\n\nYour adapted result should match the following:\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you replicate the core findings from (Carney, Cuddy, and Yap 2010) using the data from (Ranehill et al. 2015)?\nMake sure you have completed the ‘Key step’ above before starting this exercise!\nRun t-tests to analyse the effect of high vs low power posing on the following variables: powerful, testosterone_diff, cortisol_diff and num_risky_gain_pc as dependent variables.\nRead through the results of the original paper (Carney, Cuddy, and Yap 2010) - does the second study confirm the findings of the original?\n\n\n\n\n\n\n\n\n\n\nVariable\nt-value\np-value\nEffect size\nReplicates (Carney, Cuddy, and Yap 2010)?\n\n\n\n\nFeelings of power\n\n\n\nYesNo\n\n\nRisk Taking (Gain)\n\n\n\nYesNo\n\n\nTestosterone\n\n\n\nYesNo\n\n\nCortisol\n\n\n\nYesNo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nt-value\np-value\nEffect size\nReplicates (Carney, Cuddy, and Yap 2010)?\n\n\n\n\nFeelings of power\n2.40\n0.017\n0.344\nYes\n\n\nRisk Taking (Gain)\n-1.24\n0.215\n-0.176\nNo\n\n\nTestosterone\n-1.40\n0.162\n-0.200\nNo\n\n\nCortisol\n-1.10\n0.272\n-0.157\nNo",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html#assumption-checking.",
    "href": "tutorials/rmb-week-11_putting-it-together.html#assumption-checking.",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "We have the results of the replication! but before going any further we should be sure that the assumptions of our test are met.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCheck the assumptions of the high-power vs low-power t-tests for the following variables: powerful, testosterone_diff, cortisol_diff and num_risky_gain_pc.\nAre the assumptions of Normality and Homogeneity of Variance confirmed or violated?\n\n\n\n\n\n\n\n\n\n\nVariable\nNormality\nHomogeneity of Variance\nPreferred Test\n\n\n\n\n\nPowerful\nConfirmedViolated\nConfirmedViolated\nStudent’s tWelch’s tMann-Whitney U\n\n\n\nTestosterone\nConfirmedViolated\nConfirmedViolated\nStudent’s tWelch’s tMann-Whitney U\n\n\n\nCortisol\nConfirmedViolated\nConfirmedViolated\nStudent’s tWelch’s tMann-Whitney U\n\n\n\nRisk Taking (Gain)\nConfirmedViolated\nConfirmedViolated\nStudent’s tWelch’s tMann-Whitney U\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nNormality\nHomogeneity of Variance\nPreferred Test\n\n\n\n\n\nPowerful\nViolated\nViolated\nMann-Whitney U\n\n\n\nTestosterone\nViolated\nConfirmed\nMann-Whitney U\n\n\n\nCortisol\nViolated\nConfirmed\nMann-Whitney U\n\n\n\nRisk Taking (Gain)\nViolated\nConfirmed\nMann-Whitney U\n\n\n\n\n\n\n\n\n\n(Ranehill et al. 2015) do something unusual with their assumption checking. The don’t explicitly check whether the assumptions are met or not, rather they report both parametric and non-parametric statistics for all their tests (Non-parametric tests are reported in supplemental materials) and state that the conclusions would be the same irrespective of which test is would have been chosen (please do not do this in your reports!).",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html#interpreting-the-results.",
    "href": "tutorials/rmb-week-11_putting-it-together.html#interpreting-the-results.",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "It can be challenging to interpret the results of a null replication. If both the original study and replication show the same finding then things are much more straightforward….\nTo interpret a failed replication we have to consider two possibilities:\n\nThe original study was a false positive and the replication is a correct rejection.\nThe original study was a true positive and the replication is a false negative.\n\nOne of our experiments is not accurate.\nIt is very very difficult to separate these options with 100% confidence… Often a second replication is the way to converge on an answer, and perhaps a third and fourth would be better still.\nOne thing we do know is that if the effect size from the original study were accurate, then the replication should have had enough participants to see it again.\nThe same is not true the other way around. Take a look at the figures below:\n\n\n\n\n\nOriginal and replication effect size with 95% confidence intervals for that effect size. Figures by Data Colada: https://datacolada.org/37#identifier_0_910\n\n\n\nThe replication confidence interval is much smaller than the original and even crosses zero for all three key variables. This strongly indicates a very tiny or zero effect.\nThe replication effect is so small that, if it is accurate, then the original study with its smaller sample would only have a 5.9% chance of detecting it. (Analysis by Data Colada).\nSo which study do we trust? Ultimately that your decision as an individual scientist. Consider what you know about both studies and evaulate which you find more compelling.\nIn this case, there is a substantial consensus in the field that the original effects on hormones was likely a false positive, though the increase in the subjective feelings of power after a power pose replicates well. The replications have several advantages including larger sample sizes, blinded and objective experimentors, and longer pose durations.\nThe original authors have engaged in the debate and made several points in defense of their work. The original study was famous by the time the replication was carried out - perhaps the participants had heard of the original study. Similarly, the replication study participants were told what was being measured in advance which might have effected their feelings during hte study. Finally, the original study was in hte USA and hte replication in Switzerland - perhaps there is a cultural component to power posing?",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html#ethics-collaboration.",
    "href": "tutorials/rmb-week-11_putting-it-together.html#ethics-collaboration.",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "The original paper attracted huge scientific and media interest\n\n61 news outlets\n33 blogs\n2 policy sources\n110 X users\n1 peer review site\n11 Facebook pages\n8 Wikipedia pages\n1 Redditor\n11 YouTube creators\n1 Bluesky user\n\ndata from Altmetric, see here for full summary.\nThis was a mix of constructive debate, harsh criticism and genuine interest. Unfortunately a section of this debate became aggressive and personal towards Amy Cuddy to the extent that it had a serious impact on her life, health and career.\n\nIn addition to her fame and influence, Cuddy’s critics seemed obsessed with profits from her book and her fees for speaking engagements. Dollar amounts frequently emerged in conversations where academics discussed the research, and recently, one of her most vocal critics said her speaking engagement fees made him feel “ill.” One Ivy League professor tweeted, “You can listen to her speak about her irreplicable results for $25-$40,000.” In reality, her book and speaking topics have little to do with the power posing studies.\nMany of the negative comments were personal in nature and aimed at Cuddy directly. “They were comparing me to Harvey Weinstein,” Cuddy says, recalling a post to the PsychMAP Facebook group which the site owners were eventually persuaded to remove. As if that’s not bad enough, Cuddy also received death threats via direct messages on social media. “I became an untouchable in my field,” she adds, describing how colleagues were shunned for merely friending her on Facebook.\nAlthough Cuddy says Harvard wanted her to stay, she left her full-time position, because she no longer felt safe doing her work (she remains a faculty member at Harvard Business School in Executive Education). “My dean wanted me to stay, but I later told him, I couldn’t stay, because I couldn’t endure this abuse. It was relentless,” says Cuddy. Her experiences inspired her new book (still a work in progress) entitled, Bullies, Bystanders and Bravehearts aimed at helping others who find themselves facing similar experiences.\nExerpts from (Elsesser 2024).\n\n\nThis has led to a follow-up discussion […] consisting of two general camps: those who think Cuddy is being treated unfairly, and those who believe that complaints about bullying or harassment are a distraction from the most serious problem here, which is the elevation of shoddy research to a place of such profitable prominence.\nExcept from (Singal 2017)\n\nThere is perhaps a tension here between our ethical obligation to act with integrity in research and our obligation for act with respect (BPS, n.d.). Or perhaps some people are just bullies.\nHave a read through these sources and take a moment to do a quick web search for yourself.\nShare what you think about this situation on the padlet board below. what do you think of the situation surrounding this debate? how can we do better in future?\n\nTaking and receiving criticsm is a huge part of progressive constructive science but can be challenging to do well.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html#summary.",
    "href": "tutorials/rmb-week-11_putting-it-together.html#summary.",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "Replication is the gold standard for scientific progress but it is rarely as straightforward as we’d like it to be!\nA wide range of issues can arise - including statistical approaches, experimental design and sometimes interpersonal conflict. We have to navigate all of these points with high ethical standards for the scientific community to progress.\nThank you for your time this semester - I hope you have enjoyed these computer practical sessions!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "This week we will explore how to use Jamovi to test hypotheses about a dataset using t-tests. We will touch on some revision from previous weeks so please do jump back to past computer practicals for revision if required\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nIndependent samples t-tests\n\n\n\nPaired samples t-tests\n\n\n\nAssumptions of parametric tests\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nComputing a new variable from existing data\n\n\n\nComputing checks for normality and homogeneity of variance\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nExploring and understanding new datasets\n\n\n\n\n\nWe’ll be working with the dataset we collected after the break during the lecture in Week 3. This is a partial replication of a study published in the journal Psychological Science (Miller et al. 2023). The original paper found the following results summarised in the abstract.\n\nRecent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 (N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces—a phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 (N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI.\n\nWe ran a quiz that students in the lecture could join on their phones/laptops. There were two parts to the quiz:\nThe faces section included 12 faces that were either photographs or real people or AI generated images of people. The stimuli were taked from the materials released by (Miller et al. 2023) on the Open Science Framework. Responses were either ‘Real person’ or ‘AI generated’.\n\nThe confidence section asked participants to rate their confidence in their ability to do the following three things.\n\nDistinguish real photos of people from AI generated images (Experimental condition)\nDistinguish photos of happy people from photos of sad people (Emotional control condition)\nDistinguish photos of people you used to know in primary school from strangers (Memory control condition)\n\nScores were recorded on a scale from 1 (Completely confidence) to 10 (Not at all confident). The confidence section was repeated before and after the faces section to see if participants confidence changed as a result of doing the AI faces task\n\n\n\nThis week we will use both one sample and two sample t-tests to explore the following hypotheses.\n\nPeople are able to distinguish AI generated faces from real photos of humans.\nConfident people are better at distinguishing AI faces from real faces.\nPeople’s confidence in distinguishing AI generated faces will reduce after performing the task, but their confidence about emotion perception and memory will not change.\n\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to think about these hypotheses. Which statistical test is most appropriate for each? Do they call for a one-tailed or a two-tailed test?\n\n\n\n\n\nIt is critical to take some time to understand the data we work with before running critical hypothesis tests. Here we’ll take a look through the dataset to understand what information is present and if we’re happy to proceed with the analysis. This is similar to what we did in week 1 - you can refer back to the week 1 materials for additional guidance if you need it.\n\n\n\n\n\n\nKey step\n\n\n\nBefore going any further, the data file rmb-week-3_lecture-quiz-data_ai-faces.csv into a new Jamovi session.\n\n\nTake a read through the data columns. We have 26 in total with the following information.\n\n\n\n\n\n\n\nColumn Names\nDescription\n\n\n\n\nFirst Name\nParticipant ID - always ‘Anonymous’\n\n\nDataUse\nParticipant response to data re-use question\n\n\nAIConfidenceBefore\nConfidence in distinguishing AI faces from real BEFORE the task : 1 (Completely confident) to 10 (Not at all confident)\n\n\nEmoConfidenceBefore\nConfidence in distinguishing happy from sad faces BEFORE the task (Emotional control) : 1 (Completely confidence) to 10 (Not at all confident)\n\n\nMemoryConfidenceBefore\nConfidence in recognising a face from a long time ago BEFORE the task (Memory control) : 1 (Completely confidence) to 10 (Not at all confident)\n\n\nFace1_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace2_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace3_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace4_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace5_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace6_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace7_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace8_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace9_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace10_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace11_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace12_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nQuiz1\nResponse for revision quiz question\n\n\nQuiz2\nResponse for revision quiz question\n\n\nQuiz3\nResponse for revision quiz question\n\n\nAIConfidenceAfter\nConfidence in distinguishing AI faces from real AFTER the task\n\n\nEmoConfidenceAfter\nConfidence in distinguishing happy from sad faces AFTER the task (Emotional control)\n\n\nMemoryConfidenceAfter\nConfidence in recognising a face from a long time ago AFTER the task (Memory control)\n\n\n\nWork through the following questions, try to get an answer yourself before clicking to see the result. Data exploration is a critical skill that you’ll need whenever looking a new data throughout your degree.\n\n\n\n\n\n\n Data Skills - how many participants took part in the quiz?\n\n\n\n\n\nWe have 124 rows of data observations the spreadsheet.\n\n\n\n\n\n\n\n\n\n Data Skills - did everyone consent to have their data included in this practical?\n\n\n\n\n\nYes, the responses in the DataUse column are always positive - “Yes, I’m happy for my data to be included”. We removed the data with negative responses before sharing the data here.\n\n\n\n\n\n\n\n\n\n Data Skills - proportion of responsese would we expect to be accurate if participants responded randomly in the face questions.?\n\n\n\n\n\nWe would expect 50% accuracy if participants answered randomly. There were only two response options ‘Real person’ or ‘AI generated’.\n\n\n\nYou can use Descriptive Statistics to answer the following questions.\n\n\n\n\n\n\n Data Skills - which face did participants identify most accurately?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nFace 11 was most accurately identified as AI generated with 91.9% accuracy!\nFace 4 (AI) and Face 1 (Real) were close behind.\n\n\n\n\n\n\n\n\n\n Data Skills - which face did participants identify least accurately?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nFace 9 was least accurately indified as a real human with 31.8% accuracy. Face 5 (AI was second least accurate)\n\n\n\n\n\n\n\n\n\n Data Skills - did we get complete data from all participants in the face task?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nNo, quite a few participants dropped in and out during the task. We’re missing between 6 (Face 2 and 3) and 21 (Face 12) participants on each question.\n\n\n\n\n\n\n\n\n\n Data Skills - were participants most confident in their AI discrimination, emotion recognition or memory before the face task?\n\n\n\n\n\nCompute desriptive statistics for all AIConfidenceBefore, EmoConfidenceBefore and MemoryConfidenceBefore.\nParticipants were most confident in their ability to distinguish happy from sad faces in the Emotional face control condition with a score of 2.51. Participants were least confident in their ability to distinguish AI faces from real faces with a score of 4.54.\n\n\n\n\n\n\n\n\n\n Data Skills - are the confidence scores before face task normally distributed?\n\n\n\n\n\nCompute desriptive statistics for all AIConfidenceBefore, EmoConfidenceBefore and MemoryConfidenceBefore. Add the ‘Shapiro-Wilk’ statistic to the table.\nIt looks like none of these variabiles are normally distributied… The W statistics for AIConfidenceBefore is much higher than the other two, but the p-values indicate that all three show a departure from a normal distribution.\nAdd a Histogram to your descriptive plots - we can see that AI confidence looks close to normally distributed but there is a very large skew in both Emotional confidence and Memory confidence - some participants had very low confidence scores in these conditions!\n\n\n\n\n\n\nThe descriptive statistics gave us a good overview of the dataset and we can start working towards testing our hypotheses.\nOne critical piece of information is missing though! we have accuracy for each individual face but not an overall score for each participant. We’ll need to compute this new variable ourselves from the average accuracy of all twelve faces.\n\n\n\n\n\n\nData Skills - computing a variable from other columns\n\n\n\nWe can define our own variables in Jamovi using the ‘Compute’ function in the ‘Variables’ or the ‘Data’ tabs. Open a new Transformed variable.\nThis will open a menu with an option to give the new variable an name and description. Name the variable ‘PropFacesCorrect’ to indicate that it contatins the proportion of faces that the participant responded correctly on. You can add a description if you like though this is optional.\nThe variable is defined within the formula box below the name definitions. We want to compute the average accuracy across all 12 faces so we can add the formula to compute that into the box.\n\nThe formula should add all the columns together and divide the result by 12 (the total number of faces). Make sure that all the additions are grouped by parentheses! otherwise Jamovi will only divide the final value by 12 and add it to the others. This is an example of BODMAS - Brackets, Of, Division/Multiplication, Addition/Subtraction that you might have covered in maths in school. talk to your tutor to make sure that this step makes sense.\nThe formula should look something like this, I’ve removed some faces to simplify the visualisation. You should include them all.\n(Face1_Real + Face2_Real + ... + Face11_AI + Face12_AI) / 12\nOnce this is complete, you should be able to find your new column of values.\n\n\nNow, let’s take a look at our new variable. Compute some descritive statistics!\n\n\n\n\n\n\n Data Skills - how many participants gave a response to all 12 faces?\n\n\n\n\n\nWe have an N of 86 with 38 participants not responding to one or more of the faces.\n\n\n\n\n\n\n\n\n\n Data Skills - are the average accuracies normally distributed?\n\n\n\n\n\nTake a look at the Shaprio-Wilk statistic and the histogram of PropFacesCorrect, this looks like a normally distributed data variable.\n\n\n\n\n\n\n\n\n\n Data Skills - what proportion correct did the most and least accurate participants get?\n\n\n\n\n\nTake a look at the maxiumum and minumim of hte descriptive statistics, and perhaps add the table of ‘Most extreme’ data values in the Outiers section.\nTwo individuals managed to get all 12 faces correct! Two individuals were at 25% accurate corresponding to 3 out of 12 faces correct.\n\n\n\n\n\n\nOk, we’re ready to test the first hypothesis. Use the information you know about the dataset and try to find an answer!\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nTest the following hypothesis:\n\nPeople are able to distinguish AI generated faces from real photos of humans\n\nWhat sort of hypothesis is this and what is the most approprate statistical test?\nCompute the statistics, do the data support the experimental or the null hypothesis?\n\n\nWe could write a statistical version of this hypothesis as something like this\n\nPeople are able to distinguish AI generated faces from real photos of faces at an accuracy greater than chance level.\n\nWhat sort of test do you need to run?\n\n\nWe need to run a one sample t-test that compares the PropCorrectFaces variable to a chance level of 0.5 (corresponding to 50%). The results could be reported as follows\n\nA one sample t-test comparing the group average proportion of correctly identified faces (M = 0.634, SD=0.164) to chance level (proportion correct = 0.5) showed a significant effect, t(85) = 7.52, p&lt;0.001. Participants were on average more accurate than chance at distinguishing AI generated face from photos of real faces.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 1?\n\n\n\n\n\nYes, the statistical test allows us to reject the null hypothesis that participant performance was no different to chance level on this task and accept the experimental hypothesis that participants are able to distinguish AI generated faces from real photos of humans.\n\n\n\n\n\n\nNow the second hypothesis. We don’t have everything we need to test this hypothesis yet. We’ll need some way to split our participants into two groups - one with high confidence in AI face detection and one with low confidence. Time to compute another variable.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCompute a new variable named ConfidentBefore that separates the groups based on the median AI face detection confidence before the task.\n\n\nYou’ll need the median value for AIConfidenceBefore - you can compute this from descriptive statistics.\nThe computed variable will need some logical condition (using operators like ‘&gt;’, ‘&lt;’ or ‘==’) that separates participants with confidence above and below the median.\n\n\nThe median value for AIConfidenceBefore is 4, so our computed variable definition will look like this.\n\nThe values in ConfidentBefore will now be ‘True’ for people with high confidence and ‘False’ for people with low confidence. It doesn’t matter if you’ve done this the other way around - the tests will still work but the results will be flipped in the other direction (multiplied by -1)\n\n\n\n\n\nWith our new variable, we have what we need to run an independent samples t-test. This is very straightforward following the analyses we’ve run previously in the module.\nOpen the ‘Independent Sample t-test’ menu under ‘t-tests’. To run the analysis, drag PropFacesCorrect across as our dependent variable and our new ConfidentBefore variable as the grouping variable. The results should appear on the right automatically.\nOnce you have computed the core test - add the following options:\n\nDescriptives\nHomogeneity Test\nNormality Test\n\n\nLet’s think through the results\n\n\n\n\n\n\n Data Skills - are the data normally distributed?\n\n\n\n\n\nThe Shairo-Wilk statistic is not significant, indicating that the data are likely to be normally distributed\n\n\n\n\n\n\n\n\n\n Data Skills - do we have homogeneity of variance?\n\n\n\n\n\nLevene’s statistic is not significant, indicating that the variance of the two groups is comparable. We can confirm this by looking at the standard deviations in the descriptives table. 0.171 and 0.158 are fairly similar so the test report makes intuitive sense.\n\n\n\n\n\n\n\n\n\n Data Skills - which test should we report? Student’s, Welch’s or Mann-Whitney U?\n\n\n\n\n\nBoth assumptions of the standard Student’s t-test we looked at above are met by the data - we can proceed with Student’s test.\n\n\n\n\n\n\n\n\n\n Data Skills - were confident partiticpants or unconfident participants more accurate?\n\n\n\n\n\nConfident participants were more accurate! participants in this study are good at judging their ability.\nWe can see this by looking at the means in the descriptive statistics and the descriptives plot if you have run it.\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 2?\n\n\n\n\n\nYes, overall it seems like we can reject the null for hypothesis 2 and accept the experimental hypothesis that more confident participants are better at distinguishing AI faces from real human faces.\n\n\n\n\n\n\nNext we want to explore whether performing the face decision task changes peoples confidences in their abililty to detect AI generated faces. Remember that all participants categorised 12 faces with immediate correct/incorrect feedback and made confidence ratings at the start and end of the task.\nOpen the ‘Paired Sample t-test’ menu under ‘t-tests’. To run the analysis, drag AIConfidenceBefore and AIConfidenceAfter across as our pair of dependent variables. This is the format for running paired samples t-tests, the rest of the options should be familiar from our previous analyses.\nOnce you have computed the core test, do the same for the EmoConfidence and MemoryConfidence, and add the following options:\n\nDescriptives\nNormality Test\n\nThe results should appear on the right automatically.\n\nLet’s think through the results…\n\n\n\n\n\n\n Data Skills - does confidence in distinguishing AI faces from real faces change after completing the face task?\n\n\n\n\n\nYes, the results indicate that there is a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to distinguish AI faces from real faces before (M=4.48, SD=2.20) and after (M=5.66, SD=2.57) the face perception task. t(76) = -3.769, p&lt;0.001. Confidence was significantly reduced after completing the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - does confidence in distinguishing sad faces from happy faces change after completing the face task?\n\n\n\n\n\nNo, the results indicate that there is not a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to distinguish sad faces from happy faces before (M=2.57, SD=1.49) and after (M=2.74, SD=1.37) the face perception task. t(64) = -1.1017, p=0.313. Confidence was unchanged before and after the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - does confidence in recognising a face you haven’t seen for a long time change after completing the face task?\n\n\n\n\n\nNo, the results indicate that there is not a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to recognise faces of people they haven’t seen for a long time before (M=3.67, SD=1.89) and after (M=3.70, SD=1.86) the face perception task. t(66) = -0.234, p=0.816. Confidence was unchanged before and after the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - What test should we report for these comparisons? Are the parametric assumptions met?\n\n\n\n\n\nCheck the results of hte normality tests. The Shapiro-Wilk statistic is significant for all three tests! We should consider the non-parametric alternative test - add the ‘Wilcoxon Rank’ test to your analysis.\nWe should report the Wilcoxon Rank test along with the medians for non-parametric data. For example:\n\nA Wilcoxon Rank test showed a significant difference in participants confidence in their ability to distinguish AI faces from real faces before (Median=4) and after (Median=5) the face perception task. W=680, p&lt;0.001. Confidence was significantly reduced after completing the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 3?\n\n\n\n\n\nYes, overall it seems like we can reject the null for hypothesis 3 and accept the experimental hypothesis. Participants were less confident in their ability to detect AI generated faces after completing the task but their confidence in detecting emotions and remembering faces from a long time ago remains unchanged.\n\n\n\n\n\n\nLet’s use our new skills from this practical to answer one last question.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse a t-test to test this hypothesis:\n\nPeople are more accurate at identifying photos of real people compared to AI generated photos\n\nYou’ll several of the skills from this session to answer the question… think through what sort of variables you’ll need and what sort of test you’ll need.\n\n\nWe need 2 new variables to answer this question. We have already computed PropCorrectFaces in an earlier section, but we now need to make separate versions of this for AI faces and real faces…\nThe variable transforms will look something like this:\n \nWhat sort of t-test will you need?\n\n\nWe need to compute a paired samples t-test to answer the question as each participant contributes to both the AI face and real face conditions. Compute the test along with some descriptive statistics, we can report the test as follows\n\nA paired samples t-test showed a significant difference in the correct identification of AI faces (M=0.702, SD=0.198) compared to real faces (M=0.566, SD=0.229). t(85) = -4.60, p&lt;0.001. AI faces were identified more accurately than real faces.\n\n\n\n\n\n\n\n\n\n\nWe’ve computed a range of tests to statistically assess our hypotheses today! One experiment can often yield enough data to run a wide range of analyses. It is always a good idea to start with your hypotheses and predictions to break the analysis down into manageable chunks.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#the-dataset",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#the-dataset",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "We’ll be working with the dataset we collected after the break during the lecture in Week 3. This is a partial replication of a study published in the journal Psychological Science (Miller et al. 2023). The original paper found the following results summarised in the abstract.\n\nRecent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 (N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces—a phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 (N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI.\n\nWe ran a quiz that students in the lecture could join on their phones/laptops. There were two parts to the quiz:\nThe faces section included 12 faces that were either photographs or real people or AI generated images of people. The stimuli were taked from the materials released by (Miller et al. 2023) on the Open Science Framework. Responses were either ‘Real person’ or ‘AI generated’.\n\nThe confidence section asked participants to rate their confidence in their ability to do the following three things.\n\nDistinguish real photos of people from AI generated images (Experimental condition)\nDistinguish photos of happy people from photos of sad people (Emotional control condition)\nDistinguish photos of people you used to know in primary school from strangers (Memory control condition)\n\nScores were recorded on a scale from 1 (Completely confidence) to 10 (Not at all confident). The confidence section was repeated before and after the faces section to see if participants confidence changed as a result of doing the AI faces task",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#the-challenge",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#the-challenge",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "This week we will use both one sample and two sample t-tests to explore the following hypotheses.\n\nPeople are able to distinguish AI generated faces from real photos of humans.\nConfident people are better at distinguishing AI faces from real faces.\nPeople’s confidence in distinguishing AI generated faces will reduce after performing the task, but their confidence about emotion perception and memory will not change.\n\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to think about these hypotheses. Which statistical test is most appropriate for each? Do they call for a one-tailed or a two-tailed test?",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#exploring-the-data",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#exploring-the-data",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "It is critical to take some time to understand the data we work with before running critical hypothesis tests. Here we’ll take a look through the dataset to understand what information is present and if we’re happy to proceed with the analysis. This is similar to what we did in week 1 - you can refer back to the week 1 materials for additional guidance if you need it.\n\n\n\n\n\n\nKey step\n\n\n\nBefore going any further, the data file rmb-week-3_lecture-quiz-data_ai-faces.csv into a new Jamovi session.\n\n\nTake a read through the data columns. We have 26 in total with the following information.\n\n\n\n\n\n\n\nColumn Names\nDescription\n\n\n\n\nFirst Name\nParticipant ID - always ‘Anonymous’\n\n\nDataUse\nParticipant response to data re-use question\n\n\nAIConfidenceBefore\nConfidence in distinguishing AI faces from real BEFORE the task : 1 (Completely confident) to 10 (Not at all confident)\n\n\nEmoConfidenceBefore\nConfidence in distinguishing happy from sad faces BEFORE the task (Emotional control) : 1 (Completely confidence) to 10 (Not at all confident)\n\n\nMemoryConfidenceBefore\nConfidence in recognising a face from a long time ago BEFORE the task (Memory control) : 1 (Completely confidence) to 10 (Not at all confident)\n\n\nFace1_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace2_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace3_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace4_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace5_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace6_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace7_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace8_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace9_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace10_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace11_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace12_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nQuiz1\nResponse for revision quiz question\n\n\nQuiz2\nResponse for revision quiz question\n\n\nQuiz3\nResponse for revision quiz question\n\n\nAIConfidenceAfter\nConfidence in distinguishing AI faces from real AFTER the task\n\n\nEmoConfidenceAfter\nConfidence in distinguishing happy from sad faces AFTER the task (Emotional control)\n\n\nMemoryConfidenceAfter\nConfidence in recognising a face from a long time ago AFTER the task (Memory control)\n\n\n\nWork through the following questions, try to get an answer yourself before clicking to see the result. Data exploration is a critical skill that you’ll need whenever looking a new data throughout your degree.\n\n\n\n\n\n\n Data Skills - how many participants took part in the quiz?\n\n\n\n\n\nWe have 124 rows of data observations the spreadsheet.\n\n\n\n\n\n\n\n\n\n Data Skills - did everyone consent to have their data included in this practical?\n\n\n\n\n\nYes, the responses in the DataUse column are always positive - “Yes, I’m happy for my data to be included”. We removed the data with negative responses before sharing the data here.\n\n\n\n\n\n\n\n\n\n Data Skills - proportion of responsese would we expect to be accurate if participants responded randomly in the face questions.?\n\n\n\n\n\nWe would expect 50% accuracy if participants answered randomly. There were only two response options ‘Real person’ or ‘AI generated’.\n\n\n\nYou can use Descriptive Statistics to answer the following questions.\n\n\n\n\n\n\n Data Skills - which face did participants identify most accurately?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nFace 11 was most accurately identified as AI generated with 91.9% accuracy!\nFace 4 (AI) and Face 1 (Real) were close behind.\n\n\n\n\n\n\n\n\n\n Data Skills - which face did participants identify least accurately?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nFace 9 was least accurately indified as a real human with 31.8% accuracy. Face 5 (AI was second least accurate)\n\n\n\n\n\n\n\n\n\n Data Skills - did we get complete data from all participants in the face task?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nNo, quite a few participants dropped in and out during the task. We’re missing between 6 (Face 2 and 3) and 21 (Face 12) participants on each question.\n\n\n\n\n\n\n\n\n\n Data Skills - were participants most confident in their AI discrimination, emotion recognition or memory before the face task?\n\n\n\n\n\nCompute desriptive statistics for all AIConfidenceBefore, EmoConfidenceBefore and MemoryConfidenceBefore.\nParticipants were most confident in their ability to distinguish happy from sad faces in the Emotional face control condition with a score of 2.51. Participants were least confident in their ability to distinguish AI faces from real faces with a score of 4.54.\n\n\n\n\n\n\n\n\n\n Data Skills - are the confidence scores before face task normally distributed?\n\n\n\n\n\nCompute desriptive statistics for all AIConfidenceBefore, EmoConfidenceBefore and MemoryConfidenceBefore. Add the ‘Shapiro-Wilk’ statistic to the table.\nIt looks like none of these variabiles are normally distributied… The W statistics for AIConfidenceBefore is much higher than the other two, but the p-values indicate that all three show a departure from a normal distribution.\nAdd a Histogram to your descriptive plots - we can see that AI confidence looks close to normally distributed but there is a very large skew in both Emotional confidence and Memory confidence - some participants had very low confidence scores in these conditions!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#computing-overall-accuracy",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#computing-overall-accuracy",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "The descriptive statistics gave us a good overview of the dataset and we can start working towards testing our hypotheses.\nOne critical piece of information is missing though! we have accuracy for each individual face but not an overall score for each participant. We’ll need to compute this new variable ourselves from the average accuracy of all twelve faces.\n\n\n\n\n\n\nData Skills - computing a variable from other columns\n\n\n\nWe can define our own variables in Jamovi using the ‘Compute’ function in the ‘Variables’ or the ‘Data’ tabs. Open a new Transformed variable.\nThis will open a menu with an option to give the new variable an name and description. Name the variable ‘PropFacesCorrect’ to indicate that it contatins the proportion of faces that the participant responded correctly on. You can add a description if you like though this is optional.\nThe variable is defined within the formula box below the name definitions. We want to compute the average accuracy across all 12 faces so we can add the formula to compute that into the box.\n\nThe formula should add all the columns together and divide the result by 12 (the total number of faces). Make sure that all the additions are grouped by parentheses! otherwise Jamovi will only divide the final value by 12 and add it to the others. This is an example of BODMAS - Brackets, Of, Division/Multiplication, Addition/Subtraction that you might have covered in maths in school. talk to your tutor to make sure that this step makes sense.\nThe formula should look something like this, I’ve removed some faces to simplify the visualisation. You should include them all.\n(Face1_Real + Face2_Real + ... + Face11_AI + Face12_AI) / 12\nOnce this is complete, you should be able to find your new column of values.\n\n\nNow, let’s take a look at our new variable. Compute some descritive statistics!\n\n\n\n\n\n\n Data Skills - how many participants gave a response to all 12 faces?\n\n\n\n\n\nWe have an N of 86 with 38 participants not responding to one or more of the faces.\n\n\n\n\n\n\n\n\n\n Data Skills - are the average accuracies normally distributed?\n\n\n\n\n\nTake a look at the Shaprio-Wilk statistic and the histogram of PropFacesCorrect, this looks like a normally distributed data variable.\n\n\n\n\n\n\n\n\n\n Data Skills - what proportion correct did the most and least accurate participants get?\n\n\n\n\n\nTake a look at the maxiumum and minumim of hte descriptive statistics, and perhaps add the table of ‘Most extreme’ data values in the Outiers section.\nTwo individuals managed to get all 12 faces correct! Two individuals were at 25% accurate corresponding to 3 out of 12 faces correct.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-1---people-are-able-to-distinguish-ai-generated-faces-from-real-photos-of-humans",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-1---people-are-able-to-distinguish-ai-generated-faces-from-real-photos-of-humans",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "Ok, we’re ready to test the first hypothesis. Use the information you know about the dataset and try to find an answer!\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nTest the following hypothesis:\n\nPeople are able to distinguish AI generated faces from real photos of humans\n\nWhat sort of hypothesis is this and what is the most approprate statistical test?\nCompute the statistics, do the data support the experimental or the null hypothesis?\n\n\nWe could write a statistical version of this hypothesis as something like this\n\nPeople are able to distinguish AI generated faces from real photos of faces at an accuracy greater than chance level.\n\nWhat sort of test do you need to run?\n\n\nWe need to run a one sample t-test that compares the PropCorrectFaces variable to a chance level of 0.5 (corresponding to 50%). The results could be reported as follows\n\nA one sample t-test comparing the group average proportion of correctly identified faces (M = 0.634, SD=0.164) to chance level (proportion correct = 0.5) showed a significant effect, t(85) = 7.52, p&lt;0.001. Participants were on average more accurate than chance at distinguishing AI generated face from photos of real faces.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 1?\n\n\n\n\n\nYes, the statistical test allows us to reject the null hypothesis that participant performance was no different to chance level on this task and accept the experimental hypothesis that participants are able to distinguish AI generated faces from real photos of humans.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-2---confident-people-are-better-at-distinguishing-ai-faces-from-real-faces.",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-2---confident-people-are-better-at-distinguishing-ai-faces-from-real-faces.",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "Now the second hypothesis. We don’t have everything we need to test this hypothesis yet. We’ll need some way to split our participants into two groups - one with high confidence in AI face detection and one with low confidence. Time to compute another variable.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCompute a new variable named ConfidentBefore that separates the groups based on the median AI face detection confidence before the task.\n\n\nYou’ll need the median value for AIConfidenceBefore - you can compute this from descriptive statistics.\nThe computed variable will need some logical condition (using operators like ‘&gt;’, ‘&lt;’ or ‘==’) that separates participants with confidence above and below the median.\n\n\nThe median value for AIConfidenceBefore is 4, so our computed variable definition will look like this.\n\nThe values in ConfidentBefore will now be ‘True’ for people with high confidence and ‘False’ for people with low confidence. It doesn’t matter if you’ve done this the other way around - the tests will still work but the results will be flipped in the other direction (multiplied by -1)\n\n\n\n\n\nWith our new variable, we have what we need to run an independent samples t-test. This is very straightforward following the analyses we’ve run previously in the module.\nOpen the ‘Independent Sample t-test’ menu under ‘t-tests’. To run the analysis, drag PropFacesCorrect across as our dependent variable and our new ConfidentBefore variable as the grouping variable. The results should appear on the right automatically.\nOnce you have computed the core test - add the following options:\n\nDescriptives\nHomogeneity Test\nNormality Test\n\n\nLet’s think through the results\n\n\n\n\n\n\n Data Skills - are the data normally distributed?\n\n\n\n\n\nThe Shairo-Wilk statistic is not significant, indicating that the data are likely to be normally distributed\n\n\n\n\n\n\n\n\n\n Data Skills - do we have homogeneity of variance?\n\n\n\n\n\nLevene’s statistic is not significant, indicating that the variance of the two groups is comparable. We can confirm this by looking at the standard deviations in the descriptives table. 0.171 and 0.158 are fairly similar so the test report makes intuitive sense.\n\n\n\n\n\n\n\n\n\n Data Skills - which test should we report? Student’s, Welch’s or Mann-Whitney U?\n\n\n\n\n\nBoth assumptions of the standard Student’s t-test we looked at above are met by the data - we can proceed with Student’s test.\n\n\n\n\n\n\n\n\n\n Data Skills - were confident partiticpants or unconfident participants more accurate?\n\n\n\n\n\nConfident participants were more accurate! participants in this study are good at judging their ability.\nWe can see this by looking at the means in the descriptive statistics and the descriptives plot if you have run it.\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 2?\n\n\n\n\n\nYes, overall it seems like we can reject the null for hypothesis 2 and accept the experimental hypothesis that more confident participants are better at distinguishing AI faces from real human faces.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-3---peoples-confidence-in-their-ability-to-distinguish-ai-generated-faces-will-reduce-after-performing-the-task",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-3---peoples-confidence-in-their-ability-to-distinguish-ai-generated-faces-will-reduce-after-performing-the-task",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "Next we want to explore whether performing the face decision task changes peoples confidences in their abililty to detect AI generated faces. Remember that all participants categorised 12 faces with immediate correct/incorrect feedback and made confidence ratings at the start and end of the task.\nOpen the ‘Paired Sample t-test’ menu under ‘t-tests’. To run the analysis, drag AIConfidenceBefore and AIConfidenceAfter across as our pair of dependent variables. This is the format for running paired samples t-tests, the rest of the options should be familiar from our previous analyses.\nOnce you have computed the core test, do the same for the EmoConfidence and MemoryConfidence, and add the following options:\n\nDescriptives\nNormality Test\n\nThe results should appear on the right automatically.\n\nLet’s think through the results…\n\n\n\n\n\n\n Data Skills - does confidence in distinguishing AI faces from real faces change after completing the face task?\n\n\n\n\n\nYes, the results indicate that there is a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to distinguish AI faces from real faces before (M=4.48, SD=2.20) and after (M=5.66, SD=2.57) the face perception task. t(76) = -3.769, p&lt;0.001. Confidence was significantly reduced after completing the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - does confidence in distinguishing sad faces from happy faces change after completing the face task?\n\n\n\n\n\nNo, the results indicate that there is not a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to distinguish sad faces from happy faces before (M=2.57, SD=1.49) and after (M=2.74, SD=1.37) the face perception task. t(64) = -1.1017, p=0.313. Confidence was unchanged before and after the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - does confidence in recognising a face you haven’t seen for a long time change after completing the face task?\n\n\n\n\n\nNo, the results indicate that there is not a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to recognise faces of people they haven’t seen for a long time before (M=3.67, SD=1.89) and after (M=3.70, SD=1.86) the face perception task. t(66) = -0.234, p=0.816. Confidence was unchanged before and after the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - What test should we report for these comparisons? Are the parametric assumptions met?\n\n\n\n\n\nCheck the results of hte normality tests. The Shapiro-Wilk statistic is significant for all three tests! We should consider the non-parametric alternative test - add the ‘Wilcoxon Rank’ test to your analysis.\nWe should report the Wilcoxon Rank test along with the medians for non-parametric data. For example:\n\nA Wilcoxon Rank test showed a significant difference in participants confidence in their ability to distinguish AI faces from real faces before (Median=4) and after (Median=5) the face perception task. W=680, p&lt;0.001. Confidence was significantly reduced after completing the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 3?\n\n\n\n\n\nYes, overall it seems like we can reject the null for hypothesis 3 and accept the experimental hypothesis. Participants were less confident in their ability to detect AI generated faces after completing the task but their confidence in detecting emotions and remembering faces from a long time ago remains unchanged.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#bonus-hypothesis-people-are-more-accurate-at-identifying-photos-of-real-people-compared-to-ai-generated-photos.",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#bonus-hypothesis-people-are-more-accurate-at-identifying-photos-of-real-people-compared-to-ai-generated-photos.",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "Let’s use our new skills from this practical to answer one last question.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse a t-test to test this hypothesis:\n\nPeople are more accurate at identifying photos of real people compared to AI generated photos\n\nYou’ll several of the skills from this session to answer the question… think through what sort of variables you’ll need and what sort of test you’ll need.\n\n\nWe need 2 new variables to answer this question. We have already computed PropCorrectFaces in an earlier section, but we now need to make separate versions of this for AI faces and real faces…\nThe variable transforms will look something like this:\n \nWhat sort of t-test will you need?\n\n\nWe need to compute a paired samples t-test to answer the question as each participant contributes to both the AI face and real face conditions. Compute the test along with some descriptive statistics, we can report the test as follows\n\nA paired samples t-test showed a significant difference in the correct identification of AI faces (M=0.702, SD=0.198) compared to real faces (M=0.566, SD=0.229). t(85) = -4.60, p&lt;0.001. AI faces were identified more accurately than real faces.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#summary",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#summary",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "We’ve computed a range of tests to statistically assess our hypotheses today! One experiment can often yield enough data to run a wide range of analyses. It is always a good idea to start with your hypotheses and predictions to break the analysis down into manageable chunks.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis is a book created from markdown and executable code.\nSee (knuth84?) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "The sampling distribution is central to how we can move from t-values to p-values - yet it is one of the trickier parts of this course. The lectures and pre-lecture materials frequently deal with computer simulations to show the properties of the tests we’re using, so this week you will write your own simulation to help understand the sampling distribution!\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nSampling Distributions\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nSimulating data in R\n\n\n\nWriting loops in R\n\n\n\nCreating histograms in R\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nCreating shareable code to demonstrate a statistical concept\n\n\n\n\n\nTLDR: There is no data to download! we’re going to make the data ourselves.\nThe dataset this week is very simple - we’re going to create it ourselves using R. At this point, we know a lot about the types of hypotheses that we can ask when comparing the averages of groups within a dataset. We can check our understanding of different aspects of these statistical tests by simulating data samples that have known properties (eg mean and standard deviation) and checking if the outcome of the t-test is what we would expect.\nThis way we can numerically validate the tests we’re running and take a look under the hood and see what sort of processes Jamovi and R are using to compute the tests for us.\n\n\n\n\n\n\nContent Info\n\n\n\nThere is a lot of code in this session - don’t be afraid to ask for help from your tutors!\n\n\n\n\n\nToday, we’ll first introduce some concepts and functions in R to simulate a data sample and compute our own one sample t-test. With that in hand, we will set up code to run thousands of tests in one go to help us to better understand sampling distributions.\n\n\n\nThe rnorm() function in R is used to generate random numbers from a normal (Gaussian) distribution. A call to rnorm() looks like this:\nrnorm(n, mean = 0, sd = 1)\nWe can see that rnorm() takes three values as inputs\n\n\n\n\n\n\n\ninput\nDescription\n\n\n\n\nn\nThe number of random numbers you want to generate, we always need to specify this.\n\n\nmean\nThe mean (average) of the normal distribution. The default value is 0 if it isn’t specified\n\n\nsd\nThe standard deviation of the normal distribution. The default value is 1 if it isn’t specified\n\n\n\nSo we could tweak these input arguments to simulate any number of data points from any normal distribution that we wish.\n\n\n\n\n\n\nCheck Your Understanding - R code\n\n\n\nWhat code would we need to write to simulate the following?\n20 data points from a distribution with mean of 1 and standard deviation of 2\n\nR code here -&gt; .\n\n33 data points from a distribution with mean of -3 and standard deviation of 1.23\n\nR code here -&gt; .\n\nfive thousand data points from a distribution with mean of 1.789 and standard deviation of 18.2\n\nR code here -&gt; .\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nOpen Jamovi, install the Rj add-on and open a new Rj window before we go any further\n\n\nWe can use rnorm() within the Rj code window as usual. Here I’m simulating 20 data points from a distribution with a mean of 1 and a standard deviation of 2. Try running the same code in your window.\n\nThese 20 simulated numbers could now represent a data sample for a variable in an analysis. The difference is we have absolute confidence about the ‘truth’ of the population statistics underlying this data sample.\nYou may notice that you do get 20 numbers, but that you get a different 20 numbers than I did! This is as rnorm() works hard to generate random numbers that are different each time.\n\n\n\n\n\n\nKey step\n\n\n\nYou can verify the random nature of the simulated data by running your code multiple times.\nEven though the code stays the same you will get a different data sample each time. Remember that the code and the underlying distribution remain the always the same but the data sample is changing.\n\n\nThis is actually a good thing for us. Real experiments and data collection always have an element of random luck and we want our simulations to reflect this as well.\n\n\n\n\n\n\nData skills - revision on sampling and populations\n\n\n\nSampling is a fundamental process in statistics where a subset of individuals from a population is selected to estimate characteristics of the whole population. This is built from the assumption that the subset of individuals in our sample is representative of that population.\nSome samples will be more representative than others. We can do a lot of practical work and planning to get the best possible sample for our research but there is always some random chance in who ends up in a data sample and who does not.\n\nThere are two main reasons why a sample might not be representative.\nBias if the data collection has included some bias (intentional or accidental) towards a subgroup of our population then it is very likely that this will lead to an unrepresentative sample. This is not something we can solve with statistics. Good reseach design, consideration of ethics and thorough planning is the only way to avoid systematic biases.\nLuck even if we have a unbiased data collection approach, two samples that draw from the sample population can still be different due to random chance. This will always be true when we are restricted to looking at a subset of the full population - fortunately, this is something we can quantify with statistics. We’ll explore how in this session.\nThe code we write this week will simulate drawing a subset of individuals from a population. Each iteration of the code could be considered as a separate replication of the same experiment.\n\n\n\n\n\nIn week 2 we discussed how sampling relatively small amount of data from a wider population can impact our estimates of values like the mean.\nTo recap, let’s say we’re interested in computing the average attainment for secondary school children on a particular vocabulary test. Our target population might be ‘all secondary school students in the UK’ but is it practically impossible to get data from every single student currently in secondard school. We have to make do with smaller data samples of a few tens or few hundreds of students (or even more, if we’re lucky).\nOur simulations do exactly this. The parameters we pass into rnorm() define ‘population level’ characteristics in the form of a normal distribution and each time we run the function we draw a ‘sample’ of a fixed number of data points from that population. We can think of this as rerunning our data sampling every time the function is executed. This would be massively complicated, time consuming and expensive in real life, but with a few assumptions a computer can simulate it in a fraction of a second.\nWe can now start to explore how sampling variability - AKA the random differences between data samples from the same population - can impact our estimates of the mean and standard deviation.\nUpdate your code to store the data sample into a variable and compute the mean with mean() and standard deviation with sd(). The result might look like this.\nsample = rnorm(20, mean=1, sd=2)\n\n# Compute the mean\nprint('Mean:')\nprint(mean(sample))\n\n# Compute the standard deviation\nprint('Standard Deviation:')\nprint(sd(sample))\n\nRunning this code will show the mean and standard deviation of each individual data sample, try running this a few times. Notice that the values are sometimes close to our ‘population’ estimates of mean = 1 and standard deviation = 2 but also sometimes quite far away.\nTry tweaking your code to answer these questions\n\n\n\n\n\n\n Data Skills - is the estimate of the mean better or worse with a sample of 200?\n\n\n\n\n\nUpdate your code to use this data simulation\nsample = rnorm(200, mean=1, sd=2)\nRunning this a few times should show you mean values much closer to 1 and standard deviations much closer to 2 than we saw with a sample of 20 values. They still won’t be perfect though.\n\n\n\n\n\n\n\n\n\n Data Skills - is the estimate of the mean better or worse when the population standard deviation is 10?\n\n\n\n\n\nUpdate your code to use this data simulation\nsample = rnorm(200, mean=1, sd=10)\nRunning this a few times should make your estimates of the mean and standard deviation much more variable again. When variability in the population is very wide, large data samples won’t necessarily make up the difference.\n\n\n\nWe can get an informal sense of sampling variability by clicking to rerun this code a few times, but R code allows us to do much better, we can tweak the code to rerun this analysis hundreds or thousands of times to explicitly quantify the variability we see.\nTo do this, we’ll need a code element called a loop which looks something like this\nnum_simulations = 10\n\n# Loop to perform simulations\nfor (i in 1:num_simulations) {\n    print(i)\n}\nThere are two aspects to this code block. The first sets the number of simulated data samples we want to draw.\nnum_simulations = 10\nThis line sets the variable num_simulations to 10. This means you want to perform 10 iterations in the loop.\nThe second part is the loop itself:\nfor (i in 1:num_simulations) {\n    print(i)\n}\nThis is a for loop that will iterate from 1 to num_simulations (which is 10 in this case). Here’s what happens inside the loop:\n\nInitialization: The loop starts with i set to 1.\nCondition: The loop will continue to run as long as i is less than or equal to num_simulations.\nIncrement: After each iteration, i is incremented by 1.\n\nInside the loop, the print(i) statement is executed, which prints the current value of i to the console.\nThe output of this code will be the numbers from 1 to 10, each printed on a new line. The full output might look like this:\n\nPrinting the value of i is instructive but really we want our loop to include some interesting computations. Next, we should move our sampling code within the curly brackets {} of the loop.\nEverything within the those curly brackets will be repeated on every iteration of the loop. So if we includ the code to draw a sample and compute the mean, it will be repeated every time. Here is an example:\n# My loop\nnum_simulations = 10\n\nfor (i in 1:num_simulations) {\n    sample = rnorm(20, mean=1, sd=10)\n    print(mean(sample))\n}\n\nThe output of the loop is now the mean of each data sample, though the exact values you get will be slightly different to mine.\nWe can see that 20 samples drawn from a population with mean = 1, sd = 10 we have extremely variable estimates of the mean. Try reducing the standard deviation back down to 2 to see the result.\n# My loop\nnum_simulations = 10\n\nfor (i in 1:num_simulations) {\n    sample = rnorm(20, mean=1, sd=2)\n    print(mean(sample))\n}\n\nThese are much closer to our expected population value of 1.\n\n\n\nWe’re very close to completing our loop. There is one final issue to sort.\nAt the moment, the estimated sample is lost every time the code in the loop restarts - we need to store the whole set of mean values for every single simulation if we want to properly see what is going on.\nFor this we need to add two elements to the loop. Firstly we need to create a variable to store the means we estimate. We do this before the loop starts.\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nand secondly, within the loop, we need to add the estimated mean to our new mean_values variable.\n# Compute and store the mean of the data sample\nmean_values[i] = mean(sample)\nmean_values can store many numbers altogether in a list or vector. Each individual element can be accessed using the square bracket notation. For example mean_values[5] would access the fifth value in the list. The code above will store each value in to the ith position in the list so that we keep them all.\nThe final code should look like this:\n# My loop\nnum_simulations = 10\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(20, mean=1, sd=2)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\nmean_values now contains all of our simulated means that we can visualise using a histogram with the hist() function.\n\nWhich will produce a simple Histogram of the values for us. Note that, as ever, your histogram will be a little different to mine. Try rerunning the code a few times to get a sense of the variabity.\nWe can make our plot a bit more attractive and informative by specifying some additional inputs. These just change the visualisation of the histogram rather than the analysis.\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Means\", xlim=c(-2, 4),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\nThis has a lot of inputs, lets break down what they all mean\n\n\n\nInput\nDescription\n\n\n\n\nmean_values\nThe data sample to visualise\n\n\nmain\nThe main title\n\n\nxlim\nThe start and end points of the x-axis within a collection\n\n\nxlab\nThe label for the x-axis\n\n\ncol\nThe colour of the bars\n\n\nborder\nThe colour of the borders of the bars\n\n\n\nAdding these to the function call makes a much nicer and more informative image.\n\nYou can tweak the titles and axis labels to be sometime informative to you, and you can change the colours to be anything you like. Try customising the plot - update the labels and colours as you prefer them.\n\n\n\n\n\n\nClick for names of colours in R\n\n\n\n\n\nYou can use the colour names defined here to customise your histogram.\n\n\n\n\n\n\n\nOnce your histogram is customised and ready to go - we can put our code to work and understand the core concepts.\nLet’s review what we have so far.\n\nWe have constructed a loop that draws data samples from a defined population\nWe compute and store the mean of that data sample\nWe use a histogram to visualise the distribution of means estimated from all of the data samples\nThis is the sampling distribution of the mean.\n\n\n\n\n\n\n\nKey step\n\n\n\nMake sure you have an organised script ready to go at this point, try adding comments to understand what each line does. You can delete some of the older parts of the code that aren’t included in the loop if you like. Your code should look something like this.:\n# My loop\nnum_simulations = 10\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(20, mean=1, sd=2)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Means\", xlim=c(-2, 4),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\n\nRecall that the variance of the sampling distribution is one of its key parameters - it tells us how precisely our estimated means map onto the known population mean. (Recall from lecture 3 that the standard error of the mean can be computed from this number).\nSampling distributions with wider variances indicate that we have less precisely estimated the mean, whereas tighter distributions indicate that we have more confidence. Let’s use our code to illustrate this concept.\n\n\n\n\n\n\nKey step\n\n\n\nChange your code to repeat the simulation 1000 times rather than just 10, you should get a much more ‘normal’ distribution that changes less when you rerun the code.\n\n\n\nThis distribution is the sampling distribution of the mean for the simulated data. We can learn the following things:\n\nWe are most likely to estimate a value of the mean that is close to 1\nThe spread of the distribution indicates that values between 0.5 and 1.5 are still fairly common\nWe will very occasionally estimate a mean around 0 or around 2\n\nTry changing your code to answer these questions.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when each data sample has 100 data points rather than just 20?\n\n\nFor this question, you’ll need to update the first input to rnorm() to have 100 data points rather than 20.\n\n\nWith more data in each data sample - we can see that the spread in the sampling distribution has reduced. In other words, each sample has a similar estimate of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nWe will very occasionally estimate a mean around 0.5 or around 1.5, but these are rare\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when we increase the standard deviation to 10?\n\n\nFor this question, you’ll need to update the sd input to rnorm() to be 10 rather than 2.\n\n\nWith more variability in each data sample - we can see that the variance in the sampling distribution has increased. In other words, each sample can have extremely different estimates of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nIt is still common to get estimates of the mean of 0.5 or around 1.5.\nWith this much variability, some data samples have a mean estimate as far away as -1 or 3!\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nBack in week 4 we computed the following descriptives for participants confidence in a range of face processing tasks.\n\nCan you compute the sampling distribution of the mean for the AIConfidenceBefore variable?\n\n\nFor this question, you’ll need to the number of data samples, the mean and the standard deviation using the values in hte descriptive statistics.\nIts also a good idea to update the xlim of your histogram to something around c(2, 7).\n\n\nThe code and result should look something like this:\n\nBased on this, we can see that the most commonly estimated mean value is very close to our expected mean of 4.57 and that almost every data sample has a mean of between 4 and 5.\nThis is a useful way to visualise how accurate our sample mean estimates are likely to be.\n\n\n\n\n\n\n\n\nThe standard error of the mean is a very useful metric which tells us how our estimate of the mean is. We compute it using a formula most of the time if the parametric assumptions hold - but the best way to compute this is directly from the sampling distribution we have just created!\nOnce we have a sampling distribution for the mean, the standard error of the mean is simply the standard deviation of that samping distribution. Simple.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and copy your code loop into it before starting this segment.\nWe will make large changes to the code and you may want to refer back to the previous parts for revision later.\n\n\nWe can compute sampling distributions for any summary statistic that we want to. This can be useful for quantifying sampling variability properties other than the mean. One straightforward example is the sampling distribution of the maximum.\nLet’s use the heights of human women as an example. According to Our World in Data, the height of women born between 1980 and 1994 is normally distributed with a mean values of 164.7cm and the standard deviation is 7.07cm.\nWe can use these parameters to define a population that we can use to simulate our data samples. This code will draw a data sample of the height of a single person.\nrnorm(1, mean=164.7, sd=7.07)\nWe can plug this into our code to compute the sampling distributions for height. The full loop will look like this - remember to update the xlim input to the histograms as well!\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(10, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\nThis is the sampling distribution of the mean for samples of 10 data points draw from the population distribution of women’s heights. The sampling distribution shows that the average height of a group of 10 women drawn randomly from the population will nearly always fall between 160cm and 170cm.\n\nWe can adapt this code to ask other questions - for example how tall is the tallest person in a group of 10 women likely to be?\nWe can adapt our code to compute this sampling distribution of maximums as well. Here is an exmaple - look out for sections with NEW CODE indicating a change.\nCritically, we use the max() function to return the maximum value of each data sample, in exactly the same way that we used mean() to compute the average.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nmax_values = numeric(num_simulations)  # &lt;- NEW CODE\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(10, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n    max_values[i] = max(sample)  # &lt;- NEW CODE\n}\n\n# Plot the sampling distribution\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\n# NEW CODE\nhist(max_values, main = \"Sampling Distribution of Maximums\", xlim=c(130, 200),\n     xlab = \"Sample Maximum\", col = \"skyblue\", border = \"white\")\n    \n\nThis shows us that the tallest person in a group of 10 women is likely to be between 170cm and 180cm.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute the sampling distribution of the minimum value to compute the height of the shortest person in groups of 10 women?\nUpdate the code that you have already written.\n\n\nTake a look at the changes highlighted in the previous section, can you repeat those changes to include an equivalent to the maximum?\n\n\nThe following code computes and displays the sampling distribution of the minimum value.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nmax_values = numeric(num_simulations)\nmin_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(4, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n    max_values[i] = max(sample)\n    min_values[i] = min(sample)\n    \n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\nhist(max_values, main = \"Sampling Distribution of Maximums\", xlim=c(130, 200),\n     xlab = \"Sample Maximum\", col = \"skyblue\", border = \"white\")\n    \nhist(min_values, main = \"Sampling Distribution of Minimums\", xlim=c(130, 200),\n     xlab = \"Sample Minimum\", col = \"skyblue\", border = \"white\")\n\n\n\n\n\n\n\n\n\nThroughout this course, we have seen many data variables that are not normally distributed. Examples include dice rolls, reaction times and the intrusive memories from last week. These are interesting datasets but don’t follow some of the nice assumptions that we can use with normally distributed data.\nNow that we have our loops to compute sampling distributions, we can explore how much of a problem non normally distributed data is for our estimates.\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and copy your code loop from section 5 into it before starting this segment.\nWe will make large changes to the code and you may want to refer back to the previous parts for revision later.\n\n\nLet’s simulate some data for an experiment. Let’s simulate data from an experiment where participants indicate their confidence in performing some action on a scale from 1 to 10. The absolute truth of these data observations is that every participant has a equal probability of selecting any confidence from 1 to 10 - this is known as a uniform distribution and it is very different to a normal distribution.\nWe can simulate this data using runif() - note that this is an abbreviation of ‘random uniform’, not an indication to ‘run if’ something happens…\nTo use runif() we specify an n as usual in addition to the smallest and largest values that might occur.\nrunif(n, min = 1, max = 10)\nAs usual, we can slot this into our data loop to simulate our uniform distribution. Let’s start by drawing a single value during each repetition of the experiment. This will visualise our distribution.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = runif(1, min = 1, max = 10)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(0, 11),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\nYou can run this a few times to verify that the histogram shows a nearly uniform distribution where all outcomes are approximately equally likely to occur.\nNow, something odd happens when we increase the size of each data sample drawn from our population parameters. Rather than visualising the response of a single individual - lets take a look at the average response of 10 different individuals.\nUpdate the code to have an n of 10 rather than 1 in the call to runif().\n\nNow, though the raw data are uniformly distributed - the averages of uniform variables look like they are more normally distributed. Strange.\nThis is a property called the Central Limit Theorem. The Central Limit Theorem states that, given a sufficiently large sample size, the sampling distribution of the sample mean will be approximately normally distributed, regardless of the shape of the population distribution.\nThis is important as it tells us that the properties of data can change when we make computations from it. In our example, each individual response is uniformly distributed but the average response is approximately normally distributed.\nSecondly, this is why the normal distribution is so important. The Central Limit Theorem means that is weirdly likely that we will encounter normally distributed data variables - any variable that is an aggregate of other things becomes more and more normally distributed. We often deal with this sort of composite data in Psychology,\nThe learning statistics with jamovi textbook as an excellent description of central limit theorem if you want more information.\n\n\n\nWe’ve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#the-dataset",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#the-dataset",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "TLDR: There is no data to download! we’re going to make the data ourselves.\nThe dataset this week is very simple - we’re going to create it ourselves using R. At this point, we know a lot about the types of hypotheses that we can ask when comparing the averages of groups within a dataset. We can check our understanding of different aspects of these statistical tests by simulating data samples that have known properties (eg mean and standard deviation) and checking if the outcome of the t-test is what we would expect.\nThis way we can numerically validate the tests we’re running and take a look under the hood and see what sort of processes Jamovi and R are using to compute the tests for us.\n\n\n\n\n\n\nContent Info\n\n\n\nThere is a lot of code in this session - don’t be afraid to ask for help from your tutors!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#the-challenge",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#the-challenge",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "Today, we’ll first introduce some concepts and functions in R to simulate a data sample and compute our own one sample t-test. With that in hand, we will set up code to run thousands of tests in one go to help us to better understand sampling distributions.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#simulating-data-samples-in-r",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#simulating-data-samples-in-r",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "The rnorm() function in R is used to generate random numbers from a normal (Gaussian) distribution. A call to rnorm() looks like this:\nrnorm(n, mean = 0, sd = 1)\nWe can see that rnorm() takes three values as inputs\n\n\n\n\n\n\n\ninput\nDescription\n\n\n\n\nn\nThe number of random numbers you want to generate, we always need to specify this.\n\n\nmean\nThe mean (average) of the normal distribution. The default value is 0 if it isn’t specified\n\n\nsd\nThe standard deviation of the normal distribution. The default value is 1 if it isn’t specified\n\n\n\nSo we could tweak these input arguments to simulate any number of data points from any normal distribution that we wish.\n\n\n\n\n\n\nCheck Your Understanding - R code\n\n\n\nWhat code would we need to write to simulate the following?\n20 data points from a distribution with mean of 1 and standard deviation of 2\n\nR code here -&gt; .\n\n33 data points from a distribution with mean of -3 and standard deviation of 1.23\n\nR code here -&gt; .\n\nfive thousand data points from a distribution with mean of 1.789 and standard deviation of 18.2\n\nR code here -&gt; .\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nOpen Jamovi, install the Rj add-on and open a new Rj window before we go any further\n\n\nWe can use rnorm() within the Rj code window as usual. Here I’m simulating 20 data points from a distribution with a mean of 1 and a standard deviation of 2. Try running the same code in your window.\n\nThese 20 simulated numbers could now represent a data sample for a variable in an analysis. The difference is we have absolute confidence about the ‘truth’ of the population statistics underlying this data sample.\nYou may notice that you do get 20 numbers, but that you get a different 20 numbers than I did! This is as rnorm() works hard to generate random numbers that are different each time.\n\n\n\n\n\n\nKey step\n\n\n\nYou can verify the random nature of the simulated data by running your code multiple times.\nEven though the code stays the same you will get a different data sample each time. Remember that the code and the underlying distribution remain the always the same but the data sample is changing.\n\n\nThis is actually a good thing for us. Real experiments and data collection always have an element of random luck and we want our simulations to reflect this as well.\n\n\n\n\n\n\nData skills - revision on sampling and populations\n\n\n\nSampling is a fundamental process in statistics where a subset of individuals from a population is selected to estimate characteristics of the whole population. This is built from the assumption that the subset of individuals in our sample is representative of that population.\nSome samples will be more representative than others. We can do a lot of practical work and planning to get the best possible sample for our research but there is always some random chance in who ends up in a data sample and who does not.\n\nThere are two main reasons why a sample might not be representative.\nBias if the data collection has included some bias (intentional or accidental) towards a subgroup of our population then it is very likely that this will lead to an unrepresentative sample. This is not something we can solve with statistics. Good reseach design, consideration of ethics and thorough planning is the only way to avoid systematic biases.\nLuck even if we have a unbiased data collection approach, two samples that draw from the sample population can still be different due to random chance. This will always be true when we are restricted to looking at a subset of the full population - fortunately, this is something we can quantify with statistics. We’ll explore how in this session.\nThe code we write this week will simulate drawing a subset of individuals from a population. Each iteration of the code could be considered as a separate replication of the same experiment.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#estimating-the-samping-distribution-of-the-mean",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#estimating-the-samping-distribution-of-the-mean",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "In week 2 we discussed how sampling relatively small amount of data from a wider population can impact our estimates of values like the mean.\nTo recap, let’s say we’re interested in computing the average attainment for secondary school children on a particular vocabulary test. Our target population might be ‘all secondary school students in the UK’ but is it practically impossible to get data from every single student currently in secondard school. We have to make do with smaller data samples of a few tens or few hundreds of students (or even more, if we’re lucky).\nOur simulations do exactly this. The parameters we pass into rnorm() define ‘population level’ characteristics in the form of a normal distribution and each time we run the function we draw a ‘sample’ of a fixed number of data points from that population. We can think of this as rerunning our data sampling every time the function is executed. This would be massively complicated, time consuming and expensive in real life, but with a few assumptions a computer can simulate it in a fraction of a second.\nWe can now start to explore how sampling variability - AKA the random differences between data samples from the same population - can impact our estimates of the mean and standard deviation.\nUpdate your code to store the data sample into a variable and compute the mean with mean() and standard deviation with sd(). The result might look like this.\nsample = rnorm(20, mean=1, sd=2)\n\n# Compute the mean\nprint('Mean:')\nprint(mean(sample))\n\n# Compute the standard deviation\nprint('Standard Deviation:')\nprint(sd(sample))\n\nRunning this code will show the mean and standard deviation of each individual data sample, try running this a few times. Notice that the values are sometimes close to our ‘population’ estimates of mean = 1 and standard deviation = 2 but also sometimes quite far away.\nTry tweaking your code to answer these questions\n\n\n\n\n\n\n Data Skills - is the estimate of the mean better or worse with a sample of 200?\n\n\n\n\n\nUpdate your code to use this data simulation\nsample = rnorm(200, mean=1, sd=2)\nRunning this a few times should show you mean values much closer to 1 and standard deviations much closer to 2 than we saw with a sample of 20 values. They still won’t be perfect though.\n\n\n\n\n\n\n\n\n\n Data Skills - is the estimate of the mean better or worse when the population standard deviation is 10?\n\n\n\n\n\nUpdate your code to use this data simulation\nsample = rnorm(200, mean=1, sd=10)\nRunning this a few times should make your estimates of the mean and standard deviation much more variable again. When variability in the population is very wide, large data samples won’t necessarily make up the difference.\n\n\n\nWe can get an informal sense of sampling variability by clicking to rerun this code a few times, but R code allows us to do much better, we can tweak the code to rerun this analysis hundreds or thousands of times to explicitly quantify the variability we see.\nTo do this, we’ll need a code element called a loop which looks something like this\nnum_simulations = 10\n\n# Loop to perform simulations\nfor (i in 1:num_simulations) {\n    print(i)\n}\nThere are two aspects to this code block. The first sets the number of simulated data samples we want to draw.\nnum_simulations = 10\nThis line sets the variable num_simulations to 10. This means you want to perform 10 iterations in the loop.\nThe second part is the loop itself:\nfor (i in 1:num_simulations) {\n    print(i)\n}\nThis is a for loop that will iterate from 1 to num_simulations (which is 10 in this case). Here’s what happens inside the loop:\n\nInitialization: The loop starts with i set to 1.\nCondition: The loop will continue to run as long as i is less than or equal to num_simulations.\nIncrement: After each iteration, i is incremented by 1.\n\nInside the loop, the print(i) statement is executed, which prints the current value of i to the console.\nThe output of this code will be the numbers from 1 to 10, each printed on a new line. The full output might look like this:\n\nPrinting the value of i is instructive but really we want our loop to include some interesting computations. Next, we should move our sampling code within the curly brackets {} of the loop.\nEverything within the those curly brackets will be repeated on every iteration of the loop. So if we includ the code to draw a sample and compute the mean, it will be repeated every time. Here is an example:\n# My loop\nnum_simulations = 10\n\nfor (i in 1:num_simulations) {\n    sample = rnorm(20, mean=1, sd=10)\n    print(mean(sample))\n}\n\nThe output of the loop is now the mean of each data sample, though the exact values you get will be slightly different to mine.\nWe can see that 20 samples drawn from a population with mean = 1, sd = 10 we have extremely variable estimates of the mean. Try reducing the standard deviation back down to 2 to see the result.\n# My loop\nnum_simulations = 10\n\nfor (i in 1:num_simulations) {\n    sample = rnorm(20, mean=1, sd=2)\n    print(mean(sample))\n}\n\nThese are much closer to our expected population value of 1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#visualising-the-samping-distribution-of-the-mean",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#visualising-the-samping-distribution-of-the-mean",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "We’re very close to completing our loop. There is one final issue to sort.\nAt the moment, the estimated sample is lost every time the code in the loop restarts - we need to store the whole set of mean values for every single simulation if we want to properly see what is going on.\nFor this we need to add two elements to the loop. Firstly we need to create a variable to store the means we estimate. We do this before the loop starts.\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nand secondly, within the loop, we need to add the estimated mean to our new mean_values variable.\n# Compute and store the mean of the data sample\nmean_values[i] = mean(sample)\nmean_values can store many numbers altogether in a list or vector. Each individual element can be accessed using the square bracket notation. For example mean_values[5] would access the fifth value in the list. The code above will store each value in to the ith position in the list so that we keep them all.\nThe final code should look like this:\n# My loop\nnum_simulations = 10\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(20, mean=1, sd=2)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\nmean_values now contains all of our simulated means that we can visualise using a histogram with the hist() function.\n\nWhich will produce a simple Histogram of the values for us. Note that, as ever, your histogram will be a little different to mine. Try rerunning the code a few times to get a sense of the variabity.\nWe can make our plot a bit more attractive and informative by specifying some additional inputs. These just change the visualisation of the histogram rather than the analysis.\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Means\", xlim=c(-2, 4),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\nThis has a lot of inputs, lets break down what they all mean\n\n\n\nInput\nDescription\n\n\n\n\nmean_values\nThe data sample to visualise\n\n\nmain\nThe main title\n\n\nxlim\nThe start and end points of the x-axis within a collection\n\n\nxlab\nThe label for the x-axis\n\n\ncol\nThe colour of the bars\n\n\nborder\nThe colour of the borders of the bars\n\n\n\nAdding these to the function call makes a much nicer and more informative image.\n\nYou can tweak the titles and axis labels to be sometime informative to you, and you can change the colours to be anything you like. Try customising the plot - update the labels and colours as you prefer them.\n\n\n\n\n\n\nClick for names of colours in R\n\n\n\n\n\nYou can use the colour names defined here to customise your histogram.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#understanding-the-sampling-distribution-of-the-mean",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#understanding-the-sampling-distribution-of-the-mean",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "Once your histogram is customised and ready to go - we can put our code to work and understand the core concepts.\nLet’s review what we have so far.\n\nWe have constructed a loop that draws data samples from a defined population\nWe compute and store the mean of that data sample\nWe use a histogram to visualise the distribution of means estimated from all of the data samples\nThis is the sampling distribution of the mean.\n\n\n\n\n\n\n\nKey step\n\n\n\nMake sure you have an organised script ready to go at this point, try adding comments to understand what each line does. You can delete some of the older parts of the code that aren’t included in the loop if you like. Your code should look something like this.:\n# My loop\nnum_simulations = 10\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(20, mean=1, sd=2)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Means\", xlim=c(-2, 4),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\n\nRecall that the variance of the sampling distribution is one of its key parameters - it tells us how precisely our estimated means map onto the known population mean. (Recall from lecture 3 that the standard error of the mean can be computed from this number).\nSampling distributions with wider variances indicate that we have less precisely estimated the mean, whereas tighter distributions indicate that we have more confidence. Let’s use our code to illustrate this concept.\n\n\n\n\n\n\nKey step\n\n\n\nChange your code to repeat the simulation 1000 times rather than just 10, you should get a much more ‘normal’ distribution that changes less when you rerun the code.\n\n\n\nThis distribution is the sampling distribution of the mean for the simulated data. We can learn the following things:\n\nWe are most likely to estimate a value of the mean that is close to 1\nThe spread of the distribution indicates that values between 0.5 and 1.5 are still fairly common\nWe will very occasionally estimate a mean around 0 or around 2\n\nTry changing your code to answer these questions.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when each data sample has 100 data points rather than just 20?\n\n\nFor this question, you’ll need to update the first input to rnorm() to have 100 data points rather than 20.\n\n\nWith more data in each data sample - we can see that the spread in the sampling distribution has reduced. In other words, each sample has a similar estimate of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nWe will very occasionally estimate a mean around 0.5 or around 1.5, but these are rare\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when we increase the standard deviation to 10?\n\n\nFor this question, you’ll need to update the sd input to rnorm() to be 10 rather than 2.\n\n\nWith more variability in each data sample - we can see that the variance in the sampling distribution has increased. In other words, each sample can have extremely different estimates of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nIt is still common to get estimates of the mean of 0.5 or around 1.5.\nWith this much variability, some data samples have a mean estimate as far away as -1 or 3!\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nBack in week 4 we computed the following descriptives for participants confidence in a range of face processing tasks.\n\nCan you compute the sampling distribution of the mean for the AIConfidenceBefore variable?\n\n\nFor this question, you’ll need to the number of data samples, the mean and the standard deviation using the values in hte descriptive statistics.\nIts also a good idea to update the xlim of your histogram to something around c(2, 7).\n\n\nThe code and result should look something like this:\n\nBased on this, we can see that the most commonly estimated mean value is very close to our expected mean of 4.57 and that almost every data sample has a mean of between 4 and 5.\nThis is a useful way to visualise how accurate our sample mean estimates are likely to be.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#standard-error-of-the-mean",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#standard-error-of-the-mean",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "The standard error of the mean is a very useful metric which tells us how our estimate of the mean is. We compute it using a formula most of the time if the parametric assumptions hold - but the best way to compute this is directly from the sampling distribution we have just created!\nOnce we have a sampling distribution for the mean, the standard error of the mean is simply the standard deviation of that samping distribution. Simple.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#optional---sampling-distributions-of-the-maximum",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#optional---sampling-distributions-of-the-maximum",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "Key step\n\n\n\nOpen a new Rj window and copy your code loop into it before starting this segment.\nWe will make large changes to the code and you may want to refer back to the previous parts for revision later.\n\n\nWe can compute sampling distributions for any summary statistic that we want to. This can be useful for quantifying sampling variability properties other than the mean. One straightforward example is the sampling distribution of the maximum.\nLet’s use the heights of human women as an example. According to Our World in Data, the height of women born between 1980 and 1994 is normally distributed with a mean values of 164.7cm and the standard deviation is 7.07cm.\nWe can use these parameters to define a population that we can use to simulate our data samples. This code will draw a data sample of the height of a single person.\nrnorm(1, mean=164.7, sd=7.07)\nWe can plug this into our code to compute the sampling distributions for height. The full loop will look like this - remember to update the xlim input to the histograms as well!\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(10, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\nThis is the sampling distribution of the mean for samples of 10 data points draw from the population distribution of women’s heights. The sampling distribution shows that the average height of a group of 10 women drawn randomly from the population will nearly always fall between 160cm and 170cm.\n\nWe can adapt this code to ask other questions - for example how tall is the tallest person in a group of 10 women likely to be?\nWe can adapt our code to compute this sampling distribution of maximums as well. Here is an exmaple - look out for sections with NEW CODE indicating a change.\nCritically, we use the max() function to return the maximum value of each data sample, in exactly the same way that we used mean() to compute the average.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nmax_values = numeric(num_simulations)  # &lt;- NEW CODE\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(10, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n    max_values[i] = max(sample)  # &lt;- NEW CODE\n}\n\n# Plot the sampling distribution\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\n# NEW CODE\nhist(max_values, main = \"Sampling Distribution of Maximums\", xlim=c(130, 200),\n     xlab = \"Sample Maximum\", col = \"skyblue\", border = \"white\")\n    \n\nThis shows us that the tallest person in a group of 10 women is likely to be between 170cm and 180cm.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute the sampling distribution of the minimum value to compute the height of the shortest person in groups of 10 women?\nUpdate the code that you have already written.\n\n\nTake a look at the changes highlighted in the previous section, can you repeat those changes to include an equivalent to the maximum?\n\n\nThe following code computes and displays the sampling distribution of the minimum value.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nmax_values = numeric(num_simulations)\nmin_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(4, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n    max_values[i] = max(sample)\n    min_values[i] = min(sample)\n    \n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\nhist(max_values, main = \"Sampling Distribution of Maximums\", xlim=c(130, 200),\n     xlab = \"Sample Maximum\", col = \"skyblue\", border = \"white\")\n    \nhist(min_values, main = \"Sampling Distribution of Minimums\", xlim=c(130, 200),\n     xlab = \"Sample Minimum\", col = \"skyblue\", border = \"white\")",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#optional---sampling-distributions-from-non-normal-data",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#optional---sampling-distributions-from-non-normal-data",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "Throughout this course, we have seen many data variables that are not normally distributed. Examples include dice rolls, reaction times and the intrusive memories from last week. These are interesting datasets but don’t follow some of the nice assumptions that we can use with normally distributed data.\nNow that we have our loops to compute sampling distributions, we can explore how much of a problem non normally distributed data is for our estimates.\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and copy your code loop from section 5 into it before starting this segment.\nWe will make large changes to the code and you may want to refer back to the previous parts for revision later.\n\n\nLet’s simulate some data for an experiment. Let’s simulate data from an experiment where participants indicate their confidence in performing some action on a scale from 1 to 10. The absolute truth of these data observations is that every participant has a equal probability of selecting any confidence from 1 to 10 - this is known as a uniform distribution and it is very different to a normal distribution.\nWe can simulate this data using runif() - note that this is an abbreviation of ‘random uniform’, not an indication to ‘run if’ something happens…\nTo use runif() we specify an n as usual in addition to the smallest and largest values that might occur.\nrunif(n, min = 1, max = 10)\nAs usual, we can slot this into our data loop to simulate our uniform distribution. Let’s start by drawing a single value during each repetition of the experiment. This will visualise our distribution.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = runif(1, min = 1, max = 10)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(0, 11),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\nYou can run this a few times to verify that the histogram shows a nearly uniform distribution where all outcomes are approximately equally likely to occur.\nNow, something odd happens when we increase the size of each data sample drawn from our population parameters. Rather than visualising the response of a single individual - lets take a look at the average response of 10 different individuals.\nUpdate the code to have an n of 10 rather than 1 in the call to runif().\n\nNow, though the raw data are uniformly distributed - the averages of uniform variables look like they are more normally distributed. Strange.\nThis is a property called the Central Limit Theorem. The Central Limit Theorem states that, given a sufficiently large sample size, the sampling distribution of the sample mean will be approximately normally distributed, regardless of the shape of the population distribution.\nThis is important as it tells us that the properties of data can change when we make computations from it. In our example, each individual response is uniformly distributed but the average response is approximately normally distributed.\nSecondly, this is why the normal distribution is so important. The Central Limit Theorem means that is weirdly likely that we will encounter normally distributed data variables - any variable that is an aggregate of other things becomes more and more normally distributed. We often deal with this sort of composite data in Psychology,\nThe learning statistics with jamovi textbook as an excellent description of central limit theorem if you want more information.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#summary",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#summary",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "We’ve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html",
    "href": "tutorials/rmb-week-9_one-way-anova.html",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "This week we will explore how to run ANOVA analyses to compare means between two or more groups. This is an important analysis as many experimental and scientific contexts cannot be simiplified down to two groups for a t-test, and running many t-tests would increase our chances of false positive findings..\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nOne-Way ANOVA\n\n\n\nLog transforms\n\n\n\nAssumptions of ANOVA\n\n\n\nEstimated Marginal Means\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nRun and explore ANOVA analyses in Jamovi\n\n\n\nCompute data transformations using Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWork with a published dataset\n\n\n\n\n\n\n\n\n\nContent Info\n\n\n\nThis is the same dataset that we used in week 7.\n\n\n\n\n\n\n\n\nContent Info\n\n\n\nThis tutorial refers to a video in which participants watched unpleasant events to simulate a traumatic experience in the lab. The content of this video is not disclosed or dicussed in any more detail than this during this tutorial - those who are interested can find more information in methods section the published article. This tutorial does explore the emotional impact that the video has on participants.\n\n\n\n\nFlashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we’re going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind’s eye and cause distress. We investigated whether reconsolidation—the process during which memories become malleable when recalled—can be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group.\n\n\n\nThis week - we will explore two of the four conditions to test the following hypothesis:\n\nEngaging in computer game and reactivation tasks after a trumatic event will reduce number of subsequent intrusive memories\n\nThis is similar to the hypothesis from week 7, but now we have allowed for several conditions in the experiment rather than just two.\nIf we find evidence for a difference in our omnibus ANOVA test we can explore post-hoc comparisons to describe the exact difference that is present.\n\n\n\n\n\n\nKey step\n\n\n\nOpen Jamovi and load in this weeks data file before going any further.\n\n\n\n\n\nAt this point - you can probably figure out how to run the basics of an ANOVA analysis in Jamovi. There are a lot of similarities with the t-test analyses we have run.\nTo run the ANOVA, open the ANOVA -&gt; ANOVA menu from the top ribbon.\n\n\n\n\n\n\nJamovi Note!\n\n\n\nMake sure you open the ‘ANOVA’ menu not the ‘One-way ANOVA’ menu. We will run a One-way ANOVA but will use the full ANOVA menu as it contains the full set of features and analyses that we will need. The One-way ANOVA menu is a streamlined option for very simple analyses.\n\n\nOnce you have the menu open, click Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary across to the Dependent Variable box and Condition across to the ‘Fixed Factors’. The first ANOVA results table will appear in the results.\nVery simple! but remember that it takes some additional work to check over and interpret the results from an ANOVA.\n\nLet’s understand the results table in detail. First let’s quickly review the theory:\nIn an ANOVA analysis we’re comparing the sum-of-squared error between two different models - one model has all groups with the same mean and the second model gives each group its individual mean. From this we can compute three different sets of sum-sqaures.\n\n\nSum-Square Error Total - the overall variability assuming that all groups have the same mean\nSum-Square Error Within - the variability assuming that each group has its own mean\nSum-Square Error Between - the change variability attributable to giving each group its own mean\n\nThe rationale is that if the groups have distinct means then Sum-Square Within will be much smaller than Sum-Square Total. This difference is quantified with Sum-Square Between.\nThese values are listed in the ANOVA table. The ‘Sum of Squares’ for Condition is the Sum-Square Between and the ‘Sum of Squares’ for Residuals is Sum-Square Within. (This terminology can get a little complex…).\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table to complete this statement reporting the results of the ANOVA. Fill in the spaces provided.\nThe Sum-Square Error Between is  and the Sum-Square Error Within is .\n\n\nThe ANOVA table from above…\n\n\n\n\n\n\nThe next step is to compute the Mean-Squares. We need this additional step as the Sum-Squares will are senstive to the sample size and number of groups - we have to make a correction for this. The Mean-Squares are computed with two fractions.\n\\[\n\\text{Mean-Square Error Within} = \\frac{\\text{Sum-Square Error Within}}{\\text{Number of Participants} - \\text{Number of Groups}}\n\\]\n\\[\n\\text{Mean-Square Error Between} = \\frac{\\text{Sum-Square Error Between}}{\\text{Number of Groups} - 1}\n\\]\nThe ingredients for these fractions are already in our ANOVA table. The top of the fraction is the Sum of Squares we’ve seen earlier. The values on the bottom of the fraction are actually precomputed for us as the Degrees of Freedom.\nThe degrees of freedom for Condition (df between) is \\(\\text{Number of Groups} - 1\\). In our case we have four groups so the value is 3. The degrees of freedom for Residuals (df within) is \\(\\text{Number of Participants} - \\text{Number of Groups}\\). We have 72 participants and 4 groups so the value is 68.\nSo, working across the ANOVA table in rows - we can see that the value of the Mean-Square Errors in third column is simply the first column divided by the second.\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table to complete this statement reporting the results of the ANOVA. Fill in the spaces provided.\nThe Mean-Square Error Between is:\n\\(\\frac{\\text{Sum-Square Error Between}}{\\text{Number of Groups} - 1}\\) = \\(\\frac{\\text{Sum-Square Error Between}}{\\text{df between}}\\) = \\(\\frac{115}{3}\\) = \nThe Mean-Square Error Within is:\n\\(\\frac{\\text{Sum-Square Error Within}}{\\text{Number of Participants} - \\text{Number of Groups}}\\) = \\(\\frac{\\text{Sum-Square Error Between}}{\\text{df within}}\\) = \\(\\frac{686}{68}\\) = \n\n\nThe ANOVA table from above…\n\n\n\n\n\n\nWe now have the ingredients for our ANOVA! The F-statistics for the ANOVA is\n\\[\n\\text{F} = \\frac{\\text{Mean-Square Error Between}}{\\text{Mean-Square Error Within}}\n\\]\nThis is the ratio of our two Mean-Square fractions from above.\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table to complete this statement reporting the results of the ANOVA. Fill in the spaces provided.\nThe F statistic is:\n\\(\\frac{\\text{Mean-Square Error Between}}{\\text{Mean-Square Error Within}}\\) = \\(\\frac{38.3}{10.1}\\) = \n\n\nThe ANOVA table from above…\n\n\n\n\n\n\nThere are a lot of values in the ANOVA table but the relationship between them is straightforward when you know the logic behind it.\nTo report a one-way ANOVA to APA format you can follow this broad template.\n\nA one-way ANOVA was conducted to compare the effect of [independent variable] on [dependent variable]. There was a statistically significant difference in [dependent variable] between groups, F([df between], [df within]) = [F value], p = [p value].\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table to complete this statement reporting the results of the ANOVA. Fill in the spaces provided.\nA one-way ANOVA was conducted to compare the effect of the number of intrusive memories in the week after the experimenttask completed after watching the film on the number of intrusive memories in the week after the experimenttask completed after watching the film . There was a statistically significantno significant difference in number of intrusive memoriespost film task between groups, F(, ) = , p = .\n\n\nThe ANOVA table from above…\n\n\n\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new ANOVA analysis and run a one-way ANOVA to test the control condition asking whether there was a difference between groups in intrusive memories in the time before the experiment started.\nThe variable Days_Zero_Image_Based_Intrusions_in_Intrusion_Diary\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table from your new control analysis. Fill in the spaces provided.\nA one-way ANOVA was conducted to compare the effect of number of intrusive memories in the time before the experimentthe task completed after watching the film on number of intrusive memories in the time before the experimentthe task completed after watching the film. There was a statistically significantno significant difference in the number of intrusive memoriesthe task completed after watching the film between groups, F(, ) = , p = .\n\n\nThe ANOVA table from the control analysis.\n\n\n\n\n\n\n\n\n\nThere are several possible effect sizes we can use for ANOVA analyses. We will use \\(\\eta^2\\) (pronouced eta-squared) for one-way ANOVA. The Learning Statistics with Jamovi textbook has a nice summary of how to interpret \\(\\eta^2\\).\n\nThe interpretation of \\(\\eta^2\\) is equally straightforward. It refers to the proportion of the variability in the outcome variable that can be explained in terms of the predictor. A value of 0 means that there is no relationship at all between the two, whereas a value of 1 means that the relationship is perfect. Better yet, the value is very closely related to \\(R^2\\) and has an equivalent interpretation\n\nSlightly abridged quote from Learning Statistics with Jamovi\n\n\nVery straightforward indeed, we can think of \\(\\eta^2\\) as an \\(R^2\\) for the ANOVA. It is also straightforward to compute - just check the box under ‘Effect Sizes’.\n\nThere are two other options for ANOVA effect sizes:\n\npartial \\(\\eta^2\\) will be useful for more complex ANOVA designs you will work with in other modules.\n\\(\\omega^2\\) is an unbiased estimator that could be used in place of \\(\\eta^2\\). Some sources recommend that we use \\(\\omega^2\\) rather than \\(\\eta^2\\) (Albers and Lakens 2018). You could use either and might encounter either in the literature.\n\n\n\n\nAn ANOVA analysis returns a significant effect that indicates that there is a difference between the group means but doesn’t tell us anything about which groups are actually different.\nIf, and only if, the overall ANOVA returns a significant result - we can use post-hoc tests to explore what the specific differences actually are. This is a critical step for the interpretation of our results.\nIn the example above we saw that the experimental task that participants complete after watching the film does change the mean number of intrusive memories that the participants experience. The ANOVA table doesn’t say more than this - as far as we know from this analysis it is possible that playing Tetris may have increased or decreased the number of intrusive memories.\nPost-hoc tests are the answer for this - and post hoc tests are based on ‘Estimated Marginal Means’. These sound more complex than they are. Estimated marginal means are adjusted means (and associated standard errors etc) that account for the assumptions of a statistical model and for the effects of any other variables in a model.\nSo, for an ANOVA the estimated means are adjusted for the assumption of homogeneity of variance - that each group has the same variance. We do not have any other variables to adjust for in this analysis but this is relevant for future Analysis of Co-Variance (ANCOVA) models that you may encounter in future modules.\n\n\n\n\n\n\nKey step\n\n\n\nFind the ‘Estimated Marginal Means’ drop down menu and add ‘Condition’ to the Terms box to compute the estimated marginal mean for each group within Condition. Make sure to include the plots and tables in the ouputs (though the x axis labels for this particular plot aren’t very good…).\n\n\n\nThe table gives us our four marginal means summarising the mean of each group alongside a 95% confidence interval for that estimate and the Standard Error for each mean.\nThe chart gives us a more human readable summary of the means with the associated confidence intervals.\n\n\n\n\n\n\n Data Skills - which conditions do you think are likely to be significantly different?\n\n\n\n\n\nThe No-Task Control and Reactivation+Tetris conditions have the largest difference in mean, with Tetris Only and Reactivation Only somewhere in between.\n\n\n\n\n\n\n\n\n\n Data Skills - why are the standard errors for each mean all the same?\n\n\n\n\n\nRemember that these are the means computed under the assumptions of the ANOVA, not the normal means we see in the descriptive statistics.\nThis ANOVA assumes homogeneity of variance and the marginal mean can take this into account. If we computed the descriptive statistics the usual way we would see the individual estimate of the standard error for each group.\n\n\n\nSo, lets run our post-hoc tests!\n\n\n\n\n\n\nKey step\n\n\n\nFind the ‘Post-Hoc Tests’ drop down menu and add ‘Condition’ to the right hand box to compute the post hoc comparison between each pair of groups within Condition.\nMake sure to include ‘Tukey’ correction and add the Cohen’s d effect size.\n\n\n\nThe Comparison columns lists the six pairwise comparisons that can be assembled from our four conditions. As before, the standard errors (SE column) are the same for all of the comparisons. Remember that these results are based on the estimated marginal means rather than the standard means we would use outside an ANOVA.\n\n\n\n\n\n\n Data Skills - which comparisons show a significant difference?\n\n\n\n\n\nThe No-Task Control and Reactivation+Tetris conditions show a significant difference t(68) = 3.044, p-tukey = 0.017.\nThe Reactivation+Tetris and Reactivation Only conditions show a significant difference t(68) = -2.781, p-tukey = 0.034.\n\n\n\n\n\n\n\n\n\n Data Skills - which direction are the significant differences? ie which condition is larger?\n\n\n\n\n\nWe can use the estimated marginal means tables to be sure of these results.\nThe No-Task Control group (M=5.11) had more intrusive memories than the Reactivation+Tetris group (M=1.89).\nThe Reactivation+Tetris group (M=1.89) had fewer intrusive memories than the Reactivation Only group (M=4.83).\n\n\n\n\n\n\n\n\n\n Data Skills - How do the results change if we use no correction to the p-values rather than Tukey correction?\n\n\n\n\n\nYou can add the uncorrected p-value to the output table in the Post Hoc tests menu.\nThe Tukey correction aims to account for the multiple comparisons that we are making. It increases the p-values to make it less likely that we would consider a difference to be significant.\nCompare the p column with the p-tukey column. The uncorrected values are smaller - the Reactivation+Tetris to Tetris Only comparison is close to being significant in the uncorrected values.\n\n\n\n\n\n\n\nAs ever - the ANOVA makes certain assumptions about the data so that we can perform our statistics. These assumptions are similar to what we saw with t-tests and should all be in place for our ANOVA results to make a valid generalisation to the wide population.\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nMatch the ANOVA assumption to its definition.\n\n\n\n\n\n\n\nAssumption\nDefinition\n\n\n\n\nIndependence\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\nNormal distributions\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\nHomogeneity of variance\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\nCategorical factors\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\nData type\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\n\n\n\nTake a look at your lecture notes from week 8!\n\n\n\n\n\nIt is very straighforward to check these assumptions in Jamovi - find the ‘Assumption Checks’ menu and select all three options to run the Shapiro-Wilk test, Levene’s Test and the QQ plot.\n\n\n\n\n\n\n\n Data Skills - is the assumption of independence met?\n\n\n\n\n\nYes, this is a between subjects design in which each participant only contributes a single data point to this study. ANOVA analysis for repeated measures designs are possible - you might encounter these in a future module.\n\n\n\n\n\n\n\n\n\n Data Skills - is the assumption of homogeneity of variance met?\n\n\n\n\n\nNo, Levene’s test is significant indicating that the groups do not have equal variance.\nWe can explore this further by running descriptive statistics on the groups. We can see that the standard deviation of the Reactivation+Tetris group looks to be much smaller than the others. This is likely where the difference is coming from.\n\n\n\n\n\n\n\n\n\n\n Data Skills - is the for normally distributed data met?\n\n\n\n\n\nNo, Shapiro-Wilk’s test is significant indicating that the data is not normally distributed. This is confirmed by the QQ plot in which the top end of the data depart from the centre diagonal line.\nThe estimated marginal means plot from your earlier analysis can help explain why this is. Notice that the data points in that plot have a positive skew - ie a small number of participants seem to have a lot of intrusive memories.\n\n\n\n\n\n\n\n\n\n Data Skills - is the assumption of categorical factors met?\n\n\n\n\n\nYes, our Conditions variable is categorical. It contains our four groups without any implied order.\n\n\n\n\n\n\n\n\n\n Data Skills - is the assumed data type correct?\n\n\n\n\n\nYes, our Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary variable is ratio data. It contains numerical data in which zero means an absence of intrusive memories.\n\n\n\n\n\n\nLooks like our data do not meet the parametric assumptions of a standard ANOVA analysis. Its often a good idea to check assumptions early in the data analysis process…\nThere are non-parametric ANOVA analyses available - you could now run the Kruskall Wallis test on the data to perform a non parametric one way ANOVA.\nLet’s try a data transformation here though, the full parametric ANOVA is very flexible so it would be good to use it if we can.\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint - ANOVA TableHint - Assumption ChecksHint - Estimated Marginal MeansHint - Post Hoc Tests\n\n\nCompute a new data variable using the log transform on the Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary - review week 7 section 7 if you need reminder on how to do this.\nUse the results to complete this reporting of the ANOVA results.\n\nA Shaprio-Wilk test showed a significant departure from a normal distribution, W=0.877, p&lt;0.001. The data were transformed using a log transform.\nA second Shaprio-Wilk test showed that the log-transformed data is normally distributednot normally distributed, W=, p=. Levene’s test on the transformed data shows that we can assumewe can not assume homogeneity of variance across the groups, F(, ) = , p=\nA one-way ANOVA was conducted to on the transformed data to compare the effect of the number of intrusive memories in the week after the experimenttask completed after watching the film on the number of intrusive memories in the week after the experimenttask completed after watching the film. There was a statistically significantno significant difference in number of intrusive memoriespost film task between groups, F(, ) = , p = , \\(\\eta^2\\) = .\nPost hoc comparisons with Tukey correction were conducted to identify which pairs of conditions showed significant differences in their estimated marginal means. Three pairwise comparisons showed a significant difference. Note that we are reporting the log-transformed means with the post-hoc test, not the original means.\nThe ‘No-Task Control’ group (M=) had more instrusive memories than the ‘Reactivation+Tetris’ group (M=), t() = , p=, Cohen’s d=\nThe ‘Reactivation+Tetris’ group (M=) had fewer instrusive memories than the ‘Tetris Only’ group (M=), t() = , p=, Cohen’s d=.\nThe ‘Reactivation+Tetris’ group (M=) had fewer instrusive memories than the ‘Reactivation Only’ group (M=), t() = , p=, Cohen’s d=.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA is a flexible and powerful method for comparing the means of more than two groups whilst accounting for the exploding number of multiple comparisons we get from more complex experimental designs. ANOVA takes a bit more work to run, check and interpret than a t-test but the results from a single ANOVA analysis contain a huge amount of critical information about our data!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#the-dataset",
    "href": "tutorials/rmb-week-9_one-way-anova.html#the-dataset",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Flashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we’re going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind’s eye and cause distress. We investigated whether reconsolidation—the process during which memories become malleable when recalled—can be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#the-challenge",
    "href": "tutorials/rmb-week-9_one-way-anova.html#the-challenge",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "This week - we will explore two of the four conditions to test the following hypothesis:\n\nEngaging in computer game and reactivation tasks after a trumatic event will reduce number of subsequent intrusive memories\n\nThis is similar to the hypothesis from week 7, but now we have allowed for several conditions in the experiment rather than just two.\nIf we find evidence for a difference in our omnibus ANOVA test we can explore post-hoc comparisons to describe the exact difference that is present.\n\n\n\n\n\n\nKey step\n\n\n\nOpen Jamovi and load in this weeks data file before going any further.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#analysis-of-variance-in-jamovi",
    "href": "tutorials/rmb-week-9_one-way-anova.html#analysis-of-variance-in-jamovi",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "At this point - you can probably figure out how to run the basics of an ANOVA analysis in Jamovi. There are a lot of similarities with the t-test analyses we have run.\nTo run the ANOVA, open the ANOVA -&gt; ANOVA menu from the top ribbon.\n\n\n\n\n\n\nJamovi Note!\n\n\n\nMake sure you open the ‘ANOVA’ menu not the ‘One-way ANOVA’ menu. We will run a One-way ANOVA but will use the full ANOVA menu as it contains the full set of features and analyses that we will need. The One-way ANOVA menu is a streamlined option for very simple analyses.\n\n\nOnce you have the menu open, click Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary across to the Dependent Variable box and Condition across to the ‘Fixed Factors’. The first ANOVA results table will appear in the results.\nVery simple! but remember that it takes some additional work to check over and interpret the results from an ANOVA.\n\nLet’s understand the results table in detail. First let’s quickly review the theory:\nIn an ANOVA analysis we’re comparing the sum-of-squared error between two different models - one model has all groups with the same mean and the second model gives each group its individual mean. From this we can compute three different sets of sum-sqaures.\n\n\nSum-Square Error Total - the overall variability assuming that all groups have the same mean\nSum-Square Error Within - the variability assuming that each group has its own mean\nSum-Square Error Between - the change variability attributable to giving each group its own mean\n\nThe rationale is that if the groups have distinct means then Sum-Square Within will be much smaller than Sum-Square Total. This difference is quantified with Sum-Square Between.\nThese values are listed in the ANOVA table. The ‘Sum of Squares’ for Condition is the Sum-Square Between and the ‘Sum of Squares’ for Residuals is Sum-Square Within. (This terminology can get a little complex…).\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table to complete this statement reporting the results of the ANOVA. Fill in the spaces provided.\nThe Sum-Square Error Between is  and the Sum-Square Error Within is .\n\n\nThe ANOVA table from above…\n\n\n\n\n\n\nThe next step is to compute the Mean-Squares. We need this additional step as the Sum-Squares will are senstive to the sample size and number of groups - we have to make a correction for this. The Mean-Squares are computed with two fractions.\n\\[\n\\text{Mean-Square Error Within} = \\frac{\\text{Sum-Square Error Within}}{\\text{Number of Participants} - \\text{Number of Groups}}\n\\]\n\\[\n\\text{Mean-Square Error Between} = \\frac{\\text{Sum-Square Error Between}}{\\text{Number of Groups} - 1}\n\\]\nThe ingredients for these fractions are already in our ANOVA table. The top of the fraction is the Sum of Squares we’ve seen earlier. The values on the bottom of the fraction are actually precomputed for us as the Degrees of Freedom.\nThe degrees of freedom for Condition (df between) is \\(\\text{Number of Groups} - 1\\). In our case we have four groups so the value is 3. The degrees of freedom for Residuals (df within) is \\(\\text{Number of Participants} - \\text{Number of Groups}\\). We have 72 participants and 4 groups so the value is 68.\nSo, working across the ANOVA table in rows - we can see that the value of the Mean-Square Errors in third column is simply the first column divided by the second.\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table to complete this statement reporting the results of the ANOVA. Fill in the spaces provided.\nThe Mean-Square Error Between is:\n\\(\\frac{\\text{Sum-Square Error Between}}{\\text{Number of Groups} - 1}\\) = \\(\\frac{\\text{Sum-Square Error Between}}{\\text{df between}}\\) = \\(\\frac{115}{3}\\) = \nThe Mean-Square Error Within is:\n\\(\\frac{\\text{Sum-Square Error Within}}{\\text{Number of Participants} - \\text{Number of Groups}}\\) = \\(\\frac{\\text{Sum-Square Error Between}}{\\text{df within}}\\) = \\(\\frac{686}{68}\\) = \n\n\nThe ANOVA table from above…\n\n\n\n\n\n\nWe now have the ingredients for our ANOVA! The F-statistics for the ANOVA is\n\\[\n\\text{F} = \\frac{\\text{Mean-Square Error Between}}{\\text{Mean-Square Error Within}}\n\\]\nThis is the ratio of our two Mean-Square fractions from above.\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table to complete this statement reporting the results of the ANOVA. Fill in the spaces provided.\nThe F statistic is:\n\\(\\frac{\\text{Mean-Square Error Between}}{\\text{Mean-Square Error Within}}\\) = \\(\\frac{38.3}{10.1}\\) = \n\n\nThe ANOVA table from above…\n\n\n\n\n\n\nThere are a lot of values in the ANOVA table but the relationship between them is straightforward when you know the logic behind it.\nTo report a one-way ANOVA to APA format you can follow this broad template.\n\nA one-way ANOVA was conducted to compare the effect of [independent variable] on [dependent variable]. There was a statistically significant difference in [dependent variable] between groups, F([df between], [df within]) = [F value], p = [p value].\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table to complete this statement reporting the results of the ANOVA. Fill in the spaces provided.\nA one-way ANOVA was conducted to compare the effect of the number of intrusive memories in the week after the experimenttask completed after watching the film on the number of intrusive memories in the week after the experimenttask completed after watching the film . There was a statistically significantno significant difference in number of intrusive memoriespost film task between groups, F(, ) = , p = .\n\n\nThe ANOVA table from above…\n\n\n\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new ANOVA analysis and run a one-way ANOVA to test the control condition asking whether there was a difference between groups in intrusive memories in the time before the experiment started.\nThe variable Days_Zero_Image_Based_Intrusions_in_Intrusion_Diary\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table from your new control analysis. Fill in the spaces provided.\nA one-way ANOVA was conducted to compare the effect of number of intrusive memories in the time before the experimentthe task completed after watching the film on number of intrusive memories in the time before the experimentthe task completed after watching the film. There was a statistically significantno significant difference in the number of intrusive memoriesthe task completed after watching the film between groups, F(, ) = , p = .\n\n\nThe ANOVA table from the control analysis.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#effect-size-for-anova",
    "href": "tutorials/rmb-week-9_one-way-anova.html#effect-size-for-anova",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "There are several possible effect sizes we can use for ANOVA analyses. We will use \\(\\eta^2\\) (pronouced eta-squared) for one-way ANOVA. The Learning Statistics with Jamovi textbook has a nice summary of how to interpret \\(\\eta^2\\).\n\nThe interpretation of \\(\\eta^2\\) is equally straightforward. It refers to the proportion of the variability in the outcome variable that can be explained in terms of the predictor. A value of 0 means that there is no relationship at all between the two, whereas a value of 1 means that the relationship is perfect. Better yet, the value is very closely related to \\(R^2\\) and has an equivalent interpretation\n\nSlightly abridged quote from Learning Statistics with Jamovi\n\n\nVery straightforward indeed, we can think of \\(\\eta^2\\) as an \\(R^2\\) for the ANOVA. It is also straightforward to compute - just check the box under ‘Effect Sizes’.\n\nThere are two other options for ANOVA effect sizes:\n\npartial \\(\\eta^2\\) will be useful for more complex ANOVA designs you will work with in other modules.\n\\(\\omega^2\\) is an unbiased estimator that could be used in place of \\(\\eta^2\\). Some sources recommend that we use \\(\\omega^2\\) rather than \\(\\eta^2\\) (Albers and Lakens 2018). You could use either and might encounter either in the literature.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#post-hoc-tests",
    "href": "tutorials/rmb-week-9_one-way-anova.html#post-hoc-tests",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "An ANOVA analysis returns a significant effect that indicates that there is a difference between the group means but doesn’t tell us anything about which groups are actually different.\nIf, and only if, the overall ANOVA returns a significant result - we can use post-hoc tests to explore what the specific differences actually are. This is a critical step for the interpretation of our results.\nIn the example above we saw that the experimental task that participants complete after watching the film does change the mean number of intrusive memories that the participants experience. The ANOVA table doesn’t say more than this - as far as we know from this analysis it is possible that playing Tetris may have increased or decreased the number of intrusive memories.\nPost-hoc tests are the answer for this - and post hoc tests are based on ‘Estimated Marginal Means’. These sound more complex than they are. Estimated marginal means are adjusted means (and associated standard errors etc) that account for the assumptions of a statistical model and for the effects of any other variables in a model.\nSo, for an ANOVA the estimated means are adjusted for the assumption of homogeneity of variance - that each group has the same variance. We do not have any other variables to adjust for in this analysis but this is relevant for future Analysis of Co-Variance (ANCOVA) models that you may encounter in future modules.\n\n\n\n\n\n\nKey step\n\n\n\nFind the ‘Estimated Marginal Means’ drop down menu and add ‘Condition’ to the Terms box to compute the estimated marginal mean for each group within Condition. Make sure to include the plots and tables in the ouputs (though the x axis labels for this particular plot aren’t very good…).\n\n\n\nThe table gives us our four marginal means summarising the mean of each group alongside a 95% confidence interval for that estimate and the Standard Error for each mean.\nThe chart gives us a more human readable summary of the means with the associated confidence intervals.\n\n\n\n\n\n\n Data Skills - which conditions do you think are likely to be significantly different?\n\n\n\n\n\nThe No-Task Control and Reactivation+Tetris conditions have the largest difference in mean, with Tetris Only and Reactivation Only somewhere in between.\n\n\n\n\n\n\n\n\n\n Data Skills - why are the standard errors for each mean all the same?\n\n\n\n\n\nRemember that these are the means computed under the assumptions of the ANOVA, not the normal means we see in the descriptive statistics.\nThis ANOVA assumes homogeneity of variance and the marginal mean can take this into account. If we computed the descriptive statistics the usual way we would see the individual estimate of the standard error for each group.\n\n\n\nSo, lets run our post-hoc tests!\n\n\n\n\n\n\nKey step\n\n\n\nFind the ‘Post-Hoc Tests’ drop down menu and add ‘Condition’ to the right hand box to compute the post hoc comparison between each pair of groups within Condition.\nMake sure to include ‘Tukey’ correction and add the Cohen’s d effect size.\n\n\n\nThe Comparison columns lists the six pairwise comparisons that can be assembled from our four conditions. As before, the standard errors (SE column) are the same for all of the comparisons. Remember that these results are based on the estimated marginal means rather than the standard means we would use outside an ANOVA.\n\n\n\n\n\n\n Data Skills - which comparisons show a significant difference?\n\n\n\n\n\nThe No-Task Control and Reactivation+Tetris conditions show a significant difference t(68) = 3.044, p-tukey = 0.017.\nThe Reactivation+Tetris and Reactivation Only conditions show a significant difference t(68) = -2.781, p-tukey = 0.034.\n\n\n\n\n\n\n\n\n\n Data Skills - which direction are the significant differences? ie which condition is larger?\n\n\n\n\n\nWe can use the estimated marginal means tables to be sure of these results.\nThe No-Task Control group (M=5.11) had more intrusive memories than the Reactivation+Tetris group (M=1.89).\nThe Reactivation+Tetris group (M=1.89) had fewer intrusive memories than the Reactivation Only group (M=4.83).\n\n\n\n\n\n\n\n\n\n Data Skills - How do the results change if we use no correction to the p-values rather than Tukey correction?\n\n\n\n\n\nYou can add the uncorrected p-value to the output table in the Post Hoc tests menu.\nThe Tukey correction aims to account for the multiple comparisons that we are making. It increases the p-values to make it less likely that we would consider a difference to be significant.\nCompare the p column with the p-tukey column. The uncorrected values are smaller - the Reactivation+Tetris to Tetris Only comparison is close to being significant in the uncorrected values.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#checking-assumptions",
    "href": "tutorials/rmb-week-9_one-way-anova.html#checking-assumptions",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "As ever - the ANOVA makes certain assumptions about the data so that we can perform our statistics. These assumptions are similar to what we saw with t-tests and should all be in place for our ANOVA results to make a valid generalisation to the wide population.\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nMatch the ANOVA assumption to its definition.\n\n\n\n\n\n\n\nAssumption\nDefinition\n\n\n\n\nIndependence\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\nNormal distributions\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\nHomogeneity of variance\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\nCategorical factors\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\nData type\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\n\n\n\nTake a look at your lecture notes from week 8!\n\n\n\n\n\nIt is very straighforward to check these assumptions in Jamovi - find the ‘Assumption Checks’ menu and select all three options to run the Shapiro-Wilk test, Levene’s Test and the QQ plot.\n\n\n\n\n\n\n\n Data Skills - is the assumption of independence met?\n\n\n\n\n\nYes, this is a between subjects design in which each participant only contributes a single data point to this study. ANOVA analysis for repeated measures designs are possible - you might encounter these in a future module.\n\n\n\n\n\n\n\n\n\n Data Skills - is the assumption of homogeneity of variance met?\n\n\n\n\n\nNo, Levene’s test is significant indicating that the groups do not have equal variance.\nWe can explore this further by running descriptive statistics on the groups. We can see that the standard deviation of the Reactivation+Tetris group looks to be much smaller than the others. This is likely where the difference is coming from.\n\n\n\n\n\n\n\n\n\n\n Data Skills - is the for normally distributed data met?\n\n\n\n\n\nNo, Shapiro-Wilk’s test is significant indicating that the data is not normally distributed. This is confirmed by the QQ plot in which the top end of the data depart from the centre diagonal line.\nThe estimated marginal means plot from your earlier analysis can help explain why this is. Notice that the data points in that plot have a positive skew - ie a small number of participants seem to have a lot of intrusive memories.\n\n\n\n\n\n\n\n\n\n Data Skills - is the assumption of categorical factors met?\n\n\n\n\n\nYes, our Conditions variable is categorical. It contains our four groups without any implied order.\n\n\n\n\n\n\n\n\n\n Data Skills - is the assumed data type correct?\n\n\n\n\n\nYes, our Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary variable is ratio data. It contains numerical data in which zero means an absence of intrusive memories.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#anova-on-log-transformed-data",
    "href": "tutorials/rmb-week-9_one-way-anova.html#anova-on-log-transformed-data",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Looks like our data do not meet the parametric assumptions of a standard ANOVA analysis. Its often a good idea to check assumptions early in the data analysis process…\nThere are non-parametric ANOVA analyses available - you could now run the Kruskall Wallis test on the data to perform a non parametric one way ANOVA.\nLet’s try a data transformation here though, the full parametric ANOVA is very flexible so it would be good to use it if we can.\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint - ANOVA TableHint - Assumption ChecksHint - Estimated Marginal MeansHint - Post Hoc Tests\n\n\nCompute a new data variable using the log transform on the Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary - review week 7 section 7 if you need reminder on how to do this.\nUse the results to complete this reporting of the ANOVA results.\n\nA Shaprio-Wilk test showed a significant departure from a normal distribution, W=0.877, p&lt;0.001. The data were transformed using a log transform.\nA second Shaprio-Wilk test showed that the log-transformed data is normally distributednot normally distributed, W=, p=. Levene’s test on the transformed data shows that we can assumewe can not assume homogeneity of variance across the groups, F(, ) = , p=\nA one-way ANOVA was conducted to on the transformed data to compare the effect of the number of intrusive memories in the week after the experimenttask completed after watching the film on the number of intrusive memories in the week after the experimenttask completed after watching the film. There was a statistically significantno significant difference in number of intrusive memoriespost film task between groups, F(, ) = , p = , \\(\\eta^2\\) = .\nPost hoc comparisons with Tukey correction were conducted to identify which pairs of conditions showed significant differences in their estimated marginal means. Three pairwise comparisons showed a significant difference. Note that we are reporting the log-transformed means with the post-hoc test, not the original means.\nThe ‘No-Task Control’ group (M=) had more instrusive memories than the ‘Reactivation+Tetris’ group (M=), t() = , p=, Cohen’s d=\nThe ‘Reactivation+Tetris’ group (M=) had fewer instrusive memories than the ‘Tetris Only’ group (M=), t() = , p=, Cohen’s d=.\nThe ‘Reactivation+Tetris’ group (M=) had fewer instrusive memories than the ‘Reactivation Only’ group (M=), t() = , p=, Cohen’s d=.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#summary",
    "href": "tutorials/rmb-week-9_one-way-anova.html#summary",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "ANOVA is a flexible and powerful method for comparing the means of more than two groups whilst accounting for the exploding number of multiple comparisons we get from more complex experimental designs. ANOVA takes a bit more work to run, check and interpret than a t-test but the results from a single ANOVA analysis contain a huge amount of critical information about our data!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/frequently-asked-questions.html",
    "href": "tutorials/frequently-asked-questions.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "First check the start menu, if you can find both Jamovi and Rstudio then just open both from there and get started. Any programme that is not in the start menu can be opened using AppsAnywhere. This is available in the start menu of every campus computer – general information about AppsAnywhere can be found on this IT services page\n\n\n\nThis should be available as an icon on the desktop, or from within the start menu. If not, they you can try the web interface - https://apps.bham.ac.uk/ . The website may ask you to confirm that AppsAnywhere is installed on your device – you can click ‘I have it already’ if using a campus computer.\n\n\n\nOnce you have access to AppsAnywhere, you may have to log-in using your UoB credentials. The homepage of AppsAnywhere contains a search bar and will look something like this:\n\nYou can search for Jamovi and Rstudio in the search bar and click ‘Launch’ to run them. If this is the first time you’re running the app it may take a few minutes to get started. This is as AppsAnywhere is downloading and installing the programme on your computer.\n\n\n\nAppyAnywhere needs to be able to link to your desktop computer to run properly, the validation process ensures that this is working properly. Normally all you need to do is click to start validation and answer ‘Open AppsAnywhere Launcher’ when prompted by the browser.",
    "crumbs": [
      "Home",
      "Support",
      "FAQs"
    ]
  },
  {
    "objectID": "tutorials/frequently-asked-questions.html#i-cant-find-jamovirstudio-on-my-campus-computer.",
    "href": "tutorials/frequently-asked-questions.html#i-cant-find-jamovirstudio-on-my-campus-computer.",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "First check the start menu, if you can find both Jamovi and Rstudio then just open both from there and get started. Any programme that is not in the start menu can be opened using AppsAnywhere. This is available in the start menu of every campus computer – general information about AppsAnywhere can be found on this IT services page",
    "crumbs": [
      "Home",
      "Support",
      "FAQs"
    ]
  },
  {
    "objectID": "tutorials/frequently-asked-questions.html#i-cant-find-appsanywhere-on-my-campus-computer.",
    "href": "tutorials/frequently-asked-questions.html#i-cant-find-appsanywhere-on-my-campus-computer.",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "This should be available as an icon on the desktop, or from within the start menu. If not, they you can try the web interface - https://apps.bham.ac.uk/ . The website may ask you to confirm that AppsAnywhere is installed on your device – you can click ‘I have it already’ if using a campus computer.",
    "crumbs": [
      "Home",
      "Support",
      "FAQs"
    ]
  },
  {
    "objectID": "tutorials/frequently-asked-questions.html#how-do-i-use-appsanywhere",
    "href": "tutorials/frequently-asked-questions.html#how-do-i-use-appsanywhere",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Once you have access to AppsAnywhere, you may have to log-in using your UoB credentials. The homepage of AppsAnywhere contains a search bar and will look something like this:\n\nYou can search for Jamovi and Rstudio in the search bar and click ‘Launch’ to run them. If this is the first time you’re running the app it may take a few minutes to get started. This is as AppsAnywhere is downloading and installing the programme on your computer.",
    "crumbs": [
      "Home",
      "Support",
      "FAQs"
    ]
  },
  {
    "objectID": "tutorials/frequently-asked-questions.html#appsanywhere-keeps-asking-me-about-validation.",
    "href": "tutorials/frequently-asked-questions.html#appsanywhere-keeps-asking-me-about-validation.",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "AppyAnywhere needs to be able to link to your desktop computer to run properly, the validation process ensures that this is working properly. Normally all you need to do is click to start validation and answer ‘Open AppsAnywhere Launcher’ when prompted by the browser.",
    "crumbs": [
      "Home",
      "Support",
      "FAQs"
    ]
  },
  {
    "objectID": "tutorials/rma_week-9_correlations-graphs-and-exporting.html",
    "href": "tutorials/rma_week-9_correlations-graphs-and-exporting.html",
    "title": "Week 9 : Correlations, graphs and exporting",
    "section": "",
    "text": "Quantitative Methods\n\n\n\n\n\nHypthesis testing\n\n\n\nConfidence intervals and critical values\n\n\n\nMeasures of skew and normality\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Jamovi editor\n\n\n\nPerforming a correlation in Jamovi\n\n\n\nPlot basic graphs in Jamovi\n\n\n\nReport different statistical tests\n\n\n\nMake decisions about which statistical test to use based on hypotheses and data\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with openly available research data\n\n\n\nCopy/paste Jamovi output into other applications\n\n\n\n\n\n\n\n\n\nToday’s session\n\n\n\nToday you will be using Jamovi to perform a correlational analysis.\n\n\nWhen we want to test a research hypothesis, we obtain random samples of behaviour/data. We set up a null hypothesis H0: that the samples were drawn from populations with the same means (so that there will be no difference between them). We then obtain some samples and calculate the differences between the means of these samples, under the assumption that H0 is true (i.e. we obtained these means when the population means are equal).\n\nEXAMPLE: We want to test whether students drink more than the general population. To do this we collect data on the number of drinks per person in a club in one night, for two groups: students and general population. We then calculate the differences in the mean number of drinks for each of our groups. We assume that these means will be equal (null hypothesis). We are looking to disprove this hypothesis by finding differences in the means.\n\nWe then calculate the probability of getting a mean difference that is at least as large as the one we actually obtained between the means of our samples. This probability is our confidence interval. With a t-test, for example, we usually aim to say that we are 95% confident that our data were drawn from two populations with different means (i.e. there is a significant difference between the populations). To do this, we use the critical value – usually p&lt;0.05. In this case, we are saying that the probability that we would obtain the significant difference in data that we did would occur less than 5% or 5 in 100 times, if the null were true! That’s a very small chance. We can also use smaller, stricter p values like p&lt;0.01 (we would see the differences we found less than 1% of the time, if the null were true).\n\nEXAMPLE: We calculate the differences in the mean number of drinks for our student and general population groups. We find that students drink on average 2.5 more drinks than the general population, and that this difference is significant. We use a critical value of 5% and a confidence interval of 95% (these add up to 100%). That is, we can say that we are 95% confident that our data were drawn from two populations that actually have different means, and this conclusion will only be a mistake (due to chance) 5% of the time.\n\nIf the data do not lead us to reject the null hypothesis, the interpretation is more problematic – do we “accept” the null, or “retain” it, or just withhold judgement”?\n\nWe can definitely never claim to have “proved” the null hypothesis (as you cannot 100% prove a null statement)! A non-significant result is an inconclusive one – we have insufficient data to choose which hypothesis (null or alternative) is the most accurate.\n\n\n\n\n\nIf we are going to reject either the lowest or highest 5%, then we use a one-tailed (or directional) test\nIf we want to conduct a two-tailed, non-directional test of significance, then to use an overall critical value of 5% we need to reject both the lowest and highest 2.5%, making 5% altogether\nIf you are certain of the direction of your effect, you can use a one-tailed test – but generally, we use two-tailed tests (as it’s rare that we know what direction our effect will be in!)\n\n\n\n\nWe will be exploring the dataset correlation data that you can find on the CANVAS page under Week 9 of the computer labs folder of Research Methods A\nThere are five variables in the dataset\n\nAge\nIce_Cream - number of ice creams sold\nTemp - temperature\nMotivation\nStress\n\nThere are 20 days where these two variables were recorded. These are represented by the rows in the ‘data view’. We will correlate the variables ‘Temp’ and ‘Ice_Cream’\n\n\n\nThe Pearson’s Product Moment correlation is a parametric test that measures the size and significance of a relationship between two variables. In this test, the experimental data must meet certain assumptions of parametric tests. Some of these assumptions include data continuity and a normal distribution. We will discuss how to assess whether the distribution is normal in the next semester. For now, let’s assume it is normal.\n\n\n\n\n\n\nOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.\n\n\n\n\n\nThe instructions for installing and licensing JAMOVI on your personal computer are available above. To access JAMOVI once installed or when using Campus computers you will be able to find it in the start menu at the bottom left corner of your desktop (sometimes this will be in another location on your personal laptop or personal desktop PC – you know your device best!)\nOnce open, you will see the following screen. Note that if you are working with a later version of JAMOVI, the screen may look a little different e.g. in version ‘2.3.28 solid’ there is a tab called “Variables” before “Data” at the top of the screen. As researchers we work with statistical software which updates occasionally. Most of the changes are often cosmetic but sometimes options and functions move or are improved. You should be prepared to familiarise yourself with the changes each time they happen and JAMOVI update their user guides regularly to account for updates.\n\n\n\n\n\n\nKey step\n\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file Correlation_data.xlsx from canvas.\n\n\n\n\n\nNow it is time for you to explore the data. If you are unsure about any of these, please talk to the staff in your computer practical session.\n\n\n\nTo correlate the two variables click on ‘Analysis’ from the top tabs &gt; Regression &gt; Correlation Matrix\n\n\n\nTransfer ‘Ice_Cream’ and ‘Temp’ to the variables box by clicking them and pressing on the arrow button, double-clicking, or dragging them over to the box.\nFor this example, make sure that the ‘Pearson’ and ‘Report significance’ boxes are ticked. Note that Pearson is the version we use if our data meet parametric assumptions (e.g. interval or scale data, normal distribution, free from outliers etc.). If your data didn’t meet parametric assumptions, you would use Spearman instead because this test is more resilient to violations of the assumptions.\nSelect ‘correlated’ under hypothesis\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the correlation output to complete this statement reporting the results of the correlation. Fill in the spaces provided.\nThe correlation between temperature and the number of ice cream sold over 20 days is not significantstrongly significant (r () = , p &lt; ). There were  observations.\n\n\nThe correlation matrix…\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nWhat does this result tell you? Have a think about how you would interpret the result?\n\n\n\n\n\n“A Pearson’s correlation was conducted to see if there was a significant relationship between temperature and the number of ice creams sold. This revealed that there was a very strong significant positive relationship between temperature and ice cream sales, r (18)= .893, p &lt; .001. Specifically, as temperatures increased, so too did ice cream scales. Therefore, we can accept the hypothesis that there would be a significant positive relationship between temperature and ice cream scales.”\nNote, a correlation can be positive (like this one was) or negative (e.g. the r value could have read -.893 instead). If it had been negative, this would have meant that as temperature increased, ice cream sales decreased. It is very important to get the direction of the relationship right because if you get it wrong it completely changes the findings!\nIt is generally accepted that r values can be interpreted in the following ways (these numbers can be either positive or negative!): 0-0.19 = Very weak, 0.20–0.39 = Weak, 0.40–0.59 = Moderate, 0.60–0.79 = Strong, 0.80–1 = Very strong. It is extremely uncommon to have an r value of 1 because this means it is a perfect correlation (e.g. when you correlate temperature with itself because they are the same).\n\n\n\nThis correlation can be illustrated graphically. This eases the understanding of the relationship between the variables. We can illustrate the relationship using the Jamovi Scatterplot function.\n\nClick on Analyse &gt; Exploration &gt; Scatterplot\n\n\n\nMove Ice_Cream and Temp across\nClick on linear under ‘Regression Line’\nA scatterplot should appear in the window to the right\n\n\nYou can clearly see that as the temperature increases, the number of ice cream sold also raises. We know that this relationship is significant from the results of the Pearson’s Correlation.\n\n\n\nA histogram is a graph showing the frequency of different scores. Histograms are frequently used in psychological research. Histograms are useful to check whether the data is normally distributed.\n\nGo to Exploration &gt; Descriptives &gt; to generate a basic histogram of the MOTIVATION variable. Transfer MOTIVATION to the Variables box, remember to tick the Histogram box under ‘Histograms’ – A Histogram should appear in the window to the right\n\n\n\nDo the same for Stress_1\n\n\n\n\n\n\n\n\n Data Skills - Do you think the Motivation and Stress_1 data are normally distributed or is the data skewed?\n\n\n\n\n\nTo check this, look at your histograms. Can you see a classic “bell curve” shape (normally distributed data), or is some of the data pushed up at one end of the graph (skewed data)?\n\n\n\n\n\n\n\n\n\n Data Skills - Are there any outliers in the age data?\n\n\n\n\n\n\nTo check this, you will need to create a “Boxplot”.\nClick on the Boxplot box under ‘Boxplots’ and then create a boxplot for AGE by moving AGE into the variable box.\nA boxplot should appear in the window to the right\nLook at the graph, you will see the boxplot and above it there is a point on its own. This means that one participant’s age is an outlier because they are a lot older than the rest of the sample. Check back in data view to see this in the data. If age was important to your study and this participants age could confound your results, you would need to remove their data.\nImportant Note: ALWAYS keep the original JAMOVI data file in case you change your mind. Save the original and then when you make changes to your data set save this with a different name e.g. “Correlation data original”, “Correlation data outliers removed”.\n\n\n\n\n\n\n\nYou need to know how to copy results of statistical analyses into the appendices of reports in the future. Note that the only tables or figures which should ever go in the main part of the report are ones which help to illustrate your argument and they must fully adhere to APA format.\nIt is also never enough to just say ‘see table 1’. You have to interpret what tables and figures show in the text. For example, “Figure 1 shows a scatterplot depicting the correlation between temperature and ice cream sales”.\n\n\n\nClick on the bottom right search icon \nSearch for Snipping tool\nClick on ‘New’ top left\nHighlight the area of the graph or table that you want to copy\nClick ‘EDIT’ and scroll down to copy\nClick “Paste” in the Word Document\n\nMore information about APA formatting can be found here\n\n\n\n\nDownload and open the JAMOVI data file stroke_survival_data that you can find on Canvas. We want to generate some descriptive statistics on this dataset. This is a real dataset from several hospitals.\nFirstly, make sure all the variables are set at the right measurement level\n\nClick on the DATA tab\nClick on ‘Setup’\nClick on each individual variable name on the data to check the details.\n\nFrom the ‘Analysis’ tab click on ‘Exploration’ and then ‘Descriptives’ and then\nChoose two Continuous variables to explore. These will be your Variable Names in the table below. For example, if you chose age in years as a scale variable this would be entered where it says Variable 1 (both times).\nSplit your output by the two categorical variables of your choice. These categorical variables will provide your Factors in the table below. For example and gender was one of your two categorical variables you would list factor 1 as men and factor 2 as women and then report the data for men and the data for women.\n\nHINT: Enter a nominal variable into the “Split by” box. This will split your output by the categories within these variables\n\nMake sure your output includes a histogram.\nYou will see your results automatically in the window to the right.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestion\n\n\nUsing the results you just generated, fill out the following table:\n\n\n\nVariable Name\nFactors\nNumber of data points (N)\nMean (M or x)\n\n\n\n\nVariable 1\nFactor 1\nN\nM or x\n\n\n\nFactor 2\nN\nM or x\n\n\nVariable 1\nFactor 1\nN\nM or x\n\n\n\nFactor 2\nN\nM or x\n\n\nVariable 2\nFactor 1\nN\nM or x\n\n\n\nFactor 2\nN\nM or x\n\n\nVariable 2\nFactor 1\nN\nM or x\n\n\n\nFactor 2\nN\nM or x\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHint\n\n\nCorrelate three variables of your choice. In the text box below write out: * Why are you correlating these variables? Why is this relationship interesting? Justify your analysis. * What do the results tell you? Write out and interpret the results. \n\n\nHere’s the start of how to report correlations (see last week for more information): \n\n\n\n\n\n\nOpen the survey_data file. This is a real dataset from a psychological study.\nLook over the dataset. What has been recorded? What is the dataset exploring?\nSplit your dataset by one variable of your choice.\n\nHINT: Use the “Split By” function.\n\nNow explore the data using Descriptives and Correlations. Choose some variables that you find interesting, and generate Descriptives and Correlations. What do your results show you? What do they mean?\n\nWrite your results here. \n\n\n\nNow we will re-organise the data into a new JAMOVI sheet so we can compare scores at a later date based on a categorical variable of your choice.\n\nChoose a categorical variable that you are interested in\n\nE.g. You might want to know, is there a difference in optimism scores (scale) between smokers and non-smokers (categorical)?\n\nSort the dataset by this variable\n\nHINT: Use the “Split By” function to compare groups. Your data will now be organised by this categorical variable.\n\nChoose three non-categorical variables of interest. Copy all of the data from each one of these variable columns for ONE of your categories (e.g. female) into the new spreadsheet.\n\nE.g. if you’re interested in optimism scores, copy the data for optimism in females into a new variable column\nGive your new variables appropriate names, e.g. if you copied the “Total Optimism” scores for females, rename the variable “Tot_Opt_F”, for example\nRepeat for your other category/categories (e.g. copy all the male data, and rename the variables)\n\nNow you have re-organised your data…",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 9 - Correlations, Graphs and Exporting"
    ]
  },
  {
    "objectID": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#one--and-two-tailed-tests",
    "href": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#one--and-two-tailed-tests",
    "title": "Week 9 : Correlations, graphs and exporting",
    "section": "",
    "text": "If we are going to reject either the lowest or highest 5%, then we use a one-tailed (or directional) test\nIf we want to conduct a two-tailed, non-directional test of significance, then to use an overall critical value of 5% we need to reject both the lowest and highest 2.5%, making 5% altogether\nIf you are certain of the direction of your effect, you can use a one-tailed test – but generally, we use two-tailed tests (as it’s rare that we know what direction our effect will be in!)",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 9 - Correlations, Graphs and Exporting"
    ]
  },
  {
    "objectID": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#the-dataset",
    "href": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#the-dataset",
    "title": "Week 9 : Correlations, graphs and exporting",
    "section": "",
    "text": "We will be exploring the dataset correlation data that you can find on the CANVAS page under Week 9 of the computer labs folder of Research Methods A\nThere are five variables in the dataset\n\nAge\nIce_Cream - number of ice creams sold\nTemp - temperature\nMotivation\nStress\n\nThere are 20 days where these two variables were recorded. These are represented by the rows in the ‘data view’. We will correlate the variables ‘Temp’ and ‘Ice_Cream’",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 9 - Correlations, Graphs and Exporting"
    ]
  },
  {
    "objectID": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#pearsons-correlation-coefficent",
    "href": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#pearsons-correlation-coefficent",
    "title": "Week 9 : Correlations, graphs and exporting",
    "section": "",
    "text": "The Pearson’s Product Moment correlation is a parametric test that measures the size and significance of a relationship between two variables. In this test, the experimental data must meet certain assumptions of parametric tests. Some of these assumptions include data continuity and a normal distribution. We will discuss how to assess whether the distribution is normal in the next semester. For now, let’s assume it is normal.\n\n\n\n\n\n\nOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 9 - Correlations, Graphs and Exporting"
    ]
  },
  {
    "objectID": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#jamovi-basics",
    "href": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#jamovi-basics",
    "title": "Week 9 : Correlations, graphs and exporting",
    "section": "",
    "text": "The instructions for installing and licensing JAMOVI on your personal computer are available above. To access JAMOVI once installed or when using Campus computers you will be able to find it in the start menu at the bottom left corner of your desktop (sometimes this will be in another location on your personal laptop or personal desktop PC – you know your device best!)\nOnce open, you will see the following screen. Note that if you are working with a later version of JAMOVI, the screen may look a little different e.g. in version ‘2.3.28 solid’ there is a tab called “Variables” before “Data” at the top of the screen. As researchers we work with statistical software which updates occasionally. Most of the changes are often cosmetic but sometimes options and functions move or are improved. You should be prepared to familiarise yourself with the changes each time they happen and JAMOVI update their user guides regularly to account for updates.\n\n\n\n\n\n\nKey step\n\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file Correlation_data.xlsx from canvas.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 9 - Correlations, Graphs and Exporting"
    ]
  },
  {
    "objectID": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#exercises",
    "href": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#exercises",
    "title": "Week 9 : Correlations, graphs and exporting",
    "section": "",
    "text": "Now it is time for you to explore the data. If you are unsure about any of these, please talk to the staff in your computer practical session.\n\n\n\nTo correlate the two variables click on ‘Analysis’ from the top tabs &gt; Regression &gt; Correlation Matrix\n\n\n\nTransfer ‘Ice_Cream’ and ‘Temp’ to the variables box by clicking them and pressing on the arrow button, double-clicking, or dragging them over to the box.\nFor this example, make sure that the ‘Pearson’ and ‘Report significance’ boxes are ticked. Note that Pearson is the version we use if our data meet parametric assumptions (e.g. interval or scale data, normal distribution, free from outliers etc.). If your data didn’t meet parametric assumptions, you would use Spearman instead because this test is more resilient to violations of the assumptions.\nSelect ‘correlated’ under hypothesis\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the correlation output to complete this statement reporting the results of the correlation. Fill in the spaces provided.\nThe correlation between temperature and the number of ice cream sold over 20 days is not significantstrongly significant (r () = , p &lt; ). There were  observations.\n\n\nThe correlation matrix…\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nWhat does this result tell you? Have a think about how you would interpret the result?\n\n\n\n\n\n“A Pearson’s correlation was conducted to see if there was a significant relationship between temperature and the number of ice creams sold. This revealed that there was a very strong significant positive relationship between temperature and ice cream sales, r (18)= .893, p &lt; .001. Specifically, as temperatures increased, so too did ice cream scales. Therefore, we can accept the hypothesis that there would be a significant positive relationship between temperature and ice cream scales.”\nNote, a correlation can be positive (like this one was) or negative (e.g. the r value could have read -.893 instead). If it had been negative, this would have meant that as temperature increased, ice cream sales decreased. It is very important to get the direction of the relationship right because if you get it wrong it completely changes the findings!\nIt is generally accepted that r values can be interpreted in the following ways (these numbers can be either positive or negative!): 0-0.19 = Very weak, 0.20–0.39 = Weak, 0.40–0.59 = Moderate, 0.60–0.79 = Strong, 0.80–1 = Very strong. It is extremely uncommon to have an r value of 1 because this means it is a perfect correlation (e.g. when you correlate temperature with itself because they are the same).\n\n\n\nThis correlation can be illustrated graphically. This eases the understanding of the relationship between the variables. We can illustrate the relationship using the Jamovi Scatterplot function.\n\nClick on Analyse &gt; Exploration &gt; Scatterplot\n\n\n\nMove Ice_Cream and Temp across\nClick on linear under ‘Regression Line’\nA scatterplot should appear in the window to the right\n\n\nYou can clearly see that as the temperature increases, the number of ice cream sold also raises. We know that this relationship is significant from the results of the Pearson’s Correlation.\n\n\n\nA histogram is a graph showing the frequency of different scores. Histograms are frequently used in psychological research. Histograms are useful to check whether the data is normally distributed.\n\nGo to Exploration &gt; Descriptives &gt; to generate a basic histogram of the MOTIVATION variable. Transfer MOTIVATION to the Variables box, remember to tick the Histogram box under ‘Histograms’ – A Histogram should appear in the window to the right\n\n\n\nDo the same for Stress_1\n\n\n\n\n\n\n\n\n Data Skills - Do you think the Motivation and Stress_1 data are normally distributed or is the data skewed?\n\n\n\n\n\nTo check this, look at your histograms. Can you see a classic “bell curve” shape (normally distributed data), or is some of the data pushed up at one end of the graph (skewed data)?\n\n\n\n\n\n\n\n\n\n Data Skills - Are there any outliers in the age data?\n\n\n\n\n\n\nTo check this, you will need to create a “Boxplot”.\nClick on the Boxplot box under ‘Boxplots’ and then create a boxplot for AGE by moving AGE into the variable box.\nA boxplot should appear in the window to the right\nLook at the graph, you will see the boxplot and above it there is a point on its own. This means that one participant’s age is an outlier because they are a lot older than the rest of the sample. Check back in data view to see this in the data. If age was important to your study and this participants age could confound your results, you would need to remove their data.\nImportant Note: ALWAYS keep the original JAMOVI data file in case you change your mind. Save the original and then when you make changes to your data set save this with a different name e.g. “Correlation data original”, “Correlation data outliers removed”.\n\n\n\n\n\n\n\nYou need to know how to copy results of statistical analyses into the appendices of reports in the future. Note that the only tables or figures which should ever go in the main part of the report are ones which help to illustrate your argument and they must fully adhere to APA format.\nIt is also never enough to just say ‘see table 1’. You have to interpret what tables and figures show in the text. For example, “Figure 1 shows a scatterplot depicting the correlation between temperature and ice cream sales”.\n\n\n\nClick on the bottom right search icon \nSearch for Snipping tool\nClick on ‘New’ top left\nHighlight the area of the graph or table that you want to copy\nClick ‘EDIT’ and scroll down to copy\nClick “Paste” in the Word Document\n\nMore information about APA formatting can be found here\n\n\n\n\nDownload and open the JAMOVI data file stroke_survival_data that you can find on Canvas. We want to generate some descriptive statistics on this dataset. This is a real dataset from several hospitals.\nFirstly, make sure all the variables are set at the right measurement level\n\nClick on the DATA tab\nClick on ‘Setup’\nClick on each individual variable name on the data to check the details.\n\nFrom the ‘Analysis’ tab click on ‘Exploration’ and then ‘Descriptives’ and then\nChoose two Continuous variables to explore. These will be your Variable Names in the table below. For example, if you chose age in years as a scale variable this would be entered where it says Variable 1 (both times).\nSplit your output by the two categorical variables of your choice. These categorical variables will provide your Factors in the table below. For example and gender was one of your two categorical variables you would list factor 1 as men and factor 2 as women and then report the data for men and the data for women.\n\nHINT: Enter a nominal variable into the “Split by” box. This will split your output by the categories within these variables\n\nMake sure your output includes a histogram.\nYou will see your results automatically in the window to the right.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestion\n\n\nUsing the results you just generated, fill out the following table:\n\n\n\nVariable Name\nFactors\nNumber of data points (N)\nMean (M or x)\n\n\n\n\nVariable 1\nFactor 1\nN\nM or x\n\n\n\nFactor 2\nN\nM or x\n\n\nVariable 1\nFactor 1\nN\nM or x\n\n\n\nFactor 2\nN\nM or x\n\n\nVariable 2\nFactor 1\nN\nM or x\n\n\n\nFactor 2\nN\nM or x\n\n\nVariable 2\nFactor 1\nN\nM or x\n\n\n\nFactor 2\nN\nM or x\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHint\n\n\nCorrelate three variables of your choice. In the text box below write out: * Why are you correlating these variables? Why is this relationship interesting? Justify your analysis. * What do the results tell you? Write out and interpret the results. \n\n\nHere’s the start of how to report correlations (see last week for more information): \n\n\n\n\n\n\nOpen the survey_data file. This is a real dataset from a psychological study.\nLook over the dataset. What has been recorded? What is the dataset exploring?\nSplit your dataset by one variable of your choice.\n\nHINT: Use the “Split By” function.\n\nNow explore the data using Descriptives and Correlations. Choose some variables that you find interesting, and generate Descriptives and Correlations. What do your results show you? What do they mean?\n\nWrite your results here. \n\n\n\nNow we will re-organise the data into a new JAMOVI sheet so we can compare scores at a later date based on a categorical variable of your choice.\n\nChoose a categorical variable that you are interested in\n\nE.g. You might want to know, is there a difference in optimism scores (scale) between smokers and non-smokers (categorical)?\n\nSort the dataset by this variable\n\nHINT: Use the “Split By” function to compare groups. Your data will now be organised by this categorical variable.\n\nChoose three non-categorical variables of interest. Copy all of the data from each one of these variable columns for ONE of your categories (e.g. female) into the new spreadsheet.\n\nE.g. if you’re interested in optimism scores, copy the data for optimism in females into a new variable column\nGive your new variables appropriate names, e.g. if you copied the “Total Optimism” scores for females, rename the variable “Tot_Opt_F”, for example\nRepeat for your other category/categories (e.g. copy all the male data, and rename the variables)\n\nNow you have re-organised your data…",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 9 - Correlations, Graphs and Exporting"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html",
    "href": "tutorials/rmb_week-7_non-parametrics.html",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "This week we will explore how to check the assumptions of hypothesis tests and make sure that we’re using the right tests. We will introduce the idea of transforming data to squeeze the data into the shape of a normal distribution.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nAssumptions of parametric tests\n\n\n\nLog transforms and Box-Cox transforms\n\n\n\nHistograms\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nCompute data transformations using Jamovi\n\n\n\nRun non-parametric alternative tests in Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nReplicate part of a published analysis\n\n\n\n\n\n\n\n\n\nContent Info\n\n\n\nThis tutorial refers to a video in which participants watched unpleasant events to simulate a traumatic experience in the lab. The content of this video is not disclosed or dicussed in any more detail than this during this tutorial - those who are interested can find more information in methods section the published article. This tutorial does explore the emotional impact that the video has on participants.\n\n\n\n\nFlashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we’re going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind’s eye and cause distress. We investigated whether reconsolidation—the process during which memories become malleable when recalled—can be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group.\n\n\n\nThis week - we will explore two of the four conditions to test the following hypothesis:\n\nPlaying tetris will after a trumatic event will reduce number of subsequent intrusive memories\n\nAlong the way, we’ll take a detailed tour of our options when working with non-normally distributed data.\nThe theme this week is PRECISION! we need high precision when interpreting and analysing results, the exercises this week are designed to give you practice in paying close attention to the details of your data…\n\n\n\nLet’s start by getting familiar with the dataset. Load the file into Jamovi and take a look at the ‘Variables’ tab.\n\nThe creators of this dataset have helpfully included annotations for many of the variables they’ve measured. This is extremely useful for us and definitely worth a few moments for you to do the same when working with your own data.\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to read through the variable descriptions. You’ll need to refer back to this list throughout the session to find relevant variables.\n\n\nNext - take a look at the data sheet and the Condition variable. These relate to the different conditions in the experiment - all participants watched the film containing traumatic events and the four conditions describe what participants did after watching the film on day 1 of the experiment.\n\n\n\n\n\n\n\nCondition\nDescription\n\n\n\n\nNo-Task Control\nParticipants sit quietly\n\n\nReactivation+Tetris\nParticipants rewatch the film and then play Tetris\n\n\nTetris Only\nParticipants play Tetris\n\n\nReactivation Only\nParticipants rewatch the film\n\n\n\nParticipants were asked to keep an diary of any intrusive thoughts that they had prior to the experiment (day zero) and during the week after the experiment (days one to seven).\n\n\n\n\n\n\n Data Skills - which variable tracks the diary of intrusive memories?\n\n\n\n\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary contains the number of intrusive memories the participant recorded in the week after watching the film.\nDay_Zero_Image_Based_Intrusions_in_Intrusion_Diary is very similar - it contains the number of intrusive memories the participant recorded in the time prior to watching the film. This is a good measure to include so we can rule out differences in the baseline number of intrusive memories between our groups.\n\n\n\n\n\n\n\n\n\n Data Skills - what information do we have about participant’s performance when playing tetris?\n\n\n\n\n\nTetris_Total_Score contains the cumulative score of all tetris games the participant played - participants were allowed to restart their game if they finished before the time was up.\nWe also have Self_Rated_Tetris_Performance which measures how participants rated the difficulty of the tetris task and Tetris_Demand_Rating which measures how participants thought that playing tetris would affect their intrusive memories.\n\n\n\n\n\n\nLet’s start by taking a look at how participants in the conditions that played Tetris scored during their game time. Compute descriptive statistics on the Tetris_Total_Score variable and take a moment to get a sense of what it is telling you.\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the descriptive statistics of Tetris_Total_Score to answer the following questions in the spaces provided.\nThe average Tetris score was .\nThe most successful Tetris player scored  points.\nThe Tetris scores are positively skewednot skewednegatively skewed with a skewness of .\nThe Shapiro-Wilk statistic has a value of W =  and the p-value indicates that the scores are significantly non-normally distributednormally distributed\nWe should use parametricnon-parametric statistical approaches when analysing Tetris_Total_Score.\n\n\n\n\n\n\n\n\nLet’s use this data to test a hypothesis.\n\nPerformance whilst playing Tetris will be different at different times of day.\n\nWe can test this with an independent samples t-test as we’ve done in previous weeks. Set up the t-test and include the following:\n\nWelchs’ t-test\nMann-Whitney U\nEffect Size\nHomogeneity test\nNormality test\n\n\nLet’s unpack the results.\n\n\n\n\n\n\n Data Skills - does the Normality assumption check confirm what we saw in the descriptives?\n\n\n\n\n\nYes, we get the same picture but with a little less detail. Shapiro-Wilk’s test is significant indicating a non-normal distribution, but we still need the descriptive stats to understand HOW the distribution is different from a normal distribution.\nImportantly, this confirms that we should report the non-parametric alternative test - the Mann-Whitney U.\nYou could add the descriptives to the t-test page - but notice that this only includes a handful of metrics that are specifically related to t-tests (mean, standard deviation etc). We need the full list on the ‘Descriptives’ tab to investigate distributions in detail.\n\n\n\n\n\n\n\n\n\n Data Skills - what does Levene’s test indicate and does this matter for us?\n\n\n\n\n\nLevene’s test explores homogeneity of variance, the result is not significant indicating that we are safe to assume homogeneity of variance between our groups.\nHowever, the data are not normally distributed - non-parametric tests don’t have an assumption of homogeneity of variance at all so, for this analysis, Levene’s test isn’t really relevant.\n\n\n\n\n\n\n\n\n\n Data Skills - what does the Mann-Whitney U test indicate about our hypothesis?\n\n\n\n\n\nThe Mann-Whitney U test is not significant - it seems like tetris scores are similar for participants who played in the morning and those who played in the afternoon.\n\n\n\n\n\n\nOn to our main hypothesis. (James et al. 2015) were most interested in the combination of the reactivation task and the tetris intervention and their effect on intrusive memories about the film. Let’s focus on only the two conditions we can compare with a t-test.\n\n\n\n\n\n\nKey step\n\n\n\nSet up a Jamovi data filter to focus our analysis on only the two conditions “No-Task Control” and “Reactivation+Tetris” conditions. Take a look at week 3 section 4 to revise data filters if you need to.\nYou can use this code to select both conditions together:\nCondition == \"No-Task Control\" or Condition == \"Reactivation+Tetris\"\nFinally, compute an independent samples t-test to test the following:\n\nPerforming a Reactivation and Tetris task changes the number of intrusive memories in the week after watching a traumatic film.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThere are  participants in each group.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p &lt; \nLevene’s test for homogeneity of variance indicated that we cancan not assume homogeneity of variance between the No-Task and Reactivation+Tetris groups, F(1, 32) = , p = .\nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in the week after the film between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .\n\n\n\n\n\n\n\n\n\n\n\nOur hypothesis test gives statistical support to the idea that playing tetris reduces intrusive memories - but could there be other explanations? This experiment is likely not perfect and it is possible that some bad luck in the data collection could have impacted the results. In this section we’ll validate a few additional factors in the experimenta..\n\n\n(James et al. 2015) collected data variable to act as a control condition and rule out one possible confound in the experiment. Days_Zero_Image_Based_Intrusions_in_Intrusion_Diary indicates the number of intrusive memories each participant had in the time before the experiment.\nThere isn’t a huge number of participants in each condition, it is possible that one group of participants was having more or less intrusive memories to begin with - irrespective of the experimental procedure.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a t-test to test the following:\n\nThere are no differences in the number of instrusive memories between the two experimental group BEFORE the experiment started.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThe participant in the ‘No-Task Control’ group had mean of  intrusive memories prior to the experiment starting.\nThe participant in the ‘Reactivation+Tetris’ group had mean of  intrusive memories prior to the experiment starting.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p = \nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in day before between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .\n\n\n\n\n\n\n\n\n\n\n\nAnother important factor to check was that the film actually changed how the participants felt. If the film didn’t do anything to impact the participants emotions then it is possible that we don’t actually understand the source of the difference we saw in the main hypothesis.\nThe film is the main experimental manipulation that so it is critical to check that it worked as we expected.\n\n\n\n\n\n\nKey step\n\n\n\nCompute paired samples t-test’s to check the following:\n\nWatching the traumatic film will change participants emotional state.\n\nCheck ALL of participants reported sadness, hopelessness, fear, horror and anxiousness.\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nTrue or False? The data variables for these factors are Normally Distributed according to Shaprio-Wilk statistics.\n\nSadness TRUEFALSE\nHopeless TRUEFALSE\nFear TRUEFALSE\nHorror TRUEFALSE\nAxious TRUEFALSE\n\nHow did the following factors change or not after watching the film?\n\nSadness increasedstayed the samedecreased after watching the film\nHopelessless increasedstayed the samedecreased after watching the film\nFear increasedstayed the samedecreased after watching the film\nHorror increasedstayed the samedecreased after watching the film\nAxiousness increasedstayed the samedecreased after watching the film\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - did the film manipulation work?\n\n\n\n\n\nYes, all factors apart from Hopelessless showed a statistically significant increase after watching the film. This indicates that the film had a strongly impact on participants negative emotions. It seems like the experimental manipulation worked.\n\n\n\n\n\n\n\nThere are times when we might not want to use a non-parametric test, for example if we specifically want to interpret the mean of the data rather than the median or if we’re running many similar tests and we don’t want to be switching back and forth between parametric and non-parametric tests.\nWe have another option when working with non-normal data. This is to apply a reversible transformation to the data observations in a variable to change the distribution to one that is closer to being normally distributed. When this process works, we are able to run the parametric test on the transformed data rather thans swapping to the non-parametric alternative.\nThis is valid when the transformation preserves the relationships in the dataset, that is it maintains the relative order of the observations. If data point A is larger than B before transformation then it should still be larger after the transformation. It is fine if the transformation changes the numbers themselves - we just have to be careful when making an interpretation.\nWe’ll use the LOG data transformation in this section\n\n\n\n\n\n\n\nTransform\nExplanation\n\n\n\n\nLog Transform\nThe logarithm function, typically base 10 (log), is applied to each data point.\n\n\n\nOne downside of the log transform is that it has a weird value when the datapoint being transformed is zero or negative. log(0) equals negative infinity! this will break a lot of analyses that happen afterwards but fortunately there is a straightforward solution.\nWhen your data contains zeros or negatives you can simply add an offset value to make your data positive and then apply the log transform. Again, this is valid as long as we are preserving the relationships within our data by adding exactly the same offset to every single point in the dataset.\nWe can easily transform variables using Jamovi and have done something very similar when computing variables in previous weeks.\n\n\n\n\n\n\nData Skills - transforming a variable.\n\n\n\nYou can read about transforming variables in detail on the Jamovi documentation page on transforming variables. There are a few different steps but they are all straightforward.\nFirst, head to the ‘Data’ view and right click on the variable you want to transform, in this case we want Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary\n\nThis will open a dialogue box where you can define the new variable. Give your new variable a descriptive name and make sure that the ‘Source variable’ is the one that you expect. Note that you might see a ‘Circular definition’ warning until you rename your variable.\n\nNext we need to specify the transformation that we want to use. Click into the ‘using transform’ drop-down box and select “Create New Transform”.\nHere you can define the transformation that you want to apply. You can define a single transform and apply it to multiple variables. Give the transformation a descriptive name and specify that you want the transfrom to apply the LOG10() function to the $source variable. ($source is a way to tell Jamovi to use ‘whichever variable is passed into the transform’ rather than defining the transform on only 1 variable here.).\nWe also include a small offset value of +1 here. This is as some participants had no intrusive memories at all and we want to avoid issues with computing log(0) in our analysis.\n\nYou can click on the Fx logo to select functions from a list if you prefer, there are lots of pre-defined options.\n\nOnce you’re happy with the transformation, you can close the transformation definition dialogue and specify that we want to use the ‘LogTransform’ that we just defined to transfrom Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary to make our new variable.\n\nThere are a lot of steps here - but each is fairly straightforward. Ask your tutor if you get stuck.\n\n\nNow that we have our new variable we can explore what it has done. Let’s compare the descriptive statistics for the original and the transformed variable.\n\n \n\n\n\n\n\n\n Data Skills - has the log transform changed mean of the data?\n\n\n\n\n\nYes, the log transform has changed the mean by a long way. The original data shows that the average participant had 3.5 intrusive memories but that number has changed to 0.54 in the transformed data.\nThis does not mean that the number has really reduced! remember that the transformation is changing the absolute values of the observations but should be preserving the relative relationships between the data points.\nThis is fine, but we must not report the log-transformed values as the actual means. The participants still had an average of 3.5 intrusive memories whether we transformed the data or not.\n\n\n\n\n\n\n\n\n\n Data Skills - has the log transform changed the skewness and kurtosis of the data?\n\n\n\n\n\nYes, it has had a big impact. Both the skewness and kurtosis values are much closer to zero in the transformed data compared to the original. Both indicates that the transformed data are more normally distributed.\n\n\n\n\n\n\n\n\n\n Data Skills - is there a statistically significant difference from a normal distribution in the transformed data?\n\n\n\n\n\nNo, whereas the Shapiro-Wilk statistic showed a significant departure from a normal distribution for the original data (W = 0.8, p &lt; 0.001) it shows that the transformed data is now indistinguishable from a normal distribution (W = 0.953, p = 0.130)\n\n\n\nFinally - let’s see if we can use our transformed variables to compute a parametric t-test. Repeat the independent samples t-test from earlier but include all three variables in the ‘Dependent Variables’ box. Make sure to include\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary - original data\nDays_One_to_Seven_Transformed - Log transformed data\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nWith the original data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . We can proceed with Student’s t-testWelch’s t-testMann-Whitney U test.\nUsing the log-transformed data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . Levene’s test found that the we cancan not assume that the groups have homogeneity of variance, F(1, 34) = , p = . We can proceed with Student’s t-testWelch’s t-testMann-Whitney U test.\n\n\n\nNote that Welch’s t-test is valid whether or not we can assume homogeneity of variance…\n\n\n\n\n\n\n\n\nWe have explored an interesting dataset in some detail and worked with serveral different analysis approaches. Specifically, we have several options when working on checking the assumptions of our analyses. We have to be careful when computing and reporting hypothesis tests that we’re using the correct tests in a way that is supported by the data. Sometimes we can use a transform to continuous using parametric tests when the assumptions are not supported but this will not always work.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#the-dataset",
    "href": "tutorials/rmb_week-7_non-parametrics.html#the-dataset",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Flashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we’re going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind’s eye and cause distress. We investigated whether reconsolidation—the process during which memories become malleable when recalled—can be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#the-challenge",
    "href": "tutorials/rmb_week-7_non-parametrics.html#the-challenge",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "This week - we will explore two of the four conditions to test the following hypothesis:\n\nPlaying tetris will after a trumatic event will reduce number of subsequent intrusive memories\n\nAlong the way, we’ll take a detailed tour of our options when working with non-normally distributed data.\nThe theme this week is PRECISION! we need high precision when interpreting and analysing results, the exercises this week are designed to give you practice in paying close attention to the details of your data…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#dataset-description",
    "href": "tutorials/rmb_week-7_non-parametrics.html#dataset-description",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Let’s start by getting familiar with the dataset. Load the file into Jamovi and take a look at the ‘Variables’ tab.\n\nThe creators of this dataset have helpfully included annotations for many of the variables they’ve measured. This is extremely useful for us and definitely worth a few moments for you to do the same when working with your own data.\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to read through the variable descriptions. You’ll need to refer back to this list throughout the session to find relevant variables.\n\n\nNext - take a look at the data sheet and the Condition variable. These relate to the different conditions in the experiment - all participants watched the film containing traumatic events and the four conditions describe what participants did after watching the film on day 1 of the experiment.\n\n\n\n\n\n\n\nCondition\nDescription\n\n\n\n\nNo-Task Control\nParticipants sit quietly\n\n\nReactivation+Tetris\nParticipants rewatch the film and then play Tetris\n\n\nTetris Only\nParticipants play Tetris\n\n\nReactivation Only\nParticipants rewatch the film\n\n\n\nParticipants were asked to keep an diary of any intrusive thoughts that they had prior to the experiment (day zero) and during the week after the experiment (days one to seven).\n\n\n\n\n\n\n Data Skills - which variable tracks the diary of intrusive memories?\n\n\n\n\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary contains the number of intrusive memories the participant recorded in the week after watching the film.\nDay_Zero_Image_Based_Intrusions_in_Intrusion_Diary is very similar - it contains the number of intrusive memories the participant recorded in the time prior to watching the film. This is a good measure to include so we can rule out differences in the baseline number of intrusive memories between our groups.\n\n\n\n\n\n\n\n\n\n Data Skills - what information do we have about participant’s performance when playing tetris?\n\n\n\n\n\nTetris_Total_Score contains the cumulative score of all tetris games the participant played - participants were allowed to restart their game if they finished before the time was up.\nWe also have Self_Rated_Tetris_Performance which measures how participants rated the difficulty of the tetris task and Tetris_Demand_Rating which measures how participants thought that playing tetris would affect their intrusive memories.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#is-participant-performance-on-tetris-normally-distributed",
    "href": "tutorials/rmb_week-7_non-parametrics.html#is-participant-performance-on-tetris-normally-distributed",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Let’s start by taking a look at how participants in the conditions that played Tetris scored during their game time. Compute descriptive statistics on the Tetris_Total_Score variable and take a moment to get a sense of what it is telling you.\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the descriptive statistics of Tetris_Total_Score to answer the following questions in the spaces provided.\nThe average Tetris score was .\nThe most successful Tetris player scored  points.\nThe Tetris scores are positively skewednot skewednegatively skewed with a skewness of .\nThe Shapiro-Wilk statistic has a value of W =  and the p-value indicates that the scores are significantly non-normally distributednormally distributed\nWe should use parametricnon-parametric statistical approaches when analysing Tetris_Total_Score.\n\n\n\n\n\n\n\n\nLet’s use this data to test a hypothesis.\n\nPerformance whilst playing Tetris will be different at different times of day.\n\nWe can test this with an independent samples t-test as we’ve done in previous weeks. Set up the t-test and include the following:\n\nWelchs’ t-test\nMann-Whitney U\nEffect Size\nHomogeneity test\nNormality test\n\n\nLet’s unpack the results.\n\n\n\n\n\n\n Data Skills - does the Normality assumption check confirm what we saw in the descriptives?\n\n\n\n\n\nYes, we get the same picture but with a little less detail. Shapiro-Wilk’s test is significant indicating a non-normal distribution, but we still need the descriptive stats to understand HOW the distribution is different from a normal distribution.\nImportantly, this confirms that we should report the non-parametric alternative test - the Mann-Whitney U.\nYou could add the descriptives to the t-test page - but notice that this only includes a handful of metrics that are specifically related to t-tests (mean, standard deviation etc). We need the full list on the ‘Descriptives’ tab to investigate distributions in detail.\n\n\n\n\n\n\n\n\n\n Data Skills - what does Levene’s test indicate and does this matter for us?\n\n\n\n\n\nLevene’s test explores homogeneity of variance, the result is not significant indicating that we are safe to assume homogeneity of variance between our groups.\nHowever, the data are not normally distributed - non-parametric tests don’t have an assumption of homogeneity of variance at all so, for this analysis, Levene’s test isn’t really relevant.\n\n\n\n\n\n\n\n\n\n Data Skills - what does the Mann-Whitney U test indicate about our hypothesis?\n\n\n\n\n\nThe Mann-Whitney U test is not significant - it seems like tetris scores are similar for participants who played in the morning and those who played in the afternoon.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#is-tetris-effective-at-reducing-intrusive-memories",
    "href": "tutorials/rmb_week-7_non-parametrics.html#is-tetris-effective-at-reducing-intrusive-memories",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "On to our main hypothesis. (James et al. 2015) were most interested in the combination of the reactivation task and the tetris intervention and their effect on intrusive memories about the film. Let’s focus on only the two conditions we can compare with a t-test.\n\n\n\n\n\n\nKey step\n\n\n\nSet up a Jamovi data filter to focus our analysis on only the two conditions “No-Task Control” and “Reactivation+Tetris” conditions. Take a look at week 3 section 4 to revise data filters if you need to.\nYou can use this code to select both conditions together:\nCondition == \"No-Task Control\" or Condition == \"Reactivation+Tetris\"\nFinally, compute an independent samples t-test to test the following:\n\nPerforming a Reactivation and Tetris task changes the number of intrusive memories in the week after watching a traumatic film.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThere are  participants in each group.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p &lt; \nLevene’s test for homogeneity of variance indicated that we cancan not assume homogeneity of variance between the No-Task and Reactivation+Tetris groups, F(1, 32) = , p = .\nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in the week after the film between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#control-analyses-testing-for-potential-confounds.",
    "href": "tutorials/rmb_week-7_non-parametrics.html#control-analyses-testing-for-potential-confounds.",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Our hypothesis test gives statistical support to the idea that playing tetris reduces intrusive memories - but could there be other explanations? This experiment is likely not perfect and it is possible that some bad luck in the data collection could have impacted the results. In this section we’ll validate a few additional factors in the experimenta..\n\n\n(James et al. 2015) collected data variable to act as a control condition and rule out one possible confound in the experiment. Days_Zero_Image_Based_Intrusions_in_Intrusion_Diary indicates the number of intrusive memories each participant had in the time before the experiment.\nThere isn’t a huge number of participants in each condition, it is possible that one group of participants was having more or less intrusive memories to begin with - irrespective of the experimental procedure.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a t-test to test the following:\n\nThere are no differences in the number of instrusive memories between the two experimental group BEFORE the experiment started.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThe participant in the ‘No-Task Control’ group had mean of  intrusive memories prior to the experiment starting.\nThe participant in the ‘Reactivation+Tetris’ group had mean of  intrusive memories prior to the experiment starting.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p = \nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in day before between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .\n\n\n\n\n\n\n\n\n\n\n\nAnother important factor to check was that the film actually changed how the participants felt. If the film didn’t do anything to impact the participants emotions then it is possible that we don’t actually understand the source of the difference we saw in the main hypothesis.\nThe film is the main experimental manipulation that so it is critical to check that it worked as we expected.\n\n\n\n\n\n\nKey step\n\n\n\nCompute paired samples t-test’s to check the following:\n\nWatching the traumatic film will change participants emotional state.\n\nCheck ALL of participants reported sadness, hopelessness, fear, horror and anxiousness.\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nTrue or False? The data variables for these factors are Normally Distributed according to Shaprio-Wilk statistics.\n\nSadness TRUEFALSE\nHopeless TRUEFALSE\nFear TRUEFALSE\nHorror TRUEFALSE\nAxious TRUEFALSE\n\nHow did the following factors change or not after watching the film?\n\nSadness increasedstayed the samedecreased after watching the film\nHopelessless increasedstayed the samedecreased after watching the film\nFear increasedstayed the samedecreased after watching the film\nHorror increasedstayed the samedecreased after watching the film\nAxiousness increasedstayed the samedecreased after watching the film\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - did the film manipulation work?\n\n\n\n\n\nYes, all factors apart from Hopelessless showed a statistically significant increase after watching the film. This indicates that the film had a strongly impact on participants negative emotions. It seems like the experimental manipulation worked.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#transforming-non-normally-distributed-data",
    "href": "tutorials/rmb_week-7_non-parametrics.html#transforming-non-normally-distributed-data",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "There are times when we might not want to use a non-parametric test, for example if we specifically want to interpret the mean of the data rather than the median or if we’re running many similar tests and we don’t want to be switching back and forth between parametric and non-parametric tests.\nWe have another option when working with non-normal data. This is to apply a reversible transformation to the data observations in a variable to change the distribution to one that is closer to being normally distributed. When this process works, we are able to run the parametric test on the transformed data rather thans swapping to the non-parametric alternative.\nThis is valid when the transformation preserves the relationships in the dataset, that is it maintains the relative order of the observations. If data point A is larger than B before transformation then it should still be larger after the transformation. It is fine if the transformation changes the numbers themselves - we just have to be careful when making an interpretation.\nWe’ll use the LOG data transformation in this section\n\n\n\n\n\n\n\nTransform\nExplanation\n\n\n\n\nLog Transform\nThe logarithm function, typically base 10 (log), is applied to each data point.\n\n\n\nOne downside of the log transform is that it has a weird value when the datapoint being transformed is zero or negative. log(0) equals negative infinity! this will break a lot of analyses that happen afterwards but fortunately there is a straightforward solution.\nWhen your data contains zeros or negatives you can simply add an offset value to make your data positive and then apply the log transform. Again, this is valid as long as we are preserving the relationships within our data by adding exactly the same offset to every single point in the dataset.\nWe can easily transform variables using Jamovi and have done something very similar when computing variables in previous weeks.\n\n\n\n\n\n\nData Skills - transforming a variable.\n\n\n\nYou can read about transforming variables in detail on the Jamovi documentation page on transforming variables. There are a few different steps but they are all straightforward.\nFirst, head to the ‘Data’ view and right click on the variable you want to transform, in this case we want Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary\n\nThis will open a dialogue box where you can define the new variable. Give your new variable a descriptive name and make sure that the ‘Source variable’ is the one that you expect. Note that you might see a ‘Circular definition’ warning until you rename your variable.\n\nNext we need to specify the transformation that we want to use. Click into the ‘using transform’ drop-down box and select “Create New Transform”.\nHere you can define the transformation that you want to apply. You can define a single transform and apply it to multiple variables. Give the transformation a descriptive name and specify that you want the transfrom to apply the LOG10() function to the $source variable. ($source is a way to tell Jamovi to use ‘whichever variable is passed into the transform’ rather than defining the transform on only 1 variable here.).\nWe also include a small offset value of +1 here. This is as some participants had no intrusive memories at all and we want to avoid issues with computing log(0) in our analysis.\n\nYou can click on the Fx logo to select functions from a list if you prefer, there are lots of pre-defined options.\n\nOnce you’re happy with the transformation, you can close the transformation definition dialogue and specify that we want to use the ‘LogTransform’ that we just defined to transfrom Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary to make our new variable.\n\nThere are a lot of steps here - but each is fairly straightforward. Ask your tutor if you get stuck.\n\n\nNow that we have our new variable we can explore what it has done. Let’s compare the descriptive statistics for the original and the transformed variable.\n\n \n\n\n\n\n\n\n Data Skills - has the log transform changed mean of the data?\n\n\n\n\n\nYes, the log transform has changed the mean by a long way. The original data shows that the average participant had 3.5 intrusive memories but that number has changed to 0.54 in the transformed data.\nThis does not mean that the number has really reduced! remember that the transformation is changing the absolute values of the observations but should be preserving the relative relationships between the data points.\nThis is fine, but we must not report the log-transformed values as the actual means. The participants still had an average of 3.5 intrusive memories whether we transformed the data or not.\n\n\n\n\n\n\n\n\n\n Data Skills - has the log transform changed the skewness and kurtosis of the data?\n\n\n\n\n\nYes, it has had a big impact. Both the skewness and kurtosis values are much closer to zero in the transformed data compared to the original. Both indicates that the transformed data are more normally distributed.\n\n\n\n\n\n\n\n\n\n Data Skills - is there a statistically significant difference from a normal distribution in the transformed data?\n\n\n\n\n\nNo, whereas the Shapiro-Wilk statistic showed a significant departure from a normal distribution for the original data (W = 0.8, p &lt; 0.001) it shows that the transformed data is now indistinguishable from a normal distribution (W = 0.953, p = 0.130)\n\n\n\nFinally - let’s see if we can use our transformed variables to compute a parametric t-test. Repeat the independent samples t-test from earlier but include all three variables in the ‘Dependent Variables’ box. Make sure to include\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary - original data\nDays_One_to_Seven_Transformed - Log transformed data\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nWith the original data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . We can proceed with Student’s t-testWelch’s t-testMann-Whitney U test.\nUsing the log-transformed data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . Levene’s test found that the we cancan not assume that the groups have homogeneity of variance, F(1, 34) = , p = . We can proceed with Student’s t-testWelch’s t-testMann-Whitney U test.\n\n\n\nNote that Welch’s t-test is valid whether or not we can assume homogeneity of variance…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#summary",
    "href": "tutorials/rmb_week-7_non-parametrics.html#summary",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "We have explored an interesting dataset in some detail and worked with serveral different analysis approaches. Specifically, we have several options when working on checking the assumptions of our analyses. We have to be careful when computing and reporting hypothesis tests that we’re using the correct tests in a way that is supported by the data. Sometimes we can use a transform to continuous using parametric tests when the assumptions are not supported but this will not always work.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rma_week-10_tests_of_difference_Chi-Squared.html",
    "href": "tutorials/rma_week-10_tests_of_difference_Chi-Squared.html",
    "title": "Week 10 : Tests of difference: Chi-Squared",
    "section": "",
    "text": "This week you will be learning about\n\n\n\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nHypthesis testing\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Jamovi editor\n\n\n\nPerforming a chi-squared test in Jamovi\n\n\n\nPlot basic graphs in Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with openly available research data\n\n\n\n\n\n\n\n\n\nToday’s session\n\n\n\nToday you will just be familiarising yourself with the Chi-Squared test of difference. The Chi-Squared test is a non-parametric test of difference. It is used when…\n\nYou want to know about differences between groups\n\nAre there more females than males studying Psychology?\n\nYou want to know whether there is an association between two categorical variables\n\nIs there an association between gender and smoking?\n\nYour groups are independent\n\nEach observation only contributes to one cell of the analysis (e.g. gender)\n\nYou have categorical data (otherwise known as on the nominal level of measurement)\n\nFrequency or number of observations in a number of categories e.g. 10 male vs 50 females enrolled in Psychology\n\nYour data violates parametric assumptions\n\nYour data is not normally distributed\n\n\n\n\n\n\n\n\n\nThe table underneath contains information about smoking status distributed by gender. This information can be entered in to Jamovi providing that appropriate variables are constructed. In this example there are three variables, namely ‘smoking status’, ‘gender’, and ‘frequency’. In Point 2 illustrates these variables in Jamovi ‘Variable view’. Point 3 illustrates these variables and the associated data in Jamovi ‘Data view’\n\n\n\n\n\n\n\n\n\nSmokes\nDoes not smoke\n\n\n\n\nMale\n25\n            128\n\n\nFemale\n12\n            85\n\n\n\n\nTo learn how to perform a Chi-square test you may open the example data in this week’s Canvas folder and work through the steps detailed below – to open the file, simply double click on it. If you have Jamovi installed on your computer, then it will automatically open\nCheck that all relevant details are included in variable view – click on ‘variables’ in the top tool bar (it should look like the screen below). You should add your own labels here too. Also check that the values are specified for ‘gender’ and ‘smokes’.\n\n\n\nTo edit your variables, simply double tap on the variable of choice in the ‘variable view’ or tap on ‘edit’ in the tool bar\nYou will see all the details about the variable, which you can change.\n\n\n\n\nClick on ‘data’ and check that a row defines each cell of your frequency table (i.e. you should have 1 row for females who smoke, 1 for females who do not smoke, 1 for males who smoke and 1 for males who do not smoke). The total observed frequency of each category should be in the corresponding frequency column.\n\n\n\nNow you are ready to perform the Chi squared test. Click on: Frequencies &gt; Independent Samples\n\n\n\nAdd ‘Gender’ to the rows box, ‘Smokes’ to the Columns box, and ‘Frequency’ to the counts box\n\n\n\nClick on statistics and tick the Chi Squared test\n\n\n\nNext, select the cells button and select expected counts AND row, column and total\n\n\n\nYou can also play around with plots to visualise your data\n\n\n\nOn the right-hand side, you will see your results\n\n\n\n\n\nA Pearson Chi-Square was used to identify whether gender influenced smoking frequency. Results of the chi-square showed no significant difference in smoking frequency between genders (χ² (1) = .742, p = .389).\n\n\n\n\n\n\nNote\n\n\n\nYou can find the χ symbol on word by going to Insert &gt; Symbol. You might need to look under More Symbols if you haven’t used this symbol before.\n\n\n\n\n\n\nA chi-square analysis was carried out to investigate the relationship between gender and smoking frequency, the Jamovi output showing the results of the analysis is shown below.\n\n\n\n\n\n\n\n Data Skills - What are the observed frequencies for men and women who did and did not smoke?\n\n\n\n\n\nThe rows represent participants and the columns represent the different variables\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionSolution\n\n\nWhat are the observed frequencies for males and females who did and did not smoke?\n\n\n\n\n\n\n\n\n\nDid smoke\nDid not smoke\n\n\n\n\nMale\n\n\n\n\nFemale\n\n\n\n\n\n\n\n\n\n\n\nDid smoke\nDid not smoke\n\n\n\n\nMale\n50\n128\n\n\nFemale\n12\n140\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionSolution\n\n\nHow would you report the results of the chi-square analysis in APA format?\n\n\n\nA Pearson Chi-Square was used to identify whether gender influenced smoking frequency. Results of the chi-square showed a significant difference in smoking frequency between genders (χ² (1) = 21.9, p &lt; .001).\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - Are the results of the chi-square significant?\n\n\n\n\n\nYes p &lt; 0.05\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat do these results show?\n\n\n\nThink about the question we were trying to answer and what the results actually mean in real terms. Is there a difference between the groups? If there is, which group has a higher frequency of smokers?\n\n\nAs the results were statistically significant, there is an association between smoking frequency and gender, with males being more likely to be a smoker than women.\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nThe table underneath shows how often men and women play football. Enter this data into Jamovi following the same format as in the previous example.\n\nWhat are the results of the chi-square?\nAre the results significant?\nWhat do the results mean and what can you conclude?\n\n\n\n\n\nPlay football\nDo not play football\n\n\n\n\nMale\n60\n70\n\n\nFemale\n45\n80\n\n\n\n\n\nTake a look at the Jamovi output\n\n\n\n\nχ² (1) = 2.71, p = .100\nNo\nThe results of the chi-square show no significant association between gender and whether or not you play football. Neither gender is significantly more likely to play football.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 10 - Tests of Difference Chi-Squared"
    ]
  },
  {
    "objectID": "tutorials/rma_week-10_tests_of_difference_Chi-Squared.html#how-to-perform-a-chi-square-in-jamovi",
    "href": "tutorials/rma_week-10_tests_of_difference_Chi-Squared.html#how-to-perform-a-chi-square-in-jamovi",
    "title": "Week 10 : Tests of difference: Chi-Squared",
    "section": "",
    "text": "The table underneath contains information about smoking status distributed by gender. This information can be entered in to Jamovi providing that appropriate variables are constructed. In this example there are three variables, namely ‘smoking status’, ‘gender’, and ‘frequency’. In Point 2 illustrates these variables in Jamovi ‘Variable view’. Point 3 illustrates these variables and the associated data in Jamovi ‘Data view’\n\n\n\n\n\n\n\n\n\nSmokes\nDoes not smoke\n\n\n\n\nMale\n25\n            128\n\n\nFemale\n12\n            85\n\n\n\n\nTo learn how to perform a Chi-square test you may open the example data in this week’s Canvas folder and work through the steps detailed below – to open the file, simply double click on it. If you have Jamovi installed on your computer, then it will automatically open\nCheck that all relevant details are included in variable view – click on ‘variables’ in the top tool bar (it should look like the screen below). You should add your own labels here too. Also check that the values are specified for ‘gender’ and ‘smokes’.\n\n\n\nTo edit your variables, simply double tap on the variable of choice in the ‘variable view’ or tap on ‘edit’ in the tool bar\nYou will see all the details about the variable, which you can change.\n\n\n\n\nClick on ‘data’ and check that a row defines each cell of your frequency table (i.e. you should have 1 row for females who smoke, 1 for females who do not smoke, 1 for males who smoke and 1 for males who do not smoke). The total observed frequency of each category should be in the corresponding frequency column.\n\n\n\nNow you are ready to perform the Chi squared test. Click on: Frequencies &gt; Independent Samples\n\n\n\nAdd ‘Gender’ to the rows box, ‘Smokes’ to the Columns box, and ‘Frequency’ to the counts box\n\n\n\nClick on statistics and tick the Chi Squared test\n\n\n\nNext, select the cells button and select expected counts AND row, column and total\n\n\n\nYou can also play around with plots to visualise your data\n\n\n\nOn the right-hand side, you will see your results\n\n\n\n\n\nA Pearson Chi-Square was used to identify whether gender influenced smoking frequency. Results of the chi-square showed no significant difference in smoking frequency between genders (χ² (1) = .742, p = .389).\n\n\n\n\n\n\nNote\n\n\n\nYou can find the χ symbol on word by going to Insert &gt; Symbol. You might need to look under More Symbols if you haven’t used this symbol before.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 10 - Tests of Difference Chi-Squared"
    ]
  },
  {
    "objectID": "tutorials/rma_week-10_tests_of_difference_Chi-Squared.html#exercises",
    "href": "tutorials/rma_week-10_tests_of_difference_Chi-Squared.html#exercises",
    "title": "Week 10 : Tests of difference: Chi-Squared",
    "section": "",
    "text": "A chi-square analysis was carried out to investigate the relationship between gender and smoking frequency, the Jamovi output showing the results of the analysis is shown below.\n\n\n\n\n\n\n\n Data Skills - What are the observed frequencies for men and women who did and did not smoke?\n\n\n\n\n\nThe rows represent participants and the columns represent the different variables\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionSolution\n\n\nWhat are the observed frequencies for males and females who did and did not smoke?\n\n\n\n\n\n\n\n\n\nDid smoke\nDid not smoke\n\n\n\n\nMale\n\n\n\n\nFemale\n\n\n\n\n\n\n\n\n\n\n\nDid smoke\nDid not smoke\n\n\n\n\nMale\n50\n128\n\n\nFemale\n12\n140\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionSolution\n\n\nHow would you report the results of the chi-square analysis in APA format?\n\n\n\nA Pearson Chi-Square was used to identify whether gender influenced smoking frequency. Results of the chi-square showed a significant difference in smoking frequency between genders (χ² (1) = 21.9, p &lt; .001).\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - Are the results of the chi-square significant?\n\n\n\n\n\nYes p &lt; 0.05\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat do these results show?\n\n\n\nThink about the question we were trying to answer and what the results actually mean in real terms. Is there a difference between the groups? If there is, which group has a higher frequency of smokers?\n\n\nAs the results were statistically significant, there is an association between smoking frequency and gender, with males being more likely to be a smoker than women.\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nThe table underneath shows how often men and women play football. Enter this data into Jamovi following the same format as in the previous example.\n\nWhat are the results of the chi-square?\nAre the results significant?\nWhat do the results mean and what can you conclude?\n\n\n\n\n\nPlay football\nDo not play football\n\n\n\n\nMale\n60\n70\n\n\nFemale\n45\n80\n\n\n\n\n\nTake a look at the Jamovi output\n\n\n\n\nχ² (1) = 2.71, p = .100\nNo\nThe results of the chi-square show no significant association between gender and whether or not you play football. Neither gender is significantly more likely to play football.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 10 - Tests of Difference Chi-Squared"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "This week we will explore how to use Jamovi to compute and visualise descriptive statistics from a dataset. This will include some revision on Jamovi from last semester and an introduction on how we can use the R coding language to simplify complex data processing.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nMeasures of central tendency\n\n\n\nMeasures of dispersion and spread\n\n\n\nMeasures of skew and normality\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Rj editor to use R code within Jamovi\n\n\n\nLearn the basics about functions and variables in R\n\n\n\nComputing descriptive statistics using R\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nValidating and working with openly available research data\n\n\n\nCreate a reproducible data analysis script\n\n\n\n\n\n\n\n\n\n\n\nDataset\n\n\n\nNote that this is the same dataset as in week 1\n\n\nWe will be exploring data that was collected as part of the ‘Many Labs 3’ project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‘Big Five’ personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs. extravagant/careless)\nAgreeableness (friendly/compassionate vs. critical/rational)\nNeuroticism (sensitive/nervous vs. resilient/confident)\nOpenness to experience (inventive/curious vs. consistent/cautious)\nExtraversion (outgoing/energetic vs. solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.\n\n\n\nLast week, we looked at how we can spend some time understanding a new dataset and compute some descriptive statistics from it.\nNow, we’re going to learn how to use Jamovi and R to make this process more repeatable and more transparent. This will make it much easier for us to come back to the analysis and understand what steps were taken at a later date.\n\n\n\n\n\n\nOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.\n\n\n\n\n\nWe’ll be working with the Rj Add-On for Jamovi this week.\n\n\n\n\n\n\nKey step\n\n\n\nBefore continuing, install the Rj add-on for jamovi following the guide for installing Jamovi modules. Once installed you should have a new button on the Ribbon menu at the top of Jamovi…\n\n\n\nClick on this and select ‘Rj Editor’ to open a window in which we can write R code! Click on the small cog icon and make sure that ‘Output’ is set to ‘Show code and output’.\n\nWe can use the left-hand window, named Rj editor, to write commands, using the R language. These are the straightforward text commands that tell the computer a series of steps that we wanted to carry out. These commands can be edited and rerun as many times as we need, they can even be shared with collaborators and included as part of scientific publication.\nThis is a very powerful tool once we have a written set of instructions to work with rather than individually clicking through instructions. Though it takes a bit longer to write the commands, our analysis is much more transparent and reproducible once it is done.\nThe outputs of the analysis defined in the Rj editor appear on the right-hand side of the Jamovi window as usual. To check this out, let’s explore using or as a calculator.\n\n\n\n\n\n\nData Skills - R as a calculator\n\n\n\nTry typing a set of sums or equations into the Rj editor. Include one sum on each line and include as many as you like. When you’re ready, you can tell RJ to run the commands by clicking on the green triangle in the top right corner of the RJ editor. This will automatically compute. All of the sums you to find and print the results out in the right hand side window. see the example below.\nYou can use the following symbols to define your sums.\n\n\n\nSymbol\nOperation\n\n\n\n\n+\nAdd\n\n\n-\nSubtract\n\n\n*\nMultiply\n\n\n/\nDivide\n\n\n\n\n\n\n\n\n\n\nOften, we don’t just want to see the result of a calculation, we want to store the result to use again later. For this, we want to use variables. For example, here we store the result of a calculator sum in a variable named ‘y’\ny &lt;- 2 + 2\nIf you run this in Rj, you’ll notice that we no longer get the number 4 printed to the screen. This is as the output is now caught and stored rather than printed out. If we still want to see what is contained in y, we can use another helpful function named print.\ny &lt;- 2 + 2\nprint(y)\nWill now also display the content of y.\n\n\n\n\n\n\nDefinition of a variable\n\n\n\nA variable is an object that provides a labelled space to store information. The &lt;- or = symbols indicate that we’re assigning some information to a variable (the two symbols behave the same way, you could see either).\nCrucially, the information on the right hand side is assigned to a variable with the name from the left hand side.\n\n\n\n\n\n\n\n\nCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about variables in R. We suggest you try the following prompts to get started.\n\n“Write a simple description of variables in R for a beginner”\n“How can I choose a name for my variable in R”\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\nNotice that the one of the equations in my example is a bit different to the others. The final example uses a function called sqrt as a shortcut to compute the square root of the number nine.\nThe sqrt function calculates the square root of a given number. In R code, this looks something like this:\nsqrt(x)\nwith the ‘x’ replaced by any number. For example, we could compute the square root of 36 using the following command.\nsqrt(36)\nAdd this line to your Rj editor and check the result..\nMany functions, take inputs contained in parentheses written after the name of the function. These inputs are sometimes known as the arguments of a function. In the last example, 36 is an argument for the function sqrt.\nSometimes functions take several arguments, and the arguments themselves can have specific names. For example.\nround(12.345, digits = 0)\nwill round the value x to a whole number with zero digits after the decimal point. Where as\nround(12.345, digits = 2)\nwould round the same number to preserve 2 digits after the decimal point. Try running these examples in Rj with a range of different numbers to get a sense of how it works.\n\n\n\n\n\n\nDefinition of a function\n\n\n\nFunctions are used to perform specific, predefined tasks, such as calculations, data transformations, or visualization. You can think of it as a “black box” that takes in a value, performs some internal calculation, and returns the result.\n\n\n\n\n\n\n\n\nCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about functions in R. We suggest you try the following prompts to get started.\n\n“Write a simple description of functions in R for a beginner”\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\nA single analysis might have several lines of code that create, store and reuse a several different variables with several different functions. For example:\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat does this R code do? what is the value of x, y and z?\n x &lt;- 10 + 10\n y &lt;- sqrt(16)\n z &lt;- x / y\n\n\nTry copying out and running the code in Rj.\nYou can add some print functions to see the value of the intermediate variables x and y as well. Try adding print(x) on the line immediately after x is defined.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final answers:\nx is 20 and y is 4, so z = 20 / 4 which is 5.\n\n\n\n\n\n\n\nRj has a special feature that the data loaded into our spreadsheet is always contained in a special variable named data. We can refer to this data variable to view and run analyses on our dataset.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to load the data file into Jamovi and compute the descriptive statistics for the big five personality factors again.\n\n\n\nThe data we have loaded into our Jamovi session can be accessed in Rj with the data variable. Jamovi automatically makes the data available for us in the R environment.\nWe can view and interact with the data here as well. For example, to view the first few rows of the dataset you can run the head function and pass in data as an argument.\nhead(data)\nand to view the last few rows, you can do the same with the tail function.\ntail(data)\nOften, we’ll only want to work with a subset of the columns in a data frame at any given time. We can access individual columns using the dollar operator. For example, this code will access the age column within the loaded dataset.\ndata$age\nLet’s break this command down into its pieces:\n\n\n\n\n\n\n\nR code\nMeaning\n\n\n\n\ndata\nthis is the name of the whole data array we have loaded in\n\n\n$\nthis is a separator that R uses to help us access subsections of datasets\n\n\nage\nThis is the name of a single column we want to access\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you access the column of Mood scores from the same dataset?\n\n\nYou can use the head function to see the list of all available columns in the dataset. The example above accesses the ‘age’ column, you can tweak this to extract the mood data instead.\n\n\nThe final code should look like this\ndata$Mood\n\n\n\n\n\nWe can also apply functions to these variables. R includes a wide range of standard mathmatical functions that we can use for common computations. This code computes the mean of the age data in our dataset.\nmean(data$age)\nTry running this in Rj. You should get the answer 19.3 - this matches the value we computed using the descriptives analysis last week.\nSimilarly, the following function computes the standard deviation\nsd(data$age)\nIf you run this, you should get the answer 3.808.\n\n\n\nOk, back to statistics! How can we use our new R skills to compute statistics?\nThe trick is to understand that all the analyses in Jamovi are actually carried out by functions in R. Each analysis we can select from the top ribbon in Jamovi corresponds to a function that can be called in the code. Similarly, all of the options that we select in the analysis, dialogue boxes, correspond to arguments for that function. Let’s take a look in practice.\nJamovi provides a collection of R functions that correspond to each analysis dialogue box in Jamovi. This package of functions is named jmv. Thejmv function that computes descriptive statistics is called:\njmv::descriptives\nHere we first name the jmv package specify the descriptives function within it (this link is made with the two colons).\nWe can compute the descriptive statistics for age by passing the data into the function and specifying that we want to use the age variable. Try running the following:\njmv::descriptives(data, vars='age')\nand you should generate the following result table.\n\n\n\n\n\n\n\nNote!\n\n\n\nYou might notice that we still have the unusual ages in this analysis, we’ll discuss how to remove them in the next section\n\n\nJamovi provide a lot of documentation on the descriptives function. This lists every single option that can be passed into jmv::descriptives. There is an option here for every single checkbox and selector in the main Jamovi dialogue.\nThe following code additionally computes a histogram by adding the hist argument.\njmv::descriptives(data, vars='age', hist=TRUE)\n\nWe can end up specifying a lot of these options. Sometimes it can be a good idea to split the function across multiple lines. This is fine as long as the parentheses are in the right place and there are commas in between each option.\nThis code computes descriptive statistics including the skewness, interquartile range and mode.\njmv::descriptives(data, \n  vars='age', \n  skew=TRUE,\n  iqr=TRUE,\n  mode=TRUE,\n  hist=TRUE\n  )\n\nOf course, we could run these analyses by hand. The main advantage here is that we can automate the selection of critical options and reduce the possiblity of mistakes when communicating analyses within teams. As long as we have this line saved, we can rerun it on new datasets without having to recheck every box by hand.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse the examples above and the JMV documentation on the descriptives function to write a function definition that does the following.\nCompute the core descriptive statistics for the Openness variable, make sure to include the range and a boxplot.\n\n\nTake a look at the ‘range’ and ‘box’ options in the documentation page.\nBe careful with the spelling of each argument!\n\n\nThe final code should be:\njmv::descriptives(data, \n  vars='Openness', \n  range=TRUE,\n  box=TRUE\n  )\n\n\n\n\n\n\n\n\n\n\nRemember that last week we very carefully removed several participants who had entered unrealisic or impossible ages into the datasets. This may have been an accident or mistake but either way we should not use this data. Our examples so far have just used all the data.\nR and Rj make it very easy to clean data without the boring, manual work we did last week. R can do the hard work and we can avoid scrolling through very large datasets by hand. Sounds good!\nThe first thing we can do is investigate what range of responses that people actually gave when asked about their age. To do this we can use a new function called unique(). This takes a column of a dataset as an input and returns a list of all the unique responses that it contains.\n\n\n\n\n\n\nKey step\n\n\n\nLoad up an Rj Editor window (or continue in your old one) and run unique() on the age data.\nThe result should look like this:\n\n\n\nMost of these responses are fine but, as we saw last week, there are a lot of strange ones. Most importantly, we can find the extreme values that are very likely errors during data collection (-2 and 149)\nWe need to remove these from our analysis to get the most accurate results.\nTo do this we’re going to use an R library named dplyr (short for ‘Data Pliers’, see their documentation here). This is an extremely powerful library that is very widely used across the world.\nTo use dplyr, we first need to load the library into Rj by adding the following line to our code.\nlibrary(dplyr)\nThis tells Rj that we want to use the functions from this library.\nThe key function we need today is filter() - this extracts rows from a dataset based on logical conditions that we specify in the code. We are going to specify two arguments, firstly we will pass in the data and secondly we will specify the condition.\nIn this example we extract only the rows in which participants have an age above 20.\nlibrary(dplyr)\n\nfilter(data, age&gt;20)\n\nNotice that the dataset printed in the right hand window now only has values from the participants who are older than 20.\nWe can even specify multiple options - here we extract rows with participants older than 20 but still 23 or younger.\nlibrary(dplyr)\n\nfilter(data, age&gt;20, age&lt;=23)\nNext, to recreate our data tidying form last week, we need to extract the rows of participants who are older than -2 and younger than 149.\nThese values might be too specific and might not apply to every dataset. We can apply some common sense and suggest some broad, but much more sensible boundaries for a student population. We also save the result into a variable we can reuse later.\ntidy_data &lt;- filter(data, age &gt; 15, age &lt; 60)\nAnd finally, we can combine this with our call to jmv::descriptives to both filter our data to only the rows we want to use and compute the descriptive statistics on the result.\ntidy_data &lt;- filter(data, age &gt; 15, age &lt; 60)\n\njmv::descriptives(tidy_data, vars='age')\n\n\n\n\n\n\n\nKey step\n\n\n\nPractice changing the age conditions to a few different options to see the difference it makes to the descriptives analysis. What happens when you set a condition that results in zero valid participants?\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse the information in this tutorial to compute descriptive statistics for the Openness variable from Introverted participants. In other words - what is the mean Openness score for participants who are more Introverted than Extraverted.\nCompute the core descriptive statistics for the Openness variable, make sure to include the median and a histogram\n\n\nTo extract the data rows for Introverted participants we want to extract rows where the Extraversion score is less than 3.5\n\n\nThe final code should be:\nlibrary(dplyr)\n\ntidy_data &lt;- filter(data, Extraversion &lt; 3.5)\n\njmv::descriptives(tidy_data, vars='Openness')\n\n\n\n\n\n\n\n\n\n\nIt probably seems like a small difference so far. After all, we same results for our analysis using the check, boxes in Jamovi or the code in R. The Jamovi analysis is certainly quicker to run for the first time and it’s more convenient when exploring your data.\nHowever the R code has several crucial advantages.\n\nThe R code is transparent about what is actually being run to generate the results and by sharing this code, we enable others to repeat our analysis. This makes it really straightforward to run and re-run identical analyses in situations where we need to be really sure that everybody has done the same thing. For example, if we want to be certain that two data sets have been analysed in an identical way then that our code would be the best way to do this.\nR code is the best way to share what we’ve done with the rest of the world. It is common practice for modern academic public it to include the code that was used to generate the results in the paper. This way everybody who wants to check can make sure they fully understand what the authors of the paper have done.\n\nFinally, over the course of the semester we’ll see that R allows us to do much more than what is possible in Jamovi. We can define blocks of R code that can tidy-up and arrange our data, run multiple analyses and generate tables & figures all in one go. Much easier that clicking through all the menus each time we want to rerun an analysis!\n\n\n\n\n\nBlock, Jack. 1995. “A Contrarian View of the Five-Factor Approach to Personality Description.” Psychological Bulletin 117 (2): 187–215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. “Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.” Journal of Experimental Social Psychology 67: 68–82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. “A Very Brief Measure of the Big-Five Personality Domains.” Journal of Research in Personality 37 (6): 504–28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#the-dataset",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#the-dataset",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Dataset\n\n\n\nNote that this is the same dataset as in week 1\n\n\nWe will be exploring data that was collected as part of the ‘Many Labs 3’ project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‘Big Five’ personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs. extravagant/careless)\nAgreeableness (friendly/compassionate vs. critical/rational)\nNeuroticism (sensitive/nervous vs. resilient/confident)\nOpenness to experience (inventive/curious vs. consistent/cautious)\nExtraversion (outgoing/energetic vs. solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#the-challenge",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#the-challenge",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Last week, we looked at how we can spend some time understanding a new dataset and compute some descriptive statistics from it.\nNow, we’re going to learn how to use Jamovi and R to make this process more repeatable and more transparent. This will make it much easier for us to come back to the analysis and understand what steps were taken at a later date.\n\n\n\n\n\n\nOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#install-the-rj-add-on-in-jamovi",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#install-the-rj-add-on-in-jamovi",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "We’ll be working with the Rj Add-On for Jamovi this week.\n\n\n\n\n\n\nKey step\n\n\n\nBefore continuing, install the Rj add-on for jamovi following the guide for installing Jamovi modules. Once installed you should have a new button on the Ribbon menu at the top of Jamovi…\n\n\n\nClick on this and select ‘Rj Editor’ to open a window in which we can write R code! Click on the small cog icon and make sure that ‘Output’ is set to ‘Show code and output’.\n\nWe can use the left-hand window, named Rj editor, to write commands, using the R language. These are the straightforward text commands that tell the computer a series of steps that we wanted to carry out. These commands can be edited and rerun as many times as we need, they can even be shared with collaborators and included as part of scientific publication.\nThis is a very powerful tool once we have a written set of instructions to work with rather than individually clicking through instructions. Though it takes a bit longer to write the commands, our analysis is much more transparent and reproducible once it is done.\nThe outputs of the analysis defined in the Rj editor appear on the right-hand side of the Jamovi window as usual. To check this out, let’s explore using or as a calculator.\n\n\n\n\n\n\nData Skills - R as a calculator\n\n\n\nTry typing a set of sums or equations into the Rj editor. Include one sum on each line and include as many as you like. When you’re ready, you can tell RJ to run the commands by clicking on the green triangle in the top right corner of the RJ editor. This will automatically compute. All of the sums you to find and print the results out in the right hand side window. see the example below.\nYou can use the following symbols to define your sums.\n\n\n\nSymbol\nOperation\n\n\n\n\n+\nAdd\n\n\n-\nSubtract\n\n\n*\nMultiply\n\n\n/\nDivide",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#variables-and-functions",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#variables-and-functions",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Often, we don’t just want to see the result of a calculation, we want to store the result to use again later. For this, we want to use variables. For example, here we store the result of a calculator sum in a variable named ‘y’\ny &lt;- 2 + 2\nIf you run this in Rj, you’ll notice that we no longer get the number 4 printed to the screen. This is as the output is now caught and stored rather than printed out. If we still want to see what is contained in y, we can use another helpful function named print.\ny &lt;- 2 + 2\nprint(y)\nWill now also display the content of y.\n\n\n\n\n\n\nDefinition of a variable\n\n\n\nA variable is an object that provides a labelled space to store information. The &lt;- or = symbols indicate that we’re assigning some information to a variable (the two symbols behave the same way, you could see either).\nCrucially, the information on the right hand side is assigned to a variable with the name from the left hand side.\n\n\n\n\n\n\n\n\nCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about variables in R. We suggest you try the following prompts to get started.\n\n“Write a simple description of variables in R for a beginner”\n“How can I choose a name for my variable in R”\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\nNotice that the one of the equations in my example is a bit different to the others. The final example uses a function called sqrt as a shortcut to compute the square root of the number nine.\nThe sqrt function calculates the square root of a given number. In R code, this looks something like this:\nsqrt(x)\nwith the ‘x’ replaced by any number. For example, we could compute the square root of 36 using the following command.\nsqrt(36)\nAdd this line to your Rj editor and check the result..\nMany functions, take inputs contained in parentheses written after the name of the function. These inputs are sometimes known as the arguments of a function. In the last example, 36 is an argument for the function sqrt.\nSometimes functions take several arguments, and the arguments themselves can have specific names. For example.\nround(12.345, digits = 0)\nwill round the value x to a whole number with zero digits after the decimal point. Where as\nround(12.345, digits = 2)\nwould round the same number to preserve 2 digits after the decimal point. Try running these examples in Rj with a range of different numbers to get a sense of how it works.\n\n\n\n\n\n\nDefinition of a function\n\n\n\nFunctions are used to perform specific, predefined tasks, such as calculations, data transformations, or visualization. You can think of it as a “black box” that takes in a value, performs some internal calculation, and returns the result.\n\n\n\n\n\n\n\n\nCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about functions in R. We suggest you try the following prompts to get started.\n\n“Write a simple description of functions in R for a beginner”\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\nA single analysis might have several lines of code that create, store and reuse a several different variables with several different functions. For example:\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat does this R code do? what is the value of x, y and z?\n x &lt;- 10 + 10\n y &lt;- sqrt(16)\n z &lt;- x / y\n\n\nTry copying out and running the code in Rj.\nYou can add some print functions to see the value of the intermediate variables x and y as well. Try adding print(x) on the line immediately after x is defined.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final answers:\nx is 20 and y is 4, so z = 20 / 4 which is 5.\n\n\n\n\n\n\n\nRj has a special feature that the data loaded into our spreadsheet is always contained in a special variable named data. We can refer to this data variable to view and run analyses on our dataset.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#accessing-the-data-table-in-rj",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#accessing-the-data-table-in-rj",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Key step\n\n\n\nTake a moment to load the data file into Jamovi and compute the descriptive statistics for the big five personality factors again.\n\n\n\nThe data we have loaded into our Jamovi session can be accessed in Rj with the data variable. Jamovi automatically makes the data available for us in the R environment.\nWe can view and interact with the data here as well. For example, to view the first few rows of the dataset you can run the head function and pass in data as an argument.\nhead(data)\nand to view the last few rows, you can do the same with the tail function.\ntail(data)\nOften, we’ll only want to work with a subset of the columns in a data frame at any given time. We can access individual columns using the dollar operator. For example, this code will access the age column within the loaded dataset.\ndata$age\nLet’s break this command down into its pieces:\n\n\n\n\n\n\n\nR code\nMeaning\n\n\n\n\ndata\nthis is the name of the whole data array we have loaded in\n\n\n$\nthis is a separator that R uses to help us access subsections of datasets\n\n\nage\nThis is the name of a single column we want to access\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you access the column of Mood scores from the same dataset?\n\n\nYou can use the head function to see the list of all available columns in the dataset. The example above accesses the ‘age’ column, you can tweak this to extract the mood data instead.\n\n\nThe final code should look like this\ndata$Mood\n\n\n\n\n\nWe can also apply functions to these variables. R includes a wide range of standard mathmatical functions that we can use for common computations. This code computes the mean of the age data in our dataset.\nmean(data$age)\nTry running this in Rj. You should get the answer 19.3 - this matches the value we computed using the descriptives analysis last week.\nSimilarly, the following function computes the standard deviation\nsd(data$age)\nIf you run this, you should get the answer 3.808.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#running-desriptive-statistics-using-rj",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#running-desriptive-statistics-using-rj",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Ok, back to statistics! How can we use our new R skills to compute statistics?\nThe trick is to understand that all the analyses in Jamovi are actually carried out by functions in R. Each analysis we can select from the top ribbon in Jamovi corresponds to a function that can be called in the code. Similarly, all of the options that we select in the analysis, dialogue boxes, correspond to arguments for that function. Let’s take a look in practice.\nJamovi provides a collection of R functions that correspond to each analysis dialogue box in Jamovi. This package of functions is named jmv. Thejmv function that computes descriptive statistics is called:\njmv::descriptives\nHere we first name the jmv package specify the descriptives function within it (this link is made with the two colons).\nWe can compute the descriptive statistics for age by passing the data into the function and specifying that we want to use the age variable. Try running the following:\njmv::descriptives(data, vars='age')\nand you should generate the following result table.\n\n\n\n\n\n\n\nNote!\n\n\n\nYou might notice that we still have the unusual ages in this analysis, we’ll discuss how to remove them in the next section\n\n\nJamovi provide a lot of documentation on the descriptives function. This lists every single option that can be passed into jmv::descriptives. There is an option here for every single checkbox and selector in the main Jamovi dialogue.\nThe following code additionally computes a histogram by adding the hist argument.\njmv::descriptives(data, vars='age', hist=TRUE)\n\nWe can end up specifying a lot of these options. Sometimes it can be a good idea to split the function across multiple lines. This is fine as long as the parentheses are in the right place and there are commas in between each option.\nThis code computes descriptive statistics including the skewness, interquartile range and mode.\njmv::descriptives(data, \n  vars='age', \n  skew=TRUE,\n  iqr=TRUE,\n  mode=TRUE,\n  hist=TRUE\n  )\n\nOf course, we could run these analyses by hand. The main advantage here is that we can automate the selection of critical options and reduce the possiblity of mistakes when communicating analyses within teams. As long as we have this line saved, we can rerun it on new datasets without having to recheck every box by hand.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse the examples above and the JMV documentation on the descriptives function to write a function definition that does the following.\nCompute the core descriptive statistics for the Openness variable, make sure to include the range and a boxplot.\n\n\nTake a look at the ‘range’ and ‘box’ options in the documentation page.\nBe careful with the spelling of each argument!\n\n\nThe final code should be:\njmv::descriptives(data, \n  vars='Openness', \n  range=TRUE,\n  box=TRUE\n  )",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#selecting-rows-of-data-using-the-filter-function",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#selecting-rows-of-data-using-the-filter-function",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Remember that last week we very carefully removed several participants who had entered unrealisic or impossible ages into the datasets. This may have been an accident or mistake but either way we should not use this data. Our examples so far have just used all the data.\nR and Rj make it very easy to clean data without the boring, manual work we did last week. R can do the hard work and we can avoid scrolling through very large datasets by hand. Sounds good!\nThe first thing we can do is investigate what range of responses that people actually gave when asked about their age. To do this we can use a new function called unique(). This takes a column of a dataset as an input and returns a list of all the unique responses that it contains.\n\n\n\n\n\n\nKey step\n\n\n\nLoad up an Rj Editor window (or continue in your old one) and run unique() on the age data.\nThe result should look like this:\n\n\n\nMost of these responses are fine but, as we saw last week, there are a lot of strange ones. Most importantly, we can find the extreme values that are very likely errors during data collection (-2 and 149)\nWe need to remove these from our analysis to get the most accurate results.\nTo do this we’re going to use an R library named dplyr (short for ‘Data Pliers’, see their documentation here). This is an extremely powerful library that is very widely used across the world.\nTo use dplyr, we first need to load the library into Rj by adding the following line to our code.\nlibrary(dplyr)\nThis tells Rj that we want to use the functions from this library.\nThe key function we need today is filter() - this extracts rows from a dataset based on logical conditions that we specify in the code. We are going to specify two arguments, firstly we will pass in the data and secondly we will specify the condition.\nIn this example we extract only the rows in which participants have an age above 20.\nlibrary(dplyr)\n\nfilter(data, age&gt;20)\n\nNotice that the dataset printed in the right hand window now only has values from the participants who are older than 20.\nWe can even specify multiple options - here we extract rows with participants older than 20 but still 23 or younger.\nlibrary(dplyr)\n\nfilter(data, age&gt;20, age&lt;=23)\nNext, to recreate our data tidying form last week, we need to extract the rows of participants who are older than -2 and younger than 149.\nThese values might be too specific and might not apply to every dataset. We can apply some common sense and suggest some broad, but much more sensible boundaries for a student population. We also save the result into a variable we can reuse later.\ntidy_data &lt;- filter(data, age &gt; 15, age &lt; 60)\nAnd finally, we can combine this with our call to jmv::descriptives to both filter our data to only the rows we want to use and compute the descriptive statistics on the result.\ntidy_data &lt;- filter(data, age &gt; 15, age &lt; 60)\n\njmv::descriptives(tidy_data, vars='age')\n\n\n\n\n\n\n\nKey step\n\n\n\nPractice changing the age conditions to a few different options to see the difference it makes to the descriptives analysis. What happens when you set a condition that results in zero valid participants?\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse the information in this tutorial to compute descriptive statistics for the Openness variable from Introverted participants. In other words - what is the mean Openness score for participants who are more Introverted than Extraverted.\nCompute the core descriptive statistics for the Openness variable, make sure to include the median and a histogram\n\n\nTo extract the data rows for Introverted participants we want to extract rows where the Extraversion score is less than 3.5\n\n\nThe final code should be:\nlibrary(dplyr)\n\ntidy_data &lt;- filter(data, Extraversion &lt; 3.5)\n\njmv::descriptives(tidy_data, vars='Openness')",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#summary",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#summary",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "It probably seems like a small difference so far. After all, we same results for our analysis using the check, boxes in Jamovi or the code in R. The Jamovi analysis is certainly quicker to run for the first time and it’s more convenient when exploring your data.\nHowever the R code has several crucial advantages.\n\nThe R code is transparent about what is actually being run to generate the results and by sharing this code, we enable others to repeat our analysis. This makes it really straightforward to run and re-run identical analyses in situations where we need to be really sure that everybody has done the same thing. For example, if we want to be certain that two data sets have been analysed in an identical way then that our code would be the best way to do this.\nR code is the best way to share what we’ve done with the rest of the world. It is common practice for modern academic public it to include the code that was used to generate the results in the paper. This way everybody who wants to check can make sure they fully understand what the authors of the paper have done.\n\nFinally, over the course of the semester we’ll see that R allows us to do much more than what is possible in Jamovi. We can define blocks of R code that can tidy-up and arrange our data, run multiple analyses and generate tables & figures all in one go. Much easier that clicking through all the menus each time we want to rerun an analysis!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#references",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#references",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Block, Jack. 1995. “A Contrarian View of the Five-Factor Approach to Personality Description.” Psychological Bulletin 117 (2): 187–215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. “Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.” Journal of Experimental Social Psychology 67: 68–82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. “A Very Brief Measure of the Big-Five Personality Domains.” Journal of Research in Personality 37 (6): 504–28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methods Computer Practicals",
    "section": "",
    "text": "Research Methods Computer Practicals\n\nWeek 1 : Exploring new data and computing descriptives\nWeek 2 : Computing reproducible statistics using Rj\nWeek 3 : One-Sample t-tests\nWeek 4 : Two-Sample t-tests\nWeek 5 : p-values and effect sizes\nWeek 7 : Testing data distributions and Non-parametric alternative tests\nWeek 8 : Simulating Sampling Distributions\nWeek 9 : One Way Analysis of Variance (ANOVA)\nWeek 10 : Simulating Sampling Distributions for t-values\nWeek 11 : Putting it all together - replicating Power Poses"
  }
]