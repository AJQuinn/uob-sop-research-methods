[
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "tutorials/rma_week-9_correlations-graphs-and-exporting.html",
    "href": "tutorials/rma_week-9_correlations-graphs-and-exporting.html",
    "title": "Week 9 : Correlations, graphs and exporting",
    "section": "",
    "text": "Quantitative Methods\n\n\n\n\n\nHypothesis testing\n\n\n\nConfidence intervals and critical values\n\n\n\nMeasures of skew and normality\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Jamovi editor\n\n\n\nPerforming a correlation in Jamovi\n\n\n\nPlot basic graphs in Jamovi\n\n\n\nReport different statistical tests\n\n\n\nMake decisions about which statistical test to use based on hypotheses and data\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with openly available research data\n\n\n\nCopy/paste Jamovi output into other applications\n\n\n\n\n\n\n\n\n\nNoteToday’s session\n\n\n\nToday you will be using Jamovi to perform a correlational analysis.\n\n\n\n\n\nWhen we want to test a research hypothesis, we obtain random samples of behaviour/data. We set up a null hypothesis H0: that the samples were drawn from populations with the same means (so that there will be no difference between them). We then obtain some samples and calculate the differences between the means of these samples, under the assumption that H0 is true (i.e. we obtained these means when the population means are equal).\n\nEXAMPLE: We want to test whether students drink more than the general population. To do this we collect data on the number of drinks per person in a club in one night, for two groups: students and general population. We then calculate the differences in the mean number of drinks for each of our groups. We assume that these means will be equal (null hypothesis). We are looking to disprove this hypothesis by finding differences in the means.\n\nWe then calculate the probability of getting a mean difference that is at least as large as the one we actually obtained between the means of our samples. This probability is our confidence interval. With a t-test, for example, we usually aim to say that we are 95% confident that our data were drawn from two populations with different means (i.e. there is a significant difference between the populations). To do this, we use the critical value – usually p&lt;0.05. In this case, we are saying that the probability that we would obtain the significant difference in data that we did would occur less than 5% or 5 in 100 times, if the null were true! That’s a very small chance. We can also use smaller, stricter p values like p&lt;0.01 (we would see the differences we found less than 1% of the time, if the null were true).\n\nEXAMPLE: We calculate the differences in the mean number of drinks for our student and general population groups. We find that students drink on average 2.5 more drinks than the general population, and that this difference is significant. We use a critical value of 5% and a confidence interval of 95% (these add up to 100%). That is, we can say that we are 95% confident that our data were drawn from two populations that actually have different means, and this conclusion will only be a mistake (due to chance) 5% of the time.\n\nIf the data do not lead us to reject the null hypothesis, the interpretation is more problematic – do we “accept” the null, or “retain” it, or just withhold judgement”?\n\nWe can definitely never claim to have “proved” the null hypothesis (as you cannot 100% prove a null statement)! A non-significant result is an inconclusive one – we have insufficient data to choose which hypothesis (null or alternative) is the most accurate.\n\n\n\n\n\nIf we are going to reject either the lowest or highest 5%, then we use a one-tailed (or directional) test\nIf we want to conduct a two-tailed, non-directional test of significance, then to use an overall critical value of 5% we need to reject both the lowest and highest 2.5%, making 5% altogether\nIf you are certain of the direction of your effect, you can use a one-tailed test – but generally, we use two-tailed tests (as it’s rare that we know what direction our effect will be in!)\n\n\n\n\nWe will be exploring the dataset correlation data that you can find on the CANVAS page under Week 9 of the computer labs folder of Research Methods A\nThere are five variables in the dataset\n\nAge\nIce_Cream - number of ice creams sold\nTemp - temperature\nMotivation\nStress - measured at two time points\n\n\n\n\nThe Pearson’s Product Moment correlation is a parametric test that measures the size and significance of a relationship between two variables. In this test, the experimental data must meet certain assumptions of parametric tests. Some of these assumptions include data continuity and a normal distribution. We will discuss how to assess whether the distribution is normal in the next semester. For now, let’s assume it is normal.\nWe will correlate the variables ‘Temp’ (temperature) and ‘Ice_Cream’ (number of ice creams sold). There are 20 days where the two variables were recorded. These are represented by the rows in the ‘data view’.\n\n\n\n\n\n\nWarningOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file Correlation_data.xlsx from canvas.\n\n\n\n\n\nNow it is time for you to explore the data. If you are unsure about any of these, please talk to the staff in your computer practical session.\n\n\n\nTo correlate the two variables click on ‘Analysis’ from the top tabs &gt; Regression &gt; Correlation Matrix\n\n\n\nTransfer ‘Ice_Cream’ and ‘Temp’ to the variables box by clicking them and pressing on the arrow button, double-clicking, or dragging them over to the box.\nFor this example, make sure that the ‘Pearson’ and ‘Report significance’ boxes are ticked. Note that Pearson is the version we use if our data meet parametric assumptions (e.g. interval or scale data, normal distribution, free from outliers etc.). If your data didn’t meet parametric assumptions, you would use Spearman instead because this test is more resilient to violations of the assumptions.\nSelect ‘correlated’ under hypothesis\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the correlation output to complete this statement reporting the results of the correlation. Fill in the spaces provided.\nThe correlation between temperature and the number of ice cream sold over 20 days is not significantsignificant (r () = , p &lt; ). There were  observations.\n\n\nThe correlation matrix…\n\n\n\n\n\n\n\n\n\n\n\n\nNoteInterpretation\n\n\n\nWhat does this result tell you? Have a think about how you would interpret the result?\n\n\n\n\n\n“A Pearson’s correlation was conducted to see if there was a significant relationship between temperature and the number of ice creams sold. This revealed that there was a very strong significant positive relationship between temperature and ice cream sales, r (18)= .893, p &lt; .001. Specifically, as temperatures increased, so too did ice cream scales. Therefore, we can accept the hypothesis that there would be a significant positive relationship between temperature and ice cream scales.”\nNote, a correlation can be positive (like this one was) or negative (e.g. the r value could have read -.893 instead). If it had been negative, this would have meant that as temperature increased, ice cream sales decreased. It is very important to get the direction of the relationship right because if you get it wrong it completely changes the findings!\nIt is generally accepted that r values can be interpreted in the following ways (these numbers can be either positive or negative!): 0-0.19 = Very weak, 0.20–0.39 = Weak, 0.40–0.59 = Moderate, 0.60–0.79 = Strong, 0.80–1 = Very strong. It is extremely uncommon to have an r value of 1 because this means it is a perfect correlation (e.g. when you correlate temperature with itself because they are the same).\n\n\n\nThis correlation can be illustrated graphically. This eases the understanding of the relationship between the variables. We can illustrate the relationship using the Jamovi Scatterplot function.\n\nClick on Analyse &gt; Exploration &gt; Scatterplot\n\n\n\nMove Ice_Cream and Temp across\nClick on linear under ‘Regression Line’\nA scatterplot should appear in the window to the right\n\n\nYou can clearly see that as the temperature increases, the number of ice cream sold also raises. We know that this relationship is significant from the results of the Pearson’s Correlation.\n\n\n\nA histogram is a graph showing the frequency of different scores. Histograms are frequently used in psychological research. Histograms are useful to check whether the data is normally distributed.\n\nGo to Exploration &gt; Descriptives &gt; to generate a basic histogram of the MOTIVATION variable. Transfer MOTIVATION to the Variables box, remember to tick the Histogram box under ‘Histograms’ – A Histogram should appear in the window to the right\n\n\n\nDo the same for Stress_1\n\n\n\n\n\n\n\n\nCaution Data Skills - Do you think the Motivation and Stress_1 data are normally distributed or is the data skewed?\n\n\n\n\n\nTo check this, look at your histograms. Can you see a classic “bell curve” shape (normally distributed data), or is some of the data pushed up at one end of the graph (skewed data)?\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Are there any outliers in the age data?\n\n\n\n\n\n\nTo check this, you will need to create a “Boxplot”.\nClick on the Boxplot box under ‘Boxplots’ and then create a boxplot for AGE by moving AGE into the variable box.\nA boxplot should appear in the window to the right\nLook at the graph, you will see the boxplot and above it there is a point on its own. This means that one participant’s age is an outlier because they are a lot older than the rest of the sample. Check back in data view to see this in the data. If age was important to your study and this participants age could confound your results, you would need to remove their data.\nImportant Note: ALWAYS keep the original JAMOVI data file in case you change your mind. Save the original and then when you make changes to your data set save this with a different name e.g. “Correlation data original”, “Correlation data outliers removed”.\n\n\n\n\n\n\n\nYou need to know how to copy results of statistical analyses into the appendices of reports in the future. Note that the only tables or figures which should ever go in the main part of the report are ones which help to illustrate your argument and they must fully adhere to APA format.\nIt is also never enough to just say ‘see table 1’. You have to interpret what tables and figures show in the text. For example, “Figure 1 shows a scatterplot depicting the correlation between temperature and ice cream sales”.\n\n\n\nClick on the bottom right search icon \nSearch for Snipping tool\nClick on ‘New’ top left\nHighlight the area of the graph or table that you want to copy\nClick ‘EDIT’ and scroll down to copy\nClick “Paste” in the Word Document\n\nMore information about APA formatting can be found here\n\n\n\n\n\nDownload and open the JAMOVI data file stroke_survival_data that you can find on Canvas. We want to generate some descriptive statistics on this dataset. This is a real dataset from several hospitals.\nFirstly, make sure all the variables are set at the right measurement level\n\nClick on the DATA tab\nClick on ‘Setup’\nClick on each individual variable name on the data to check the details.\n\nFrom the ‘Analysis’ tab click on ‘Exploration’ and then ‘Descriptives’ and then\nFill out the tables in the question box below. You will take two continuous variables and two categorical variables at a time and run some descriptive statistics.\n\nHINT: Enter a nominal variable into the “Split by” box. This will split your output by the categories within these variables\n\nMake sure your output includes a histogram.\nYou will see your results automatically in the window to the right.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nContinuous: age and income; Categorical: sex and diabetesContinuous: Age and cost; Categorical: smoker and obesity\n\n\nUsing the results you just generated, fill out the following table:\n\n\n\n\n\n\n\n\n\nVariable Name\nFactors\nNumber of data points (N)\nMean (M or x̅)\n\n\n\n\nAge (years)\nMale\n\n\n\n\n\nFemale\n\n\n\n\nAge (years)\nDiabetic\n\n\n\n\n\nNon-diabetic\n\n\n\n\nIncome\nMale\n\n\n\n\n\nFemale\n\n\n\n\nIncome\nDiabetic\n\n\n\n\n\nNon-diabetic\n\n\n\n\n\n\n\nUsing the results you just generated, fill out the following table:\n\n\n\n\n\n\n\n\n\nVariable Name\nFactors\nNumber of data points (N)\nMean (M or x̅)\n\n\n\n\nAge (years)\nSmoker\n\n\n\n\n\nNon-smoker\n\n\n\n\nAge (years)\nObese\n\n\n\n\n\nNot obese\n\n\n\n\nCost\nSmoker\n\n\n\n\n\nNon-smoker\n\n\n\n\nCost\nObese\n\n\n\n\n\nNot obese\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are multiple variables in the data set. Which variables would be suitable for correlational analysis?\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nWhich variables?\n\n\nThe stroke dataset has 123 pairs of variables which are suitable for correlational analysis.\nThese variables are age and smoking; age and cost; cost and smokingage and cost; age and income; income and costpatid and age; patid and active; age and active\n\n\n\n\n\n\nOnly once you have completed the above, expand the following question!\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\n\n\nCorrelation: Age and incomeCorrelation: Income and costCorrelation: Age and cost\n\n\nThe correlation between age and income is not significantsignificant (r () = , p = ). There were  observations.\n\n\nThe correlation between income and cost is not significantsignificant (r () = , p = ). There were  observations.\n\n\nThe correlation between age and cost is not significantsignificant (r () = , p &lt; ). There were  observations.\n\n\n\n\n\n\n\n\n\n\nOpen the survey_data file. This is a real dataset from a psychological study.\nLook over the dataset. Try to work out what has been recorded. What is the dataset exploring?\nExplore the variables ‘toptim’ and ‘tlifesat’ using Descriptives, split by both the sex and smoke variable, and fill out the information below.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nContinuous: toptim and tlifesat; Categorical: sex and smoke\n\n\nUsing the results you just generated, fill out the following table:\n\n\n\n\n\n\n\n\n\nVariable Name\nFactors\nNumber of data points (N)\nMean (M or x̅)\n\n\n\n\ntoptim\nMale\n\n\n\n\n\nFemale\n\n\n\n\ntoptim\nSmoker\n\n\n\n\n\nNon-smoker\n\n\n\n\ntlifesat\nMale\n\n\n\n\n\nFemale\n\n\n\n\ntlifesat\nSmoker\n\n\n\n\n\nNon-smoker\n\n\n\n\n\n\n\n\n\n\n\nNow run a correlation bewteen ‘toptim’ and ‘tlifesat’. Report your results in the question box below.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nCorrelation: toptim and tlifesatResults interpretation\n\n\nThe correlation between optimism and life satisfaction is not significantsignificant (r () = , p &lt; ). There were  observations.\n\n\nThe results indicate a positivenegative correlation between optimism and life satisfaction. This means that as optimism increasesdecreases, life satisfaction increasesdecreases.\n\n\n\n\n\n\nExplore the rest of the data using descriptives and correlations. This time you can use variables of your choice. Choose some variables that you find interesting and think about what your results show you. What interpretations can you make from your results?\n\n\n\n\nNow we will re-organise the data into a new JAMOVI sheet so we can compare scores at a later date based on a categorical variable of your choice.\n\nFor example, we might want to know if the correlation between optimism scores and age is different for males and females (categorical)\nTo do this we need to split the dataset into the categories of our categorical variable; in this case males and females.\n\nHINT: We are going to filter the dataset, this will be covered further in RMB but the steps will be listed here for you to follow\n\nClick filters in the data tab and type sex == 1 in the box. This will give you the data for the male participants. Ensure that you click the eye icon. This ensures that you only copy the filtered data.\n\n\n\nCopy the data in the age column for the male participants and open a new Jamovi sheet.\n\n\n\nPaste the age data for the male participants into the first column in your new Jamovi sheet. Double click the variable name to open the variable editor and change the name to something that makes sense, this example uses ‘age_male’.\n\n\n\nGo back to your original Jamovi sheet and reopen the filter. Change the code to sex == 2. This will give you the female data.\n\n\n\nCopy the female age data over to your new Jamovi sheet and change the variable name to something sensible like ‘age_female’.\n\n\n\nRepeat the steps to get the optimism data for males and females.\n\n\n\nNow you have re-organised your data you can run the correlation\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nCorrelation: optimism and age - maleCorrelation: optimism and age - female\n\n\nThe correlation between optimism and age for males is not significantsignificant (r () = , p = ). There were  observations.\n\n\nThe correlation between optimism and age for females is not significantsignificant (r () = , p &lt; ). There were  observations.\n\n\n\n\n\n\nNow you know how to filter your data and run correlations. Spend the rest of the session exploring the other variables in the dataset.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 9 - Correlations, Graphs and Exporting"
    ]
  },
  {
    "objectID": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#hypothesis-testing",
    "href": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#hypothesis-testing",
    "title": "Week 9 : Correlations, graphs and exporting",
    "section": "",
    "text": "When we want to test a research hypothesis, we obtain random samples of behaviour/data. We set up a null hypothesis H0: that the samples were drawn from populations with the same means (so that there will be no difference between them). We then obtain some samples and calculate the differences between the means of these samples, under the assumption that H0 is true (i.e. we obtained these means when the population means are equal).\n\nEXAMPLE: We want to test whether students drink more than the general population. To do this we collect data on the number of drinks per person in a club in one night, for two groups: students and general population. We then calculate the differences in the mean number of drinks for each of our groups. We assume that these means will be equal (null hypothesis). We are looking to disprove this hypothesis by finding differences in the means.\n\nWe then calculate the probability of getting a mean difference that is at least as large as the one we actually obtained between the means of our samples. This probability is our confidence interval. With a t-test, for example, we usually aim to say that we are 95% confident that our data were drawn from two populations with different means (i.e. there is a significant difference between the populations). To do this, we use the critical value – usually p&lt;0.05. In this case, we are saying that the probability that we would obtain the significant difference in data that we did would occur less than 5% or 5 in 100 times, if the null were true! That’s a very small chance. We can also use smaller, stricter p values like p&lt;0.01 (we would see the differences we found less than 1% of the time, if the null were true).\n\nEXAMPLE: We calculate the differences in the mean number of drinks for our student and general population groups. We find that students drink on average 2.5 more drinks than the general population, and that this difference is significant. We use a critical value of 5% and a confidence interval of 95% (these add up to 100%). That is, we can say that we are 95% confident that our data were drawn from two populations that actually have different means, and this conclusion will only be a mistake (due to chance) 5% of the time.\n\nIf the data do not lead us to reject the null hypothesis, the interpretation is more problematic – do we “accept” the null, or “retain” it, or just withhold judgement”?\n\nWe can definitely never claim to have “proved” the null hypothesis (as you cannot 100% prove a null statement)! A non-significant result is an inconclusive one – we have insufficient data to choose which hypothesis (null or alternative) is the most accurate.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 9 - Correlations, Graphs and Exporting"
    ]
  },
  {
    "objectID": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#one--and-two-tailed-tests",
    "href": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#one--and-two-tailed-tests",
    "title": "Week 9 : Correlations, graphs and exporting",
    "section": "",
    "text": "If we are going to reject either the lowest or highest 5%, then we use a one-tailed (or directional) test\nIf we want to conduct a two-tailed, non-directional test of significance, then to use an overall critical value of 5% we need to reject both the lowest and highest 2.5%, making 5% altogether\nIf you are certain of the direction of your effect, you can use a one-tailed test – but generally, we use two-tailed tests (as it’s rare that we know what direction our effect will be in!)",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 9 - Correlations, Graphs and Exporting"
    ]
  },
  {
    "objectID": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#the-dataset",
    "href": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#the-dataset",
    "title": "Week 9 : Correlations, graphs and exporting",
    "section": "",
    "text": "We will be exploring the dataset correlation data that you can find on the CANVAS page under Week 9 of the computer labs folder of Research Methods A\nThere are five variables in the dataset\n\nAge\nIce_Cream - number of ice creams sold\nTemp - temperature\nMotivation\nStress - measured at two time points",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 9 - Correlations, Graphs and Exporting"
    ]
  },
  {
    "objectID": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#pearsons-correlation-coefficent",
    "href": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#pearsons-correlation-coefficent",
    "title": "Week 9 : Correlations, graphs and exporting",
    "section": "",
    "text": "The Pearson’s Product Moment correlation is a parametric test that measures the size and significance of a relationship between two variables. In this test, the experimental data must meet certain assumptions of parametric tests. Some of these assumptions include data continuity and a normal distribution. We will discuss how to assess whether the distribution is normal in the next semester. For now, let’s assume it is normal.\nWe will correlate the variables ‘Temp’ (temperature) and ‘Ice_Cream’ (number of ice creams sold). There are 20 days where the two variables were recorded. These are represented by the rows in the ‘data view’.\n\n\n\n\n\n\nWarningOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file Correlation_data.xlsx from canvas.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 9 - Correlations, Graphs and Exporting"
    ]
  },
  {
    "objectID": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#exercises",
    "href": "tutorials/rma_week-9_correlations-graphs-and-exporting.html#exercises",
    "title": "Week 9 : Correlations, graphs and exporting",
    "section": "",
    "text": "Now it is time for you to explore the data. If you are unsure about any of these, please talk to the staff in your computer practical session.\n\n\n\nTo correlate the two variables click on ‘Analysis’ from the top tabs &gt; Regression &gt; Correlation Matrix\n\n\n\nTransfer ‘Ice_Cream’ and ‘Temp’ to the variables box by clicking them and pressing on the arrow button, double-clicking, or dragging them over to the box.\nFor this example, make sure that the ‘Pearson’ and ‘Report significance’ boxes are ticked. Note that Pearson is the version we use if our data meet parametric assumptions (e.g. interval or scale data, normal distribution, free from outliers etc.). If your data didn’t meet parametric assumptions, you would use Spearman instead because this test is more resilient to violations of the assumptions.\nSelect ‘correlated’ under hypothesis\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the correlation output to complete this statement reporting the results of the correlation. Fill in the spaces provided.\nThe correlation between temperature and the number of ice cream sold over 20 days is not significantsignificant (r () = , p &lt; ). There were  observations.\n\n\nThe correlation matrix…\n\n\n\n\n\n\n\n\n\n\n\n\nNoteInterpretation\n\n\n\nWhat does this result tell you? Have a think about how you would interpret the result?\n\n\n\n\n\n“A Pearson’s correlation was conducted to see if there was a significant relationship between temperature and the number of ice creams sold. This revealed that there was a very strong significant positive relationship between temperature and ice cream sales, r (18)= .893, p &lt; .001. Specifically, as temperatures increased, so too did ice cream scales. Therefore, we can accept the hypothesis that there would be a significant positive relationship between temperature and ice cream scales.”\nNote, a correlation can be positive (like this one was) or negative (e.g. the r value could have read -.893 instead). If it had been negative, this would have meant that as temperature increased, ice cream sales decreased. It is very important to get the direction of the relationship right because if you get it wrong it completely changes the findings!\nIt is generally accepted that r values can be interpreted in the following ways (these numbers can be either positive or negative!): 0-0.19 = Very weak, 0.20–0.39 = Weak, 0.40–0.59 = Moderate, 0.60–0.79 = Strong, 0.80–1 = Very strong. It is extremely uncommon to have an r value of 1 because this means it is a perfect correlation (e.g. when you correlate temperature with itself because they are the same).\n\n\n\nThis correlation can be illustrated graphically. This eases the understanding of the relationship between the variables. We can illustrate the relationship using the Jamovi Scatterplot function.\n\nClick on Analyse &gt; Exploration &gt; Scatterplot\n\n\n\nMove Ice_Cream and Temp across\nClick on linear under ‘Regression Line’\nA scatterplot should appear in the window to the right\n\n\nYou can clearly see that as the temperature increases, the number of ice cream sold also raises. We know that this relationship is significant from the results of the Pearson’s Correlation.\n\n\n\nA histogram is a graph showing the frequency of different scores. Histograms are frequently used in psychological research. Histograms are useful to check whether the data is normally distributed.\n\nGo to Exploration &gt; Descriptives &gt; to generate a basic histogram of the MOTIVATION variable. Transfer MOTIVATION to the Variables box, remember to tick the Histogram box under ‘Histograms’ – A Histogram should appear in the window to the right\n\n\n\nDo the same for Stress_1\n\n\n\n\n\n\n\n\nCaution Data Skills - Do you think the Motivation and Stress_1 data are normally distributed or is the data skewed?\n\n\n\n\n\nTo check this, look at your histograms. Can you see a classic “bell curve” shape (normally distributed data), or is some of the data pushed up at one end of the graph (skewed data)?\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Are there any outliers in the age data?\n\n\n\n\n\n\nTo check this, you will need to create a “Boxplot”.\nClick on the Boxplot box under ‘Boxplots’ and then create a boxplot for AGE by moving AGE into the variable box.\nA boxplot should appear in the window to the right\nLook at the graph, you will see the boxplot and above it there is a point on its own. This means that one participant’s age is an outlier because they are a lot older than the rest of the sample. Check back in data view to see this in the data. If age was important to your study and this participants age could confound your results, you would need to remove their data.\nImportant Note: ALWAYS keep the original JAMOVI data file in case you change your mind. Save the original and then when you make changes to your data set save this with a different name e.g. “Correlation data original”, “Correlation data outliers removed”.\n\n\n\n\n\n\n\nYou need to know how to copy results of statistical analyses into the appendices of reports in the future. Note that the only tables or figures which should ever go in the main part of the report are ones which help to illustrate your argument and they must fully adhere to APA format.\nIt is also never enough to just say ‘see table 1’. You have to interpret what tables and figures show in the text. For example, “Figure 1 shows a scatterplot depicting the correlation between temperature and ice cream sales”.\n\n\n\nClick on the bottom right search icon \nSearch for Snipping tool\nClick on ‘New’ top left\nHighlight the area of the graph or table that you want to copy\nClick ‘EDIT’ and scroll down to copy\nClick “Paste” in the Word Document\n\nMore information about APA formatting can be found here\n\n\n\n\n\nDownload and open the JAMOVI data file stroke_survival_data that you can find on Canvas. We want to generate some descriptive statistics on this dataset. This is a real dataset from several hospitals.\nFirstly, make sure all the variables are set at the right measurement level\n\nClick on the DATA tab\nClick on ‘Setup’\nClick on each individual variable name on the data to check the details.\n\nFrom the ‘Analysis’ tab click on ‘Exploration’ and then ‘Descriptives’ and then\nFill out the tables in the question box below. You will take two continuous variables and two categorical variables at a time and run some descriptive statistics.\n\nHINT: Enter a nominal variable into the “Split by” box. This will split your output by the categories within these variables\n\nMake sure your output includes a histogram.\nYou will see your results automatically in the window to the right.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nContinuous: age and income; Categorical: sex and diabetesContinuous: Age and cost; Categorical: smoker and obesity\n\n\nUsing the results you just generated, fill out the following table:\n\n\n\n\n\n\n\n\n\nVariable Name\nFactors\nNumber of data points (N)\nMean (M or x̅)\n\n\n\n\nAge (years)\nMale\n\n\n\n\n\nFemale\n\n\n\n\nAge (years)\nDiabetic\n\n\n\n\n\nNon-diabetic\n\n\n\n\nIncome\nMale\n\n\n\n\n\nFemale\n\n\n\n\nIncome\nDiabetic\n\n\n\n\n\nNon-diabetic\n\n\n\n\n\n\n\nUsing the results you just generated, fill out the following table:\n\n\n\n\n\n\n\n\n\nVariable Name\nFactors\nNumber of data points (N)\nMean (M or x̅)\n\n\n\n\nAge (years)\nSmoker\n\n\n\n\n\nNon-smoker\n\n\n\n\nAge (years)\nObese\n\n\n\n\n\nNot obese\n\n\n\n\nCost\nSmoker\n\n\n\n\n\nNon-smoker\n\n\n\n\nCost\nObese\n\n\n\n\n\nNot obese\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are multiple variables in the data set. Which variables would be suitable for correlational analysis?\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nWhich variables?\n\n\nThe stroke dataset has 123 pairs of variables which are suitable for correlational analysis.\nThese variables are age and smoking; age and cost; cost and smokingage and cost; age and income; income and costpatid and age; patid and active; age and active\n\n\n\n\n\n\nOnly once you have completed the above, expand the following question!\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\n\n\nCorrelation: Age and incomeCorrelation: Income and costCorrelation: Age and cost\n\n\nThe correlation between age and income is not significantsignificant (r () = , p = ). There were  observations.\n\n\nThe correlation between income and cost is not significantsignificant (r () = , p = ). There were  observations.\n\n\nThe correlation between age and cost is not significantsignificant (r () = , p &lt; ). There were  observations.\n\n\n\n\n\n\n\n\n\n\nOpen the survey_data file. This is a real dataset from a psychological study.\nLook over the dataset. Try to work out what has been recorded. What is the dataset exploring?\nExplore the variables ‘toptim’ and ‘tlifesat’ using Descriptives, split by both the sex and smoke variable, and fill out the information below.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nContinuous: toptim and tlifesat; Categorical: sex and smoke\n\n\nUsing the results you just generated, fill out the following table:\n\n\n\n\n\n\n\n\n\nVariable Name\nFactors\nNumber of data points (N)\nMean (M or x̅)\n\n\n\n\ntoptim\nMale\n\n\n\n\n\nFemale\n\n\n\n\ntoptim\nSmoker\n\n\n\n\n\nNon-smoker\n\n\n\n\ntlifesat\nMale\n\n\n\n\n\nFemale\n\n\n\n\ntlifesat\nSmoker\n\n\n\n\n\nNon-smoker\n\n\n\n\n\n\n\n\n\n\n\nNow run a correlation bewteen ‘toptim’ and ‘tlifesat’. Report your results in the question box below.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nCorrelation: toptim and tlifesatResults interpretation\n\n\nThe correlation between optimism and life satisfaction is not significantsignificant (r () = , p &lt; ). There were  observations.\n\n\nThe results indicate a positivenegative correlation between optimism and life satisfaction. This means that as optimism increasesdecreases, life satisfaction increasesdecreases.\n\n\n\n\n\n\nExplore the rest of the data using descriptives and correlations. This time you can use variables of your choice. Choose some variables that you find interesting and think about what your results show you. What interpretations can you make from your results?\n\n\n\n\nNow we will re-organise the data into a new JAMOVI sheet so we can compare scores at a later date based on a categorical variable of your choice.\n\nFor example, we might want to know if the correlation between optimism scores and age is different for males and females (categorical)\nTo do this we need to split the dataset into the categories of our categorical variable; in this case males and females.\n\nHINT: We are going to filter the dataset, this will be covered further in RMB but the steps will be listed here for you to follow\n\nClick filters in the data tab and type sex == 1 in the box. This will give you the data for the male participants. Ensure that you click the eye icon. This ensures that you only copy the filtered data.\n\n\n\nCopy the data in the age column for the male participants and open a new Jamovi sheet.\n\n\n\nPaste the age data for the male participants into the first column in your new Jamovi sheet. Double click the variable name to open the variable editor and change the name to something that makes sense, this example uses ‘age_male’.\n\n\n\nGo back to your original Jamovi sheet and reopen the filter. Change the code to sex == 2. This will give you the female data.\n\n\n\nCopy the female age data over to your new Jamovi sheet and change the variable name to something sensible like ‘age_female’.\n\n\n\nRepeat the steps to get the optimism data for males and females.\n\n\n\nNow you have re-organised your data you can run the correlation\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nCorrelation: optimism and age - maleCorrelation: optimism and age - female\n\n\nThe correlation between optimism and age for males is not significantsignificant (r () = , p = ). There were  observations.\n\n\nThe correlation between optimism and age for females is not significantsignificant (r () = , p &lt; ). There were  observations.\n\n\n\n\n\n\nNow you know how to filter your data and run correlations. Spend the rest of the session exploring the other variables in the dataset.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 9 - Correlations, Graphs and Exporting"
    ]
  },
  {
    "objectID": "tutorials/rmd_week-1_item-wording.html",
    "href": "tutorials/rmd_week-1_item-wording.html",
    "title": "Week 1 : Item Wording/Vetting and Partial Correlation",
    "section": "",
    "text": "When you have completed this workshop, you should be able to:\n\nUnderstand the need for careful wording in questionnaire design, distinguishing between different question features that must be considered.\nDistinguish between well-worded and poorly-worded items.\nCreate a scatter plot in Jamovi to observe and describe relationships between variables.\nCreate a scatter plot in Jamovi to observe and describe the relationship between two variables when data are grouped by a third variable.\nConduct bivariate correlation between pairs of variables in Jamovi, interpret the output and write-up the findings.\nConduct partial correlations between two variables in Jamovi, interpret the output and write-up the findings.\nExamine correlational data using scatter plots to make decisions about how to test the relationship between variables.\n\n\n\n\nHere is a list of things to bear in mind when writing questionnaire items. Read through and make sure you understand each potential limitation and how to avoid them\nThings to bear in mind: Explanation :\n\n\n\n\n\n\n\nThings to bear in mind:\nExplanation:\n\n\n\n\nUse simple language\nAvoid jargon, complicated language or technical terms, keep it simple so anyone can understand\n\n\nKeep questions short\nLong questions confuse/bore participants, keep questions as short as possible\n\n\nAvoid double barrelled questions\nAsking two things in one question confuses participants and produces meaningless data. If you want to ask two things, split in to two questions\n\n\nAvoid leading questions\nWe want to know what our participants think and feel, we do not want our own thoughts reflected back at us because we have led participants. Don’t imply one answer is ‘right/better’\n\n\nAvoid double negatives\n“I never was, nor neither will be…” Double negatives are confusing and confusion leads to meaningless data. Consider wording carefully.\n\n\nIs the respondent likely to know the answer?\nIf the question is knowledge-based, participants should be able to answer. Keep in mind what your participants can be reasonably expected to know based on who they are.\n\n\nAre the meanings of words clear\nSome words have more than one meaning, so you need to make sure that meaning is clear. For example, a bat can mean a flying mammal, a cricket bat, to hit something away etc.\n\n\nAvoid ‘prestige bias’\nThe perceived prestige of a person, brand, job etc can influence how we respond to questions about them. Avoid implying that something or someone is superior if you want to obtain genuine responses\n\n\nAvoid ‘conformity bias’ and social desirability\nWe often conform to behaviours we think are most common or most desirable but this might not be a true reflection of what participants actually do. Emphasise the importance of understanding their normal behaviour/attitudes, not what they think they should be doing/thinking\n\n\nAvoid ambiguity\nJust like words can have more than one meaning, questions can often be interpreted several different ways. Make sure your question is clear and participants know what they should be doing\n\n\nIs the context clear?\nWords and questions can have different meanings in different contexts, it should be clear which context you are asking about\n\n\nAvoid questions that create opinions\nQuestionnaires should be designed in a way to explore what participants think and feel, we should not be trying to change how they think or feel (interventions for behaviour change come after understanding)\n\n\n\n\n\n\n\n\n\nTipItem Vetting Task\n\n\n\n\nQuestionHint\n\n\nLook at the list of questions/items below that were designed to explore life as a student. A participant would respond with their level of agreement to the items below using a 7-point Likert scale ranging from ‘strongly agree’ to ‘strongly disagree’. Read through the items and see if you think any should be removed or re-written. Make a list of alterations (15 min)\n\nI carefully considered other options before choosing to come to university.\nBy working diligently, students earn the minimal funding they receive.\nIt is not the case that students are not as well off as they used to be.\nLike successful graduates, I believe students should pay for their own education.\nRadical politicians should be allowed to cut student incomes to poverty levels.\nUniversity will provide me with either a wider view of life or a new path in life’s big journey.\nOne hundred pounds is a good amount.\nUniversity life is providing me with new opportunities.\nMy degree is the most important area of my life.\nIndolent student politicians are responsible for decimating maintenance grants.\nI balance my studying and socialising well.\nI make an important contribution to the University community.\nAfter getting my degree I will have a better chance in the British or international job market.\nIt is an honest person that believes all education should be free\nFree education is an important cornerstone to our society.\nI believe that students should receive more financial support.\nQuestionnaires about student opinions are a waste of time.\nWhen I started at University I believed I would ultimately improve my quality of life by undertaking further education.\n\nIf you are not sure about any of these you can ask staff in your workshop, but ultimately you will need to make the decision because you need to be able to defend the methodological and statistical choices you make.\n\n\nReview the table of things to bear in mind and consider whether they apply to each of the questions in the list.\n\n\n\n\n\n \n\n\n\nDownload and open the data file academicsuccess from Canvas. You were shown how to import data into Jamovi in previous research methods modules, if you have forgotten please refer to this page\nOnce open the data should look like this\n\nRight now, you are in data view. The data are fictional and created to show that students who are more engaged in their studies are more likely to be academically successful than students who are less enagaged.\nThe data has four columns:\n\nengagement - how engaged the students are in their studies\nacadsuccess - rating of academic success\nexternalLoC - self reported external locus of control (sometimes referred to as Externality)\nemployed - which participants were in employment after graduation\n\nYou are going to test this hypothesis with a correlational analysis which you should be familiar with from earlier modules.\n\n\nHere we will show you how to amend the type of data each variable represents in Jamovi.\nClick on “Variables”. Here you can see all of your variable names. The symbols next to the name of each variable tells you what type of data Jamovi has stored this variable as (ordinal, nominal or continuous).\n\nIf you double click on the variable name, a new section will appear which lets you edit them.\nYou can change the name of the variable, the type of measurement (nominal, ordinal, continuous, or participant ID). You can state if the data type is text, and integer, or allows decimals.\n\nIf data is nominal or ordinal, you can include levels. Levels are the possible options people have to choose from. For example, responses on a Likert scale or saying if they are employed.\n\n\nClick “Data” on the top of screen to go back and view the raw data.\n\n\n\nBefore correlating two variables, it is always a good idea to produce a scatter plot of the relationship between two variables. Scatter plots help you to decide which method for calculating a correlation coefficient to use. Scatter plots also tell you if you should perform a correlation analysis at all.\nEach point on a scatter plot represents an individual in the sample. The point’s position on the X and Y axes corresponds to that individual’s scores on the two variables.\n\nSelect analysis -&gt; exploration -&gt; scatterplot\n\n\n\nYou will see your variables in this screen. Move the “engagement” and “acadsuccess” variables to the x & y axis. You can do this by dragging the variable names to the x / y axis box, or selecting the variable and clicking the arrows next to the box.\n\n\n\nOnce you have populated both x and y axis labels, the scatterplot with automatically appear.\n\n\n\n\n\n\n\n\nTipTest your understanding\n\n\n\n\nBased on the scatterplot, answer these questions\n\n\n\nIs the relationship approximately linear? YesNo\nIs the relationship likely to be strong? YesNo\nIs the relationship likely to be positive or negative? PositiveNegative\nAre there any obvious outliers? Yes: engagement ~5 and acadsuccess ~10Yes: engagement ~0.75 and acadsuccess ~10No\n\n\n\n\n\n\n\n\n\nThe scatterplot does not show if the correlation between the two variables is statistically significant. We can test this in the following way:\n\nAt the top of the screen select analysis -&gt; regression -&gt; correlation matrix\n\n\n\nA new window will appear\n\n\n\nIn this window move your two variables (engagement and acadsuccess) to the empty box.\n\n\n\nJamovi will automatically populate your results based on a default statistic. However, you must make sure the correct parametric test has run. Underneath the variables box, you will see an option to select from different tests.\n\nCorrelation coefficients: You have the option to choose Pearsons if your data is suitable for parametric analysis. For non-parametric analysis you have the option to select Spearman or Kendall’s tau-b. For this data click Pearsons.\nHypothesis: If your hypothesis has direction of prediction such as a positive or negative correlation between the two variables, you can select either “Correlated Positively” or “Correlated Negatively” as a specific test. If you have no prediction as to the direction of the relationship, you can keep “Correlated”. For this data we should select “Correlated”.\n\n\n\n\nThe results will automatically load on the right-hand screen as you make your selection.\n\n\n\n\n\n\nFor Pearson’s correlations, you can work out the variance explained in the data by the relationship between the variables by squaring the correlation coefficient. E.g., (0.713)2 = 0.508. Converted to a percentage this is roughly 51% of the variance explained (rounded up)\n\n\n\nThe following is an example of a correlation write-up, using the above example for the correlation between engagement and academic success. Remember that your results might be different if you have cleaned the data in any way (and that is fine – just remember to tell us what you did in your reports).\n“Consistent with the hypothesis, there was a strong positive correlation between study engagement and academic success, r(18) = .713, p &lt; .001, two-tailed. The relationship accounts for 51% of the variance observed.”\n\n\n\n\n\n\nTipTest your understanding\n\n\n\n\nQuestions\n\n\nRun a new correlation analysis to investigate the correlation between externality (external locus of control) and engagement. After that, do the same for externality and academic success. What do you find?\nExternality and engagement:\n\nWhat is the value of the Person’s correlation coefficient? \nIs the correlation significant? YesNo\nIf so at what level? 0.0010.010.050.1\n\nExternality and academic success:\n\nWhat is the value of the Person’s correlation coefficient? \nIs the correlation significant? YesNo\nIf so at what level? p &lt; 0.001p = 0.05p = 0.01\n\nAPA Results\nThere was a strong positivenegative correlation between externality and engagement, r() = , p , two-tailed. The relationship accounts for % of the variance observed.\nThere was a strong positivenegative correlation between externality and academic success, r() = , p , two-tailed. The relationship accounts for % of the variance observed.\n\n\n\n\n\n\n\n\nThis section continues to use the academicsuccess data set.\nSix months after finishing their degrees, the students in this fictional study indicate whether they were in graduate employment. This information is in the academicsuccess file under the “employed” column.\n\nYour task is to see if the correlation differed for people who were in graduate employment after finishing their degree. To see this, you will need to create a grouped scatter plot.\n\nYou should have already loaded the academicsuccess file. The steps are mostly the same as when you created the correlation coefficient in the previous section.\nSelect analysis -&gt; exploration -&gt; scatterplot\n\n\n\nIn the x and y axis place the variables “engagement” and “acadsuccess”. In the group section place the variable “employed”.\n\n\n\nJamovi will display a new scatterplot where employed and unemployed participants have their own colour to identify their group.\n\n\n\n\n\n\n\n\nTipTest your understanding\n\n\n\n\nQuestions\n\n\nBased on the grouped scatter plot…\n\nWhat do you notice about this plot? (Remember this is a fake data set!)\nWhat does this suggest to you about the data set as a whole?\n\n\n\n\n\n\n\n\n\nThe academicsuccess datasheet contains three continuous variables, the third being externality (labelled “externalLoC”). Externality is correlated with both engagement and academic success. It is possible that the apparent relationship between engagement and academic success is completely due to the relationship that they each have with externality. That is, externality might be acting as a mediating variable between the other two. Alternatively, the relationship between engagement and academic success might have little to do with externality. We can test this by performing a partial correlation which lets us see what the correlation between two variables (here engagement and academic success) would be if we were to hold a third variable (here externality) constant.\n\nAt the top of the screen select Analysis -&gt; Regression -&gt; Partial Correlation (ensure that the academicsuccess data set is loaded into Jamovi)\n\n\n\nDrag the “engagement” and “acadsuccess” variables into the “Variables” box\n\n\n\nDrag the “externalLoC” variable into the “Control Variables” box. This tells the analysis which variables you are controlling for.\n\n\n\nUnderneath the variable sections, you can select the type of test you want to run. You must make sure the correct parametric test has run.\n\nCorrelation Coefficients: You have the option to choose Pearsons if your data is suitable for parametric tests. For non-parametric tests you have the option to select Spearman or Kendall’s tau-b. For this data click Pearsons.\nHypothesis: If your hypothesis has direction of prediction, such as a positive or negative correlation between the two variables, you can select either “Correlated Positively” or “Correlated Negatively” as a specific test. If you have no prediction as to the direction of the relationship, you can keep “Correlated”, which you should select for this data.\nCorrelation type: A partial correlation will measure the relationship between two variables, when controlling for another variable. A semipartial correlation will correlate the variable you are controlling for with just one of other variables, to see if that influences the relationship the remaining variable. For this data you should use a partial correlation.\nAdditional options: tick “report significance” to see the P value for every correlation. Note that this test does not calculate degrees of freedom, so you will need to tick “N” for when you do this further below.\n\n\n\n\nYou can now interpret the results that are presented\n\n\n\n\n\nJamovi does not give you the degrees of freedom (df) in partial correlation analysis. You will need to calculate it manually using the equation below (see explanation in bullet points).\n\\[\ndf=n-k-p\n\\]\n\nn: the number of datapoints or observations. You can see this by ticking “N” under “Additional Options” in the above analysis. As Jamovi is dynamic, you will be able to see the number of data points (20) appear in the partial correlation matrix.\nk: the number of variables, including the control variables. In this case we have 3 variables in the partial correlation (“engagement”, “acadsuccess” and “externality”)\np: number of parameters that are being used. Here we are measuring one coefficient between “engagement” and “acadsuccess”. You are then measuring a second coefficient between the two variables and the control variable, externality (note this second parameter is not included in semipartial correlations). Therefore, here the parameter is 2.\n\nWith this calculation you can confirm that the degrees of freedom is 15.\n\n\n\nThe following is an example of a partial correlation write-up. It refers to the above example for the correlation between engagement and academic success, controlling for externality.\n“When externality was held constant, the partial correlation between study engagement and academic success was not found to be significant, r(15) = .235, p = .334, two-tailed. The results suggest that academic success is unrelated to study engagement when controlling for externality.”\n\n\n\nJamovi can easily calculate correlations between several pairs of variables at once. Download and open the stress data file on Canvas. The file contains made-up data of 100 people. Variables include:\n\nStress – How much stress participants experience\nAge – Participants age\nWork_Hours – The number of hours participants work per week\nChildren – The number children participants have\nLeisure - The amount of money participants spend on leisure per week\nExercise - The amount of time per week participants spend exercising\nSalary - Participants’ salaries.\n\nCalculate Person’s correlations for all pairings of the variables.\nYou can do this by adding all of the variables into the correlation dialog box at once. The steps are the same as when you calculate the correlation coefficient, as shown above.\n\nAt the top of the screen select Analysis -&gt; Regression -&gt; Correlation Matrix\n\n\n\nWhen selecting your variables, rather than just selecting two variables, select all of them.\n\n\n\nMake sure you select the correct tests as explained in correlation coefficient section. Read the above steps again to remind yourself what they are.\nYou can tell which pair each entry in the correlation matrix applies to by looking at the row and column headings. Since the order of variables in correlation makes no difference, only one ordering needs to be recorded for each pair when reporting multiple correlations (e.g. it is fine to report salary correlated with exercise, you don’t need to also report exercise correlated with salary). See the table below for a basic layout for a table of multiple correlations. The first column shows the correlation coefficients for stress versus each of the other variables in turn. [NOTE: The table is not presented in APA format]\n\n\n\n\n\n\n\nTipTest your understanding\n\n\n\n\nQuestions\n\n\nAnswer the following questions based on the table above\n\nAre the following correlations significant at the .01 level? (i.e. p = .01 or p &lt; .01)\n\nStress and age: YesNo Stress and work hours: YesNo Stress and leisure: YesNo Stress and exercise: YesNo Stress and salary: YesNo Stress and children: YesNo Age and work hours: YesNo Age and leisure: YesNo Age and exercise: YesNo Age and salary: YesNo Age and children: YesNo Work hours and leisure: YesNo Work hours and exercise: YesNo Work hours and salary: YesNo Work hours and children: YesNo Leisure and exercise: YesNo Leisure and salary: YesNo Leisure and children: YesNo Exercise and salary: YesNo Exercise and children: YesNo Salary and children: YesNo\n\nWhich variable relates most strongly to the amount of money spent on leisure? StressAgeWork hoursExerciseSalaryChildren\nWhat percentage of the variance in the data is explained by this relationship? %\nComplete the following sentence to describe this correlation as if in a report.\n\nThere was a strong positivenegative correlation between StressAgeWork hoursExerciseSalaryChildren and money spent on leisure, r() = , p , two-tailed.\n\n\n\n\n\n\n\n\n\n\n\nTipTest your understanding\n\n\n\n\nQuestions\n\n\nCreate a scatterplot of the relationship between Age (x) and Stress (y) and answer the following questions.\n\nIs the correlation coefficient for Age and Stress significant? YesNo\nIs correlation an appropriate test for measuring the relationship between Age and Stress? YesNo\nWhy? There is a linear relationship between the variablesThe relationship between the variables doesn’t look linear\nFill in the sentence below to report the correlation as if for a report. Think about any cautionary notes that you think should be applied.\n\nThere was a positivea negativeno correlation between Age and Stress, r() = , p = , two-tailed.\n\n\n\n\n\n\n\n\nDownload and open the speed data file. Imagine that the resting heart rates of 10 people have been measured. The time taken by each person to run 100m is also measured. The theory is that fit people will have low resting heart rates and will also be able to run quickly. Create a scatter plot for this data.\n\n\n\n\n\n\nTipTest your understanding\n\n\n\n\nQuestions\n\n\nCalculate both Pearson’s and Spearman’s correlation coefficients.\n\nWhat is the value for Pearson’s correlation for these variables? \nWhat is the value for Spearman’s correlation for these variables? \nAre either of the coefficients significant? YesNo\nFill in the sentence below to report the correlation as if for a report.\n\nPearson’s: There was a positivea negativeno significant correlation between speed and heart rate, r() = , p = , one-tailedtwo-tailed.\nSpearman’s: There was a positivea negativeno significant correlation between speed and heart rate, r() = , p = , one-tailedtwo-tailed.",
    "crumbs": [
      "Home",
      "Research Methods D",
      "Week 1 - Item Wording"
    ]
  },
  {
    "objectID": "tutorials/rmd_week-1_item-wording.html#learning-objectives",
    "href": "tutorials/rmd_week-1_item-wording.html#learning-objectives",
    "title": "Week 1 : Item Wording/Vetting and Partial Correlation",
    "section": "",
    "text": "When you have completed this workshop, you should be able to:\n\nUnderstand the need for careful wording in questionnaire design, distinguishing between different question features that must be considered.\nDistinguish between well-worded and poorly-worded items.\nCreate a scatter plot in Jamovi to observe and describe relationships between variables.\nCreate a scatter plot in Jamovi to observe and describe the relationship between two variables when data are grouped by a third variable.\nConduct bivariate correlation between pairs of variables in Jamovi, interpret the output and write-up the findings.\nConduct partial correlations between two variables in Jamovi, interpret the output and write-up the findings.\nExamine correlational data using scatter plots to make decisions about how to test the relationship between variables.",
    "crumbs": [
      "Home",
      "Research Methods D",
      "Week 1 - Item Wording"
    ]
  },
  {
    "objectID": "tutorials/rmd_week-1_item-wording.html#item-wording",
    "href": "tutorials/rmd_week-1_item-wording.html#item-wording",
    "title": "Week 1 : Item Wording/Vetting and Partial Correlation",
    "section": "",
    "text": "Here is a list of things to bear in mind when writing questionnaire items. Read through and make sure you understand each potential limitation and how to avoid them\nThings to bear in mind: Explanation :\n\n\n\n\n\n\n\nThings to bear in mind:\nExplanation:\n\n\n\n\nUse simple language\nAvoid jargon, complicated language or technical terms, keep it simple so anyone can understand\n\n\nKeep questions short\nLong questions confuse/bore participants, keep questions as short as possible\n\n\nAvoid double barrelled questions\nAsking two things in one question confuses participants and produces meaningless data. If you want to ask two things, split in to two questions\n\n\nAvoid leading questions\nWe want to know what our participants think and feel, we do not want our own thoughts reflected back at us because we have led participants. Don’t imply one answer is ‘right/better’\n\n\nAvoid double negatives\n“I never was, nor neither will be…” Double negatives are confusing and confusion leads to meaningless data. Consider wording carefully.\n\n\nIs the respondent likely to know the answer?\nIf the question is knowledge-based, participants should be able to answer. Keep in mind what your participants can be reasonably expected to know based on who they are.\n\n\nAre the meanings of words clear\nSome words have more than one meaning, so you need to make sure that meaning is clear. For example, a bat can mean a flying mammal, a cricket bat, to hit something away etc.\n\n\nAvoid ‘prestige bias’\nThe perceived prestige of a person, brand, job etc can influence how we respond to questions about them. Avoid implying that something or someone is superior if you want to obtain genuine responses\n\n\nAvoid ‘conformity bias’ and social desirability\nWe often conform to behaviours we think are most common or most desirable but this might not be a true reflection of what participants actually do. Emphasise the importance of understanding their normal behaviour/attitudes, not what they think they should be doing/thinking\n\n\nAvoid ambiguity\nJust like words can have more than one meaning, questions can often be interpreted several different ways. Make sure your question is clear and participants know what they should be doing\n\n\nIs the context clear?\nWords and questions can have different meanings in different contexts, it should be clear which context you are asking about\n\n\nAvoid questions that create opinions\nQuestionnaires should be designed in a way to explore what participants think and feel, we should not be trying to change how they think or feel (interventions for behaviour change come after understanding)\n\n\n\n\n\n\n\n\n\nTipItem Vetting Task\n\n\n\n\nQuestionHint\n\n\nLook at the list of questions/items below that were designed to explore life as a student. A participant would respond with their level of agreement to the items below using a 7-point Likert scale ranging from ‘strongly agree’ to ‘strongly disagree’. Read through the items and see if you think any should be removed or re-written. Make a list of alterations (15 min)\n\nI carefully considered other options before choosing to come to university.\nBy working diligently, students earn the minimal funding they receive.\nIt is not the case that students are not as well off as they used to be.\nLike successful graduates, I believe students should pay for their own education.\nRadical politicians should be allowed to cut student incomes to poverty levels.\nUniversity will provide me with either a wider view of life or a new path in life’s big journey.\nOne hundred pounds is a good amount.\nUniversity life is providing me with new opportunities.\nMy degree is the most important area of my life.\nIndolent student politicians are responsible for decimating maintenance grants.\nI balance my studying and socialising well.\nI make an important contribution to the University community.\nAfter getting my degree I will have a better chance in the British or international job market.\nIt is an honest person that believes all education should be free\nFree education is an important cornerstone to our society.\nI believe that students should receive more financial support.\nQuestionnaires about student opinions are a waste of time.\nWhen I started at University I believed I would ultimately improve my quality of life by undertaking further education.\n\nIf you are not sure about any of these you can ask staff in your workshop, but ultimately you will need to make the decision because you need to be able to defend the methodological and statistical choices you make.\n\n\nReview the table of things to bear in mind and consider whether they apply to each of the questions in the list.",
    "crumbs": [
      "Home",
      "Research Methods D",
      "Week 1 - Item Wording"
    ]
  },
  {
    "objectID": "tutorials/rmd_week-1_item-wording.html#jamovi-tasks-correlation",
    "href": "tutorials/rmd_week-1_item-wording.html#jamovi-tasks-correlation",
    "title": "Week 1 : Item Wording/Vetting and Partial Correlation",
    "section": "",
    "text": "Download and open the data file academicsuccess from Canvas. You were shown how to import data into Jamovi in previous research methods modules, if you have forgotten please refer to this page\nOnce open the data should look like this\n\nRight now, you are in data view. The data are fictional and created to show that students who are more engaged in their studies are more likely to be academically successful than students who are less enagaged.\nThe data has four columns:\n\nengagement - how engaged the students are in their studies\nacadsuccess - rating of academic success\nexternalLoC - self reported external locus of control (sometimes referred to as Externality)\nemployed - which participants were in employment after graduation\n\nYou are going to test this hypothesis with a correlational analysis which you should be familiar with from earlier modules.\n\n\nHere we will show you how to amend the type of data each variable represents in Jamovi.\nClick on “Variables”. Here you can see all of your variable names. The symbols next to the name of each variable tells you what type of data Jamovi has stored this variable as (ordinal, nominal or continuous).\n\nIf you double click on the variable name, a new section will appear which lets you edit them.\nYou can change the name of the variable, the type of measurement (nominal, ordinal, continuous, or participant ID). You can state if the data type is text, and integer, or allows decimals.\n\nIf data is nominal or ordinal, you can include levels. Levels are the possible options people have to choose from. For example, responses on a Likert scale or saying if they are employed.\n\n\nClick “Data” on the top of screen to go back and view the raw data.\n\n\n\nBefore correlating two variables, it is always a good idea to produce a scatter plot of the relationship between two variables. Scatter plots help you to decide which method for calculating a correlation coefficient to use. Scatter plots also tell you if you should perform a correlation analysis at all.\nEach point on a scatter plot represents an individual in the sample. The point’s position on the X and Y axes corresponds to that individual’s scores on the two variables.\n\nSelect analysis -&gt; exploration -&gt; scatterplot\n\n\n\nYou will see your variables in this screen. Move the “engagement” and “acadsuccess” variables to the x & y axis. You can do this by dragging the variable names to the x / y axis box, or selecting the variable and clicking the arrows next to the box.\n\n\n\nOnce you have populated both x and y axis labels, the scatterplot with automatically appear.\n\n\n\n\n\n\n\n\nTipTest your understanding\n\n\n\n\nBased on the scatterplot, answer these questions\n\n\n\nIs the relationship approximately linear? YesNo\nIs the relationship likely to be strong? YesNo\nIs the relationship likely to be positive or negative? PositiveNegative\nAre there any obvious outliers? Yes: engagement ~5 and acadsuccess ~10Yes: engagement ~0.75 and acadsuccess ~10No\n\n\n\n\n\n\n\n\n\nThe scatterplot does not show if the correlation between the two variables is statistically significant. We can test this in the following way:\n\nAt the top of the screen select analysis -&gt; regression -&gt; correlation matrix\n\n\n\nA new window will appear\n\n\n\nIn this window move your two variables (engagement and acadsuccess) to the empty box.\n\n\n\nJamovi will automatically populate your results based on a default statistic. However, you must make sure the correct parametric test has run. Underneath the variables box, you will see an option to select from different tests.\n\nCorrelation coefficients: You have the option to choose Pearsons if your data is suitable for parametric analysis. For non-parametric analysis you have the option to select Spearman or Kendall’s tau-b. For this data click Pearsons.\nHypothesis: If your hypothesis has direction of prediction such as a positive or negative correlation between the two variables, you can select either “Correlated Positively” or “Correlated Negatively” as a specific test. If you have no prediction as to the direction of the relationship, you can keep “Correlated”. For this data we should select “Correlated”.\n\n\n\n\nThe results will automatically load on the right-hand screen as you make your selection.\n\n\n\n\n\n\nFor Pearson’s correlations, you can work out the variance explained in the data by the relationship between the variables by squaring the correlation coefficient. E.g., (0.713)2 = 0.508. Converted to a percentage this is roughly 51% of the variance explained (rounded up)\n\n\n\nThe following is an example of a correlation write-up, using the above example for the correlation between engagement and academic success. Remember that your results might be different if you have cleaned the data in any way (and that is fine – just remember to tell us what you did in your reports).\n“Consistent with the hypothesis, there was a strong positive correlation between study engagement and academic success, r(18) = .713, p &lt; .001, two-tailed. The relationship accounts for 51% of the variance observed.”\n\n\n\n\n\n\nTipTest your understanding\n\n\n\n\nQuestions\n\n\nRun a new correlation analysis to investigate the correlation between externality (external locus of control) and engagement. After that, do the same for externality and academic success. What do you find?\nExternality and engagement:\n\nWhat is the value of the Person’s correlation coefficient? \nIs the correlation significant? YesNo\nIf so at what level? 0.0010.010.050.1\n\nExternality and academic success:\n\nWhat is the value of the Person’s correlation coefficient? \nIs the correlation significant? YesNo\nIf so at what level? p &lt; 0.001p = 0.05p = 0.01\n\nAPA Results\nThere was a strong positivenegative correlation between externality and engagement, r() = , p , two-tailed. The relationship accounts for % of the variance observed.\nThere was a strong positivenegative correlation between externality and academic success, r() = , p , two-tailed. The relationship accounts for % of the variance observed.\n\n\n\n\n\n\n\n\nThis section continues to use the academicsuccess data set.\nSix months after finishing their degrees, the students in this fictional study indicate whether they were in graduate employment. This information is in the academicsuccess file under the “employed” column.\n\nYour task is to see if the correlation differed for people who were in graduate employment after finishing their degree. To see this, you will need to create a grouped scatter plot.\n\nYou should have already loaded the academicsuccess file. The steps are mostly the same as when you created the correlation coefficient in the previous section.\nSelect analysis -&gt; exploration -&gt; scatterplot\n\n\n\nIn the x and y axis place the variables “engagement” and “acadsuccess”. In the group section place the variable “employed”.\n\n\n\nJamovi will display a new scatterplot where employed and unemployed participants have their own colour to identify their group.\n\n\n\n\n\n\n\n\nTipTest your understanding\n\n\n\n\nQuestions\n\n\nBased on the grouped scatter plot…\n\nWhat do you notice about this plot? (Remember this is a fake data set!)\nWhat does this suggest to you about the data set as a whole?\n\n\n\n\n\n\n\n\n\nThe academicsuccess datasheet contains three continuous variables, the third being externality (labelled “externalLoC”). Externality is correlated with both engagement and academic success. It is possible that the apparent relationship between engagement and academic success is completely due to the relationship that they each have with externality. That is, externality might be acting as a mediating variable between the other two. Alternatively, the relationship between engagement and academic success might have little to do with externality. We can test this by performing a partial correlation which lets us see what the correlation between two variables (here engagement and academic success) would be if we were to hold a third variable (here externality) constant.\n\nAt the top of the screen select Analysis -&gt; Regression -&gt; Partial Correlation (ensure that the academicsuccess data set is loaded into Jamovi)\n\n\n\nDrag the “engagement” and “acadsuccess” variables into the “Variables” box\n\n\n\nDrag the “externalLoC” variable into the “Control Variables” box. This tells the analysis which variables you are controlling for.\n\n\n\nUnderneath the variable sections, you can select the type of test you want to run. You must make sure the correct parametric test has run.\n\nCorrelation Coefficients: You have the option to choose Pearsons if your data is suitable for parametric tests. For non-parametric tests you have the option to select Spearman or Kendall’s tau-b. For this data click Pearsons.\nHypothesis: If your hypothesis has direction of prediction, such as a positive or negative correlation between the two variables, you can select either “Correlated Positively” or “Correlated Negatively” as a specific test. If you have no prediction as to the direction of the relationship, you can keep “Correlated”, which you should select for this data.\nCorrelation type: A partial correlation will measure the relationship between two variables, when controlling for another variable. A semipartial correlation will correlate the variable you are controlling for with just one of other variables, to see if that influences the relationship the remaining variable. For this data you should use a partial correlation.\nAdditional options: tick “report significance” to see the P value for every correlation. Note that this test does not calculate degrees of freedom, so you will need to tick “N” for when you do this further below.\n\n\n\n\nYou can now interpret the results that are presented\n\n\n\n\n\nJamovi does not give you the degrees of freedom (df) in partial correlation analysis. You will need to calculate it manually using the equation below (see explanation in bullet points).\n\\[\ndf=n-k-p\n\\]\n\nn: the number of datapoints or observations. You can see this by ticking “N” under “Additional Options” in the above analysis. As Jamovi is dynamic, you will be able to see the number of data points (20) appear in the partial correlation matrix.\nk: the number of variables, including the control variables. In this case we have 3 variables in the partial correlation (“engagement”, “acadsuccess” and “externality”)\np: number of parameters that are being used. Here we are measuring one coefficient between “engagement” and “acadsuccess”. You are then measuring a second coefficient between the two variables and the control variable, externality (note this second parameter is not included in semipartial correlations). Therefore, here the parameter is 2.\n\nWith this calculation you can confirm that the degrees of freedom is 15.\n\n\n\nThe following is an example of a partial correlation write-up. It refers to the above example for the correlation between engagement and academic success, controlling for externality.\n“When externality was held constant, the partial correlation between study engagement and academic success was not found to be significant, r(15) = .235, p = .334, two-tailed. The results suggest that academic success is unrelated to study engagement when controlling for externality.”\n\n\n\nJamovi can easily calculate correlations between several pairs of variables at once. Download and open the stress data file on Canvas. The file contains made-up data of 100 people. Variables include:\n\nStress – How much stress participants experience\nAge – Participants age\nWork_Hours – The number of hours participants work per week\nChildren – The number children participants have\nLeisure - The amount of money participants spend on leisure per week\nExercise - The amount of time per week participants spend exercising\nSalary - Participants’ salaries.\n\nCalculate Person’s correlations for all pairings of the variables.\nYou can do this by adding all of the variables into the correlation dialog box at once. The steps are the same as when you calculate the correlation coefficient, as shown above.\n\nAt the top of the screen select Analysis -&gt; Regression -&gt; Correlation Matrix\n\n\n\nWhen selecting your variables, rather than just selecting two variables, select all of them.\n\n\n\nMake sure you select the correct tests as explained in correlation coefficient section. Read the above steps again to remind yourself what they are.\nYou can tell which pair each entry in the correlation matrix applies to by looking at the row and column headings. Since the order of variables in correlation makes no difference, only one ordering needs to be recorded for each pair when reporting multiple correlations (e.g. it is fine to report salary correlated with exercise, you don’t need to also report exercise correlated with salary). See the table below for a basic layout for a table of multiple correlations. The first column shows the correlation coefficients for stress versus each of the other variables in turn. [NOTE: The table is not presented in APA format]\n\n\n\n\n\n\n\nTipTest your understanding\n\n\n\n\nQuestions\n\n\nAnswer the following questions based on the table above\n\nAre the following correlations significant at the .01 level? (i.e. p = .01 or p &lt; .01)\n\nStress and age: YesNo Stress and work hours: YesNo Stress and leisure: YesNo Stress and exercise: YesNo Stress and salary: YesNo Stress and children: YesNo Age and work hours: YesNo Age and leisure: YesNo Age and exercise: YesNo Age and salary: YesNo Age and children: YesNo Work hours and leisure: YesNo Work hours and exercise: YesNo Work hours and salary: YesNo Work hours and children: YesNo Leisure and exercise: YesNo Leisure and salary: YesNo Leisure and children: YesNo Exercise and salary: YesNo Exercise and children: YesNo Salary and children: YesNo\n\nWhich variable relates most strongly to the amount of money spent on leisure? StressAgeWork hoursExerciseSalaryChildren\nWhat percentage of the variance in the data is explained by this relationship? %\nComplete the following sentence to describe this correlation as if in a report.\n\nThere was a strong positivenegative correlation between StressAgeWork hoursExerciseSalaryChildren and money spent on leisure, r() = , p , two-tailed.\n\n\n\n\n\n\n\n\n\n\n\nTipTest your understanding\n\n\n\n\nQuestions\n\n\nCreate a scatterplot of the relationship between Age (x) and Stress (y) and answer the following questions.\n\nIs the correlation coefficient for Age and Stress significant? YesNo\nIs correlation an appropriate test for measuring the relationship between Age and Stress? YesNo\nWhy? There is a linear relationship between the variablesThe relationship between the variables doesn’t look linear\nFill in the sentence below to report the correlation as if for a report. Think about any cautionary notes that you think should be applied.\n\nThere was a positivea negativeno correlation between Age and Stress, r() = , p = , two-tailed.\n\n\n\n\n\n\n\n\nDownload and open the speed data file. Imagine that the resting heart rates of 10 people have been measured. The time taken by each person to run 100m is also measured. The theory is that fit people will have low resting heart rates and will also be able to run quickly. Create a scatter plot for this data.\n\n\n\n\n\n\nTipTest your understanding\n\n\n\n\nQuestions\n\n\nCalculate both Pearson’s and Spearman’s correlation coefficients.\n\nWhat is the value for Pearson’s correlation for these variables? \nWhat is the value for Spearman’s correlation for these variables? \nAre either of the coefficients significant? YesNo\nFill in the sentence below to report the correlation as if for a report.\n\nPearson’s: There was a positivea negativeno significant correlation between speed and heart rate, r() = , p = , one-tailedtwo-tailed.\nSpearman’s: There was a positivea negativeno significant correlation between speed and heart rate, r() = , p = , one-tailedtwo-tailed.",
    "crumbs": [
      "Home",
      "Research Methods D",
      "Week 1 - Item Wording"
    ]
  },
  {
    "objectID": "tutorials/rma_week-8_introduction-to-jamovi.html",
    "href": "tutorials/rma_week-8_introduction-to-jamovi.html",
    "title": "Week 8 : Introduction to Jamovi",
    "section": "",
    "text": "Welcome to this practical session! This week you will be introduced to Jamovi. You will use Jamovi throughout your degree for statistical analysis, so it is very important to get familiar with it!\nJAMOVI has inbuilt functions which can be used to investigate and manipulate data, and to perform the majority of statistical tests used in psychological research. Each task you perform in JAMOVI will generate an output which will support your research decisions, such as choosing a statistical test or deciding whether to reject a hypothesis.\nJAMOVI will already be on the computers on Campus but if you want to use it at home too you will need to download and install it on to your personal laptop or desktop PC. The software is free and we recommend that you install it at home because you will need to use it a lot over the next three years, including for many of your assignments. As you can see on the JAMOVI website, there are versions for WindowsOS, MacOS and ChromeOS: https://www.jamovi.org/download.html\nYou can find instructions for how to install JAMOVI on your personal laptop or desktop PC here: https://docs.jamovi.org/_pages/um_1_installation.html\nThere is also a Cloud version here: https://www.jamovi.org/cloud.html\nYou will also be able to find a lot of information about how to use JAMOVI on their website and all of this is free to access. You should explore their website but this page could be particularly useful because it is a comprehensive User Guide: https://www.jamovi.org/user-manual.html\n\n\n\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nMeasures of central tendency\n\n\n\nMeasures of dispersion and spread\n\n\n\nMeasures of skew and normality\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Jamovi editor\n\n\n\nComputing descriptive statistics in Jamovi\n\n\n\nPlot basic graphs in Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with openly available research data\n\n\n\n\n\n\n\n\n\nNoteToday’s session\n\n\n\nToday you will just be familiarising yourself with the Jamovi interface and its layout / basic functions. Next week we will move on to using Jamovi for statistical analysis of data – so make sure you are comfortable with the layout and using the basic functions this week!\n\n\n\n\n\nWe will be exploring the dataset employees data that you can find on the CANVAS page under Week 8 of the computer labs folder of Research Methods A\n\n\n\nWe have a dataset of employees in a big data table. This raw data needs to be summarised and visualised so that we can turn this massive block of numbers into something intuitive.\nWe can generate a concise overview of the main characteristics of the data by computing and visualising a few key features from the data.\n\n\n\n\n\n\nWarningOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.\n\n\n\n\n\nThe instructions for installing and licensing JAMOVI on your personal computer are available above. To access JAMOVI once installed or when using Campus computers you will be able to find it in the start menu at the bottom left corner of your desktop (sometimes this will be in another location on your personal laptop or personal desktop PC – you know your device best!)\nOnce open, you will see the following screen. Note that if you are working with a later version of JAMOVI, the screen may look a little different e.g. in version ‘2.3.28 solid’ there is a tab called “Variables” before “Data” at the top of the screen. As researchers we work with statistical software which updates occasionally. Most of the changes are often cosmetic but sometimes options and functions move or are improved. You should be prepared to familiarise yourself with the changes each time they happen and JAMOVI update their user guides regularly to account for updates.\n\n\n\n\n\n\nTipData Skills - Jamovi interface\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file Employees data.xlsx from canvas.\nIn Jamovi, click the 3 horizontal lines in the top left corner, click file, and click open. Find your data file and click to open.\n\nClick on the data tab and then on set-up to see the variable information\n\nTry clicking on different column names to change the information you are presented with.\n\nTry changing the labels for gender from 0 and 1 to something meaningful.\n\n\n\nWhen you start to do things in JAMOVI another screen will also come up to the right of the data view. This is the Output screen which keeps track of what you are doing and all of your results. When you have undertaken some analysis you can click on the results from this screen and cut and paste them into your reports.\n\n\n\nNow it is time for you to explore the data. If you are unsure about any of these, please talk to the staff in your computer practical session.\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nLooking at JAMOVI, what do each row and column represent?\nEach row represents one participants dataone variablea level of a variable.\nEach column represents one participants dataone variablea level of a variable.\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nHow many variables are there in this data set?\nThere are  variables in the dataset.\n\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - What are the variables called?\n\n\n\n\n\nThe variable are called gender, bdate, educ, jobcat, salary, salbegin, prevexp, minority\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHint\n\n\nHow many levels are there in each variable?\nGender: .\nJobcat: .\nMinority: .\nOnly the variables that are categorical have levels as these are the only variables that represent groups.\n\n\nCheck how many values are specified for each variable. E.g. Gender in this example only has two levels, male and female (which are represented by 1s and 0s) – look at the other variables as well. It is actually bad practice by the researcher to only list two potential labels for gender identity. Our research should be inclusive; this question isn’t inclusive because it excludes anyone who does not identify with one of these labels.\nWhen you come to do your own research, it is incredibly important that you design your study well before you collect any data. How you design your study will determine what analyses you can perform, what hypotheses you can test and what conclusions can be drawn. If you don’t design your study in a way which allows you to collect the data you need, you won’t be able to analyse it how you intended, test your hypotheses or accomplish your aims/objectives. Therefore, you need to get it right because you can’t change it afterwards.\n\n\n\n\n\n\n\n\n\n\n\nCaution Ethical Consideration - How could you ask a question about gender identity in a more inclusive way?\n\n\n\n\n\nYou could ask participants to state their gender without putting multiple choice constraints on this.\nYou could ask what gender participants identify as.\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nWhat type of variables (ordinal, scale (continuous), nominal) are there in the dataset?\nGender: ordinalscale (continuous)nominalID.\nBdate: ordinalscale (continuous)nominalID.\nEduc: ordinalscale (continuous)nominalID.\nJobcat: ordinalscale (continuous)nominalID.\nSalary: ordinalscale (continuous)nominalID.\nSalbegin: ordinalscale (continuous)nominalID.\nPrevexp: ordinalscale (continuous)nominalID.\nMinority: ordinalscale (continuous)nominalID.\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nYour variables are not always imported as the correct variable type. In this case Bdate has been imported as ID which isn’t one of our variable types. What should you change this to?\nBdate: ordinalscale (continuous)nominal.\nPlease change this on your dataset.\n\n\n\n\n\n\n\n\n\n\n\nNoteNote\n\n\n\nWhen conducting statistical analysis for your research, you need to ensure that your variables are set to the right type. Otherwise, your analysis may not work correctly.\n\n\n\n\n\n\n\n\nCaution Data Skills - What does the variable ‘Minority’ mean? Is it useful? Is there any information in the dataset to tell us what the numbers mean?\n\n\n\n\n\nThe variable gives us information about whether the participant is part of a minority group. It isn’t very useful without labels. There is no information in the dataset that would allow us to discern what the numbers mean either.\nPlease add labels to your ‘Minority’ variable where 0 = Not a minority and 1 = Minority\n\n\n\n\n\n\nFor the next few exercises you need to run some short analysis to get some descriptive statistics. Go to Analyse -&gt; Exploration -&gt; Descriptive statistics. Put Education and Job Category into the right-hand box. See the results appear in the right-hand window.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nWhat is the mean education?\nMean educ: .\n\n\n\n\n\nNow try putting gender in the ‘split by’ box and see what happens to the output\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nWhat is the mean education split by gender?\nMean education men: .\nMean education women: .\n\n\n\n\n\nAdd labels to the variable JobCat where 1 = Manual, 2 = Office and 3 = Corporate. Go to Analyse -&gt; Exploration -&gt; Descriptive statistics. Put Salary into the right-hand box and Press OK. Look at the output\n\nNow do the same again but split by JobCat.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nWhat are the mean salaries for each group?\nGroup 1: .\nGroup 2: .\nGroup 3: .\n\n\n\n\n\nOn the same screen click ‘plot’ and then tick the box for histogram and bar plot. You’ll see the graphs appear in the screen to the right. Have a play around with the different options and see what you can produce.\n\n\n\n\n\n\n\n\n\n\nNoteKey step - Saving data in Jamovi\n\n\n\nIn JAMOVI Click on the 3 horizontal lines in the top left corner to File &gt; Save as.\nGive the file an appropriate name and click on ‘Browse’ to decide where you want to save it – then just click on ‘Save’",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 8 - Introduction to Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rma_week-8_introduction-to-jamovi.html#the-dataset",
    "href": "tutorials/rma_week-8_introduction-to-jamovi.html#the-dataset",
    "title": "Week 8 : Introduction to Jamovi",
    "section": "",
    "text": "We will be exploring the dataset employees data that you can find on the CANVAS page under Week 8 of the computer labs folder of Research Methods A",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 8 - Introduction to Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rma_week-8_introduction-to-jamovi.html#the-challenge",
    "href": "tutorials/rma_week-8_introduction-to-jamovi.html#the-challenge",
    "title": "Week 8 : Introduction to Jamovi",
    "section": "",
    "text": "We have a dataset of employees in a big data table. This raw data needs to be summarised and visualised so that we can turn this massive block of numbers into something intuitive.\nWe can generate a concise overview of the main characteristics of the data by computing and visualising a few key features from the data.\n\n\n\n\n\n\nWarningOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 8 - Introduction to Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rma_week-8_introduction-to-jamovi.html#jamovi-basics",
    "href": "tutorials/rma_week-8_introduction-to-jamovi.html#jamovi-basics",
    "title": "Week 8 : Introduction to Jamovi",
    "section": "",
    "text": "The instructions for installing and licensing JAMOVI on your personal computer are available above. To access JAMOVI once installed or when using Campus computers you will be able to find it in the start menu at the bottom left corner of your desktop (sometimes this will be in another location on your personal laptop or personal desktop PC – you know your device best!)\nOnce open, you will see the following screen. Note that if you are working with a later version of JAMOVI, the screen may look a little different e.g. in version ‘2.3.28 solid’ there is a tab called “Variables” before “Data” at the top of the screen. As researchers we work with statistical software which updates occasionally. Most of the changes are often cosmetic but sometimes options and functions move or are improved. You should be prepared to familiarise yourself with the changes each time they happen and JAMOVI update their user guides regularly to account for updates.\n\n\n\n\n\n\nTipData Skills - Jamovi interface",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 8 - Introduction to Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rma_week-8_introduction-to-jamovi.html#how-to-open-an-excel-file-in-jamovi",
    "href": "tutorials/rma_week-8_introduction-to-jamovi.html#how-to-open-an-excel-file-in-jamovi",
    "title": "Week 8 : Introduction to Jamovi",
    "section": "",
    "text": "NoteKey step\n\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file Employees data.xlsx from canvas.\nIn Jamovi, click the 3 horizontal lines in the top left corner, click file, and click open. Find your data file and click to open.\n\nClick on the data tab and then on set-up to see the variable information\n\nTry clicking on different column names to change the information you are presented with.\n\nTry changing the labels for gender from 0 and 1 to something meaningful.\n\n\n\nWhen you start to do things in JAMOVI another screen will also come up to the right of the data view. This is the Output screen which keeps track of what you are doing and all of your results. When you have undertaken some analysis you can click on the results from this screen and cut and paste them into your reports.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 8 - Introduction to Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rma_week-8_introduction-to-jamovi.html#exercises",
    "href": "tutorials/rma_week-8_introduction-to-jamovi.html#exercises",
    "title": "Week 8 : Introduction to Jamovi",
    "section": "",
    "text": "Now it is time for you to explore the data. If you are unsure about any of these, please talk to the staff in your computer practical session.\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nLooking at JAMOVI, what do each row and column represent?\nEach row represents one participants dataone variablea level of a variable.\nEach column represents one participants dataone variablea level of a variable.\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nHow many variables are there in this data set?\nThere are  variables in the dataset.\n\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - What are the variables called?\n\n\n\n\n\nThe variable are called gender, bdate, educ, jobcat, salary, salbegin, prevexp, minority\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHint\n\n\nHow many levels are there in each variable?\nGender: .\nJobcat: .\nMinority: .\nOnly the variables that are categorical have levels as these are the only variables that represent groups.\n\n\nCheck how many values are specified for each variable. E.g. Gender in this example only has two levels, male and female (which are represented by 1s and 0s) – look at the other variables as well. It is actually bad practice by the researcher to only list two potential labels for gender identity. Our research should be inclusive; this question isn’t inclusive because it excludes anyone who does not identify with one of these labels.\nWhen you come to do your own research, it is incredibly important that you design your study well before you collect any data. How you design your study will determine what analyses you can perform, what hypotheses you can test and what conclusions can be drawn. If you don’t design your study in a way which allows you to collect the data you need, you won’t be able to analyse it how you intended, test your hypotheses or accomplish your aims/objectives. Therefore, you need to get it right because you can’t change it afterwards.\n\n\n\n\n\n\n\n\n\n\n\nCaution Ethical Consideration - How could you ask a question about gender identity in a more inclusive way?\n\n\n\n\n\nYou could ask participants to state their gender without putting multiple choice constraints on this.\nYou could ask what gender participants identify as.\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nWhat type of variables (ordinal, scale (continuous), nominal) are there in the dataset?\nGender: ordinalscale (continuous)nominalID.\nBdate: ordinalscale (continuous)nominalID.\nEduc: ordinalscale (continuous)nominalID.\nJobcat: ordinalscale (continuous)nominalID.\nSalary: ordinalscale (continuous)nominalID.\nSalbegin: ordinalscale (continuous)nominalID.\nPrevexp: ordinalscale (continuous)nominalID.\nMinority: ordinalscale (continuous)nominalID.\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nYour variables are not always imported as the correct variable type. In this case Bdate has been imported as ID which isn’t one of our variable types. What should you change this to?\nBdate: ordinalscale (continuous)nominal.\nPlease change this on your dataset.\n\n\n\n\n\n\n\n\n\n\n\nNoteNote\n\n\n\nWhen conducting statistical analysis for your research, you need to ensure that your variables are set to the right type. Otherwise, your analysis may not work correctly.\n\n\n\n\n\n\n\n\nCaution Data Skills - What does the variable ‘Minority’ mean? Is it useful? Is there any information in the dataset to tell us what the numbers mean?\n\n\n\n\n\nThe variable gives us information about whether the participant is part of a minority group. It isn’t very useful without labels. There is no information in the dataset that would allow us to discern what the numbers mean either.\nPlease add labels to your ‘Minority’ variable where 0 = Not a minority and 1 = Minority",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 8 - Introduction to Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rma_week-8_introduction-to-jamovi.html#descriptive-statistics-exercises",
    "href": "tutorials/rma_week-8_introduction-to-jamovi.html#descriptive-statistics-exercises",
    "title": "Week 8 : Introduction to Jamovi",
    "section": "",
    "text": "For the next few exercises you need to run some short analysis to get some descriptive statistics. Go to Analyse -&gt; Exploration -&gt; Descriptive statistics. Put Education and Job Category into the right-hand box. See the results appear in the right-hand window.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nWhat is the mean education?\nMean educ: .\n\n\n\n\n\nNow try putting gender in the ‘split by’ box and see what happens to the output\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nWhat is the mean education split by gender?\nMean education men: .\nMean education women: .\n\n\n\n\n\nAdd labels to the variable JobCat where 1 = Manual, 2 = Office and 3 = Corporate. Go to Analyse -&gt; Exploration -&gt; Descriptive statistics. Put Salary into the right-hand box and Press OK. Look at the output\n\nNow do the same again but split by JobCat.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nWhat are the mean salaries for each group?\nGroup 1: .\nGroup 2: .\nGroup 3: .\n\n\n\n\n\nOn the same screen click ‘plot’ and then tick the box for histogram and bar plot. You’ll see the graphs appear in the screen to the right. Have a play around with the different options and see what you can produce.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 8 - Introduction to Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rma_week-8_introduction-to-jamovi.html#how-to-save-your-work-in-jamovi",
    "href": "tutorials/rma_week-8_introduction-to-jamovi.html#how-to-save-your-work-in-jamovi",
    "title": "Week 8 : Introduction to Jamovi",
    "section": "",
    "text": "NoteKey step - Saving data in Jamovi\n\n\n\nIn JAMOVI Click on the 3 horizontal lines in the top left corner to File &gt; Save as.\nGive the file an appropriate name and click on ‘Browse’ to decide where you want to save it – then just click on ‘Save’",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 8 - Introduction to Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-2_review-of-inferential-tests.html",
    "href": "tutorials/rmc_week-2_review-of-inferential-tests.html",
    "title": "Week 2 : Review of Inferential Tests",
    "section": "",
    "text": "Quantitative Methods\n\n\n\n\n\nIndependent t-tests (and Mann-Whitney)\n\n\n\nPaired samples t-test (and Wilcoxon)\n\n\n\nOne-way between participants ANOVA\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Jamovi editor\n\n\n\nSetting up data files for between- and within-participants designs\n\n\n\nComputing descriptive statistics in Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with openly available research data\n\n\n\n\n\n\n\n\n\nNoteToday’s session\n\n\n\nBelow is summary of several of the inferential tests covered at Level 1. As part of your preparation for Level 2 and your Level 3 Project, make sure that you are confident in how (and in what context) to carry out these tests on Jamovi and how to interpret the outputs. Some of the language used here may be a little different from what you learned in RMB but the statistical and theoretical content is effectively the same.\n\n\n\n\n\n\n\n\nAn independent t-test is used to assess whether there are significant differences between two independent or separate groups (between-participants design) for a specific measure or DV, e.g. a comparison of psychology and human neuroscience students’ self-rating of statistics expertise.\nIn this example, we test the two-sided hypothesis that there is a difference in the self-rated level of research methods expertise of psychology and human neuroscience students in response to a question asking participants to rate their expertise on a scale of 0 to 100.\n\nWalk-through example 1 Using RMC_NM1_WT1_ind_t_test.sav in Jamovi:\n\nScreen the data using Exploration.\n\nIf it looks as if the whole dataset is normally distributed, proceed with the t-test.\nIf non-normal, check if this might be due to one or two outliers.\n\nIf so, remove the outlier(s) and explore the data again. Is it normally distributed now?\nIf not move to Mann-Whitney.\n\n\n\n[For the rest of the walk through, follow the instructions for the t-test.]\n\nGo to the Analyses menu. Select T-Tests. Select Independent-Samples T Test…\n\n\n\nIn the dialog box, move the dependent variable into the box labelled ‘Dependent Variables’\nMove the grouping variable into the box labelled Grouping Variable\nEnsure that ‘Student’s’ is checked\nUnder the additional satatistics menu make sure that the ‘mean difference’ with ‘confidence interval’; ‘effect size’, with ‘confidence interval’; and ‘descriptives’ are checked.\n\n\nIf you have carried out the analysis correctly, your output should consist of three tables. * The Independent Samples T-Test table shows us the results of the t-test. Looking at the first three columns we can determine if the results of the t-test are significant. The next four columns give us some more information about the differences between the two groups. The last four columns provide information about how large an observed effect is. When we report a t-test with a significant outcome, we should also report the effect size. (If the t-test is not significant, you do not need to report Cohen’s d.) - The larger the observed effect, the greater Cohen’s d becomes. Read the value from the Effect Size column. - A common interpretation of effect sizes using Cohen’s d is that d = 0.2 is a small effect, d = 0.5 is a medium effect, and d = 0.8 is a large effect. Essentially, d is measured in units of standard deviations. A d greater than 1 means that the difference between groups is greater than one standard deviation.\n\nThe Assumptions table shows the Levene’s Test for Equality of Variances. This test shows whether the variances of the two groups are equal. When looking at this test, we are testing the assumption that the variances are statistically different. In Levene’s Test…\n\nIf p is &gt; (greater than) 0.05, equal variances are assumed, and we use the t-test results from the Student’s t-test\nIf p is &lt; (smaller than) 0.05, unequal variances are assumed, and we need to rerun the analysis by clicking ‘Welch’s’ in the Tests menu and use the new values.\n\nThe Group Descriptives table shows the number of participants, the mean score, the median score, and the standard deviations of what we have measured as well as the standard error of the mean for each group/condition. Check here that the groups/conditions match those you intended to compare…\n\n\nReporting p-values in RMC/NCM1 * Please give exact probabilities (p=…) to 3 decimal places, wherever possible. You may see very small p-values reported as &lt; .001 but never 0. - This is important as we can never have a p-value of 0. It would mean that there is no chance that the data have come out the way they did by sampling error, which cannot be the case: sampling error is always a possibility.\n\n\n\n\nThe Mann-Whitney test is used when there are different participants in each condition and parametric assumptions are not met. Therefore, it is a non-parametric equivalent of an independent t-test. For example, it can be used with data that are not normally distributed and with data that are ordinal.\nInstead of using means and standard deviations, the Mann-Whitney test looks at the ranking positions of the values from each of the two conditions. The test works out the ranks for the scores in each condition and compares them. The test statistic for a Mann-Whitney test is the U-value.\nIn this example, we test the hypothesis that ‘politically correct’ language (pc) makes the ideas expressed feel less familiar to participants compared to ‘non-politically correct’ language (npc). Participants were given as series of sentences and asked to rate them for familiarity of the idea expressed on a 5-point scale (with 1 = very unfamiliar and 5 = very familiar).\n\nWalk-through Example 2 Using RMC_NM1_WT2_Mann-Whitney.sav in Jamovi: a) Screen the data using Exploration. Here the data are not normally distributed (and they are ordinal), so Mann-Whitney is a more appropriate test to use. Record medians for the different groups and minimum and maximum scores. These are needed when reporting Mann-Whitney test results. b) Go to the Analyses -&gt; T-Tests -&gt; Independent T-Test\n\n\nEnter the DV into the ‘Dependent Variables’ box. Enter the IV into the ‘Grouping Variable’ box.\nIn the ‘Tests’ section, deselect ‘Student’s’ and select ‘Mann-Whitney U’.\nCheck the box next to ‘Descriptives’ and ‘Effect Size’.\n\n\nThe Jamovi output should look like this: * The Independent Samples T-Test table shows the value of Mann-Whitney’s U to be 59.0. The rank biserial correlation provides a measure of effect size (rpb = .476). The p-value is .019. We conclude that there is a significant difference in familiarity ratings for pc compared to npc sentences. To check the direction of the difference and get some idea of this effect, we would examine the medians for these data. * The Group Descriptives table shows the number of participants in each group and presents the mean, median, standard deviation and standard error of the mean for each group. The mean scores for the groups are NOT reported. Report median scores instead.\n\n\n\n\n\n\n\n\nA paired samples t-test is used to assess whether there are significant differences between two conditions which were completed by the same, single group of participants (within-participants design or repeated measures), e.g. comparison of a pre- and post-intervention measurement.\nIn this example, the hypothesis tested is that people’s performance increased significantly for RMB compared to their performance for RMA (assessed as separate modules). This is a one-sided hypothesis as a specific direction for the difference is predicted.\n\nWalk-Through Example 3 Using RMC_NM1_WT3_paired_t_test.sav in Jamovi: a) Screen the data using Exploration -&gt; Descriptives. If it looks to be normally distributed, proceed with t-test. (If not move to Wilcoxon.)\n[For the rest of the walk through, follow the instructions for the t-test.]\n\nGo to the Analyses tab. Select T-Tests. Select Paired Samples T Test\n\n\n\n\nMove the first variable of the pair you wish to compare into the Paired Variables box.\nRepeat with the second variable.\nMake sure ‘Student’s’, ‘Mean Difference’, ‘Effect size’ and ‘Confidence interval’, and ‘Descriptives’ are checked.\nAs the hypothesis is one-tailed we need to specify this in the Hypothesis section. As we think that scores for RMA will be lower than RMB we should check ‘Measure 1 &lt; Measure 2’ here.\n\n\nThe output of a paired samples t-test consists of two tables. * The Paired Samples T-Test table shows the t-value, the degrees of freedom (df), the p-value, the mean difference between conditions (condition entered first minus condition entered second), the standard error of the difference and effect size information. - When interpreting a one-sided t-test, consider the direction of the difference: is it in the predicted direction? We hypothesised that the RMB mark would be higher than the RMA mark. The direction of the difference suggests that this is the case: The mean difference (RMA mark minus RMB mark) was negative – i.e. RMB mark was higher on average than RMA mark.\n\nThe Descriptives table provides the descriptive statistics for the two conditions: number of participants, means, medians, standard deviations, and standard error of the mean for each condition.\n\n\n\n\n\n\nThe Wilcoxon Matched Pairs Signed Ranks Test is used when a paired-samples t-test cannot be used, because the data do not meet the requirements of parametric tests.\nThe Wilcoxon test makes use of the fact that the same participants are performing in both conditions. It does this by taking note of the differences in scores for each data-pair. The differences are ranked from lowest to highest.\nSome differences will be positive, some negative, and some 0 (ties) – the latter are ignored as they provide no information. If there is no significant difference between the two conditions, there would be a similar number of positive and negative differences, as the differences between one participant and another would cancel each other out.\nIn this example, we test the hypothesis that primary school children recognise more emotions than they express (i.e. spontaneously label or mention) in a story description task.\n\nWalk-Through Example 4 Using RMC_WT4_Wilcoxon.sav in Jamovi:\n\nScreen the data using Exploration -&gt; Descriptives. Record medians for the different conditions and minimum and maximum scores.\nGo to the Analyses -&gt; T-Tests -&gt; Paired Samples -Test\nEnter the first level of the DV into the ‘Paired Variables’ box. Enter the second level of the DV into the ‘Paired Variables’ list.\nIn the ‘Tests’ section, select ‘Wilcoxon rank’.\nSelect ‘Effect size’\nWe have specified a one-way hypothesis that children will recognise more emotions that they express so we should check ‘Measure 1 &lt; Measure 2’.\n\n\n\nThe Jamovi output should look like this:\n\nThe Paired Samples T-Test table shows the significance of the test. The T-score is under the heading ‘Statistic’. The rank biserial correlation provides a measure of effect size (rpb = -.783). The difference is significant, at p &lt; .001. We conclude that there is a significant difference between emotions expressed and emotions recognised with children recognising more emotions than they express.\n\n\n\n\n\n\n\n\nTipActivity 1 - Choosing and running the appropriate test\n\n\n\n\nCheck Your Understanding - Activity 1 Question 1Check Your Understanding - Activity 1 Question 2Check Your Understanding - Activity 1 Question 3\n\n\nUsing RMC_NM1_Wksh2_choosing_tests.sav in Jamovi and what has just been covered, test the following hypothesis:\nOlder students look forward to the module more than younger ones\nTest used: independent t-testpaired samples t-testMann-Whitney testWilcoxon test\nIs this hypothesis one-sided or two-sided? (Which direction if one-sided?): two-sidedone-sided, group 1 &lt; group 2one-sided, group 1 &gt; group 2\nKey descriptives Older students: mean = , median = , sd = \nYounger students: mean = , median = , sd = \nReport the result If you chose to use a parametric test, fill out the parametric question and if you chose to use a non-parametric test then fill out the non-parametric version\nParametric: t() = ; p =  (two-sidedone-sided); d =  Non-parametric: U = ; N = ; p =  (two-sidedone-sided); rrb = \nHypothesis accepted? yesno\n\n\nUsing RMC_NM1_Wksh2_choosing_tests.sav in Jamovi and what has just been covered, test the following hypothesis:\nPsychology students come from large families than Human Neuroscience students.\nTest used: independent t-testpaired samples t-testMann-Whitney testWilcoxon test\nIs this hypothesis one-sided or two-sided? (Which direction if one-sided?): two-sidedone-sided, group 1 &lt; group 2one-sided, group 1 &gt; group 2\nKey descriptives Psychology students: median = , sd = \nHuman neuroscience students: median = , sd = \nReport the result U = ; N = ; p =  (two-sidedone-sided); rrb = \nHypothesis accepted? yesno\n\n\nUsing RMC_NM1_Wksh2_choosing_tests.sav in Jamovi and what has just been covered, test the following hypothesis:\nPerformance scores are higher after practice than before practice.\nTest used: independent t-testpaired samples t-testMann-Whitney testWilcoxon test\nIs this hypothesis one-sided or two-sided? (Which direction if one-sided?): two-sidedone-sided, measure 1 &lt; measure 2one-sided, measure 1 &gt; measure 2\nKey descriptives Before practice: median = , sd = \nAfter practice: median = , sd = \nReport the result T = ; N = ; p  (two-sidedone-sided); rrb = \nHypothesis accepted? yesno\n\n\n\n\n\n\n\n\n\n\nThis section is about reviewing how to carry out one-way ANOVAs (for data that meet parametric assumptions) in Jamovi.\nOne-way ANOVAs have a single factor (IV) which has more than two levels (groups), and a single DV.\nOne-way ANOVAs are carried out differently in Jamovi depending on whether the IV is a within- or between-participants factor. You will have had some experience in RMB with one-way between-participants ANOVA – i.e. comparing two or more groups. The following sections go over how to perform one-way between-participants ANOVA. In the next workshop, one-way within-participants ANOVA will be introduced.\n\n\n\n\nIn this example experiment, students were asked to decide which category a displayed word belonged to (e.g. Is ‘horse’ an animal or furniture?). Response times were recorded.\nBefore the categorization task, the participants performed 1 of 3 priming tasks:\n\nThe “surface” group looked at a list of words and decided if each one contained an ‘e’.\nThe “deep” group looked at a list of words and decided if each one was a living thing.\nThe “no prime” group did not look at any words before (control group).\n\nHypotheses tested were:\n\nParticipants who have previously processed a word at a deep level will categorise it more quickly than those who have processed it at a surface level or in the no prime condition.\nIf surface processing has any effect on later categorization, the response times in the surface condition should be faster than in the no prime condition.\n\n\nWalk-Through Example 5 “The effect of priming on a categorization task” a) Download and open the RMC_NM1_1way_between_ANOVA.sav from the Canvas folder. b) Click on the “Data” tab and look at the values in the ‘priming’ and ‘RT’ column.\n\n\nClick Analyses -&gt; Exploration -&gt; Descriptives. In the dialog box, enter the DV into the ‘Variables’ box. Enter the IV into the ‘Split by’ box.\nGo to ‘Plots’. Check ‘Histograms’ and ‘Box plot’ with ‘Label outliers’.\n\n\n\n\n\n\n\nNoteNote\n\n\n\nIn this example the data are broadly normally distributed with no major outliers and so meet data requirements for ANOVA.\n\n\n\nGo to Analyses -&gt; ANOVA -&gt; ANOVA\n\n\n\n\nIn the dialog box, enter the DV into the ‘Dependent Variables’ box. Enter the IV in the ‘Fixed Factors’ box.\nUnder Effect Size check the ‘partial η2’ box.\n\n\n\nClick on the ‘Post Hoc Tests’ menu. Add ‘priming’ to the right-side box. Select ‘Bonferroni’ under ‘Correction’ and check the ‘Cohen’s d’ box under Effect Size.\n\n\n\nClick on the ‘Estimated Marginal Means’ menu. Add ‘priming’ to the ‘Term 1’ box. Select ‘Marginal means plots’ and ‘Marginal means tables’ under ‘Output’ and change the ‘Error bars’ drop down to ‘Standard Error’.\n\n\n\n\n\n\nThe means and standard errors for each group can be found in this table (scroll down as this will be at the bottom). The standard deviations and number of participants in each group can be found in the descriptive exploration we did earlier.\n\n\nFrom this table and the Descriptives table from performing the Exploration analysis answer the following questions:\n\n\n\n\n\n\nTipJamovi Output One-way ANOVA (between) questions\n\n\n\n\nQuestion 1Question 2Question 3\n\n\nWhich priming group responded the fastest? surfacedeepno-prime\n\n\nWhich priming group responded the slowest? surfacedeepno-prime\n\n\nWhich group showed the largest variation in response time? surfacedeepno-prime\n\n\n\n\n\n\nThe ANOVA table shows if the mean differences between the levels/conditions of the IV are statistically significant.\n\n\n\n\n\n\n\n\nTipJamovi Output One-way ANOVA (between) questions\n\n\n\n\nQuestion 4\n\n\nIs there a significant effect of priming upon response times in the categorization task? yesno\n\n\n\n\n\n\nThe next table presents the post-hoc test and looks at the mean differences between conditions to determine which of these differences are statistically significant.\n\n\n\n\n\n\n\n\nTipJamovi Output One-way ANOVA (between) questions\n\n\n\n\nQuestion 5\n\n\nWhich (if any) condition(s) is significantly different from “no prime”? surface and deepsurface onlydeep onlyneither are significant\nWhich (if any) condition(s) is significantly different from “deep”? surface and no primesurface onlyno prime onlyneither are significant\n\n\n\n\n\n\nThe final part of the output is a plot of the means which is a useful tool to understand the differences between groups. The white circles represent the mean and error bars represent whatever you have set them to (standard error of the mean in this case).\n\nFigure 1 Mean Categorisation Times (ms) for the Surface, Deep and No-Prime Conditions\n\n\n\n\n\n\n\nNoteNote\n\n\n\nA chart is called a ‘Figure’ in APA style and should be labelled with an informative heading. The heading should appear immediately above the figure as in the example above. In a report you also need refer to the figure when you want the reader to look at it. You should always comment on what the chart shows in the main text.\n\n\n\n\n\n\nIn reporting ANOVA results, include:\n\nInformation about the descriptive statistics for the different groups, possibly including a figure to illustrate the findings\nA summary of the test statistic including probability and effect size\nComment on whether the findings supported the hypothesis or not.\n\nHere is an example write-up for the ANOVA in Walk-Through Example 5. (This report includes a table of means instead of a figure)\n\n\n\n\n\n\n\nTipOne-way ANOVA Write Up\n\n\n\n\nFill in the blanks\n\n\n(Any data preparation, e.g. how the response times were calculated or how the data were trimmed or if and how many outliers were removed, should be presented in the section of the method about data preparation and data analysis.)\nIn a report, an ANOVA results section could look like this:\nResults\nAs predicted, the mean categorization times were lowest in the ‘deep’ group, followed by the ‘surface’ group, with processing times being slowest in the ‘no prime’ group. A one-way between-participants ANOVA found significant differences in categorization times between the three priming conditions (see Table 1).\nTable 1 Means (Standard Deviations in Parentheses), and One-Way Analysis of Variance Statistics for Categorization Times (ms) for the Priming Conditions.\n\n\n\n\n\n\n\n\n\n\n\nMeasure\n\nPriming Condition\n\nF(2,[])\nη2p\n\n\n\n\n\nSurface\nDeep\nNo-prime\n\n\n\n\nCategorisation time\n (141)\n1289 (223)\n1682 ()\n28.1***\n.44\n\n\n\n*** p &lt; .001\nTo locate these differences, a Bonferroni post hoc test was performed. The surface and no prime groups differed significantly (mean difference = 294 ms, p &lt; .001) as did the  and no prime groups (mean difference = 393 ms, p &lt; .001). However, there was no difference between deep and  priming (mean difference = 99 ms, p = ).\nThese findings supportpartially supportdo not support the hypothesis in that both surface and deep priming led to an advantage in the categorization task compared to the no prime condition, but there was no evidence that deep priming produced a greater advantage than surface priming.",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 2 - Review of inferential tests"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-2_review-of-inferential-tests.html#comparing-2-independent-groups-on-jamovi",
    "href": "tutorials/rmc_week-2_review-of-inferential-tests.html#comparing-2-independent-groups-on-jamovi",
    "title": "Week 2 : Review of Inferential Tests",
    "section": "",
    "text": "An independent t-test is used to assess whether there are significant differences between two independent or separate groups (between-participants design) for a specific measure or DV, e.g. a comparison of psychology and human neuroscience students’ self-rating of statistics expertise.\nIn this example, we test the two-sided hypothesis that there is a difference in the self-rated level of research methods expertise of psychology and human neuroscience students in response to a question asking participants to rate their expertise on a scale of 0 to 100.\n\nWalk-through example 1 Using RMC_NM1_WT1_ind_t_test.sav in Jamovi:\n\nScreen the data using Exploration.\n\nIf it looks as if the whole dataset is normally distributed, proceed with the t-test.\nIf non-normal, check if this might be due to one or two outliers.\n\nIf so, remove the outlier(s) and explore the data again. Is it normally distributed now?\nIf not move to Mann-Whitney.\n\n\n\n[For the rest of the walk through, follow the instructions for the t-test.]\n\nGo to the Analyses menu. Select T-Tests. Select Independent-Samples T Test…\n\n\n\nIn the dialog box, move the dependent variable into the box labelled ‘Dependent Variables’\nMove the grouping variable into the box labelled Grouping Variable\nEnsure that ‘Student’s’ is checked\nUnder the additional satatistics menu make sure that the ‘mean difference’ with ‘confidence interval’; ‘effect size’, with ‘confidence interval’; and ‘descriptives’ are checked.\n\n\nIf you have carried out the analysis correctly, your output should consist of three tables. * The Independent Samples T-Test table shows us the results of the t-test. Looking at the first three columns we can determine if the results of the t-test are significant. The next four columns give us some more information about the differences between the two groups. The last four columns provide information about how large an observed effect is. When we report a t-test with a significant outcome, we should also report the effect size. (If the t-test is not significant, you do not need to report Cohen’s d.) - The larger the observed effect, the greater Cohen’s d becomes. Read the value from the Effect Size column. - A common interpretation of effect sizes using Cohen’s d is that d = 0.2 is a small effect, d = 0.5 is a medium effect, and d = 0.8 is a large effect. Essentially, d is measured in units of standard deviations. A d greater than 1 means that the difference between groups is greater than one standard deviation.\n\nThe Assumptions table shows the Levene’s Test for Equality of Variances. This test shows whether the variances of the two groups are equal. When looking at this test, we are testing the assumption that the variances are statistically different. In Levene’s Test…\n\nIf p is &gt; (greater than) 0.05, equal variances are assumed, and we use the t-test results from the Student’s t-test\nIf p is &lt; (smaller than) 0.05, unequal variances are assumed, and we need to rerun the analysis by clicking ‘Welch’s’ in the Tests menu and use the new values.\n\nThe Group Descriptives table shows the number of participants, the mean score, the median score, and the standard deviations of what we have measured as well as the standard error of the mean for each group/condition. Check here that the groups/conditions match those you intended to compare…\n\n\nReporting p-values in RMC/NCM1 * Please give exact probabilities (p=…) to 3 decimal places, wherever possible. You may see very small p-values reported as &lt; .001 but never 0. - This is important as we can never have a p-value of 0. It would mean that there is no chance that the data have come out the way they did by sampling error, which cannot be the case: sampling error is always a possibility.\n\n\n\n\nThe Mann-Whitney test is used when there are different participants in each condition and parametric assumptions are not met. Therefore, it is a non-parametric equivalent of an independent t-test. For example, it can be used with data that are not normally distributed and with data that are ordinal.\nInstead of using means and standard deviations, the Mann-Whitney test looks at the ranking positions of the values from each of the two conditions. The test works out the ranks for the scores in each condition and compares them. The test statistic for a Mann-Whitney test is the U-value.\nIn this example, we test the hypothesis that ‘politically correct’ language (pc) makes the ideas expressed feel less familiar to participants compared to ‘non-politically correct’ language (npc). Participants were given as series of sentences and asked to rate them for familiarity of the idea expressed on a 5-point scale (with 1 = very unfamiliar and 5 = very familiar).\n\nWalk-through Example 2 Using RMC_NM1_WT2_Mann-Whitney.sav in Jamovi: a) Screen the data using Exploration. Here the data are not normally distributed (and they are ordinal), so Mann-Whitney is a more appropriate test to use. Record medians for the different groups and minimum and maximum scores. These are needed when reporting Mann-Whitney test results. b) Go to the Analyses -&gt; T-Tests -&gt; Independent T-Test\n\n\nEnter the DV into the ‘Dependent Variables’ box. Enter the IV into the ‘Grouping Variable’ box.\nIn the ‘Tests’ section, deselect ‘Student’s’ and select ‘Mann-Whitney U’.\nCheck the box next to ‘Descriptives’ and ‘Effect Size’.\n\n\nThe Jamovi output should look like this: * The Independent Samples T-Test table shows the value of Mann-Whitney’s U to be 59.0. The rank biserial correlation provides a measure of effect size (rpb = .476). The p-value is .019. We conclude that there is a significant difference in familiarity ratings for pc compared to npc sentences. To check the direction of the difference and get some idea of this effect, we would examine the medians for these data. * The Group Descriptives table shows the number of participants in each group and presents the mean, median, standard deviation and standard error of the mean for each group. The mean scores for the groups are NOT reported. Report median scores instead.",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 2 - Review of inferential tests"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-2_review-of-inferential-tests.html#comparing-2-related-groups-on-jamovi",
    "href": "tutorials/rmc_week-2_review-of-inferential-tests.html#comparing-2-related-groups-on-jamovi",
    "title": "Week 2 : Review of Inferential Tests",
    "section": "",
    "text": "A paired samples t-test is used to assess whether there are significant differences between two conditions which were completed by the same, single group of participants (within-participants design or repeated measures), e.g. comparison of a pre- and post-intervention measurement.\nIn this example, the hypothesis tested is that people’s performance increased significantly for RMB compared to their performance for RMA (assessed as separate modules). This is a one-sided hypothesis as a specific direction for the difference is predicted.\n\nWalk-Through Example 3 Using RMC_NM1_WT3_paired_t_test.sav in Jamovi: a) Screen the data using Exploration -&gt; Descriptives. If it looks to be normally distributed, proceed with t-test. (If not move to Wilcoxon.)\n[For the rest of the walk through, follow the instructions for the t-test.]\n\nGo to the Analyses tab. Select T-Tests. Select Paired Samples T Test\n\n\n\n\nMove the first variable of the pair you wish to compare into the Paired Variables box.\nRepeat with the second variable.\nMake sure ‘Student’s’, ‘Mean Difference’, ‘Effect size’ and ‘Confidence interval’, and ‘Descriptives’ are checked.\nAs the hypothesis is one-tailed we need to specify this in the Hypothesis section. As we think that scores for RMA will be lower than RMB we should check ‘Measure 1 &lt; Measure 2’ here.\n\n\nThe output of a paired samples t-test consists of two tables. * The Paired Samples T-Test table shows the t-value, the degrees of freedom (df), the p-value, the mean difference between conditions (condition entered first minus condition entered second), the standard error of the difference and effect size information. - When interpreting a one-sided t-test, consider the direction of the difference: is it in the predicted direction? We hypothesised that the RMB mark would be higher than the RMA mark. The direction of the difference suggests that this is the case: The mean difference (RMA mark minus RMB mark) was negative – i.e. RMB mark was higher on average than RMA mark.\n\nThe Descriptives table provides the descriptive statistics for the two conditions: number of participants, means, medians, standard deviations, and standard error of the mean for each condition.\n\n\n\n\n\n\nThe Wilcoxon Matched Pairs Signed Ranks Test is used when a paired-samples t-test cannot be used, because the data do not meet the requirements of parametric tests.\nThe Wilcoxon test makes use of the fact that the same participants are performing in both conditions. It does this by taking note of the differences in scores for each data-pair. The differences are ranked from lowest to highest.\nSome differences will be positive, some negative, and some 0 (ties) – the latter are ignored as they provide no information. If there is no significant difference between the two conditions, there would be a similar number of positive and negative differences, as the differences between one participant and another would cancel each other out.\nIn this example, we test the hypothesis that primary school children recognise more emotions than they express (i.e. spontaneously label or mention) in a story description task.\n\nWalk-Through Example 4 Using RMC_WT4_Wilcoxon.sav in Jamovi:\n\nScreen the data using Exploration -&gt; Descriptives. Record medians for the different conditions and minimum and maximum scores.\nGo to the Analyses -&gt; T-Tests -&gt; Paired Samples -Test\nEnter the first level of the DV into the ‘Paired Variables’ box. Enter the second level of the DV into the ‘Paired Variables’ list.\nIn the ‘Tests’ section, select ‘Wilcoxon rank’.\nSelect ‘Effect size’\nWe have specified a one-way hypothesis that children will recognise more emotions that they express so we should check ‘Measure 1 &lt; Measure 2’.\n\n\n\nThe Jamovi output should look like this:\n\nThe Paired Samples T-Test table shows the significance of the test. The T-score is under the heading ‘Statistic’. The rank biserial correlation provides a measure of effect size (rpb = -.783). The difference is significant, at p &lt; .001. We conclude that there is a significant difference between emotions expressed and emotions recognised with children recognising more emotions than they express.\n\n\n\n\n\n\n\n\nTipActivity 1 - Choosing and running the appropriate test\n\n\n\n\nCheck Your Understanding - Activity 1 Question 1Check Your Understanding - Activity 1 Question 2Check Your Understanding - Activity 1 Question 3\n\n\nUsing RMC_NM1_Wksh2_choosing_tests.sav in Jamovi and what has just been covered, test the following hypothesis:\nOlder students look forward to the module more than younger ones\nTest used: independent t-testpaired samples t-testMann-Whitney testWilcoxon test\nIs this hypothesis one-sided or two-sided? (Which direction if one-sided?): two-sidedone-sided, group 1 &lt; group 2one-sided, group 1 &gt; group 2\nKey descriptives Older students: mean = , median = , sd = \nYounger students: mean = , median = , sd = \nReport the result If you chose to use a parametric test, fill out the parametric question and if you chose to use a non-parametric test then fill out the non-parametric version\nParametric: t() = ; p =  (two-sidedone-sided); d =  Non-parametric: U = ; N = ; p =  (two-sidedone-sided); rrb = \nHypothesis accepted? yesno\n\n\nUsing RMC_NM1_Wksh2_choosing_tests.sav in Jamovi and what has just been covered, test the following hypothesis:\nPsychology students come from large families than Human Neuroscience students.\nTest used: independent t-testpaired samples t-testMann-Whitney testWilcoxon test\nIs this hypothesis one-sided or two-sided? (Which direction if one-sided?): two-sidedone-sided, group 1 &lt; group 2one-sided, group 1 &gt; group 2\nKey descriptives Psychology students: median = , sd = \nHuman neuroscience students: median = , sd = \nReport the result U = ; N = ; p =  (two-sidedone-sided); rrb = \nHypothesis accepted? yesno\n\n\nUsing RMC_NM1_Wksh2_choosing_tests.sav in Jamovi and what has just been covered, test the following hypothesis:\nPerformance scores are higher after practice than before practice.\nTest used: independent t-testpaired samples t-testMann-Whitney testWilcoxon test\nIs this hypothesis one-sided or two-sided? (Which direction if one-sided?): two-sidedone-sided, measure 1 &lt; measure 2one-sided, measure 1 &gt; measure 2\nKey descriptives Before practice: median = , sd = \nAfter practice: median = , sd = \nReport the result T = ; N = ; p  (two-sidedone-sided); rrb = \nHypothesis accepted? yesno",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 2 - Review of inferential tests"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-2_review-of-inferential-tests.html#comparing-more-than-2-groups-on-jamovi",
    "href": "tutorials/rmc_week-2_review-of-inferential-tests.html#comparing-more-than-2-groups-on-jamovi",
    "title": "Week 2 : Review of Inferential Tests",
    "section": "",
    "text": "This section is about reviewing how to carry out one-way ANOVAs (for data that meet parametric assumptions) in Jamovi.\nOne-way ANOVAs have a single factor (IV) which has more than two levels (groups), and a single DV.\nOne-way ANOVAs are carried out differently in Jamovi depending on whether the IV is a within- or between-participants factor. You will have had some experience in RMB with one-way between-participants ANOVA – i.e. comparing two or more groups. The following sections go over how to perform one-way between-participants ANOVA. In the next workshop, one-way within-participants ANOVA will be introduced.\n\n\n\n\nIn this example experiment, students were asked to decide which category a displayed word belonged to (e.g. Is ‘horse’ an animal or furniture?). Response times were recorded.\nBefore the categorization task, the participants performed 1 of 3 priming tasks:\n\nThe “surface” group looked at a list of words and decided if each one contained an ‘e’.\nThe “deep” group looked at a list of words and decided if each one was a living thing.\nThe “no prime” group did not look at any words before (control group).\n\nHypotheses tested were:\n\nParticipants who have previously processed a word at a deep level will categorise it more quickly than those who have processed it at a surface level or in the no prime condition.\nIf surface processing has any effect on later categorization, the response times in the surface condition should be faster than in the no prime condition.\n\n\nWalk-Through Example 5 “The effect of priming on a categorization task” a) Download and open the RMC_NM1_1way_between_ANOVA.sav from the Canvas folder. b) Click on the “Data” tab and look at the values in the ‘priming’ and ‘RT’ column.\n\n\nClick Analyses -&gt; Exploration -&gt; Descriptives. In the dialog box, enter the DV into the ‘Variables’ box. Enter the IV into the ‘Split by’ box.\nGo to ‘Plots’. Check ‘Histograms’ and ‘Box plot’ with ‘Label outliers’.\n\n\n\n\n\n\n\nNoteNote\n\n\n\nIn this example the data are broadly normally distributed with no major outliers and so meet data requirements for ANOVA.\n\n\n\nGo to Analyses -&gt; ANOVA -&gt; ANOVA\n\n\n\n\nIn the dialog box, enter the DV into the ‘Dependent Variables’ box. Enter the IV in the ‘Fixed Factors’ box.\nUnder Effect Size check the ‘partial η2’ box.\n\n\n\nClick on the ‘Post Hoc Tests’ menu. Add ‘priming’ to the right-side box. Select ‘Bonferroni’ under ‘Correction’ and check the ‘Cohen’s d’ box under Effect Size.\n\n\n\nClick on the ‘Estimated Marginal Means’ menu. Add ‘priming’ to the ‘Term 1’ box. Select ‘Marginal means plots’ and ‘Marginal means tables’ under ‘Output’ and change the ‘Error bars’ drop down to ‘Standard Error’.\n\n\n\n\n\n\nThe means and standard errors for each group can be found in this table (scroll down as this will be at the bottom). The standard deviations and number of participants in each group can be found in the descriptive exploration we did earlier.\n\n\nFrom this table and the Descriptives table from performing the Exploration analysis answer the following questions:\n\n\n\n\n\n\nTipJamovi Output One-way ANOVA (between) questions\n\n\n\n\nQuestion 1Question 2Question 3\n\n\nWhich priming group responded the fastest? surfacedeepno-prime\n\n\nWhich priming group responded the slowest? surfacedeepno-prime\n\n\nWhich group showed the largest variation in response time? surfacedeepno-prime\n\n\n\n\n\n\nThe ANOVA table shows if the mean differences between the levels/conditions of the IV are statistically significant.\n\n\n\n\n\n\n\n\nTipJamovi Output One-way ANOVA (between) questions\n\n\n\n\nQuestion 4\n\n\nIs there a significant effect of priming upon response times in the categorization task? yesno\n\n\n\n\n\n\nThe next table presents the post-hoc test and looks at the mean differences between conditions to determine which of these differences are statistically significant.\n\n\n\n\n\n\n\n\nTipJamovi Output One-way ANOVA (between) questions\n\n\n\n\nQuestion 5\n\n\nWhich (if any) condition(s) is significantly different from “no prime”? surface and deepsurface onlydeep onlyneither are significant\nWhich (if any) condition(s) is significantly different from “deep”? surface and no primesurface onlyno prime onlyneither are significant\n\n\n\n\n\n\nThe final part of the output is a plot of the means which is a useful tool to understand the differences between groups. The white circles represent the mean and error bars represent whatever you have set them to (standard error of the mean in this case).\n\nFigure 1 Mean Categorisation Times (ms) for the Surface, Deep and No-Prime Conditions\n\n\n\n\n\n\n\nNoteNote\n\n\n\nA chart is called a ‘Figure’ in APA style and should be labelled with an informative heading. The heading should appear immediately above the figure as in the example above. In a report you also need refer to the figure when you want the reader to look at it. You should always comment on what the chart shows in the main text.\n\n\n\n\n\n\nIn reporting ANOVA results, include:\n\nInformation about the descriptive statistics for the different groups, possibly including a figure to illustrate the findings\nA summary of the test statistic including probability and effect size\nComment on whether the findings supported the hypothesis or not.\n\nHere is an example write-up for the ANOVA in Walk-Through Example 5. (This report includes a table of means instead of a figure)\n\n\n\n\n\n\n\nTipOne-way ANOVA Write Up\n\n\n\n\nFill in the blanks\n\n\n(Any data preparation, e.g. how the response times were calculated or how the data were trimmed or if and how many outliers were removed, should be presented in the section of the method about data preparation and data analysis.)\nIn a report, an ANOVA results section could look like this:\nResults\nAs predicted, the mean categorization times were lowest in the ‘deep’ group, followed by the ‘surface’ group, with processing times being slowest in the ‘no prime’ group. A one-way between-participants ANOVA found significant differences in categorization times between the three priming conditions (see Table 1).\nTable 1 Means (Standard Deviations in Parentheses), and One-Way Analysis of Variance Statistics for Categorization Times (ms) for the Priming Conditions.\n\n\n\n\n\n\n\n\n\n\n\nMeasure\n\nPriming Condition\n\nF(2,[])\nη2p\n\n\n\n\n\nSurface\nDeep\nNo-prime\n\n\n\n\nCategorisation time\n (141)\n1289 (223)\n1682 ()\n28.1***\n.44\n\n\n\n*** p &lt; .001\nTo locate these differences, a Bonferroni post hoc test was performed. The surface and no prime groups differed significantly (mean difference = 294 ms, p &lt; .001) as did the  and no prime groups (mean difference = 393 ms, p &lt; .001). However, there was no difference between deep and  priming (mean difference = 99 ms, p = ).\nThese findings supportpartially supportdo not support the hypothesis in that both surface and deep priming led to an advantage in the categorization task compared to the no prime condition, but there was no evidence that deep priming produced a greater advantage than surface priming.",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 2 - Review of inferential tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "This week we will explore how to use Jamovi to compute and visualise descriptive statistics from a dataset. This will include some revision on Jamovi from last semester and an introduction on how we can use the R coding language to simplify complex data processing.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nMeasures of central tendency\n\n\n\nMeasures of dispersion and spread\n\n\n\nMeasures of skew and normality\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Rj editor to use R code within Jamovi\n\n\n\nLearn the basics about functions and variables in R\n\n\n\nComputing descriptive statistics using R\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nValidating and working with openly available research data\n\n\n\nCreate a reproducible data analysis script\n\n\n\n\n\nMetaphors are often used to help people understand abstract concepts in terms of perceptual experiences. Though this is often useful, it can bias perception and decision making in unexpected ways. For example, North and South are abstract concepts that have a spatial association (north = up, south = down). Participants in the USA have a biased expectation that richer people will live in the north of a city (Meier et al. 2011). However, these expectations are not uniform across all people and cultures and we should not naively generalise this result to all people. A follow up study comparing this effect in participants from the USA and Hong Kong replicated the original effect for American participants but found the opposite effect in people from Hong Kong (Huang, Tse, and Cho 2014).\nThe Many Labs 2 project (Klein et al. 2018) performed an additional replication of this effect across a wide range of countries. Participants were presented with the following map outline of a city:\n\nand asked to indicate where they though a fictional person would live within the city. Participants were shown one of two descriptions of the person.\n\n“Dr. Bennett lives in the city. He is a wealthy businessman who has travelled the world. He inherited a significant amount of money from a Great Aunt, and was educated at the best schools growing up. He enjoys fine dining and going to the theater on weekends.”\n\nOr\n\n“Mr. Bennett lives in the city. He is unemployed. He was born and raised in the city he now calls home. He struggles to pay the rent each month, and dropped out of high school before graduation. He enjoys a good hot dog and a six pack of beers when he can.”\n\nThe first description is a caricature of a person with high Socio-Economic Status (SES) whilst the second is a caricature of someone with a low SES. The hypothesis is that people in the United States and Hong Kong have different demographic knowledge that may shape their metaphoric link between positive association and cardinal direction (north vs. south).\n\n\n\nThis week we will use one-sample t-tests to explore whether participants from the UK would expect people with low or high SES to live north or south of the centre of the city.\n\n\n\n\n\n\nNoteA new hypothesis?\n\n\n\nBefore we start, think about what you would predict about participants from the UK. Is there a reason to expect that people in the UK would associate the north of a city with high SES or vice versa?\nMake a note of your answer and the reasons behind it. Note that there isn’t necessarily a correct answer to this question and we won’t ask you for it later.\nIt is good practice to be clear about our predictions and expectations before starting a data analysis.\n\n\n\n\n\nLet’s start by loading the dataset into Jamovi and checking out the data. You can use the file named RMB-2024_computer-practical_huang-2014_country-UK_cond-All.csv.\nThis dataset has several columns of data. The main column of interest is currently named variable. This represents the y-axis position of the persons expectation about where the fictional character would live. A value of zero represents the middle of the city, positive values represent more northern locations and negative values represent more southern locations.\nThe other key variable is named factor. This indicates which group the participant was allocated to. This can be either ‘high’ or ‘low’ depending on whether the participant saw a description of a person with stereotypes of high or low socioeconomic status.\nThis is open-access data and the column names aren’t very informative. It would be better to rename them to something more considered so that we can avoid mistakes later.\n\n\n\n\n\n\nNoteKey step\n\n\n\nRename the following variables using the editor under the ‘Variables’ tab.\n\n‘variable’ should be renamed to ‘NorthSouth_Position’, and\n‘factor’ should be renamed to ‘SES_Story’.\n\nDon’t include any spaces in the new variable names! this can cause an issue later.\n\n\n\nNext, take a look through the data itself. Remember the steps we took in the last couple of weeks. Have a careful scan through the data values and see if you can spot anything that needs to be fixed.\n\n\n\n\n\n\nCaution Data Skills - how many participants are included in the dataset?\n\n\n\n\n\nWe have 136 rows of data observations the spreadsheet.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - what experimental design was used in this experiment?\n\n\n\n\n\nThis is a between-subjects design, though that might not be obvious from the data alone! Checking through the paper methods we can see that each participant was either in the high or low SES condition.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - do we need to tidy up this dataset?\n\n\n\n\n\nNo, in this case the dataset looks very clean. We appear to have data for everyone without any obvious typos or missing information. Looks good to go.\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nCompute some descriptive statistics on the NorthSouth column. Make sure that you include the standard error of the mean!\nYou can split the descriptive statistics using a second variable. This\n\n\n\nNext, let’s think about what these values can tell us.\n\n\n\n\n\n\nCaution Data Skills - do we have an equal number of data points in each condition?\n\n\n\n\n\nNo, we have 75 participants in the High condition and 61 participants in the Low condition.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - which condition has a wider variation in NorthSouth Positions chosen by participants?\n\n\n\n\n\nWe can see this from the standard deviations. The High condition has a standard deviation of 63.2 and the Low condition has a standard deviation of 77.8.\nThe responses in the Low SES story condition were more variable.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - which condition has a more northern mean position?\n\n\n\n\n\nRecall from the introduction that positive values indicate a position in the north of the city. The High condition has a mean value of -39.0 and the Low condition has a mean value of 18.6\nThe responses in the Low SES story condition were more northern.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - can we tell from the descriptives whether this is a significant difference?\n\n\n\n\n\nNo! remember that descriptive statistics only summarise the data as it is. They cannot tell us whether we would expect what we see in that data to generalise beyond the specific data sample.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - are we more confident in our estimate of the mean for the ‘North’ or ‘South’ condition?\n\n\n\n\n\nThe standard error of the mean tells us how well we think our estimated mean from the data sample represents the ‘true’ population mean.\nThe standard error of the mean for the ‘Low’ group is higher (9.96 vs 7.3 for ‘High’) - indicating that we are less confident that this is a good representation of our population mean. Note that it could still be a good estimate, we’re just relatively less confident than we are for the ‘High’ condition.\n\n\n\n\n\n\nNext, we’re going to ask whether the participants in the either the High or Low SES conditions had a preference towards locating our fictional character in the North or South of the city. This is our one-sample t-test. The NorthSouth_Position is our data variable and the centre point of the city is the comparison value.\nFirst, we need to split out the data so that we can run the t-test using either the Low or High SES conditions, not using both mixed together.\nWe can do this in Jamovi using a ‘Filter’. Jamovi provides some great documentation on data filters on their website. Take a few minutes and read through the info on ‘Row Filters’ on this page.\nThe most important thing about filters is that they are reversible. We can apply a filter to run an analysis on a subset of the data without actually changing the dataset itself. This means we could make several different filters and turn them on and off to repeat the same analyses on different sections of the dataset. Convenient!\n\n\n\n\n\n\nNoteKey step\n\n\n\nFollowing the guidance on the Jamovi documentation on data filters, create a filter to select ONLY the participants who heard the low SES story.\nThe result should exclude the first 75 rows from your dataset, you can confirm this by looking at the ticks and crosses in the ‘Filter’ column, and by inspecting the values in the very bottom of the window.\n\n\n\nNote that your analyses in results windows will automatically update when you apply a filter to the data. With the filter applied, you will only see the descriptive statistics for the ‘Low’ SES_Story groups.\nIf you click the ‘active’ toggle in the ‘Filter’ window you can turn the filter off and your analysis will return to its initial state.\n\n\n\n\n\n\nNoteKey step\n\n\n\nWith your first Filter turned OFF, create a filter to select ONLY the participants who heard the High SES story.\nThe result should now exclude the last 61 rows from your dataset.\n You can turn your filters on and off to see how this changes the data available for the analysis, though turning both filters on will exclude all the data in this case…\n\n\n\n\n\nNow we’re ready to test our hypothesis! remember that the equation for a one-sample t-test is the following:\n\\[\nt = \\frac{\\text{The sample mean} - \\text{Comparison Value}}{\\text{The standard error of the mean}}\n\\]\nWe’re looking at the difference between our observed mean and a comparison value as a ratio to the precision to which we have estimated our observed mean.\nOur descriptive statistics output contains the mean and standard error of the mean for both groups. Our comparison value is the centre of the city, indicated by a score of 0 (zero).\nWe all the values required for a one-sample t-test already in the descriptive statistics! For the ‘Low’ SES_Story condition, we have:\n\nGroup Mean = 18.6\nComparison Value = 0\nStandard error of the mean = 9.96\n\nSo, the one sample t-test value is:\n\\[\nt = \\frac{18.6 - 0}{9.96} = 1.867\n\\]\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute the t-value for the High SES Story group using a calculator or Rj?\n\n\nFirst, get the three values that you’ll need for the computation. These are the comparison value and the mean and standard error of the mean for the High SES group.\n\n\nThe individual values are\n\nGroup Mean = -39.0\nComparison Value = 0\nStandard error of the mean = 7.30\n\nso the t-value is\n\\[\nt = \\frac{-39.0 - 0}{7.30} = -5.342\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nBefore going any further - make sure that your data filter is set to include only the data from the ‘Low’ SES_Story condition.\n\n\nOf course, we will normally compute the t-test using Jamovi as rather than relying on hand calculations. We can do this by opening the ‘One Sample T-Test’ option under the ‘Analysis -&gt; T-Tests’ menu on the top ribbon.\nOpen the One Sample T-Test option now, and drag the NorthSouth_Position variable over to the ‘dependent variables’ box. The result should appear on the right hand side. Before we inspect the result, we should check and add a number of important options to our analysis.\n\n\nHypothesis - test value it is critical that we select the correct hypothesis or our test will be meaningless. Our ‘test value’, also known as a ‘comparison value’, is set to zero by default. This is correct for our analysis but may need setting to something else for other analyses. Take care to check it makes sense each time.\nHypothesis - test tail we can also specify whether we’re going to perform a one- or two-tailed test. Here we want to specify a two-tailed test in which the mean does not equal the test value.\nAssumption Checks we should add a test to check that our data are normally distributed. Selecting the ‘Normality test’ box will compute a Shapio-Wilks statistic for you\nAdditional Statistics there are a number of important options here. For now, add the ‘Descriptive plots’ option to your analysis. This will show us a visualisation of the estimated mean and its confidence intervals.\n\nLet’s take a look at the outputs, first the t-test table.\n\nThis contains the information we need to report our t-test: the t-value, the degrees of freedom and the p-value.\n\nt(60) = 1.87, p = 0.066\n\nThis t-value should match the number you computed by hand earlier on. Note that the footnote of the table reminds us of the hypothesis that we specified. Make sure that this matches what you expect!\nNext, the normality check:\n\nThis reports the Shapiro-Wilk W statistic and associated p-value. In this case the p-value is not significant which indicates that the data are normally distributed and we’re happy to go ahead with the parametric t-test.\nNext, the descriptives plot:\n\nThis is a really useful visualisation of our test. We can see the mean of the data (black dot) and the test value (dotted line). We also have the 95% confidence intervals of the estimated mean. We can see that the confidence intervals overlap with the test value - this tells us that it is likely our test will not be significant - according to our confidence intervals, the true population mean might include our test value zero which would indicate no real difference.\n\n\n\n\n\n\nNoteKey step\n\n\n\nChange your data filter to compute the one-sample t-test for the ‘High’ SES_Story condition. The test should update once your filter is on without you having to respecify the test.\n\n\n\n\n\n\n\n\nCaution Data Skills - Is the mean position of participants in the ‘High’ condition significantly different to zero?\n\n\n\n\n\nYes, we do have a significant difference - we could report our test as follows\n\nt(74) = -5.35, p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Is it valid to use parametric statistics on this data?\n\n\n\n\n\nThe Shapiro Wilk statistic has a p-value of 0.038 which is less than p = 0.05. This suggets that our data are not normally distributed. We should inspect the data in more detail and consider using a non-parametric alternative.\nThe Wilcoxon Rank test is the non-parametric alternative for One Sample t-test. You can add this to the analysis to see if it gives the same result as the parametric test.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Do the confidence intervals of the estimated mean cross zero?\n\n\n\n\n\nNo - in the ‘High’ condition the confidence intervals of the mean are a long way from zero.\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nGo back to your notes from the start of the session. Do the results match your predictions about whether a UK data sample would expect a high or low socio-economic status people to live in the north or south of the city?\n\n\n\n\n\n\nFinally - we should make a reproducible version of our analysis using Rj.\n\n\n\n\n\n\nNoteKey step\n\n\n\nOpen a new Rj window and add the following line to load the dplyr library.\nlibrary(dplyr)\n\n\nFirstly, we’ll need to replicate our data filter to select only the ‘High’ or ‘Low’ conditions for our analysis. We covered this in detail last week - if you need a refresher, take a look at the information on selecting rows of data using the filter function from week 2\nLet’s build a filter to select only the ‘High’ condition.\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\nNext - turn on Jamovi’s syntax mode to display the R code associated with our analyses. This should create some R code at the top of each results block.\n\nWe can right click on this code to copy it before pasting it into the Rj window. Our code should then look like this:\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\n\njmv::ttestOneS(\n    data = data,\n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\n\n\n\n\n\n\nWarning\n\n\n\nSome people have encountered an issue with running this code block - if your R session stalls whilst running the code above, please try again whilst removing plots=TRUE.\nApologies - we’re working on a fix for this.\n\n\nThis command runs the function ttestOneS which computes the One Sample t-test. Jamovi has automatically detected that we want to add the normality test and the descriptive plots and included these options in the function call (see norm=TRUE and plots=TRUE).\nThere is a keyword argument in R for every option in Jamovi - take a look at the jmv library documentation for ttestOneS to see a full list.\nBefore running the code we need to make one adjustment. The function runs on the full dataset data by default but we want to use our new filtered data high_ses instead. Update the function to take in the filtered data as an input.\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\n\njmv::ttestOneS(\n    data = high_ses,\n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\nNow running the code should output result tables that match our previous analysis. Importantly the form of data preprocessing (the filtering) and the test itself are all clearly specified in the code. Making it easier for others to understand what analysis we have run.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you add a code block to compute the one-sample t-test for only the ‘Low’ SES_Story condition\n\n\nRemember that you’ll need to write a new filter line to create a copy of the dataset with only the ‘Low’ values for SES_Story. You can save this into a variable and pass it in as the data argument to ttestOneS\n\n\nYour final code should look like this:\nlow_ses &lt;- filter(data, SES_Story == 'Low')\n\njmv::ttestOneS(\n    data = low_ses, \n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\n\n\n\n\n\nYou can keep adding R code to your script to run multiple analyses at once, this way we can run the t-test for both conditions with out having to go back and forth in Jamovi toggling the data filters on and off.\nIt takes a little more setup but the R code has some big advantages once you get used to it!\n\n\n\nWe’ve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#the-dataset",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#the-dataset",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Metaphors are often used to help people understand abstract concepts in terms of perceptual experiences. Though this is often useful, it can bias perception and decision making in unexpected ways. For example, North and South are abstract concepts that have a spatial association (north = up, south = down). Participants in the USA have a biased expectation that richer people will live in the north of a city (Meier et al. 2011). However, these expectations are not uniform across all people and cultures and we should not naively generalise this result to all people. A follow up study comparing this effect in participants from the USA and Hong Kong replicated the original effect for American participants but found the opposite effect in people from Hong Kong (Huang, Tse, and Cho 2014).\nThe Many Labs 2 project (Klein et al. 2018) performed an additional replication of this effect across a wide range of countries. Participants were presented with the following map outline of a city:\n\nand asked to indicate where they though a fictional person would live within the city. Participants were shown one of two descriptions of the person.\n\n“Dr. Bennett lives in the city. He is a wealthy businessman who has travelled the world. He inherited a significant amount of money from a Great Aunt, and was educated at the best schools growing up. He enjoys fine dining and going to the theater on weekends.”\n\nOr\n\n“Mr. Bennett lives in the city. He is unemployed. He was born and raised in the city he now calls home. He struggles to pay the rent each month, and dropped out of high school before graduation. He enjoys a good hot dog and a six pack of beers when he can.”\n\nThe first description is a caricature of a person with high Socio-Economic Status (SES) whilst the second is a caricature of someone with a low SES. The hypothesis is that people in the United States and Hong Kong have different demographic knowledge that may shape their metaphoric link between positive association and cardinal direction (north vs. south).",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#the-challenge",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#the-challenge",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "This week we will use one-sample t-tests to explore whether participants from the UK would expect people with low or high SES to live north or south of the centre of the city.\n\n\n\n\n\n\nNoteA new hypothesis?\n\n\n\nBefore we start, think about what you would predict about participants from the UK. Is there a reason to expect that people in the UK would associate the north of a city with high SES or vice versa?\nMake a note of your answer and the reasons behind it. Note that there isn’t necessarily a correct answer to this question and we won’t ask you for it later.\nIt is good practice to be clear about our predictions and expectations before starting a data analysis.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#getting-started-with-descriptive-statistics",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#getting-started-with-descriptive-statistics",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Let’s start by loading the dataset into Jamovi and checking out the data. You can use the file named RMB-2024_computer-practical_huang-2014_country-UK_cond-All.csv.\nThis dataset has several columns of data. The main column of interest is currently named variable. This represents the y-axis position of the persons expectation about where the fictional character would live. A value of zero represents the middle of the city, positive values represent more northern locations and negative values represent more southern locations.\nThe other key variable is named factor. This indicates which group the participant was allocated to. This can be either ‘high’ or ‘low’ depending on whether the participant saw a description of a person with stereotypes of high or low socioeconomic status.\nThis is open-access data and the column names aren’t very informative. It would be better to rename them to something more considered so that we can avoid mistakes later.\n\n\n\n\n\n\nNoteKey step\n\n\n\nRename the following variables using the editor under the ‘Variables’ tab.\n\n‘variable’ should be renamed to ‘NorthSouth_Position’, and\n‘factor’ should be renamed to ‘SES_Story’.\n\nDon’t include any spaces in the new variable names! this can cause an issue later.\n\n\n\nNext, take a look through the data itself. Remember the steps we took in the last couple of weeks. Have a careful scan through the data values and see if you can spot anything that needs to be fixed.\n\n\n\n\n\n\nCaution Data Skills - how many participants are included in the dataset?\n\n\n\n\n\nWe have 136 rows of data observations the spreadsheet.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - what experimental design was used in this experiment?\n\n\n\n\n\nThis is a between-subjects design, though that might not be obvious from the data alone! Checking through the paper methods we can see that each participant was either in the high or low SES condition.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - do we need to tidy up this dataset?\n\n\n\n\n\nNo, in this case the dataset looks very clean. We appear to have data for everyone without any obvious typos or missing information. Looks good to go.\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nCompute some descriptive statistics on the NorthSouth column. Make sure that you include the standard error of the mean!\nYou can split the descriptive statistics using a second variable. This\n\n\n\nNext, let’s think about what these values can tell us.\n\n\n\n\n\n\nCaution Data Skills - do we have an equal number of data points in each condition?\n\n\n\n\n\nNo, we have 75 participants in the High condition and 61 participants in the Low condition.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - which condition has a wider variation in NorthSouth Positions chosen by participants?\n\n\n\n\n\nWe can see this from the standard deviations. The High condition has a standard deviation of 63.2 and the Low condition has a standard deviation of 77.8.\nThe responses in the Low SES story condition were more variable.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - which condition has a more northern mean position?\n\n\n\n\n\nRecall from the introduction that positive values indicate a position in the north of the city. The High condition has a mean value of -39.0 and the Low condition has a mean value of 18.6\nThe responses in the Low SES story condition were more northern.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - can we tell from the descriptives whether this is a significant difference?\n\n\n\n\n\nNo! remember that descriptive statistics only summarise the data as it is. They cannot tell us whether we would expect what we see in that data to generalise beyond the specific data sample.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - are we more confident in our estimate of the mean for the ‘North’ or ‘South’ condition?\n\n\n\n\n\nThe standard error of the mean tells us how well we think our estimated mean from the data sample represents the ‘true’ population mean.\nThe standard error of the mean for the ‘Low’ group is higher (9.96 vs 7.3 for ‘High’) - indicating that we are less confident that this is a good representation of our population mean. Note that it could still be a good estimate, we’re just relatively less confident than we are for the ‘High’ condition.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#data-filters-in-jamovi",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#data-filters-in-jamovi",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Next, we’re going to ask whether the participants in the either the High or Low SES conditions had a preference towards locating our fictional character in the North or South of the city. This is our one-sample t-test. The NorthSouth_Position is our data variable and the centre point of the city is the comparison value.\nFirst, we need to split out the data so that we can run the t-test using either the Low or High SES conditions, not using both mixed together.\nWe can do this in Jamovi using a ‘Filter’. Jamovi provides some great documentation on data filters on their website. Take a few minutes and read through the info on ‘Row Filters’ on this page.\nThe most important thing about filters is that they are reversible. We can apply a filter to run an analysis on a subset of the data without actually changing the dataset itself. This means we could make several different filters and turn them on and off to repeat the same analyses on different sections of the dataset. Convenient!\n\n\n\n\n\n\nNoteKey step\n\n\n\nFollowing the guidance on the Jamovi documentation on data filters, create a filter to select ONLY the participants who heard the low SES story.\nThe result should exclude the first 75 rows from your dataset, you can confirm this by looking at the ticks and crosses in the ‘Filter’ column, and by inspecting the values in the very bottom of the window.\n\n\n\nNote that your analyses in results windows will automatically update when you apply a filter to the data. With the filter applied, you will only see the descriptive statistics for the ‘Low’ SES_Story groups.\nIf you click the ‘active’ toggle in the ‘Filter’ window you can turn the filter off and your analysis will return to its initial state.\n\n\n\n\n\n\nNoteKey step\n\n\n\nWith your first Filter turned OFF, create a filter to select ONLY the participants who heard the High SES story.\nThe result should now exclude the last 61 rows from your dataset.\n You can turn your filters on and off to see how this changes the data available for the analysis, though turning both filters on will exclude all the data in this case…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#one-sample-t-tests",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#one-sample-t-tests",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Now we’re ready to test our hypothesis! remember that the equation for a one-sample t-test is the following:\n\\[\nt = \\frac{\\text{The sample mean} - \\text{Comparison Value}}{\\text{The standard error of the mean}}\n\\]\nWe’re looking at the difference between our observed mean and a comparison value as a ratio to the precision to which we have estimated our observed mean.\nOur descriptive statistics output contains the mean and standard error of the mean for both groups. Our comparison value is the centre of the city, indicated by a score of 0 (zero).\nWe all the values required for a one-sample t-test already in the descriptive statistics! For the ‘Low’ SES_Story condition, we have:\n\nGroup Mean = 18.6\nComparison Value = 0\nStandard error of the mean = 9.96\n\nSo, the one sample t-test value is:\n\\[\nt = \\frac{18.6 - 0}{9.96} = 1.867\n\\]\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute the t-value for the High SES Story group using a calculator or Rj?\n\n\nFirst, get the three values that you’ll need for the computation. These are the comparison value and the mean and standard error of the mean for the High SES group.\n\n\nThe individual values are\n\nGroup Mean = -39.0\nComparison Value = 0\nStandard error of the mean = 7.30\n\nso the t-value is\n\\[\nt = \\frac{-39.0 - 0}{7.30} = -5.342\n\\]",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#one-sample-t-tests-in-jamovi",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#one-sample-t-tests-in-jamovi",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "NoteKey step\n\n\n\nBefore going any further - make sure that your data filter is set to include only the data from the ‘Low’ SES_Story condition.\n\n\nOf course, we will normally compute the t-test using Jamovi as rather than relying on hand calculations. We can do this by opening the ‘One Sample T-Test’ option under the ‘Analysis -&gt; T-Tests’ menu on the top ribbon.\nOpen the One Sample T-Test option now, and drag the NorthSouth_Position variable over to the ‘dependent variables’ box. The result should appear on the right hand side. Before we inspect the result, we should check and add a number of important options to our analysis.\n\n\nHypothesis - test value it is critical that we select the correct hypothesis or our test will be meaningless. Our ‘test value’, also known as a ‘comparison value’, is set to zero by default. This is correct for our analysis but may need setting to something else for other analyses. Take care to check it makes sense each time.\nHypothesis - test tail we can also specify whether we’re going to perform a one- or two-tailed test. Here we want to specify a two-tailed test in which the mean does not equal the test value.\nAssumption Checks we should add a test to check that our data are normally distributed. Selecting the ‘Normality test’ box will compute a Shapio-Wilks statistic for you\nAdditional Statistics there are a number of important options here. For now, add the ‘Descriptive plots’ option to your analysis. This will show us a visualisation of the estimated mean and its confidence intervals.\n\nLet’s take a look at the outputs, first the t-test table.\n\nThis contains the information we need to report our t-test: the t-value, the degrees of freedom and the p-value.\n\nt(60) = 1.87, p = 0.066\n\nThis t-value should match the number you computed by hand earlier on. Note that the footnote of the table reminds us of the hypothesis that we specified. Make sure that this matches what you expect!\nNext, the normality check:\n\nThis reports the Shapiro-Wilk W statistic and associated p-value. In this case the p-value is not significant which indicates that the data are normally distributed and we’re happy to go ahead with the parametric t-test.\nNext, the descriptives plot:\n\nThis is a really useful visualisation of our test. We can see the mean of the data (black dot) and the test value (dotted line). We also have the 95% confidence intervals of the estimated mean. We can see that the confidence intervals overlap with the test value - this tells us that it is likely our test will not be significant - according to our confidence intervals, the true population mean might include our test value zero which would indicate no real difference.\n\n\n\n\n\n\nNoteKey step\n\n\n\nChange your data filter to compute the one-sample t-test for the ‘High’ SES_Story condition. The test should update once your filter is on without you having to respecify the test.\n\n\n\n\n\n\n\n\nCaution Data Skills - Is the mean position of participants in the ‘High’ condition significantly different to zero?\n\n\n\n\n\nYes, we do have a significant difference - we could report our test as follows\n\nt(74) = -5.35, p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Is it valid to use parametric statistics on this data?\n\n\n\n\n\nThe Shapiro Wilk statistic has a p-value of 0.038 which is less than p = 0.05. This suggets that our data are not normally distributed. We should inspect the data in more detail and consider using a non-parametric alternative.\nThe Wilcoxon Rank test is the non-parametric alternative for One Sample t-test. You can add this to the analysis to see if it gives the same result as the parametric test.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Do the confidence intervals of the estimated mean cross zero?\n\n\n\n\n\nNo - in the ‘High’ condition the confidence intervals of the mean are a long way from zero.\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nGo back to your notes from the start of the session. Do the results match your predictions about whether a UK data sample would expect a high or low socio-economic status people to live in the north or south of the city?",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#reproducible-one-sample-t-tests",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#reproducible-one-sample-t-tests",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Finally - we should make a reproducible version of our analysis using Rj.\n\n\n\n\n\n\nNoteKey step\n\n\n\nOpen a new Rj window and add the following line to load the dplyr library.\nlibrary(dplyr)\n\n\nFirstly, we’ll need to replicate our data filter to select only the ‘High’ or ‘Low’ conditions for our analysis. We covered this in detail last week - if you need a refresher, take a look at the information on selecting rows of data using the filter function from week 2\nLet’s build a filter to select only the ‘High’ condition.\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\nNext - turn on Jamovi’s syntax mode to display the R code associated with our analyses. This should create some R code at the top of each results block.\n\nWe can right click on this code to copy it before pasting it into the Rj window. Our code should then look like this:\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\n\njmv::ttestOneS(\n    data = data,\n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\n\n\n\n\n\n\nWarning\n\n\n\nSome people have encountered an issue with running this code block - if your R session stalls whilst running the code above, please try again whilst removing plots=TRUE.\nApologies - we’re working on a fix for this.\n\n\nThis command runs the function ttestOneS which computes the One Sample t-test. Jamovi has automatically detected that we want to add the normality test and the descriptive plots and included these options in the function call (see norm=TRUE and plots=TRUE).\nThere is a keyword argument in R for every option in Jamovi - take a look at the jmv library documentation for ttestOneS to see a full list.\nBefore running the code we need to make one adjustment. The function runs on the full dataset data by default but we want to use our new filtered data high_ses instead. Update the function to take in the filtered data as an input.\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\n\njmv::ttestOneS(\n    data = high_ses,\n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\nNow running the code should output result tables that match our previous analysis. Importantly the form of data preprocessing (the filtering) and the test itself are all clearly specified in the code. Making it easier for others to understand what analysis we have run.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you add a code block to compute the one-sample t-test for only the ‘Low’ SES_Story condition\n\n\nRemember that you’ll need to write a new filter line to create a copy of the dataset with only the ‘Low’ values for SES_Story. You can save this into a variable and pass it in as the data argument to ttestOneS\n\n\nYour final code should look like this:\nlow_ses &lt;- filter(data, SES_Story == 'Low')\n\njmv::ttestOneS(\n    data = low_ses, \n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\n\n\n\n\n\nYou can keep adding R code to your script to run multiple analyses at once, this way we can run the t-test for both conditions with out having to go back and forth in Jamovi toggling the data filters on and off.\nIt takes a little more setup but the R code has some big advantages once you get used to it!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#summary",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#summary",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "We’ve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html",
    "href": "tutorials/rmb-week-9_one-way-anova.html",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "This week we will explore how to run ANOVA analyses to compare means between two or more groups. This is an important analysis as many experimental and scientific contexts cannot be simiplified down to two groups for a t-test, and running many t-tests would increase our chances of false positive findings..\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nOne-Way ANOVA\n\n\n\nLog transforms\n\n\n\nAssumptions of ANOVA\n\n\n\nEstimated Marginal Means\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nRun and explore ANOVA analyses in Jamovi\n\n\n\nCompute data transformations using Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWork with a published dataset\n\n\n\n\n\n\n\n\n\nNoteContent Info\n\n\n\nThis is the same dataset that we used in week 7.\n\n\n\n\n\n\n\n\nNoteContent Info\n\n\n\nThis tutorial refers to a video in which participants watched unpleasant events to simulate a traumatic experience in the lab. The content of this video is not disclosed or dicussed in any more detail than this during this tutorial - those who are interested can find more information in methods section the published article. This tutorial does explore the emotional impact that the video has on participants.\n\n\n\n\nFlashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we’re going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind’s eye and cause distress. We investigated whether reconsolidation—the process during which memories become malleable when recalled—can be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group.\n\n\n\nThis week - we will explore two of the four conditions to test the following hypothesis:\n\nEngaging in computer game and reactivation tasks after a trumatic event will reduce number of subsequent intrusive memories\n\nThis is similar to the hypothesis from week 7, but now we have allowed for several conditions in the experiment rather than just two.\nIf we find evidence for a difference in our omnibus ANOVA test we can explore post-hoc comparisons to describe the exact difference that is present.\n\n\n\n\n\n\nNoteKey step\n\n\n\nOpen Jamovi and load in this weeks data file before going any further.\n\n\n\n\n\nAt this point - you can probably figure out how to run the basics of an ANOVA analysis in Jamovi. There are a lot of similarities with the t-test analyses we have run.\nTo run the ANOVA, open the ANOVA -&gt; ANOVA menu from the top ribbon.\n\n\n\n\n\n\nWarningJamovi Note!\n\n\n\nMake sure you open the ‘ANOVA’ menu not the ‘One-way ANOVA’ menu. We will run a One-way ANOVA but will use the full ANOVA menu as it contains the full set of features and analyses that we will need. The One-way ANOVA menu is a streamlined option for very simple analyses.\n\n\nOnce you have the menu open, click Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary across to the Dependent Variable box and Condition across to the ‘Fixed Factors’. The first ANOVA results table will appear in the results.\nVery simple! but remember that it takes some additional work to check over and interpret the results from an ANOVA.\n\nLet’s understand the results table in detail. First let’s quickly review the theory:\nIn an ANOVA analysis we’re comparing the sum-of-squared error between two different models - one model has all groups with the same mean and the second model gives each group its individual mean. From this we can compute three different sets of sum-sqaures.\n\n\nSum-Square Error Total - the overall variability assuming that all groups have the same mean\nSum-Square Error Within - the variability assuming that each group has its own mean\nSum-Square Error Between - the change variability attributable to giving each group its own mean\n\nThe rationale is that if the groups have distinct means then Sum-Square Within will be much smaller than Sum-Square Total. This difference is quantified with Sum-Square Between.\nThese values are listed in the ANOVA table. The ‘Sum of Squares’ for Condition is the Sum-Square Between and the ‘Sum of Squares’ for Residuals is Sum-Square Within. (This terminology can get a little complex…).\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table to complete this statement reporting the results of the ANOVA. Fill in the spaces provided.\nThe Sum-Square Error Between is  and the Sum-Square Error Within is .\n\n\nThe ANOVA table from above…\n\n\n\n\n\n\nThe next step is to compute the Mean-Squares. We need this additional step as the Sum-Squares will are senstive to the sample size and number of groups - we have to make a correction for this. The Mean-Squares are computed with two fractions.\n\\[\n\\text{Mean-Square Error Within} = \\frac{\\text{Sum-Square Error Within}}{\\text{Number of Participants} - \\text{Number of Groups}}\n\\]\n\\[\n\\text{Mean-Square Error Between} = \\frac{\\text{Sum-Square Error Between}}{\\text{Number of Groups} - 1}\n\\]\nThe ingredients for these fractions are already in our ANOVA table. The top of the fraction is the Sum of Squares we’ve seen earlier. The values on the bottom of the fraction are actually precomputed for us as the Degrees of Freedom.\nThe degrees of freedom for Condition (df between) is \\(\\text{Number of Groups} - 1\\). In our case we have four groups so the value is 3. The degrees of freedom for Residuals (df within) is \\(\\text{Number of Participants} - \\text{Number of Groups}\\). We have 72 participants and 4 groups so the value is 68.\nSo, working across the ANOVA table in rows - we can see that the value of the Mean-Square Errors in third column is simply the first column divided by the second.\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table to complete this statement reporting the results of the ANOVA. Fill in the spaces provided.\nThe Mean-Square Error Between is:\n\\(\\frac{\\text{Sum-Square Error Between}}{\\text{Number of Groups} - 1}\\) = \\(\\frac{\\text{Sum-Square Error Between}}{\\text{df between}}\\) = \\(\\frac{115}{3}\\) = \nThe Mean-Square Error Within is:\n\\(\\frac{\\text{Sum-Square Error Within}}{\\text{Number of Participants} - \\text{Number of Groups}}\\) = \\(\\frac{\\text{Sum-Square Error Between}}{\\text{df within}}\\) = \\(\\frac{686}{68}\\) = \n\n\nThe ANOVA table from above…\n\n\n\n\n\n\nWe now have the ingredients for our ANOVA! The F-statistics for the ANOVA is\n\\[\n\\text{F} = \\frac{\\text{Mean-Square Error Between}}{\\text{Mean-Square Error Within}}\n\\]\nThis is the ratio of our two Mean-Square fractions from above.\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table to complete this statement reporting the results of the ANOVA. Fill in the spaces provided.\nThe F statistic is:\n\\(\\frac{\\text{Mean-Square Error Between}}{\\text{Mean-Square Error Within}}\\) = \\(\\frac{38.3}{10.1}\\) = \n\n\nThe ANOVA table from above…\n\n\n\n\n\n\nThere are a lot of values in the ANOVA table but the relationship between them is straightforward when you know the logic behind it.\nTo report a one-way ANOVA to APA format you can follow this broad template.\n\nA one-way ANOVA was conducted to compare the effect of [independent variable] on [dependent variable]. There was a statistically significant difference in [dependent variable] between groups, F([df between], [df within]) = [F value], p = [p value].\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table to complete this statement reporting the results of the ANOVA. Fill in the spaces provided.\nA one-way ANOVA was conducted to compare the effect of the number of intrusive memories in the week after the experimenttask completed after watching the film on the number of intrusive memories in the week after the experimenttask completed after watching the film . There was a statistically significantno significant difference in number of intrusive memoriespost film task between groups, F(, ) = , p = .\n\n\nThe ANOVA table from above…\n\n\n\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nOpen a new ANOVA analysis and run a one-way ANOVA to test the control condition asking whether there was a difference between groups in intrusive memories in the time before the experiment started.\nThe variable Days_Zero_Image_Based_Intrusions_in_Intrusion_Diary\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table from your new control analysis. Fill in the spaces provided.\nA one-way ANOVA was conducted to compare the effect of number of intrusive memories in the time before the experimentthe task completed after watching the film on number of intrusive memories in the time before the experimentthe task completed after watching the film. There was a statistically significantno significant difference in the number of intrusive memoriesthe task completed after watching the film between groups, F(, ) = , p = .\n\n\nThe ANOVA table from the control analysis.\n\n\n\n\n\n\n\n\n\nThere are several possible effect sizes we can use for ANOVA analyses. We will use \\(\\eta^2\\) (pronouced eta-squared) for one-way ANOVA. The Learning Statistics with Jamovi textbook has a nice summary of how to interpret \\(\\eta^2\\).\n\nThe interpretation of \\(\\eta^2\\) is equally straightforward. It refers to the proportion of the variability in the outcome variable that can be explained in terms of the predictor. A value of 0 means that there is no relationship at all between the two, whereas a value of 1 means that the relationship is perfect. Better yet, the value is very closely related to \\(R^2\\) and has an equivalent interpretation\n\nSlightly abridged quote from Learning Statistics with Jamovi\n\n\nVery straightforward indeed, we can think of \\(\\eta^2\\) as an \\(R^2\\) for the ANOVA. It is also straightforward to compute - just check the box under ‘Effect Sizes’.\n\nThere are two other options for ANOVA effect sizes:\n\npartial \\(\\eta^2\\) will be useful for more complex ANOVA designs you will work with in other modules.\n\\(\\omega^2\\) is an unbiased estimator that could be used in place of \\(\\eta^2\\). Some sources recommend that we use \\(\\omega^2\\) rather than \\(\\eta^2\\) (Albers and Lakens 2018). You could use either and might encounter either in the literature.\n\n\n\n\nAn ANOVA analysis returns a significant effect that indicates that there is a difference between the group means but doesn’t tell us anything about which groups are actually different.\nIf, and only if, the overall ANOVA returns a significant result - we can use post-hoc tests to explore what the specific differences actually are. This is a critical step for the interpretation of our results.\nIn the example above we saw that the experimental task that participants complete after watching the film does change the mean number of intrusive memories that the participants experience. The ANOVA table doesn’t say more than this - as far as we know from this analysis it is possible that playing Tetris may have increased or decreased the number of intrusive memories.\nPost-hoc tests are the answer for this - and post hoc tests are based on ‘Estimated Marginal Means’. These sound more complex than they are. Estimated marginal means are adjusted means (and associated standard errors etc) that account for the assumptions of a statistical model and for the effects of any other variables in a model.\nSo, for an ANOVA the estimated means are adjusted for the assumption of homogeneity of variance - that each group has the same variance. We do not have any other variables to adjust for in this analysis but this is relevant for future Analysis of Co-Variance (ANCOVA) models that you may encounter in future modules.\n\n\n\n\n\n\nNoteKey step\n\n\n\nFind the ‘Estimated Marginal Means’ drop down menu and add ‘Condition’ to the Terms box to compute the estimated marginal mean for each group within Condition. Make sure to include the plots and tables in the ouputs (though the x axis labels for this particular plot aren’t very good…).\n\n\n\nThe table gives us our four marginal means summarising the mean of each group alongside a 95% confidence interval for that estimate and the Standard Error for each mean.\nThe chart gives us a more human readable summary of the means with the associated confidence intervals.\n\n\n\n\n\n\nCaution Data Skills - which conditions do you think are likely to be significantly different?\n\n\n\n\n\nThe No-Task Control and Reactivation+Tetris conditions have the largest difference in mean, with Tetris Only and Reactivation Only somewhere in between.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - why are the standard errors for each mean all the same?\n\n\n\n\n\nRemember that these are the means computed under the assumptions of the ANOVA, not the normal means we see in the descriptive statistics.\nThis ANOVA assumes homogeneity of variance and the marginal mean can take this into account. If we computed the descriptive statistics the usual way we would see the individual estimate of the standard error for each group.\n\n\n\nSo, lets run our post-hoc tests!\n\n\n\n\n\n\nNoteKey step\n\n\n\nFind the ‘Post-Hoc Tests’ drop down menu and add ‘Condition’ to the right hand box to compute the post hoc comparison between each pair of groups within Condition.\nMake sure to include ‘Tukey’ correction and add the Cohen’s d effect size.\n\n\n\nThe Comparison columns lists the six pairwise comparisons that can be assembled from our four conditions. As before, the standard errors (SE column) are the same for all of the comparisons. Remember that these results are based on the estimated marginal means rather than the standard means we would use outside an ANOVA.\n\n\n\n\n\n\nCaution Data Skills - which comparisons show a significant difference?\n\n\n\n\n\nThe No-Task Control and Reactivation+Tetris conditions show a significant difference t(68) = 3.044, p-tukey = 0.017.\nThe Reactivation+Tetris and Reactivation Only conditions show a significant difference t(68) = -2.781, p-tukey = 0.034.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - which direction are the significant differences? ie which condition is larger?\n\n\n\n\n\nWe can use the estimated marginal means tables to be sure of these results.\nThe No-Task Control group (M=5.11) had more intrusive memories than the Reactivation+Tetris group (M=1.89).\nThe Reactivation+Tetris group (M=1.89) had fewer intrusive memories than the Reactivation Only group (M=4.83).\n\n\n\n\n\n\n\n\n\nCaution Data Skills - How do the results change if we use no correction to the p-values rather than Tukey correction?\n\n\n\n\n\nYou can add the uncorrected p-value to the output table in the Post Hoc tests menu.\nThe Tukey correction aims to account for the multiple comparisons that we are making. It increases the p-values to make it less likely that we would consider a difference to be significant.\nCompare the p column with the p-tukey column. The uncorrected values are smaller - the Reactivation+Tetris to Tetris Only comparison is close to being significant in the uncorrected values.\n\n\n\n\n\n\n\nAs ever - the ANOVA makes certain assumptions about the data so that we can perform our statistics. These assumptions are similar to what we saw with t-tests and should all be in place for our ANOVA results to make a valid generalisation to the wide population.\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nMatch the ANOVA assumption to its definition.\n\n\n\n\n\n\n\nAssumption\nDefinition\n\n\n\n\nIndependence\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\nNormal distributions\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\nHomogeneity of variance\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\nCategorical factors\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\nData type\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\n\n\n\nTake a look at your lecture notes from week 8!\n\n\n\n\n\nIt is very straighforward to check these assumptions in Jamovi - find the ‘Assumption Checks’ menu and select all three options to run the Shapiro-Wilk test, Levene’s Test and the QQ plot.\n\n\n\n\n\n\n\nCaution Data Skills - is the assumption of independence met?\n\n\n\n\n\nYes, this is a between subjects design in which each participant only contributes a single data point to this study. ANOVA analysis for repeated measures designs are possible - you might encounter these in a future module.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - is the assumption of homogeneity of variance met?\n\n\n\n\n\nNo, Levene’s test is significant indicating that the groups do not have equal variance.\nWe can explore this further by running descriptive statistics on the groups. We can see that the standard deviation of the Reactivation+Tetris group looks to be much smaller than the others. This is likely where the difference is coming from.\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - is the for normally distributed data met?\n\n\n\n\n\nNo, Shapiro-Wilk’s test is significant indicating that the data is not normally distributed. This is confirmed by the QQ plot in which the top end of the data depart from the centre diagonal line.\nThe estimated marginal means plot from your earlier analysis can help explain why this is. Notice that the data points in that plot have a positive skew - ie a small number of participants seem to have a lot of intrusive memories.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - is the assumption of categorical factors met?\n\n\n\n\n\nYes, our Conditions variable is categorical. It contains our four groups without any implied order.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - is the assumed data type correct?\n\n\n\n\n\nYes, our Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary variable is ratio data. It contains numerical data in which zero means an absence of intrusive memories.\n\n\n\n\n\n\nLooks like our data do not meet the parametric assumptions of a standard ANOVA analysis. Its often a good idea to check assumptions early in the data analysis process…\nThere are non-parametric ANOVA analyses available - you could now run the Kruskall Wallis test on the data to perform a non parametric one way ANOVA.\nLet’s try a data transformation here though, the full parametric ANOVA is very flexible so it would be good to use it if we can.\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint - ANOVA TableHint - Assumption ChecksHint - Estimated Marginal MeansHint - Post Hoc Tests\n\n\nCompute a new data variable using the log transform on the Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary - review week 7 section 7 if you need reminder on how to do this.\nUse the results to complete this reporting of the ANOVA results.\n\nA Shaprio-Wilk test showed a significant departure from a normal distribution, W=0.877, p&lt;0.001. The data were transformed using a log transform.\nA second Shaprio-Wilk test showed that the log-transformed data is normally distributednot normally distributed, W=, p=. Levene’s test on the transformed data shows that we can assumewe can not assume homogeneity of variance across the groups, F(, ) = , p=\nA one-way ANOVA was conducted to on the transformed data to compare the effect of the number of intrusive memories in the week after the experimenttask completed after watching the film on the number of intrusive memories in the week after the experimenttask completed after watching the film. There was a statistically significantno significant difference in number of intrusive memoriespost film task between groups, F(, ) = , p = , \\(\\eta^2\\) = .\nPost hoc comparisons with Tukey correction were conducted to identify which pairs of conditions showed significant differences in their estimated marginal means. Three pairwise comparisons showed a significant difference. Note that we are reporting the log-transformed means with the post-hoc test, not the original means.\nThe ‘No-Task Control’ group (M=) had more instrusive memories than the ‘Reactivation+Tetris’ group (M=), t() = , p=, Cohen’s d=\nThe ‘Reactivation+Tetris’ group (M=) had fewer instrusive memories than the ‘Tetris Only’ group (M=), t() = , p=, Cohen’s d=.\nThe ‘Reactivation+Tetris’ group (M=) had fewer instrusive memories than the ‘Reactivation Only’ group (M=), t() = , p=, Cohen’s d=.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA is a flexible and powerful method for comparing the means of more than two groups whilst accounting for the exploding number of multiple comparisons we get from more complex experimental designs. ANOVA takes a bit more work to run, check and interpret than a t-test but the results from a single ANOVA analysis contain a huge amount of critical information about our data!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#the-dataset",
    "href": "tutorials/rmb-week-9_one-way-anova.html#the-dataset",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Flashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we’re going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind’s eye and cause distress. We investigated whether reconsolidation—the process during which memories become malleable when recalled—can be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#the-challenge",
    "href": "tutorials/rmb-week-9_one-way-anova.html#the-challenge",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "This week - we will explore two of the four conditions to test the following hypothesis:\n\nEngaging in computer game and reactivation tasks after a trumatic event will reduce number of subsequent intrusive memories\n\nThis is similar to the hypothesis from week 7, but now we have allowed for several conditions in the experiment rather than just two.\nIf we find evidence for a difference in our omnibus ANOVA test we can explore post-hoc comparisons to describe the exact difference that is present.\n\n\n\n\n\n\nNoteKey step\n\n\n\nOpen Jamovi and load in this weeks data file before going any further.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#analysis-of-variance-in-jamovi",
    "href": "tutorials/rmb-week-9_one-way-anova.html#analysis-of-variance-in-jamovi",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "At this point - you can probably figure out how to run the basics of an ANOVA analysis in Jamovi. There are a lot of similarities with the t-test analyses we have run.\nTo run the ANOVA, open the ANOVA -&gt; ANOVA menu from the top ribbon.\n\n\n\n\n\n\nWarningJamovi Note!\n\n\n\nMake sure you open the ‘ANOVA’ menu not the ‘One-way ANOVA’ menu. We will run a One-way ANOVA but will use the full ANOVA menu as it contains the full set of features and analyses that we will need. The One-way ANOVA menu is a streamlined option for very simple analyses.\n\n\nOnce you have the menu open, click Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary across to the Dependent Variable box and Condition across to the ‘Fixed Factors’. The first ANOVA results table will appear in the results.\nVery simple! but remember that it takes some additional work to check over and interpret the results from an ANOVA.\n\nLet’s understand the results table in detail. First let’s quickly review the theory:\nIn an ANOVA analysis we’re comparing the sum-of-squared error between two different models - one model has all groups with the same mean and the second model gives each group its individual mean. From this we can compute three different sets of sum-sqaures.\n\n\nSum-Square Error Total - the overall variability assuming that all groups have the same mean\nSum-Square Error Within - the variability assuming that each group has its own mean\nSum-Square Error Between - the change variability attributable to giving each group its own mean\n\nThe rationale is that if the groups have distinct means then Sum-Square Within will be much smaller than Sum-Square Total. This difference is quantified with Sum-Square Between.\nThese values are listed in the ANOVA table. The ‘Sum of Squares’ for Condition is the Sum-Square Between and the ‘Sum of Squares’ for Residuals is Sum-Square Within. (This terminology can get a little complex…).\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table to complete this statement reporting the results of the ANOVA. Fill in the spaces provided.\nThe Sum-Square Error Between is  and the Sum-Square Error Within is .\n\n\nThe ANOVA table from above…\n\n\n\n\n\n\nThe next step is to compute the Mean-Squares. We need this additional step as the Sum-Squares will are senstive to the sample size and number of groups - we have to make a correction for this. The Mean-Squares are computed with two fractions.\n\\[\n\\text{Mean-Square Error Within} = \\frac{\\text{Sum-Square Error Within}}{\\text{Number of Participants} - \\text{Number of Groups}}\n\\]\n\\[\n\\text{Mean-Square Error Between} = \\frac{\\text{Sum-Square Error Between}}{\\text{Number of Groups} - 1}\n\\]\nThe ingredients for these fractions are already in our ANOVA table. The top of the fraction is the Sum of Squares we’ve seen earlier. The values on the bottom of the fraction are actually precomputed for us as the Degrees of Freedom.\nThe degrees of freedom for Condition (df between) is \\(\\text{Number of Groups} - 1\\). In our case we have four groups so the value is 3. The degrees of freedom for Residuals (df within) is \\(\\text{Number of Participants} - \\text{Number of Groups}\\). We have 72 participants and 4 groups so the value is 68.\nSo, working across the ANOVA table in rows - we can see that the value of the Mean-Square Errors in third column is simply the first column divided by the second.\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table to complete this statement reporting the results of the ANOVA. Fill in the spaces provided.\nThe Mean-Square Error Between is:\n\\(\\frac{\\text{Sum-Square Error Between}}{\\text{Number of Groups} - 1}\\) = \\(\\frac{\\text{Sum-Square Error Between}}{\\text{df between}}\\) = \\(\\frac{115}{3}\\) = \nThe Mean-Square Error Within is:\n\\(\\frac{\\text{Sum-Square Error Within}}{\\text{Number of Participants} - \\text{Number of Groups}}\\) = \\(\\frac{\\text{Sum-Square Error Between}}{\\text{df within}}\\) = \\(\\frac{686}{68}\\) = \n\n\nThe ANOVA table from above…\n\n\n\n\n\n\nWe now have the ingredients for our ANOVA! The F-statistics for the ANOVA is\n\\[\n\\text{F} = \\frac{\\text{Mean-Square Error Between}}{\\text{Mean-Square Error Within}}\n\\]\nThis is the ratio of our two Mean-Square fractions from above.\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table to complete this statement reporting the results of the ANOVA. Fill in the spaces provided.\nThe F statistic is:\n\\(\\frac{\\text{Mean-Square Error Between}}{\\text{Mean-Square Error Within}}\\) = \\(\\frac{38.3}{10.1}\\) = \n\n\nThe ANOVA table from above…\n\n\n\n\n\n\nThere are a lot of values in the ANOVA table but the relationship between them is straightforward when you know the logic behind it.\nTo report a one-way ANOVA to APA format you can follow this broad template.\n\nA one-way ANOVA was conducted to compare the effect of [independent variable] on [dependent variable]. There was a statistically significant difference in [dependent variable] between groups, F([df between], [df within]) = [F value], p = [p value].\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table to complete this statement reporting the results of the ANOVA. Fill in the spaces provided.\nA one-way ANOVA was conducted to compare the effect of the number of intrusive memories in the week after the experimenttask completed after watching the film on the number of intrusive memories in the week after the experimenttask completed after watching the film . There was a statistically significantno significant difference in number of intrusive memoriespost film task between groups, F(, ) = , p = .\n\n\nThe ANOVA table from above…\n\n\n\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nOpen a new ANOVA analysis and run a one-way ANOVA to test the control condition asking whether there was a difference between groups in intrusive memories in the time before the experiment started.\nThe variable Days_Zero_Image_Based_Intrusions_in_Intrusion_Diary\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the ANOVA table from your new control analysis. Fill in the spaces provided.\nA one-way ANOVA was conducted to compare the effect of number of intrusive memories in the time before the experimentthe task completed after watching the film on number of intrusive memories in the time before the experimentthe task completed after watching the film. There was a statistically significantno significant difference in the number of intrusive memoriesthe task completed after watching the film between groups, F(, ) = , p = .\n\n\nThe ANOVA table from the control analysis.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#effect-size-for-anova",
    "href": "tutorials/rmb-week-9_one-way-anova.html#effect-size-for-anova",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "There are several possible effect sizes we can use for ANOVA analyses. We will use \\(\\eta^2\\) (pronouced eta-squared) for one-way ANOVA. The Learning Statistics with Jamovi textbook has a nice summary of how to interpret \\(\\eta^2\\).\n\nThe interpretation of \\(\\eta^2\\) is equally straightforward. It refers to the proportion of the variability in the outcome variable that can be explained in terms of the predictor. A value of 0 means that there is no relationship at all between the two, whereas a value of 1 means that the relationship is perfect. Better yet, the value is very closely related to \\(R^2\\) and has an equivalent interpretation\n\nSlightly abridged quote from Learning Statistics with Jamovi\n\n\nVery straightforward indeed, we can think of \\(\\eta^2\\) as an \\(R^2\\) for the ANOVA. It is also straightforward to compute - just check the box under ‘Effect Sizes’.\n\nThere are two other options for ANOVA effect sizes:\n\npartial \\(\\eta^2\\) will be useful for more complex ANOVA designs you will work with in other modules.\n\\(\\omega^2\\) is an unbiased estimator that could be used in place of \\(\\eta^2\\). Some sources recommend that we use \\(\\omega^2\\) rather than \\(\\eta^2\\) (Albers and Lakens 2018). You could use either and might encounter either in the literature.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#post-hoc-tests",
    "href": "tutorials/rmb-week-9_one-way-anova.html#post-hoc-tests",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "An ANOVA analysis returns a significant effect that indicates that there is a difference between the group means but doesn’t tell us anything about which groups are actually different.\nIf, and only if, the overall ANOVA returns a significant result - we can use post-hoc tests to explore what the specific differences actually are. This is a critical step for the interpretation of our results.\nIn the example above we saw that the experimental task that participants complete after watching the film does change the mean number of intrusive memories that the participants experience. The ANOVA table doesn’t say more than this - as far as we know from this analysis it is possible that playing Tetris may have increased or decreased the number of intrusive memories.\nPost-hoc tests are the answer for this - and post hoc tests are based on ‘Estimated Marginal Means’. These sound more complex than they are. Estimated marginal means are adjusted means (and associated standard errors etc) that account for the assumptions of a statistical model and for the effects of any other variables in a model.\nSo, for an ANOVA the estimated means are adjusted for the assumption of homogeneity of variance - that each group has the same variance. We do not have any other variables to adjust for in this analysis but this is relevant for future Analysis of Co-Variance (ANCOVA) models that you may encounter in future modules.\n\n\n\n\n\n\nNoteKey step\n\n\n\nFind the ‘Estimated Marginal Means’ drop down menu and add ‘Condition’ to the Terms box to compute the estimated marginal mean for each group within Condition. Make sure to include the plots and tables in the ouputs (though the x axis labels for this particular plot aren’t very good…).\n\n\n\nThe table gives us our four marginal means summarising the mean of each group alongside a 95% confidence interval for that estimate and the Standard Error for each mean.\nThe chart gives us a more human readable summary of the means with the associated confidence intervals.\n\n\n\n\n\n\nCaution Data Skills - which conditions do you think are likely to be significantly different?\n\n\n\n\n\nThe No-Task Control and Reactivation+Tetris conditions have the largest difference in mean, with Tetris Only and Reactivation Only somewhere in between.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - why are the standard errors for each mean all the same?\n\n\n\n\n\nRemember that these are the means computed under the assumptions of the ANOVA, not the normal means we see in the descriptive statistics.\nThis ANOVA assumes homogeneity of variance and the marginal mean can take this into account. If we computed the descriptive statistics the usual way we would see the individual estimate of the standard error for each group.\n\n\n\nSo, lets run our post-hoc tests!\n\n\n\n\n\n\nNoteKey step\n\n\n\nFind the ‘Post-Hoc Tests’ drop down menu and add ‘Condition’ to the right hand box to compute the post hoc comparison between each pair of groups within Condition.\nMake sure to include ‘Tukey’ correction and add the Cohen’s d effect size.\n\n\n\nThe Comparison columns lists the six pairwise comparisons that can be assembled from our four conditions. As before, the standard errors (SE column) are the same for all of the comparisons. Remember that these results are based on the estimated marginal means rather than the standard means we would use outside an ANOVA.\n\n\n\n\n\n\nCaution Data Skills - which comparisons show a significant difference?\n\n\n\n\n\nThe No-Task Control and Reactivation+Tetris conditions show a significant difference t(68) = 3.044, p-tukey = 0.017.\nThe Reactivation+Tetris and Reactivation Only conditions show a significant difference t(68) = -2.781, p-tukey = 0.034.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - which direction are the significant differences? ie which condition is larger?\n\n\n\n\n\nWe can use the estimated marginal means tables to be sure of these results.\nThe No-Task Control group (M=5.11) had more intrusive memories than the Reactivation+Tetris group (M=1.89).\nThe Reactivation+Tetris group (M=1.89) had fewer intrusive memories than the Reactivation Only group (M=4.83).\n\n\n\n\n\n\n\n\n\nCaution Data Skills - How do the results change if we use no correction to the p-values rather than Tukey correction?\n\n\n\n\n\nYou can add the uncorrected p-value to the output table in the Post Hoc tests menu.\nThe Tukey correction aims to account for the multiple comparisons that we are making. It increases the p-values to make it less likely that we would consider a difference to be significant.\nCompare the p column with the p-tukey column. The uncorrected values are smaller - the Reactivation+Tetris to Tetris Only comparison is close to being significant in the uncorrected values.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#checking-assumptions",
    "href": "tutorials/rmb-week-9_one-way-anova.html#checking-assumptions",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "As ever - the ANOVA makes certain assumptions about the data so that we can perform our statistics. These assumptions are similar to what we saw with t-tests and should all be in place for our ANOVA results to make a valid generalisation to the wide population.\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nMatch the ANOVA assumption to its definition.\n\n\n\n\n\n\n\nAssumption\nDefinition\n\n\n\n\nIndependence\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\nNormal distributions\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\nHomogeneity of variance\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\nCategorical factors\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\nData type\nInterval or Ratio data observationsdata must be reasonably normally distributedgroups should each have equal variancedata observations must be unrelatedpredicting factors must be divided into separate groups\n\n\n\n\n\nTake a look at your lecture notes from week 8!\n\n\n\n\n\nIt is very straighforward to check these assumptions in Jamovi - find the ‘Assumption Checks’ menu and select all three options to run the Shapiro-Wilk test, Levene’s Test and the QQ plot.\n\n\n\n\n\n\n\nCaution Data Skills - is the assumption of independence met?\n\n\n\n\n\nYes, this is a between subjects design in which each participant only contributes a single data point to this study. ANOVA analysis for repeated measures designs are possible - you might encounter these in a future module.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - is the assumption of homogeneity of variance met?\n\n\n\n\n\nNo, Levene’s test is significant indicating that the groups do not have equal variance.\nWe can explore this further by running descriptive statistics on the groups. We can see that the standard deviation of the Reactivation+Tetris group looks to be much smaller than the others. This is likely where the difference is coming from.\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - is the for normally distributed data met?\n\n\n\n\n\nNo, Shapiro-Wilk’s test is significant indicating that the data is not normally distributed. This is confirmed by the QQ plot in which the top end of the data depart from the centre diagonal line.\nThe estimated marginal means plot from your earlier analysis can help explain why this is. Notice that the data points in that plot have a positive skew - ie a small number of participants seem to have a lot of intrusive memories.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - is the assumption of categorical factors met?\n\n\n\n\n\nYes, our Conditions variable is categorical. It contains our four groups without any implied order.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - is the assumed data type correct?\n\n\n\n\n\nYes, our Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary variable is ratio data. It contains numerical data in which zero means an absence of intrusive memories.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#anova-on-log-transformed-data",
    "href": "tutorials/rmb-week-9_one-way-anova.html#anova-on-log-transformed-data",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Looks like our data do not meet the parametric assumptions of a standard ANOVA analysis. Its often a good idea to check assumptions early in the data analysis process…\nThere are non-parametric ANOVA analyses available - you could now run the Kruskall Wallis test on the data to perform a non parametric one way ANOVA.\nLet’s try a data transformation here though, the full parametric ANOVA is very flexible so it would be good to use it if we can.\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint - ANOVA TableHint - Assumption ChecksHint - Estimated Marginal MeansHint - Post Hoc Tests\n\n\nCompute a new data variable using the log transform on the Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary - review week 7 section 7 if you need reminder on how to do this.\nUse the results to complete this reporting of the ANOVA results.\n\nA Shaprio-Wilk test showed a significant departure from a normal distribution, W=0.877, p&lt;0.001. The data were transformed using a log transform.\nA second Shaprio-Wilk test showed that the log-transformed data is normally distributednot normally distributed, W=, p=. Levene’s test on the transformed data shows that we can assumewe can not assume homogeneity of variance across the groups, F(, ) = , p=\nA one-way ANOVA was conducted to on the transformed data to compare the effect of the number of intrusive memories in the week after the experimenttask completed after watching the film on the number of intrusive memories in the week after the experimenttask completed after watching the film. There was a statistically significantno significant difference in number of intrusive memoriespost film task between groups, F(, ) = , p = , \\(\\eta^2\\) = .\nPost hoc comparisons with Tukey correction were conducted to identify which pairs of conditions showed significant differences in their estimated marginal means. Three pairwise comparisons showed a significant difference. Note that we are reporting the log-transformed means with the post-hoc test, not the original means.\nThe ‘No-Task Control’ group (M=) had more instrusive memories than the ‘Reactivation+Tetris’ group (M=), t() = , p=, Cohen’s d=\nThe ‘Reactivation+Tetris’ group (M=) had fewer instrusive memories than the ‘Tetris Only’ group (M=), t() = , p=, Cohen’s d=.\nThe ‘Reactivation+Tetris’ group (M=) had fewer instrusive memories than the ‘Reactivation Only’ group (M=), t() = , p=, Cohen’s d=.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#summary",
    "href": "tutorials/rmb-week-9_one-way-anova.html#summary",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "ANOVA is a flexible and powerful method for comparing the means of more than two groups whilst accounting for the exploding number of multiple comparisons we get from more complex experimental designs. ANOVA takes a bit more work to run, check and interpret than a t-test but the results from a single ANOVA analysis contain a huge amount of critical information about our data!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 9 - One-way ANOVA"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html",
    "href": "tutorials/rmb-week-11_putting-it-together.html",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "This week we will explore some data replicating a famous but contentious experiment. We will combine your skills from across the course to explore different aspects of the study.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nRevision of t-tests\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nRevision of reproducible analyses\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with published scientific data\n\n\n\n\n\n\n\n\n\n\n\nEthics\n\n\n\n\n\nQuestionable research practices\n\n\n\nConstructive criticism\n\n\n\n\n\nWe will be working with a replication of a vary famous study showing that our body posture can have far reaching impacts on our attitudes and even our physiology! amazing…\nFrom the abstract of the original paper:\nCarney, D. R., Cuddy, A. J. C., & Yap, A. J. (2010). Power Posing: Brief Nonverbal Displays Affect Neuroendocrine Levels and Risk Tolerance. Psychological Science, 21(10), 1363-1368. https://doi.org/10.1177/0956797610383437\n\nHumans and other animals express power through open, expansive postures, and they express powerlessness through closed, contractive postures. But can these postures actually cause power? The results of this study confirmed our prediction that posing in high-power nonverbal displays (as opposed to low-power nonverbal displays) would cause neuroendocrine and behavioral changes for both male and female participants: High-power posers experienced elevations in testosterone, decreases in cortisol, and increased feelings of power and tolerance for risk; low-power posers exhibited the opposite pattern. In short, posing in displays of power caused advantaged and adaptive psychological, physiological, and behavioral changes, and these findings suggest that embodiment extends beyond mere thinking and feeling, to physiology and subsequent behavioral choices. That a person can, by assuming two simple 1-min poses, embody power and instantly become more powerful has real-world, actionable implications. (Carney, Cuddy, and Yap 2010)\n\nThis is a compelling result! Perhaps not so suprising that power poses would change our mood - but to cause a change hormones is amazing.\nThe authors quickly became famous with this result. The second author Amy Cuddy gave high profile talks (TED talk with 74 million views: Video & transcript), wrote books and appeared widely in the media as a result of this work.\n\n\n\nAmy Cuddy at Pop Tech 2011 - Photo by Erik Hersman.\n\n\nThese results attracted several replications attempts and the following debate became so intense that it was attracting attention in the mainstream media - including articles summarising the story in the New York Times magazine (Dominus 2017) and the British Psychological Society (Loncar 2021). It even has it’s own Wikipedia page (Wikipedia 2025) and lead to some very awkward photos of british politicians (Wheeler 2018).\nWe’ll be focusing on one of these replications that tried to faithfully repeat the original methodology (Ranehill et al. 2015) with a large, independent data sample.\n\n\n\nWe will look at how the replication might have been planned, analyse the data and make a shareable version of the analysis.\nWe will also use this story to think about some ethical issues around good scientific practices and how we can encourage collaborative and constructive scientific debate.\n\n\n\nLet’s understand the original experiment in a bit more detail. Read through the introduction and methods section of (Carney, Cuddy, and Yap 2010) and answer the following questions.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nRead the introduction (you can focus on the hypotheses) and methods section of (Carney, Cuddy, and Yap 2010) and answer the following questions.\n\n\nThe authors predict that high-power poses will lead to increasesdecreases in testosterone.\nThe authors predict that high-power poses will lead to increasesdecreases in the stress hormone cortisol.\nThe authors predict that high-power poses will lead to increasesdecreases in subjective feelings of power.\nThe authors predict that high-power poses will lead to increasesdecreases in tolerance to risk.\n\n\n\nWhat is the design of this study? Between subjectsWithin subjects.\nHow were participants assigned to groups? In order of recruitmentRandomlyBased on their preference.\nWhat is the independent variable (or grouping factor)? GamblingCortisolPoseTestosteroneelectrocardiography?\nWhich of these is NOT a dependent variable of interest? GamblingCortisolTestosteroneelectrocardiography?\nAre participants told the true purpose of the study? YesNo\nDid the experimentors know which condition the participant they were testing was in? YesNo\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nRead the results section of (Carney, Cuddy, and Yap 2010). Identify the reported statistics for the high-power/low-power pose groups for the following effects: Testosterone, Cortisol and Feelings of Power.\nEnter the reported effect sizes (r values) in the table below and use this online tool to convert them to Cohen’d D values.\nFinally - use jPower in Jamovi to estimate the sample size you would need to have 90% power to detect this effect again with an type 1 error rate of 0.05. We covered this in Week 5 Section 7\n\n\n\n\n\n\n\n\n\nHypothesis Test\nReported Effect Size (r)\nConverted Effect Size (Cohen’s d)\nSample size per group for 90% power\n\n\n\n\nTestosterone\n\n\n\n\n\nCortisol\n\n\n\n\n\nFeelings of power\n\n\n\n\n\n\n\n\n(Carney, Cuddy, and Yap 2010) report the following in their results.\n\nAs hypothesized, high-power poses caused an increase in testosterone compared with low-power poses, which caused a decrease in testosterone, F(1, 39) = 4.29, p &lt; .05; r = .34 (Fig. 3). Also as hypothesized, high-power poses caused a decrease in cortisol compared with low-power poses, which caused an increase in cortisol, F(1, 38) = 7.45, p &lt; .02; r = .43 Finally, high-power posers reported feeling significantly more “powerful” and “in charge” (M = 2.57, SD = 0.81) than low-power posers did (M = 1.83, SD = 0.81), F(1, 41) = 9.53, p &lt; .01; r = .44.\n\n(Carney, Cuddy, and Yap 2010) reported ANOVAs with r values as effect sizes - but remember that we can convert the effect sizes to Cohen’s d using the online tool.\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Test\nReported Effect Size (r)\nConverted Effect Size (Cohen’s d)\nSample size per group for 90% power\n\n\n\n\nTestosterone\n0.34\n0.723\n42\n\n\nCortisol\n0.43\n0.953\n25\n\n\nFeelings of power\n0.44\n0.980\n23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - The replication study has a planned sample size of 100 - is this enough to have a 90% chance of detecting the effect if is is there?\n\n\n\n\n\nYes, this should be plenty. The smallest reported effect in the original paper is for Testosterone, which suggests that two groups of 42 participants should be sufficient.\n\n\n\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nMake sure you have downloaded the dataset and loaded it into Jamovi before starting this section.\nTake a moment to have a look through the dataset paying particular attention to the ‘Variables’ tab.\n\n\nNext, we’re going to look at the dataset from (Ranehill et al. 2015) from the replication study. The research team were very careful to closely replicate the original methods.\nThere were a small number of changes in the replication compared to the original study:\n\nWhereas participants in Carney et al.’s study held each position for 1 min, we extended this time to 3 min.\nSecond, Carney et al. manually configured participants’ positions before leaving the room, whereas participants in our study received instructions on a computer. As in Carney et al.’s study, participants were recorded while posing; the recordings verified that participants complied with task instructions. Because the instructions were given via computer, the experimenter was blind to experimental condition, which negated potential experimenter effects.\nThird, the filler task in our study involved constructing words from letters and spaces; in Carney et al.’s study, the task was to form impressions of faces.\nFinally, we did not use deception, but informed participants briefly in the consent form that the study investigated whether physical position influences hormone levels and behavior (the instructions and consent form are in the Supplemental Material).\n\nThese are reasonable changes that aim to improve the orignal design (blinded experimentors, longer pose duration) without making substantial changes to its purpose.\nLet’s take a look at the data.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nIn the original study, (Carney, Cuddy, and Yap 2010) reported that:\n\nHigh-power posers reported feeling significantly more “powerful” and “in charge” (M = 2.57, SD = 0.81) than low-power posers did (M = 1.83, SD = 0.81), F(1, 41) = 9.53, p &lt; .01; r = .44.\n\nCan you replicate this analysis using the powerful and hp_condition (1 is ‘high power pose’ and 0 is ‘low power pose’) variables in the replication dataset from (Ranehill et al. 2015)? (Carney, Cuddy, and Yap 2010) used an ANOVA but you should use a t-test.\nUse the results to fill in the gaps in the reporting below.\n\nThe results replicatedo not replicate the original finding about subjective feeling. High-power posers (M = , SD = ) reported feeling less powerful thanjust as powerful asmore powerful than low-power posers did (M = , SD = ), t() = , p = ; Cohen’s d = .\n\n\n\n\n\n\n\nHigh-power posers reported feeling significantly more “powerful” (M = 2.22, SD = 0.753) than low-power posers did (M = 1.978, SD = 0.663), t(193) = -2.40, p = 0.017; Cohen’s d = -0.344.\n\n\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Compare your results above to those reported in the replication paper - are there any differences?\n\n\n\n\n\nNo! oddly they do not… (Ranehill et al. 2015) report the following:\n\nUsing two-tailed t tests, we replicated Carney et al.’s finding that participants in the high-power condition self-reported, on average, higher feelings of power than did participants in the low-power condition (mean difference = 0.245, 95% confidence interval, or CI = [0.044, 0.446]), t(193) = 2.399, p = .017, Cohen’s d = 0.344. This suggests that the power-posing manipulation worked.\n\nMost of the reported values are the same, but but notice that the sign of the t-value is different!\nWe can confirm that this is a weird variable issue by checking hte descriptive statistics. The direction of our effect is definitely the same as what is reported in the paper.\n\n\n\n\n\n\n\n\n\nNoteKey step - open after answering the previous question\n\n\n\n\n\nSomething strange has happened - it seems like (Ranehill et al. 2015) must have computed the t-test the other way around compared to our analysis. They have a positive t-value and ours is negative.\nRemember that the direction of a t-test (ie whether the t-statistics is positive or negative) doesn’t effect the p-values but does change the interpretation of the result a little.\nThis is because the order of subtraction makes a difference to the answer - if we do 2 + 3 or 3 + 2 we get the same answer however 2 - 3 and 3 - 2 are not the same.\nThe means of our groups are 1.98 for low power and 2.22 for high power poses. (Ranehill et al. 2015) report a positive t-value of 2.399 - so they must have run the equivalent of high power minus low power (2.22 - 1.98) and have a positive t-value indicating that the mean for high power is larger.\nIn contrast, our negative t-value indicates we have run low power minus high power (1.98 - 2.22) and have a negative t-value indicating that the mean for low power is smaller.\nThe order of the conditions in the t-test is controlled from the data varaible view. Double click on hp_condition in the ‘Variables’ tab, or on the column header of the ‘Data’ tab. We should need a ‘Nominal’ variable with two ‘Levels’ - 0 and 1. The conditions in the t-test are sorted in the order of this ‘Levels’ list.\nYou can reorder the levels to put 1 first and 0 second - which should update your t-test live to match (Ranehill et al. 2015)’s result.\n\nWe could make this even easier to remember by changing the names of the Levels to something more memorable. This is often a good idea when data has used an abstract numerical coding. Statistics is about precision and anything we can do to make the interpretation easier is a good thing.\n\nYour adapted result should match the following:\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you replicate the core findings from (Carney, Cuddy, and Yap 2010) using the data from (Ranehill et al. 2015)?\nMake sure you have completed the ‘Key step’ above before starting this exercise!\nRun t-tests to analyse the effect of high vs low power posing on the following variables: powerful, testosterone_diff, cortisol_diff and num_risky_gain_pc as dependent variables.\nRead through the results of the original paper (Carney, Cuddy, and Yap 2010) - does the second study confirm the findings of the original?\n\n\n\n\n\n\n\n\n\n\nVariable\nt-value\np-value\nEffect size\nReplicates (Carney, Cuddy, and Yap 2010)?\n\n\n\n\nFeelings of power\n\n\n\nYesNo\n\n\nRisk Taking (Gain)\n\n\n\nYesNo\n\n\nTestosterone\n\n\n\nYesNo\n\n\nCortisol\n\n\n\nYesNo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nt-value\np-value\nEffect size\nReplicates (Carney, Cuddy, and Yap 2010)?\n\n\n\n\nFeelings of power\n2.40\n0.017\n0.344\nYes\n\n\nRisk Taking (Gain)\n-1.24\n0.215\n-0.176\nNo\n\n\nTestosterone\n-1.40\n0.162\n-0.200\nNo\n\n\nCortisol\n-1.10\n0.272\n-0.157\nNo\n\n\n\n\n\n\n\n\n\n\n\nWe have the results of the replication! but before going any further we should be sure that the assumptions of our test are met.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCheck the assumptions of the high-power vs low-power t-tests for the following variables: powerful, testosterone_diff, cortisol_diff and num_risky_gain_pc.\nAre the assumptions of Normality and Homogeneity of Variance confirmed or violated?\n\n\n\n\n\n\n\n\n\n\nVariable\nNormality\nHomogeneity of Variance\nPreferred Test\n\n\n\n\n\nPowerful\nConfirmedViolated\nConfirmedViolated\nStudent’s tWelch’s tMann-Whitney U\n\n\n\nTestosterone\nConfirmedViolated\nConfirmedViolated\nStudent’s tWelch’s tMann-Whitney U\n\n\n\nCortisol\nConfirmedViolated\nConfirmedViolated\nStudent’s tWelch’s tMann-Whitney U\n\n\n\nRisk Taking (Gain)\nConfirmedViolated\nConfirmedViolated\nStudent’s tWelch’s tMann-Whitney U\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nNormality\nHomogeneity of Variance\nPreferred Test\n\n\n\n\n\nPowerful\nViolated\nViolated\nMann-Whitney U\n\n\n\nTestosterone\nViolated\nConfirmed\nMann-Whitney U\n\n\n\nCortisol\nViolated\nConfirmed\nMann-Whitney U\n\n\n\nRisk Taking (Gain)\nViolated\nConfirmed\nMann-Whitney U\n\n\n\n\n\n\n\n\n\n(Ranehill et al. 2015) do something unusual with their assumption checking. The don’t explicitly check whether the assumptions are met or not, rather they report both parametric and non-parametric statistics for all their tests (Non-parametric tests are reported in supplemental materials) and state that the conclusions would be the same irrespective of which test is would have been chosen (please do not do this in your reports!).\n\n\n\nIt can be challenging to interpret the results of a null replication. If both the original study and replication show the same finding then things are much more straightforward….\nTo interpret a failed replication we have to consider two possibilities:\n\nThe original study was a false positive and the replication is a correct rejection.\nThe original study was a true positive and the replication is a false negative.\n\nOne of our experiments is not accurate.\nIt is very very difficult to separate these options with 100% confidence… Often a second replication is the way to converge on an answer, and perhaps a third and fourth would be better still.\nOne thing we do know is that if the effect size from the original study were accurate, then the replication should have had enough participants to see it again.\nThe same is not true the other way around. Take a look at the figures below:\n\n\n\n\n\nOriginal and replication effect size with 95% confidence intervals for that effect size. Figures by Data Colada: https://datacolada.org/37#identifier_0_910\n\n\n\nThe replication confidence interval is much smaller than the original and even crosses zero for all three key variables. This strongly indicates a very tiny or zero effect.\nThe replication effect is so small that, if it is accurate, then the original study with its smaller sample would only have a 5.9% chance of detecting it. (Analysis by Data Colada).\nSo which study do we trust? Ultimately that your decision as an individual scientist. Consider what you know about both studies and evaulate which you find more compelling.\nIn this case, there is a substantial consensus in the field that the original effects on hormones was likely a false positive, though the increase in the subjective feelings of power after a power pose replicates well. The replications have several advantages including larger sample sizes, blinded and objective experimentors, and longer pose durations.\nThe original authors have engaged in the debate and made several points in defense of their work. The original study was famous by the time the replication was carried out - perhaps the participants had heard of the original study. Similarly, the replication study participants were told what was being measured in advance which might have effected their feelings during hte study. Finally, the original study was in hte USA and hte replication in Switzerland - perhaps there is a cultural component to power posing?\n\n\n\nThe original paper attracted huge scientific and media interest\n\n61 news outlets\n33 blogs\n2 policy sources\n110 X users\n1 peer review site\n11 Facebook pages\n8 Wikipedia pages\n1 Redditor\n11 YouTube creators\n1 Bluesky user\n\ndata from Altmetric, see here for full summary.\nThis was a mix of constructive debate, harsh criticism and genuine interest. Unfortunately a section of this debate became aggressive and personal towards Amy Cuddy to the extent that it had a serious impact on her life, health and career.\n\nIn addition to her fame and influence, Cuddy’s critics seemed obsessed with profits from her book and her fees for speaking engagements. Dollar amounts frequently emerged in conversations where academics discussed the research, and recently, one of her most vocal critics said her speaking engagement fees made him feel “ill.” One Ivy League professor tweeted, “You can listen to her speak about her irreplicable results for $25-$40,000.” In reality, her book and speaking topics have little to do with the power posing studies.\nMany of the negative comments were personal in nature and aimed at Cuddy directly. “They were comparing me to Harvey Weinstein,” Cuddy says, recalling a post to the PsychMAP Facebook group which the site owners were eventually persuaded to remove. As if that’s not bad enough, Cuddy also received death threats via direct messages on social media. “I became an untouchable in my field,” she adds, describing how colleagues were shunned for merely friending her on Facebook.\nAlthough Cuddy says Harvard wanted her to stay, she left her full-time position, because she no longer felt safe doing her work (she remains a faculty member at Harvard Business School in Executive Education). “My dean wanted me to stay, but I later told him, I couldn’t stay, because I couldn’t endure this abuse. It was relentless,” says Cuddy. Her experiences inspired her new book (still a work in progress) entitled, Bullies, Bystanders and Bravehearts aimed at helping others who find themselves facing similar experiences.\nExerpts from (Elsesser 2024).\n\n\nThis has led to a follow-up discussion […] consisting of two general camps: those who think Cuddy is being treated unfairly, and those who believe that complaints about bullying or harassment are a distraction from the most serious problem here, which is the elevation of shoddy research to a place of such profitable prominence.\nExcept from (Singal 2017)\n\nThere is perhaps a tension here between our ethical obligation to act with integrity in research and our obligation for act with respect (BPS, n.d.). Or perhaps some people are just bullies.\nHave a read through these sources and take a moment to do a quick web search for yourself.\nShare what you think about this situation on the padlet board below. what do you think of the situation surrounding this debate? how can we do better in future?\n\nTaking and receiving criticsm is a huge part of progressive constructive science but can be challenging to do well.\n\n\n\nReplication is the gold standard for scientific progress but it is rarely as straightforward as we’d like it to be!\nA wide range of issues can arise - including statistical approaches, experimental design and sometimes interpersonal conflict. We have to navigate all of these points with high ethical standards for the scientific community to progress.\nThank you for your time this semester - I hope you have enjoyed these computer practical sessions!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html#the-dataset",
    "href": "tutorials/rmb-week-11_putting-it-together.html#the-dataset",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "We will be working with a replication of a vary famous study showing that our body posture can have far reaching impacts on our attitudes and even our physiology! amazing…\nFrom the abstract of the original paper:\nCarney, D. R., Cuddy, A. J. C., & Yap, A. J. (2010). Power Posing: Brief Nonverbal Displays Affect Neuroendocrine Levels and Risk Tolerance. Psychological Science, 21(10), 1363-1368. https://doi.org/10.1177/0956797610383437\n\nHumans and other animals express power through open, expansive postures, and they express powerlessness through closed, contractive postures. But can these postures actually cause power? The results of this study confirmed our prediction that posing in high-power nonverbal displays (as opposed to low-power nonverbal displays) would cause neuroendocrine and behavioral changes for both male and female participants: High-power posers experienced elevations in testosterone, decreases in cortisol, and increased feelings of power and tolerance for risk; low-power posers exhibited the opposite pattern. In short, posing in displays of power caused advantaged and adaptive psychological, physiological, and behavioral changes, and these findings suggest that embodiment extends beyond mere thinking and feeling, to physiology and subsequent behavioral choices. That a person can, by assuming two simple 1-min poses, embody power and instantly become more powerful has real-world, actionable implications. (Carney, Cuddy, and Yap 2010)\n\nThis is a compelling result! Perhaps not so suprising that power poses would change our mood - but to cause a change hormones is amazing.\nThe authors quickly became famous with this result. The second author Amy Cuddy gave high profile talks (TED talk with 74 million views: Video & transcript), wrote books and appeared widely in the media as a result of this work.\n\n\n\nAmy Cuddy at Pop Tech 2011 - Photo by Erik Hersman.\n\n\nThese results attracted several replications attempts and the following debate became so intense that it was attracting attention in the mainstream media - including articles summarising the story in the New York Times magazine (Dominus 2017) and the British Psychological Society (Loncar 2021). It even has it’s own Wikipedia page (Wikipedia 2025) and lead to some very awkward photos of british politicians (Wheeler 2018).\nWe’ll be focusing on one of these replications that tried to faithfully repeat the original methodology (Ranehill et al. 2015) with a large, independent data sample.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html#the-challenge.",
    "href": "tutorials/rmb-week-11_putting-it-together.html#the-challenge.",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "We will look at how the replication might have been planned, analyse the data and make a shareable version of the analysis.\nWe will also use this story to think about some ethical issues around good scientific practices and how we can encourage collaborative and constructive scientific debate.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html#understanding-power-posing-and-planning-a-replication.",
    "href": "tutorials/rmb-week-11_putting-it-together.html#understanding-power-posing-and-planning-a-replication.",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "Let’s understand the original experiment in a bit more detail. Read through the introduction and methods section of (Carney, Cuddy, and Yap 2010) and answer the following questions.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nRead the introduction (you can focus on the hypotheses) and methods section of (Carney, Cuddy, and Yap 2010) and answer the following questions.\n\n\nThe authors predict that high-power poses will lead to increasesdecreases in testosterone.\nThe authors predict that high-power poses will lead to increasesdecreases in the stress hormone cortisol.\nThe authors predict that high-power poses will lead to increasesdecreases in subjective feelings of power.\nThe authors predict that high-power poses will lead to increasesdecreases in tolerance to risk.\n\n\n\nWhat is the design of this study? Between subjectsWithin subjects.\nHow were participants assigned to groups? In order of recruitmentRandomlyBased on their preference.\nWhat is the independent variable (or grouping factor)? GamblingCortisolPoseTestosteroneelectrocardiography?\nWhich of these is NOT a dependent variable of interest? GamblingCortisolTestosteroneelectrocardiography?\nAre participants told the true purpose of the study? YesNo\nDid the experimentors know which condition the participant they were testing was in? YesNo\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nRead the results section of (Carney, Cuddy, and Yap 2010). Identify the reported statistics for the high-power/low-power pose groups for the following effects: Testosterone, Cortisol and Feelings of Power.\nEnter the reported effect sizes (r values) in the table below and use this online tool to convert them to Cohen’d D values.\nFinally - use jPower in Jamovi to estimate the sample size you would need to have 90% power to detect this effect again with an type 1 error rate of 0.05. We covered this in Week 5 Section 7\n\n\n\n\n\n\n\n\n\nHypothesis Test\nReported Effect Size (r)\nConverted Effect Size (Cohen’s d)\nSample size per group for 90% power\n\n\n\n\nTestosterone\n\n\n\n\n\nCortisol\n\n\n\n\n\nFeelings of power\n\n\n\n\n\n\n\n\n(Carney, Cuddy, and Yap 2010) report the following in their results.\n\nAs hypothesized, high-power poses caused an increase in testosterone compared with low-power poses, which caused a decrease in testosterone, F(1, 39) = 4.29, p &lt; .05; r = .34 (Fig. 3). Also as hypothesized, high-power poses caused a decrease in cortisol compared with low-power poses, which caused an increase in cortisol, F(1, 38) = 7.45, p &lt; .02; r = .43 Finally, high-power posers reported feeling significantly more “powerful” and “in charge” (M = 2.57, SD = 0.81) than low-power posers did (M = 1.83, SD = 0.81), F(1, 41) = 9.53, p &lt; .01; r = .44.\n\n(Carney, Cuddy, and Yap 2010) reported ANOVAs with r values as effect sizes - but remember that we can convert the effect sizes to Cohen’s d using the online tool.\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Test\nReported Effect Size (r)\nConverted Effect Size (Cohen’s d)\nSample size per group for 90% power\n\n\n\n\nTestosterone\n0.34\n0.723\n42\n\n\nCortisol\n0.43\n0.953\n25\n\n\nFeelings of power\n0.44\n0.980\n23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - The replication study has a planned sample size of 100 - is this enough to have a 90% chance of detecting the effect if is is there?\n\n\n\n\n\nYes, this should be plenty. The smallest reported effect in the original paper is for Testosterone, which suggests that two groups of 42 participants should be sufficient.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html#analysing-the-replication-data.",
    "href": "tutorials/rmb-week-11_putting-it-together.html#analysing-the-replication-data.",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "NoteKey step\n\n\n\nMake sure you have downloaded the dataset and loaded it into Jamovi before starting this section.\nTake a moment to have a look through the dataset paying particular attention to the ‘Variables’ tab.\n\n\nNext, we’re going to look at the dataset from (Ranehill et al. 2015) from the replication study. The research team were very careful to closely replicate the original methods.\nThere were a small number of changes in the replication compared to the original study:\n\nWhereas participants in Carney et al.’s study held each position for 1 min, we extended this time to 3 min.\nSecond, Carney et al. manually configured participants’ positions before leaving the room, whereas participants in our study received instructions on a computer. As in Carney et al.’s study, participants were recorded while posing; the recordings verified that participants complied with task instructions. Because the instructions were given via computer, the experimenter was blind to experimental condition, which negated potential experimenter effects.\nThird, the filler task in our study involved constructing words from letters and spaces; in Carney et al.’s study, the task was to form impressions of faces.\nFinally, we did not use deception, but informed participants briefly in the consent form that the study investigated whether physical position influences hormone levels and behavior (the instructions and consent form are in the Supplemental Material).\n\nThese are reasonable changes that aim to improve the orignal design (blinded experimentors, longer pose duration) without making substantial changes to its purpose.\nLet’s take a look at the data.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nIn the original study, (Carney, Cuddy, and Yap 2010) reported that:\n\nHigh-power posers reported feeling significantly more “powerful” and “in charge” (M = 2.57, SD = 0.81) than low-power posers did (M = 1.83, SD = 0.81), F(1, 41) = 9.53, p &lt; .01; r = .44.\n\nCan you replicate this analysis using the powerful and hp_condition (1 is ‘high power pose’ and 0 is ‘low power pose’) variables in the replication dataset from (Ranehill et al. 2015)? (Carney, Cuddy, and Yap 2010) used an ANOVA but you should use a t-test.\nUse the results to fill in the gaps in the reporting below.\n\nThe results replicatedo not replicate the original finding about subjective feeling. High-power posers (M = , SD = ) reported feeling less powerful thanjust as powerful asmore powerful than low-power posers did (M = , SD = ), t() = , p = ; Cohen’s d = .\n\n\n\n\n\n\n\nHigh-power posers reported feeling significantly more “powerful” (M = 2.22, SD = 0.753) than low-power posers did (M = 1.978, SD = 0.663), t(193) = -2.40, p = 0.017; Cohen’s d = -0.344.\n\n\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Compare your results above to those reported in the replication paper - are there any differences?\n\n\n\n\n\nNo! oddly they do not… (Ranehill et al. 2015) report the following:\n\nUsing two-tailed t tests, we replicated Carney et al.’s finding that participants in the high-power condition self-reported, on average, higher feelings of power than did participants in the low-power condition (mean difference = 0.245, 95% confidence interval, or CI = [0.044, 0.446]), t(193) = 2.399, p = .017, Cohen’s d = 0.344. This suggests that the power-posing manipulation worked.\n\nMost of the reported values are the same, but but notice that the sign of the t-value is different!\nWe can confirm that this is a weird variable issue by checking hte descriptive statistics. The direction of our effect is definitely the same as what is reported in the paper.\n\n\n\n\n\n\n\n\n\nNoteKey step - open after answering the previous question\n\n\n\n\n\nSomething strange has happened - it seems like (Ranehill et al. 2015) must have computed the t-test the other way around compared to our analysis. They have a positive t-value and ours is negative.\nRemember that the direction of a t-test (ie whether the t-statistics is positive or negative) doesn’t effect the p-values but does change the interpretation of the result a little.\nThis is because the order of subtraction makes a difference to the answer - if we do 2 + 3 or 3 + 2 we get the same answer however 2 - 3 and 3 - 2 are not the same.\nThe means of our groups are 1.98 for low power and 2.22 for high power poses. (Ranehill et al. 2015) report a positive t-value of 2.399 - so they must have run the equivalent of high power minus low power (2.22 - 1.98) and have a positive t-value indicating that the mean for high power is larger.\nIn contrast, our negative t-value indicates we have run low power minus high power (1.98 - 2.22) and have a negative t-value indicating that the mean for low power is smaller.\nThe order of the conditions in the t-test is controlled from the data varaible view. Double click on hp_condition in the ‘Variables’ tab, or on the column header of the ‘Data’ tab. We should need a ‘Nominal’ variable with two ‘Levels’ - 0 and 1. The conditions in the t-test are sorted in the order of this ‘Levels’ list.\nYou can reorder the levels to put 1 first and 0 second - which should update your t-test live to match (Ranehill et al. 2015)’s result.\n\nWe could make this even easier to remember by changing the names of the Levels to something more memorable. This is often a good idea when data has used an abstract numerical coding. Statistics is about precision and anything we can do to make the interpretation easier is a good thing.\n\nYour adapted result should match the following:\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you replicate the core findings from (Carney, Cuddy, and Yap 2010) using the data from (Ranehill et al. 2015)?\nMake sure you have completed the ‘Key step’ above before starting this exercise!\nRun t-tests to analyse the effect of high vs low power posing on the following variables: powerful, testosterone_diff, cortisol_diff and num_risky_gain_pc as dependent variables.\nRead through the results of the original paper (Carney, Cuddy, and Yap 2010) - does the second study confirm the findings of the original?\n\n\n\n\n\n\n\n\n\n\nVariable\nt-value\np-value\nEffect size\nReplicates (Carney, Cuddy, and Yap 2010)?\n\n\n\n\nFeelings of power\n\n\n\nYesNo\n\n\nRisk Taking (Gain)\n\n\n\nYesNo\n\n\nTestosterone\n\n\n\nYesNo\n\n\nCortisol\n\n\n\nYesNo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nt-value\np-value\nEffect size\nReplicates (Carney, Cuddy, and Yap 2010)?\n\n\n\n\nFeelings of power\n2.40\n0.017\n0.344\nYes\n\n\nRisk Taking (Gain)\n-1.24\n0.215\n-0.176\nNo\n\n\nTestosterone\n-1.40\n0.162\n-0.200\nNo\n\n\nCortisol\n-1.10\n0.272\n-0.157\nNo",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html#assumption-checking.",
    "href": "tutorials/rmb-week-11_putting-it-together.html#assumption-checking.",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "We have the results of the replication! but before going any further we should be sure that the assumptions of our test are met.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCheck the assumptions of the high-power vs low-power t-tests for the following variables: powerful, testosterone_diff, cortisol_diff and num_risky_gain_pc.\nAre the assumptions of Normality and Homogeneity of Variance confirmed or violated?\n\n\n\n\n\n\n\n\n\n\nVariable\nNormality\nHomogeneity of Variance\nPreferred Test\n\n\n\n\n\nPowerful\nConfirmedViolated\nConfirmedViolated\nStudent’s tWelch’s tMann-Whitney U\n\n\n\nTestosterone\nConfirmedViolated\nConfirmedViolated\nStudent’s tWelch’s tMann-Whitney U\n\n\n\nCortisol\nConfirmedViolated\nConfirmedViolated\nStudent’s tWelch’s tMann-Whitney U\n\n\n\nRisk Taking (Gain)\nConfirmedViolated\nConfirmedViolated\nStudent’s tWelch’s tMann-Whitney U\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nNormality\nHomogeneity of Variance\nPreferred Test\n\n\n\n\n\nPowerful\nViolated\nViolated\nMann-Whitney U\n\n\n\nTestosterone\nViolated\nConfirmed\nMann-Whitney U\n\n\n\nCortisol\nViolated\nConfirmed\nMann-Whitney U\n\n\n\nRisk Taking (Gain)\nViolated\nConfirmed\nMann-Whitney U\n\n\n\n\n\n\n\n\n\n(Ranehill et al. 2015) do something unusual with their assumption checking. The don’t explicitly check whether the assumptions are met or not, rather they report both parametric and non-parametric statistics for all their tests (Non-parametric tests are reported in supplemental materials) and state that the conclusions would be the same irrespective of which test is would have been chosen (please do not do this in your reports!).",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html#interpreting-the-results.",
    "href": "tutorials/rmb-week-11_putting-it-together.html#interpreting-the-results.",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "It can be challenging to interpret the results of a null replication. If both the original study and replication show the same finding then things are much more straightforward….\nTo interpret a failed replication we have to consider two possibilities:\n\nThe original study was a false positive and the replication is a correct rejection.\nThe original study was a true positive and the replication is a false negative.\n\nOne of our experiments is not accurate.\nIt is very very difficult to separate these options with 100% confidence… Often a second replication is the way to converge on an answer, and perhaps a third and fourth would be better still.\nOne thing we do know is that if the effect size from the original study were accurate, then the replication should have had enough participants to see it again.\nThe same is not true the other way around. Take a look at the figures below:\n\n\n\n\n\nOriginal and replication effect size with 95% confidence intervals for that effect size. Figures by Data Colada: https://datacolada.org/37#identifier_0_910\n\n\n\nThe replication confidence interval is much smaller than the original and even crosses zero for all three key variables. This strongly indicates a very tiny or zero effect.\nThe replication effect is so small that, if it is accurate, then the original study with its smaller sample would only have a 5.9% chance of detecting it. (Analysis by Data Colada).\nSo which study do we trust? Ultimately that your decision as an individual scientist. Consider what you know about both studies and evaulate which you find more compelling.\nIn this case, there is a substantial consensus in the field that the original effects on hormones was likely a false positive, though the increase in the subjective feelings of power after a power pose replicates well. The replications have several advantages including larger sample sizes, blinded and objective experimentors, and longer pose durations.\nThe original authors have engaged in the debate and made several points in defense of their work. The original study was famous by the time the replication was carried out - perhaps the participants had heard of the original study. Similarly, the replication study participants were told what was being measured in advance which might have effected their feelings during hte study. Finally, the original study was in hte USA and hte replication in Switzerland - perhaps there is a cultural component to power posing?",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html#ethics-collaboration.",
    "href": "tutorials/rmb-week-11_putting-it-together.html#ethics-collaboration.",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "The original paper attracted huge scientific and media interest\n\n61 news outlets\n33 blogs\n2 policy sources\n110 X users\n1 peer review site\n11 Facebook pages\n8 Wikipedia pages\n1 Redditor\n11 YouTube creators\n1 Bluesky user\n\ndata from Altmetric, see here for full summary.\nThis was a mix of constructive debate, harsh criticism and genuine interest. Unfortunately a section of this debate became aggressive and personal towards Amy Cuddy to the extent that it had a serious impact on her life, health and career.\n\nIn addition to her fame and influence, Cuddy’s critics seemed obsessed with profits from her book and her fees for speaking engagements. Dollar amounts frequently emerged in conversations where academics discussed the research, and recently, one of her most vocal critics said her speaking engagement fees made him feel “ill.” One Ivy League professor tweeted, “You can listen to her speak about her irreplicable results for $25-$40,000.” In reality, her book and speaking topics have little to do with the power posing studies.\nMany of the negative comments were personal in nature and aimed at Cuddy directly. “They were comparing me to Harvey Weinstein,” Cuddy says, recalling a post to the PsychMAP Facebook group which the site owners were eventually persuaded to remove. As if that’s not bad enough, Cuddy also received death threats via direct messages on social media. “I became an untouchable in my field,” she adds, describing how colleagues were shunned for merely friending her on Facebook.\nAlthough Cuddy says Harvard wanted her to stay, she left her full-time position, because she no longer felt safe doing her work (she remains a faculty member at Harvard Business School in Executive Education). “My dean wanted me to stay, but I later told him, I couldn’t stay, because I couldn’t endure this abuse. It was relentless,” says Cuddy. Her experiences inspired her new book (still a work in progress) entitled, Bullies, Bystanders and Bravehearts aimed at helping others who find themselves facing similar experiences.\nExerpts from (Elsesser 2024).\n\n\nThis has led to a follow-up discussion […] consisting of two general camps: those who think Cuddy is being treated unfairly, and those who believe that complaints about bullying or harassment are a distraction from the most serious problem here, which is the elevation of shoddy research to a place of such profitable prominence.\nExcept from (Singal 2017)\n\nThere is perhaps a tension here between our ethical obligation to act with integrity in research and our obligation for act with respect (BPS, n.d.). Or perhaps some people are just bullies.\nHave a read through these sources and take a moment to do a quick web search for yourself.\nShare what you think about this situation on the padlet board below. what do you think of the situation surrounding this debate? how can we do better in future?\n\nTaking and receiving criticsm is a huge part of progressive constructive science but can be challenging to do well.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-11_putting-it-together.html#summary.",
    "href": "tutorials/rmb-week-11_putting-it-together.html#summary.",
    "title": "Week 11 : Putting it all together - replicating Power Poses!",
    "section": "",
    "text": "Replication is the gold standard for scientific progress but it is rarely as straightforward as we’d like it to be!\nA wide range of issues can arise - including statistical approaches, experimental design and sometimes interpersonal conflict. We have to navigate all of these points with high ethical standards for the scientific community to progress.\nThank you for your time this semester - I hope you have enjoyed these computer practical sessions!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 11 - Putting it all together"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "This week we will explore how to use Jamovi to compute and visualise descriptive statistics from a dataset. This will include some revision on Jamovi from last semester and an introduction on how we can use the R coding language to simplify complex data processing.\n\n\n\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nMeasures of central tendency (Mean and Median)\n\n\n\nMeasures of dispersion and spread (Standard Deviation and Interquartile Range)\n\n\n\nMeasures of skew and normality\n\n\n\nRecognising different types of distribution\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nExploring and cleaning datasets Jamovi\n\n\n\nComputing descriptive statistics in Jamovi\n\n\n\nComputing descriptive plots in Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with openly available research data\n\n\n\n\n\n\nWe will be exploring data that was collected as part of the ‘Many Labs 3’ project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‘Big Five’ personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs. extravagant/careless)\nAgreeableness (friendly/compassionate vs. critical/rational)\nNeuroticism (sensitive/nervous vs. resilient/confident)\nOpenness to experience (inventive/curious vs. consistent/cautious)\nExtraversion (outgoing/energetic vs. solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.\n\n\n\nWe have data from a sample of students in a big data table. This raw data needs to be summarised and visualised so that we can turn this massive block of numbers into something intuitive.\nWe can generate a concise overview of the main characteristics of the data by computing and visualising few key features from the data.\n\n\n\n\n\n\nWarningOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.\n\n\n\n\n\n\n\n\n\n\n\n\nWarningRevision from RM-A\n\n\n\nThis computer practical will have some revision from RM-A, please check your notes from RM-A for help with loading data and handling variables.\nIf you aren’t familiar with Jamovi, or would like a refresher - please take a quick look at the following topics in the Jamovi user guide\n\nFirst steps in Jamovi\nAnalyses\nThe spreadsheet\n\nPlease ask your tutor for help in the computer practical sessions if you need any assistance.\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file ML3AllSites_RMB.csv from canvas.\nYou can import csv files directly into Jamovi. There is more information in the Jamovi Textbook\nIt’s easy to open csv files in Jamovi. From the top left menu (the button with three parallel lines) choose ‘Open’ and browse to where you have stored the csv file on your computer. If you’re on a Mac, it’ll look like the usual Finder window that you use to choose a file; on Windows it looks like an Explorer window. Find the one you want, then click on the ‘Open’ button.\n\n\n\nThis is the original dataset provided by the authors of the paper! It is a massive dataset - take a moment to get familiar with it by answering the following questions.\nSome of the questions will require you to run a descriptive statistics analysis on the age variable. Run what ever you need to explore the data and the questions.\n\n\n\n\n\n\nCaution Data Skills - How many participants are included in the dataset?\n\n\n\n\n\nThere are 2402 participants in total - this is the number of rows in the dataset\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Is the dataset complete? do we have all information from everyone?\n\n\n\n\n\nNo, unfortunately some data is missing from some participants. You can see empty cells in quite a few places across the dataset. This means that some analyses may have an N of less than 2402 for analyses using some of these variables.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Take a look at the configuration of the Age variable. Is it the correct type?\n\n\n\n\n\nJamovi has to make an educated guess at what type of data is included in each column. Looks like it has correctly set the age variable to be ‘Continuous’.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - How many participants do not have age information?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe N for the age descriptives is 2402 and there are 0 missing participants. This is a large sample and there is age information for everyone.\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - What is the average and standard deviation of the age of the participants?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe mean age is 19.3 years, typical for a student population. The standard deviation is 3.81.\nThis means that a participant who is 19.3+3.81 = 23.11 years old would be one standard deviation above the average age of the sample.\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Do you think all the information in the age variable is accurate?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu. Try checking the box to show the 5 most extreme values in the dataset under the ‘Statistics -&gt; Outliers’ submenu.\nTake a close look at the variable and its descriptive statistics.\nThe youngest age in the dataset is -2 and the oldest age is 149! neither of these can be true and are probably typos (or jokes) made during data collection. Some other values are unusual but not impossible - the second youngest student is 13. We do not know what has happened during data collection to lead to this as this is not our dataset. We should be careful about interpreting the mean whilst these variables are included…\n\n\n\n\nThis process of exploring and understanding your data is a critical first step when working on a new analysis. Real data is rarely in perfect condition - we have to take some time and apply common sense to understand the data and what challenges there might be when analysing it.\nIf we do identify issues, such are large amounts of missing data, incorrect data types or mistakes in data entry, we can normally fix them - but only if they’ve been spotted! If we assume our data is perfect and go straight to the analysis then we could end up including a 149 year old student in our sample…\n\n\n\nIts probably not a good idea to include the data from the participants with impossible ages in our analysis. One simple step we can take is just to remove those rows from our analysis.\n\n\n\n\n\n\nTipData Skills - removing rows from a dataset\n\n\n\nRemove the two participants with impossible ages, you can use the extreme values table in the ‘Descriptives’ output to help find the row numbers. You can remove a row in Jamovi by right clicking on the row number (far left of the Data view) and clicking ‘Delete Row’.\n\nThis will not change the original data file (unless you export the data back out again..), it will only affect the view of the data loaded into Jamovi. If you make a mistake you can simply load the original file and start again.\nNotice that the bottom of the data viewer tells you how many rows have been deleted in that dataset.\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nNow you can recompute the descriptive statistics of your age variable and see what has changed.\n\n\n\nThe mean is pretty much the same - still 19.3 - but the standard deviation is much smaller now. It has reduced from 3.81 to 2.70. That is a close to 25% reduction in the estimate of the variability of this data - big difference. Including outliers like these can have a large impact on our analyses.\n\n\n\n\n\n\nCaution Critical thinking - Are you happy with the data in the age variable now?\n\n\n\n\n\nTough question! take a look at the Minimum value - we still have one participant who is 13 years old. Whilst this is more likely than finding a -2 or 149 year old student, it is still extremely unlikely. It is harder to make a judgement for this participant than the others. The first two participants had impossible ages, whereas a 13 year old student is just very unlikely. Sometimes we have to make a judgement call.\nLet’s delete row 709 to remove this participant as well.\nThe recomputed descriptive statistics now has an N=2399. Removing the 13 year old hasn’t changed our mean or standard deviation estimates very much - indicating that it was less important to remove this data point than the first two.\nThe youngest participant is now 16 which is more realistic (though still very impressive for that 16 year old!).\n\n\n\nThis process can be intricate and involves a lot of clicking in Jamovi - it would be easy to make a mistake and not notice that something has gone wrong. We’ll explore some faster and more transparent ways to do this using R code during this course.\n\n\n\n\nOk, let’s move on to personality factors. As we said at the top, the big five factors are recorded on a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. So someone with an Extraversion score or 6 might be very outgoing where as someone with a score of 1 would be more of an introvert.\n\n\n\n\n\n\nNoteKey step\n\n\n\nCompute the descriptive statistics for the big 5 personality factors.\nTake some time to consider the data quality - use the questions we answered with age as a guide for what to think about.\n\n\n\nNotice that we have a few missing data values in these variables. Jamovi will remove these from any analyses so they shouldn’t affect our descriptive statistics.\n\n\n\n\n\n\nCaution Critical thinking - Are you happy to use the data in the big 5 personality factors?\n\n\n\n\n\nYes, these look reasonable. The data range matches what we would expect from the rating scales (values between 1 and 7) and there is only a small amount of missing data in each of the five variables.\nWe don’t need to do any further data cleaning based on these descriptive statistics.\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding 1\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nYou have reproduced the descriptive statistics for the big 5 personality factors. Now you can do the same for some of the other factors in Table 2 of section 3.5 in Ebersole et al. (2016).\nSpecifically - try and compute the mean and standard deviations for the following factors (Descriptions of the factors are the the cited section of Ebersole et al. (2016)):\n\nSelf Esteem (see section 3.5.1)\nDaily Mood (see section 3.5.3)\nNeed for cognition (see section 3.5.5)\n\nCan you find variables (columns) that correspond to these factors?\nNote: Open datasets sometimes use shorthand for variable names rather than the names used in the manuscript. This is not ideal but does happen in practice! we should be ready take a little time to explore and understand a new dataset when working with it.\n\n\nIn this case the variables in the dataset to use are:\n\n\n\nIn Manuscript\nIn Jamovi\n\n\n\n\nSelf Esteem\n‘SelfEsteem’\n\n\nDaily Mood.\n‘Mood’\n\n\nNeed for cognition\n‘NFC’\n\n\n\nTalk to your tutor if you’re getting stuck.\n\n\nYou should be able to reproduce the following table. Verify that the computed Mean and Standard Deviations match those published in table 2 of Ebersole et al (2016)\n\nThe final mean values are very close to the values in Ebersole et al. (2016) - but not identical. Looks like the original paper kept in all the data that had missing or unusual age values.\n\n\n\n\n\nNext, let’s think about the distributions of these variables. Its great to think about summarising a dataset with its mean value, but this doesn’t tell us about the wider distribution.\n\n\n\n\nNext we’ll have a look at computing histograms from this data. Remember that histograms divide the data range into intervals called bins. The choice of bin width and number of bins can affect the appearance and interpretation of the histogram. Most of the time we can use the automatic bin definitions provided by Jamovi.\nWe can use Histograms to explore a few key factors of the wider data distributions that are hard to see from the descriptive statistics tables.\n\nShape: The shape of the histogram can reveal the distribution pattern of the data, such as normal distribution, skewness, or bimodality.\nOutliers: Histograms can help identify outliers or unusual data points that fall outside the general pattern.\n\nJamovi makes it very simple to compute a histogram. Open the ‘Plots’ sub-menu of the descriptive statistics tab and select the ‘Histogram’ checkbox.\n\n\n\n\n\n\nNoteKey step\n\n\n\nCompute a histogram of the participant ages\n\n\n\nThe descriptives tables give us a numerical summary of the data - but this can be hard to build an intuitive understanding of the dataset. A histogram gives us something immediately interpretable on a more ‘human’ level. Have a think about the following questions.\n\n\n\n\n\n\nCaution Data Skills - What is the average age of the pariticpants in this dataset?\n\n\n\n\n\nWe can see that the most common ages are clustered around the late teenage years and early 20s. We can’t tell the exact mean from the histogram alone - we need to compute the descriptives table for this - but histograms are good to get a quick ‘sense’ of the data.\n\n\n\nIf you haven’t heard about the ‘normal distribution’ in your previous studies, this is a good opportunity to ask Copilot to give you a simple explanation.\n\n\n\n\n\n\nCaution Data Skills - Do the ages look normally distributed?\n\n\n\n\n\nThere is a long tail to this distribution with a very small number of participants in their 30s, 40s and 50s. This is a strong positive skew and likely indicates that the data are not normally distributed.\nThis would be hard to tell from the descriptives table alone.\nOverall, this is a good match for what we would expect from a student population at a large university.\n\n\n\nWe can get an alternative perspective by visualising our data using a ‘Box Plot’.\n\n\n\n\n\n\nNoteKey step\n\n\n\nCompute a Box plot of the participant ages\n\n\n\nThe boxplot visualises the median and interquartile range (25th to 75th percentile) of the data, as well as highlighting outliers. This gives us less resolution around the middle of the distribution close to the median - but does give a much clearer view of the long tail in the ages of the student data sample.\n\n\n\n\n\n\nCaution Data Skills - How old is the oldest participant in the dataset??\n\n\n\n\n\nThe most extreme outliers in the dataset are labelled with their row ID in the boxplot. The largest age value is in row 1081 - it looks like this is around 55 in the boxplot.\nWe can check the corresponding row in the data table to get some more precision. This tells us that the oldest participant is exactly 54 years old.\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding 2\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nCompute histograms and/or boxplots for the big-5 personality factors and take some notes their distributions. Think about the following questions for each factor:\n\nIs the distribution symmetric?\nIf not, is there a positive or a negative skew?\nAre there any outliers?\n\nWrite some quick notes on your answers - we’ll use these again in the next section\n\n\nIt might be easiest to do this one factor at a time to avoid being swamped with figures!\nCreate the figure for a single factor, make some notes and make the next figure with the next factor. You can consider saving the figure into a notebook or document if you want to make a more controlled comparison.\nIf you aren’t sure about any of the concepts - this is a good opportunity to ask your tutors in the session - or use Copilot to give you a simple explanation.\n\n\nHere are my notes about each distribution:\n\nOpenness, this does not look like a normal distribution - there is a negative skew in the data.\nConscientiousness, this is definitely not a normal distribution - there is a strong negative skew.\nExtraversion, this is close to being a normal distribution - perhaps a very slight negative skew.\nAgreeableness, this does not look like a normal distribution - there is a negative skew in the data.\nNeuroticism, this does not look like a normal distribution - there is a positive skew in the data.\n\nNone of the factors have obvious outliers.\n\n\n\n\n\n\n\n\nAs a final step, we can see if our intuitions about the distribution of big-5 personality factors match with more formal statistics measures checking for normal distributions and skewness.\n\n\n\n\n\n\nNoteKey step\n\n\n\nCompute statistics Shapero-Wilk’s test for normality and for ‘Skewness’. You can add both of these tests from the ‘Statistics’ sub-menu of the Descriptives analysis.\n\n\n\n\n\nThe Shapiro-Wilk test evaluates whether a sample comes from a normally distributed population. It returns 2 values:\n\n‘W’ The W statistic measures how well the data conforms to a normal distribution. It ranges from 0 to 1 where values close to 1 indicate the data is normally distributed.\n‘p’ is a p-value indicating the probability that we could see a value of W at least as large as the one we have measured if the data really were normally distributed. Typically, we would say that the data are not normally distributed if p is less than or equal to 0.05.\n\n\n\n\n\n\n\nCaution Data Skills - Do our notes from the histograms match the Shapiro-Wilk statistics?\n\n\n\n\n\nYes, If we look at the W statistics, then we see that Extraversion has the highest value indicating that it has the most ‘normal’ data distribution whilst the other four factors are similar. This matches what we saw with the histograms.\nThe p-values from the formal hypothesis test indicate that none of the 5 factors have normal distributions. We couldn’t tell this from our informal evaulation of the histograms and box plots.\n\n\n\n\n\n\nThe skewness metric returns a number close to zero if the data are symmetrical around zero, and a number close to +1 or -1 if the data are skewed. The sign of the skewness value indicates whether there is a positive or a negative skew.\n\n\n\n\n\n\nCaution Data Skills - Do our notes from the histograms match the skewness statistic?\n\n\n\n\n\nYes, its very close. We can see that ‘Extraversion’ has the least skew with a Skewness metric very close to zero (-0.0714). In contrast, ‘Conscientiousness’ has the most skew. Its value of ‘-0.656’ indicates a very substantial negative skewness in the distribution.\nOpenness and Agreeableness have moderate negative skews, whilst Neuroticism has a moderate positive skew.\n\n\n\n\n\n\n\nIn this session we have gone through the process of exploring a dataset from a published scientific article. We have computed a range of descriptive statistics and used histograms and boxplots to visualise the dataset.\nIt is common for publicly available data to contain little mistakes and issues. We need to go through a process like this with every new dataset and it should be like second nature by the end of this module. Remember to use common sense and practical thinking to identify issues and think about possible solutions.\nRemember this process when collecting your own data. Think carefully about how you can avoid typos and mistakes when collecting a dataset. It can save you and your collaborators a lot of time in future!\n\n\n\n\n\nBlock, Jack. 1995. “A Contrarian View of the Five-Factor Approach to Personality Description.” Psychological Bulletin 117 (2): 187–215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. “Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.” Journal of Experimental Social Psychology 67: 68–82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. “A Very Brief Measure of the Big-Five Personality Domains.” Journal of Research in Personality 37 (6): 504–28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#the-dataset",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#the-dataset",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "We will be exploring data that was collected as part of the ‘Many Labs 3’ project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‘Big Five’ personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs. extravagant/careless)\nAgreeableness (friendly/compassionate vs. critical/rational)\nNeuroticism (sensitive/nervous vs. resilient/confident)\nOpenness to experience (inventive/curious vs. consistent/cautious)\nExtraversion (outgoing/energetic vs. solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#the-challenge",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#the-challenge",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "We have data from a sample of students in a big data table. This raw data needs to be summarised and visualised so that we can turn this massive block of numbers into something intuitive.\nWe can generate a concise overview of the main characteristics of the data by computing and visualising few key features from the data.\n\n\n\n\n\n\nWarningOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#exploring-a-new-dataset-in-jamovi",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#exploring-a-new-dataset-in-jamovi",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "WarningRevision from RM-A\n\n\n\nThis computer practical will have some revision from RM-A, please check your notes from RM-A for help with loading data and handling variables.\nIf you aren’t familiar with Jamovi, or would like a refresher - please take a quick look at the following topics in the Jamovi user guide\n\nFirst steps in Jamovi\nAnalyses\nThe spreadsheet\n\nPlease ask your tutor for help in the computer practical sessions if you need any assistance.\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file ML3AllSites_RMB.csv from canvas.\nYou can import csv files directly into Jamovi. There is more information in the Jamovi Textbook\nIt’s easy to open csv files in Jamovi. From the top left menu (the button with three parallel lines) choose ‘Open’ and browse to where you have stored the csv file on your computer. If you’re on a Mac, it’ll look like the usual Finder window that you use to choose a file; on Windows it looks like an Explorer window. Find the one you want, then click on the ‘Open’ button.\n\n\n\nThis is the original dataset provided by the authors of the paper! It is a massive dataset - take a moment to get familiar with it by answering the following questions.\nSome of the questions will require you to run a descriptive statistics analysis on the age variable. Run what ever you need to explore the data and the questions.\n\n\n\n\n\n\nCaution Data Skills - How many participants are included in the dataset?\n\n\n\n\n\nThere are 2402 participants in total - this is the number of rows in the dataset\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Is the dataset complete? do we have all information from everyone?\n\n\n\n\n\nNo, unfortunately some data is missing from some participants. You can see empty cells in quite a few places across the dataset. This means that some analyses may have an N of less than 2402 for analyses using some of these variables.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Take a look at the configuration of the Age variable. Is it the correct type?\n\n\n\n\n\nJamovi has to make an educated guess at what type of data is included in each column. Looks like it has correctly set the age variable to be ‘Continuous’.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - How many participants do not have age information?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe N for the age descriptives is 2402 and there are 0 missing participants. This is a large sample and there is age information for everyone.\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - What is the average and standard deviation of the age of the participants?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe mean age is 19.3 years, typical for a student population. The standard deviation is 3.81.\nThis means that a participant who is 19.3+3.81 = 23.11 years old would be one standard deviation above the average age of the sample.\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Do you think all the information in the age variable is accurate?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu. Try checking the box to show the 5 most extreme values in the dataset under the ‘Statistics -&gt; Outliers’ submenu.\nTake a close look at the variable and its descriptive statistics.\nThe youngest age in the dataset is -2 and the oldest age is 149! neither of these can be true and are probably typos (or jokes) made during data collection. Some other values are unusual but not impossible - the second youngest student is 13. We do not know what has happened during data collection to lead to this as this is not our dataset. We should be careful about interpreting the mean whilst these variables are included…\n\n\n\n\nThis process of exploring and understanding your data is a critical first step when working on a new analysis. Real data is rarely in perfect condition - we have to take some time and apply common sense to understand the data and what challenges there might be when analysing it.\nIf we do identify issues, such are large amounts of missing data, incorrect data types or mistakes in data entry, we can normally fix them - but only if they’ve been spotted! If we assume our data is perfect and go straight to the analysis then we could end up including a 149 year old student in our sample…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#data-cleaning",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#data-cleaning",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Its probably not a good idea to include the data from the participants with impossible ages in our analysis. One simple step we can take is just to remove those rows from our analysis.\n\n\n\n\n\n\nTipData Skills - removing rows from a dataset\n\n\n\nRemove the two participants with impossible ages, you can use the extreme values table in the ‘Descriptives’ output to help find the row numbers. You can remove a row in Jamovi by right clicking on the row number (far left of the Data view) and clicking ‘Delete Row’.\n\nThis will not change the original data file (unless you export the data back out again..), it will only affect the view of the data loaded into Jamovi. If you make a mistake you can simply load the original file and start again.\nNotice that the bottom of the data viewer tells you how many rows have been deleted in that dataset.\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nNow you can recompute the descriptive statistics of your age variable and see what has changed.\n\n\n\nThe mean is pretty much the same - still 19.3 - but the standard deviation is much smaller now. It has reduced from 3.81 to 2.70. That is a close to 25% reduction in the estimate of the variability of this data - big difference. Including outliers like these can have a large impact on our analyses.\n\n\n\n\n\n\nCaution Critical thinking - Are you happy with the data in the age variable now?\n\n\n\n\n\nTough question! take a look at the Minimum value - we still have one participant who is 13 years old. Whilst this is more likely than finding a -2 or 149 year old student, it is still extremely unlikely. It is harder to make a judgement for this participant than the others. The first two participants had impossible ages, whereas a 13 year old student is just very unlikely. Sometimes we have to make a judgement call.\nLet’s delete row 709 to remove this participant as well.\nThe recomputed descriptive statistics now has an N=2399. Removing the 13 year old hasn’t changed our mean or standard deviation estimates very much - indicating that it was less important to remove this data point than the first two.\nThe youngest participant is now 16 which is more realistic (though still very impressive for that 16 year old!).\n\n\n\nThis process can be intricate and involves a lot of clicking in Jamovi - it would be easy to make a mistake and not notice that something has gone wrong. We’ll explore some faster and more transparent ways to do this using R code during this course.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#descriptives-from-big-5-personality-factors",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#descriptives-from-big-5-personality-factors",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Ok, let’s move on to personality factors. As we said at the top, the big five factors are recorded on a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. So someone with an Extraversion score or 6 might be very outgoing where as someone with a score of 1 would be more of an introvert.\n\n\n\n\n\n\nNoteKey step\n\n\n\nCompute the descriptive statistics for the big 5 personality factors.\nTake some time to consider the data quality - use the questions we answered with age as a guide for what to think about.\n\n\n\nNotice that we have a few missing data values in these variables. Jamovi will remove these from any analyses so they shouldn’t affect our descriptive statistics.\n\n\n\n\n\n\nCaution Critical thinking - Are you happy to use the data in the big 5 personality factors?\n\n\n\n\n\nYes, these look reasonable. The data range matches what we would expect from the rating scales (values between 1 and 7) and there is only a small amount of missing data in each of the five variables.\nWe don’t need to do any further data cleaning based on these descriptive statistics.\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding 1\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nYou have reproduced the descriptive statistics for the big 5 personality factors. Now you can do the same for some of the other factors in Table 2 of section 3.5 in Ebersole et al. (2016).\nSpecifically - try and compute the mean and standard deviations for the following factors (Descriptions of the factors are the the cited section of Ebersole et al. (2016)):\n\nSelf Esteem (see section 3.5.1)\nDaily Mood (see section 3.5.3)\nNeed for cognition (see section 3.5.5)\n\nCan you find variables (columns) that correspond to these factors?\nNote: Open datasets sometimes use shorthand for variable names rather than the names used in the manuscript. This is not ideal but does happen in practice! we should be ready take a little time to explore and understand a new dataset when working with it.\n\n\nIn this case the variables in the dataset to use are:\n\n\n\nIn Manuscript\nIn Jamovi\n\n\n\n\nSelf Esteem\n‘SelfEsteem’\n\n\nDaily Mood.\n‘Mood’\n\n\nNeed for cognition\n‘NFC’\n\n\n\nTalk to your tutor if you’re getting stuck.\n\n\nYou should be able to reproduce the following table. Verify that the computed Mean and Standard Deviations match those published in table 2 of Ebersole et al (2016)\n\nThe final mean values are very close to the values in Ebersole et al. (2016) - but not identical. Looks like the original paper kept in all the data that had missing or unusual age values.\n\n\n\n\n\nNext, let’s think about the distributions of these variables. Its great to think about summarising a dataset with its mean value, but this doesn’t tell us about the wider distribution.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#visualising-distributions",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#visualising-distributions",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Next we’ll have a look at computing histograms from this data. Remember that histograms divide the data range into intervals called bins. The choice of bin width and number of bins can affect the appearance and interpretation of the histogram. Most of the time we can use the automatic bin definitions provided by Jamovi.\nWe can use Histograms to explore a few key factors of the wider data distributions that are hard to see from the descriptive statistics tables.\n\nShape: The shape of the histogram can reveal the distribution pattern of the data, such as normal distribution, skewness, or bimodality.\nOutliers: Histograms can help identify outliers or unusual data points that fall outside the general pattern.\n\nJamovi makes it very simple to compute a histogram. Open the ‘Plots’ sub-menu of the descriptive statistics tab and select the ‘Histogram’ checkbox.\n\n\n\n\n\n\nNoteKey step\n\n\n\nCompute a histogram of the participant ages\n\n\n\nThe descriptives tables give us a numerical summary of the data - but this can be hard to build an intuitive understanding of the dataset. A histogram gives us something immediately interpretable on a more ‘human’ level. Have a think about the following questions.\n\n\n\n\n\n\nCaution Data Skills - What is the average age of the pariticpants in this dataset?\n\n\n\n\n\nWe can see that the most common ages are clustered around the late teenage years and early 20s. We can’t tell the exact mean from the histogram alone - we need to compute the descriptives table for this - but histograms are good to get a quick ‘sense’ of the data.\n\n\n\nIf you haven’t heard about the ‘normal distribution’ in your previous studies, this is a good opportunity to ask Copilot to give you a simple explanation.\n\n\n\n\n\n\nCaution Data Skills - Do the ages look normally distributed?\n\n\n\n\n\nThere is a long tail to this distribution with a very small number of participants in their 30s, 40s and 50s. This is a strong positive skew and likely indicates that the data are not normally distributed.\nThis would be hard to tell from the descriptives table alone.\nOverall, this is a good match for what we would expect from a student population at a large university.\n\n\n\nWe can get an alternative perspective by visualising our data using a ‘Box Plot’.\n\n\n\n\n\n\nNoteKey step\n\n\n\nCompute a Box plot of the participant ages\n\n\n\nThe boxplot visualises the median and interquartile range (25th to 75th percentile) of the data, as well as highlighting outliers. This gives us less resolution around the middle of the distribution close to the median - but does give a much clearer view of the long tail in the ages of the student data sample.\n\n\n\n\n\n\nCaution Data Skills - How old is the oldest participant in the dataset??\n\n\n\n\n\nThe most extreme outliers in the dataset are labelled with their row ID in the boxplot. The largest age value is in row 1081 - it looks like this is around 55 in the boxplot.\nWe can check the corresponding row in the data table to get some more precision. This tells us that the oldest participant is exactly 54 years old.\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding 2\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nCompute histograms and/or boxplots for the big-5 personality factors and take some notes their distributions. Think about the following questions for each factor:\n\nIs the distribution symmetric?\nIf not, is there a positive or a negative skew?\nAre there any outliers?\n\nWrite some quick notes on your answers - we’ll use these again in the next section\n\n\nIt might be easiest to do this one factor at a time to avoid being swamped with figures!\nCreate the figure for a single factor, make some notes and make the next figure with the next factor. You can consider saving the figure into a notebook or document if you want to make a more controlled comparison.\nIf you aren’t sure about any of the concepts - this is a good opportunity to ask your tutors in the session - or use Copilot to give you a simple explanation.\n\n\nHere are my notes about each distribution:\n\nOpenness, this does not look like a normal distribution - there is a negative skew in the data.\nConscientiousness, this is definitely not a normal distribution - there is a strong negative skew.\nExtraversion, this is close to being a normal distribution - perhaps a very slight negative skew.\nAgreeableness, this does not look like a normal distribution - there is a negative skew in the data.\nNeuroticism, this does not look like a normal distribution - there is a positive skew in the data.\n\nNone of the factors have obvious outliers.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#computing-statistics-for-normality-and-skewness.",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#computing-statistics-for-normality-and-skewness.",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "As a final step, we can see if our intuitions about the distribution of big-5 personality factors match with more formal statistics measures checking for normal distributions and skewness.\n\n\n\n\n\n\nNoteKey step\n\n\n\nCompute statistics Shapero-Wilk’s test for normality and for ‘Skewness’. You can add both of these tests from the ‘Statistics’ sub-menu of the Descriptives analysis.\n\n\n\n\n\nThe Shapiro-Wilk test evaluates whether a sample comes from a normally distributed population. It returns 2 values:\n\n‘W’ The W statistic measures how well the data conforms to a normal distribution. It ranges from 0 to 1 where values close to 1 indicate the data is normally distributed.\n‘p’ is a p-value indicating the probability that we could see a value of W at least as large as the one we have measured if the data really were normally distributed. Typically, we would say that the data are not normally distributed if p is less than or equal to 0.05.\n\n\n\n\n\n\n\nCaution Data Skills - Do our notes from the histograms match the Shapiro-Wilk statistics?\n\n\n\n\n\nYes, If we look at the W statistics, then we see that Extraversion has the highest value indicating that it has the most ‘normal’ data distribution whilst the other four factors are similar. This matches what we saw with the histograms.\nThe p-values from the formal hypothesis test indicate that none of the 5 factors have normal distributions. We couldn’t tell this from our informal evaulation of the histograms and box plots.\n\n\n\n\n\n\nThe skewness metric returns a number close to zero if the data are symmetrical around zero, and a number close to +1 or -1 if the data are skewed. The sign of the skewness value indicates whether there is a positive or a negative skew.\n\n\n\n\n\n\nCaution Data Skills - Do our notes from the histograms match the skewness statistic?\n\n\n\n\n\nYes, its very close. We can see that ‘Extraversion’ has the least skew with a Skewness metric very close to zero (-0.0714). In contrast, ‘Conscientiousness’ has the most skew. Its value of ‘-0.656’ indicates a very substantial negative skewness in the distribution.\nOpenness and Agreeableness have moderate negative skews, whilst Neuroticism has a moderate positive skew.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#summary",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#summary",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "In this session we have gone through the process of exploring a dataset from a published scientific article. We have computed a range of descriptive statistics and used histograms and boxplots to visualise the dataset.\nIt is common for publicly available data to contain little mistakes and issues. We need to go through a process like this with every new dataset and it should be like second nature by the end of this module. Remember to use common sense and practical thinking to identify issues and think about possible solutions.\nRemember this process when collecting your own data. Think carefully about how you can avoid typos and mistakes when collecting a dataset. It can save you and your collaborators a lot of time in future!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#references",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#references",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Block, Jack. 1995. “A Contrarian View of the Five-Factor Approach to Personality Description.” Psychological Bulletin 117 (2): 187–215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. “Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.” Journal of Experimental Social Psychology 67: 68–82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. “A Very Brief Measure of the Big-Five Personality Domains.” Journal of Research in Personality 37 (6): 504–28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "support/faculty.html",
    "href": "support/faculty.html",
    "title": "Faculty Pages",
    "section": "",
    "text": "These pages are built using the following tools:\n\n\nQuarto is an open-source scientific and technical publishing system that lets us publish reproducible web pages from plain text pages.\nQuarto defines and builds the website structure in a way that automatically builds links and interactive elements. The github page is already configured to do this automatically when changes are made.\nQuarto defines lots of page organisation options on its documentation\n\n\n\nQuarto works with a slight tweak of Markdown - Markdown is a lightweight language for creating formatted text. The Markdown guide contains a list of syntax options that define how text is formatted.\nYou can use all markdown within Quarto.\n\n\n\nwebexercises is a very lightweight R package that instructors easily create interactive web pages that students can use in self-guided learning.\n\n\n\n\n\n\nYou can edit the website directly using github’s in-browser editor\n\n\n\nThese are the steps to build a local version of the website that you can update and change.\n\nInstall R\nInstall Quarto CLI\nInstall VS Code\nInstall & verify Quarto Add-On in VS Code\n\nQuarto docs\nVS Code Add On\n\nDownload code from github and open folder in VS Code\n\ngithub desktop\ntextbook source code\n\nOpen Project folder in VS Code\nStart R and isntall virtual environment\n\ninstall.packages('renv')\nrenv::install() # Takes about 6 minutes..\n\nYou can the render and preview pages in VS Code\n\nquarto docs on render and preview\n\n\n\n\n\n\n\n\nAny images must be included in the github repository within the ‘images’ subfolder - though they may be organised in any way within that folder.\nWe can then include the images with the following markdown\nThe code:\n\n![](../images/rmb-week-4_computer-practical/rmb-week-4_computer-practical_img0.png)\n\nThe result: \nFull documentation on including images is on the figures page of the quarto documentation\nThe most important option is including additional arguments in braces - {} - for scaling and resizing the image to suit your needs. eg, images can be scales by pixels, percentage of page width or inches/cm.\n\n![Elephant](elephant.png){width=300}\n![Elephant](elephant.png){width=80%}\n![Elephant](elephant.png){width=4in}\n\n\n\n\nAny papers that you want to cite should be included in bibtex format in references.bib on github.\nThis tool will easily generate valid bibtext from a doi link: doi2bib.org\nFor example, if we add the following bibtext reference to references.bib:\n@article{Holmes2009_CanPlaying,\n  title = {Can Playing the Computer Game “Tetris” Reduce the Build-Up of Flashbacks for Trauma? A Proposal from Cognitive Science},\n  volume = {4},\n  ISSN = {1932-6203},\n  url = {http://dx.doi.org/10.1371/journal.pone.0004153},\n  DOI = {10.1371/journal.pone.0004153},\n  number = {1},\n  journal = {PLoS ONE},\n  publisher = {Public Library of Science (PLoS)},\n  author = {Holmes,  Emily A. and James,  Ella L. and Coode-Bate,  Thomas and Deeprose,  Catherine},\n  editor = {Bell,  Vaughan},\n  year = {2009},\n  month = jan,\n  pages = {e4153}\n}\nWe can then create a citation by enclosing the article tag in square brackets and the @ sign - eg [@Holmes2009_CanPlaying]\nThe code:\n\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks [@Holmes2009_CanPlaying]\n\nThe result: One unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009)\nAny referenced materials will be listed at the bottom of the page automatically.\n\n\n\nCallouts are an excellent way to draw extra attention to certain concepts, or to more clearly indicate that certain content is supplemental or applicable to only some scenarios.\nThe Quarto documentation is the main source of info on callout blocks\nThe research methods pages mainly use two types of callout\n\n\nContent information provides context about the content.\nThe code:\n\n::: {.callout-note appearance=\"simple\"}\n# Content Info\n\nThis is the same dataset that we used in week 7.\n\n:::\n\nThe result:\n\n\n\n\n\n\nNoteContent Info\n\n\n\nThis is the same dataset that we used in week 7.\n\n\n\n\n\nKey steps are practical actions that students must complete before progressing to the next section\nThe code:\n\n::: {.callout-note appearance=\"simple\"}\n# Key step\n\nOpen a new ANOVA analysis and run a one-way ANOVA to test the control condition asking whether there was a difference between groups in intrusive memories in the time before the experiment started.\n\nThe variable `Days_Zero_Image_Based_Intrusions_in_Intrusion_Diary`\n\n:::\n\nThe result:\n\n\n\n\n\n\nNoteKey step\n\n\n\nOpen a new ANOVA analysis and run a one-way ANOVA to test the control condition asking whether there was a difference between groups in intrusive memories in the time before the experiment started.\nThe variable Days_Zero_Image_Based_Intrusions_in_Intrusion_Diary\n\n\n\n\n\n\nData skills blocks are slightly modified callout blocks that ask a question with a non-numerical answer that the student should try to explore.\nThe format is similar to the callout blocks above but using a callout-caution style to change the colour and specifying collapse=\"true\" to hide and show the answer.\nThe code\n\n::: {.callout-caution icon=\"false\" collapse=\"true\"}\n## &lt;i class=\"bi bi-question-lg\"&gt;&lt;/i&gt; Data Skills - Is the dataset complete? do we have all information from everyone?\n\nNo, unfortunately some data is missing from some participants. You can see empty cells in quite a few places across the dataset. This means that some analyses may have an N of less than 2402 for analyses using some of these variables.\n:::\n\nThe Result:\n\n\n\n\n\n\nCaution Data Skills - Is the dataset complete? do we have all information from everyone?\n\n\n\n\n\nNo, unfortunately some data is missing from some participants. You can see empty cells in quite a few places across the dataset. This means that some analyses may have an N of less than 2402 for analyses using some of these variables.\n\n\n\nThe question is on the line starting with the following:\n\n## &lt;i class=\"bi bi-question-lg\"&gt;&lt;/i&gt; Data Skills -\n\nThe html adds a question mark icon and probably shouldn’t be changed.\n\n\n\nEquations can be added using latex/MathJax format enclosed in $$ signs. For example:\nThe code:\n\n$$\nt = \\frac{18.6 - 0}{9.96} = 1.867\n$$\n\nThe result:\n\\[\nt = \\frac{18.6 - 0}{9.96} = 1.867\n\\]\nWe can also do this in-line with a single dollar sign. For example the code $X^2$ creates \\(X^2\\).\n\n\nThe big one! ‘Check your understanding’ boxes are larger quiz elements that pose a bigger challenge and might take some minutes to solve.\nThese are large elements with several components. In general, you can copy the section below and just change the content underneath the Question/Hint/Answer subheadings\nThe subheading content can contain any other elements as normal - eg images, references and equations.\nThe code:\n\n:::: callout-tip\n## Check Your Understanding\n\n::: panel-tabset\n## Question\n\nHere is my question\n\n## Hint\n\nHere is a hint!\n\n## Solution\n\nHere is the answer - perhaps with an image.\n\n![](../images/rmb-week-4_computer-practical/rmb-week-4_computer-practical_img5.png){width=\"400px\"}\n\n:::\n::::\n\nThe result:\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHere is my question\n\n\nHere is a hint!\n\n\nHere is the answer - perhaps with an image.\n\n\n\n\n\n\n\n\n\nThe webexercises page is the source documentation for interactive quiz elements. We tend to use the following:\n\n\nThese elements prompt the student to enter a precise value. This uses an R function called fitb which takes the correct response as the input.\nThe code:\n\nthe p-value is `r fitb(0.151)`\n\nThe result:\nthe p-value is \nNote that the backticks `r at the start and ‘`’ at the end must be exactly in place.\n\n\n\nThe multiple choice dropdown uses a similar format\nThe code:\n\nThe study used a `r mcq(c(answer = 'Between subjects', 'Within subjects'))` design.\n\nThe result:\nThe study used a Between subjectsWithin subjects design.\nThe mcq function takes an input as an R collection c() containing all of the options and the correct answer tagged with answer =.",
    "crumbs": [
      "Home",
      "Support",
      "Faculty"
    ]
  },
  {
    "objectID": "support/faculty.html#software",
    "href": "support/faculty.html#software",
    "title": "Faculty Pages",
    "section": "",
    "text": "These pages are built using the following tools:\n\n\nQuarto is an open-source scientific and technical publishing system that lets us publish reproducible web pages from plain text pages.\nQuarto defines and builds the website structure in a way that automatically builds links and interactive elements. The github page is already configured to do this automatically when changes are made.\nQuarto defines lots of page organisation options on its documentation\n\n\n\nQuarto works with a slight tweak of Markdown - Markdown is a lightweight language for creating formatted text. The Markdown guide contains a list of syntax options that define how text is formatted.\nYou can use all markdown within Quarto.\n\n\n\nwebexercises is a very lightweight R package that instructors easily create interactive web pages that students can use in self-guided learning.",
    "crumbs": [
      "Home",
      "Support",
      "Faculty"
    ]
  },
  {
    "objectID": "support/faculty.html#writing-and-changing-content",
    "href": "support/faculty.html#writing-and-changing-content",
    "title": "Faculty Pages",
    "section": "",
    "text": "You can edit the website directly using github’s in-browser editor\n\n\n\nThese are the steps to build a local version of the website that you can update and change.\n\nInstall R\nInstall Quarto CLI\nInstall VS Code\nInstall & verify Quarto Add-On in VS Code\n\nQuarto docs\nVS Code Add On\n\nDownload code from github and open folder in VS Code\n\ngithub desktop\ntextbook source code\n\nOpen Project folder in VS Code\nStart R and isntall virtual environment\n\ninstall.packages('renv')\nrenv::install() # Takes about 6 minutes..\n\nYou can the render and preview pages in VS Code\n\nquarto docs on render and preview",
    "crumbs": [
      "Home",
      "Support",
      "Faculty"
    ]
  },
  {
    "objectID": "support/faculty.html#format-library",
    "href": "support/faculty.html#format-library",
    "title": "Faculty Pages",
    "section": "",
    "text": "Any images must be included in the github repository within the ‘images’ subfolder - though they may be organised in any way within that folder.\nWe can then include the images with the following markdown\nThe code:\n\n![](../images/rmb-week-4_computer-practical/rmb-week-4_computer-practical_img0.png)\n\nThe result: \nFull documentation on including images is on the figures page of the quarto documentation\nThe most important option is including additional arguments in braces - {} - for scaling and resizing the image to suit your needs. eg, images can be scales by pixels, percentage of page width or inches/cm.\n\n![Elephant](elephant.png){width=300}\n![Elephant](elephant.png){width=80%}\n![Elephant](elephant.png){width=4in}\n\n\n\n\nAny papers that you want to cite should be included in bibtex format in references.bib on github.\nThis tool will easily generate valid bibtext from a doi link: doi2bib.org\nFor example, if we add the following bibtext reference to references.bib:\n@article{Holmes2009_CanPlaying,\n  title = {Can Playing the Computer Game “Tetris” Reduce the Build-Up of Flashbacks for Trauma? A Proposal from Cognitive Science},\n  volume = {4},\n  ISSN = {1932-6203},\n  url = {http://dx.doi.org/10.1371/journal.pone.0004153},\n  DOI = {10.1371/journal.pone.0004153},\n  number = {1},\n  journal = {PLoS ONE},\n  publisher = {Public Library of Science (PLoS)},\n  author = {Holmes,  Emily A. and James,  Ella L. and Coode-Bate,  Thomas and Deeprose,  Catherine},\n  editor = {Bell,  Vaughan},\n  year = {2009},\n  month = jan,\n  pages = {e4153}\n}\nWe can then create a citation by enclosing the article tag in square brackets and the @ sign - eg [@Holmes2009_CanPlaying]\nThe code:\n\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks [@Holmes2009_CanPlaying]\n\nThe result: One unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009)\nAny referenced materials will be listed at the bottom of the page automatically.\n\n\n\nCallouts are an excellent way to draw extra attention to certain concepts, or to more clearly indicate that certain content is supplemental or applicable to only some scenarios.\nThe Quarto documentation is the main source of info on callout blocks\nThe research methods pages mainly use two types of callout\n\n\nContent information provides context about the content.\nThe code:\n\n::: {.callout-note appearance=\"simple\"}\n# Content Info\n\nThis is the same dataset that we used in week 7.\n\n:::\n\nThe result:\n\n\n\n\n\n\nNoteContent Info\n\n\n\nThis is the same dataset that we used in week 7.\n\n\n\n\n\nKey steps are practical actions that students must complete before progressing to the next section\nThe code:\n\n::: {.callout-note appearance=\"simple\"}\n# Key step\n\nOpen a new ANOVA analysis and run a one-way ANOVA to test the control condition asking whether there was a difference between groups in intrusive memories in the time before the experiment started.\n\nThe variable `Days_Zero_Image_Based_Intrusions_in_Intrusion_Diary`\n\n:::\n\nThe result:\n\n\n\n\n\n\nNoteKey step\n\n\n\nOpen a new ANOVA analysis and run a one-way ANOVA to test the control condition asking whether there was a difference between groups in intrusive memories in the time before the experiment started.\nThe variable Days_Zero_Image_Based_Intrusions_in_Intrusion_Diary\n\n\n\n\n\n\nData skills blocks are slightly modified callout blocks that ask a question with a non-numerical answer that the student should try to explore.\nThe format is similar to the callout blocks above but using a callout-caution style to change the colour and specifying collapse=\"true\" to hide and show the answer.\nThe code\n\n::: {.callout-caution icon=\"false\" collapse=\"true\"}\n## &lt;i class=\"bi bi-question-lg\"&gt;&lt;/i&gt; Data Skills - Is the dataset complete? do we have all information from everyone?\n\nNo, unfortunately some data is missing from some participants. You can see empty cells in quite a few places across the dataset. This means that some analyses may have an N of less than 2402 for analyses using some of these variables.\n:::\n\nThe Result:\n\n\n\n\n\n\nCaution Data Skills - Is the dataset complete? do we have all information from everyone?\n\n\n\n\n\nNo, unfortunately some data is missing from some participants. You can see empty cells in quite a few places across the dataset. This means that some analyses may have an N of less than 2402 for analyses using some of these variables.\n\n\n\nThe question is on the line starting with the following:\n\n## &lt;i class=\"bi bi-question-lg\"&gt;&lt;/i&gt; Data Skills -\n\nThe html adds a question mark icon and probably shouldn’t be changed.\n\n\n\nEquations can be added using latex/MathJax format enclosed in $$ signs. For example:\nThe code:\n\n$$\nt = \\frac{18.6 - 0}{9.96} = 1.867\n$$\n\nThe result:\n\\[\nt = \\frac{18.6 - 0}{9.96} = 1.867\n\\]\nWe can also do this in-line with a single dollar sign. For example the code $X^2$ creates \\(X^2\\).\n\n\nThe big one! ‘Check your understanding’ boxes are larger quiz elements that pose a bigger challenge and might take some minutes to solve.\nThese are large elements with several components. In general, you can copy the section below and just change the content underneath the Question/Hint/Answer subheadings\nThe subheading content can contain any other elements as normal - eg images, references and equations.\nThe code:\n\n:::: callout-tip\n## Check Your Understanding\n\n::: panel-tabset\n## Question\n\nHere is my question\n\n## Hint\n\nHere is a hint!\n\n## Solution\n\nHere is the answer - perhaps with an image.\n\n![](../images/rmb-week-4_computer-practical/rmb-week-4_computer-practical_img5.png){width=\"400px\"}\n\n:::\n::::\n\nThe result:\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHere is my question\n\n\nHere is a hint!\n\n\nHere is the answer - perhaps with an image.\n\n\n\n\n\n\n\n\n\nThe webexercises page is the source documentation for interactive quiz elements. We tend to use the following:\n\n\nThese elements prompt the student to enter a precise value. This uses an R function called fitb which takes the correct response as the input.\nThe code:\n\nthe p-value is `r fitb(0.151)`\n\nThe result:\nthe p-value is \nNote that the backticks `r at the start and ‘`’ at the end must be exactly in place.\n\n\n\nThe multiple choice dropdown uses a similar format\nThe code:\n\nThe study used a `r mcq(c(answer = 'Between subjects', 'Within subjects'))` design.\n\nThe result:\nThe study used a Between subjectsWithin subjects design.\nThe mcq function takes an input as an R collection c() containing all of the options and the correct answer tagged with answer =.",
    "crumbs": [
      "Home",
      "Support",
      "Faculty"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis is a book created from markdown and executable code.\nSee (knuth84?) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methods - Handbook for Quantitative Methods",
    "section": "",
    "text": "Welcome to outr interactive textbook for quantitative research methods!\n\n\n\n\n\n\nNote\n\n\n\nFor students at the University of Birmingham, these pages are embedded throughout your canvas courses. The pages on this website are a subset of the course material.\nCanvas should be used as your primary resource as it contains the complete overview, timelines, materials and assessments for your module.\n\n\n\n\n\nWeek 8 : Introduction to Jamovi\nWeek 9 : Correlations, Graphs and Exporting\nWeek 10 : Tests of Difference: Chi Squared\n\n\n\n\n\nWeek 1 : Exploring new data and computing descriptives\nWeek 2 : Computing reproducible statistics using Rj\nWeek 3 : One-Sample t-tests\nWeek 4 : Two-Sample t-tests\nWeek 5 : p-values and effect sizes\nWeek 7 : Testing data distributions and Non-parametric alternative tests\nWeek 8 : Simulating Sampling Distributions\nWeek 9 : One Way Analysis of Variance (ANOVA)\nWeek 10 : Simulating Sampling Distributions for t-values\nWeek 11 : Putting it all together - replicating Power Poses\n\n\n\n\n\nWeek 1 : Data handling\nWeek 2 : Review of inferential tests\nWeek 3 : More ANOVAs with Jamovi\nWeek 4 : Factorial ANOVA with Jamovi\n\n\n\n\n\nWeek 1 : Item Wording and Partial Correlation\nWeek 2 : Item Analysis\nWeek 3 : Factor Analysis"
  },
  {
    "objectID": "index.html#research-methods-a",
    "href": "index.html#research-methods-a",
    "title": "Research Methods - Handbook for Quantitative Methods",
    "section": "",
    "text": "Week 8 : Introduction to Jamovi\nWeek 9 : Correlations, Graphs and Exporting\nWeek 10 : Tests of Difference: Chi Squared"
  },
  {
    "objectID": "index.html#research-methods-b",
    "href": "index.html#research-methods-b",
    "title": "Research Methods - Handbook for Quantitative Methods",
    "section": "",
    "text": "Week 1 : Exploring new data and computing descriptives\nWeek 2 : Computing reproducible statistics using Rj\nWeek 3 : One-Sample t-tests\nWeek 4 : Two-Sample t-tests\nWeek 5 : p-values and effect sizes\nWeek 7 : Testing data distributions and Non-parametric alternative tests\nWeek 8 : Simulating Sampling Distributions\nWeek 9 : One Way Analysis of Variance (ANOVA)\nWeek 10 : Simulating Sampling Distributions for t-values\nWeek 11 : Putting it all together - replicating Power Poses"
  },
  {
    "objectID": "index.html#research-methods-c",
    "href": "index.html#research-methods-c",
    "title": "Research Methods - Handbook for Quantitative Methods",
    "section": "",
    "text": "Week 1 : Data handling\nWeek 2 : Review of inferential tests\nWeek 3 : More ANOVAs with Jamovi\nWeek 4 : Factorial ANOVA with Jamovi"
  },
  {
    "objectID": "index.html#research-methods-d",
    "href": "index.html#research-methods-d",
    "title": "Research Methods - Handbook for Quantitative Methods",
    "section": "",
    "text": "Week 1 : Item Wording and Partial Correlation\nWeek 2 : Item Analysis\nWeek 3 : Factor Analysis"
  },
  {
    "objectID": "support/copilot.html",
    "href": "support/copilot.html",
    "title": "Using Copilot at University of Birmingham",
    "section": "",
    "text": "Microsoft Copilot is now available to use for University of Birmingham members. See the University documentation pages for guidelines on ethical use of Generative AI.\nYou are welcome to use Copilot to support your learning of statistics and coding throughout the Research Methods B computer practical sessions. In particular, Copilot can be a useful way to get a second perspective on different technical concepts, to help generate data analysis code or to understand where data analysis code might have gone wrong.\nThroughout the course - we will suggest places where you might benefit from interacting with Copilot to support your learning. Look out for dialogue boxes like this one.\n\n\n\n\n\n\nTipCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about variables in R. We suggest you try the following prompts to get started.\n\n“Write a simple description of variables in R for a beginner”\n“How can I choose a name for my variable in R”\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\n\n\n\n\n\n\nImportantEthical use of Copilot\n\n\n\nTools like Copilot can be a valuable learning aid, however it is critical that you familiarise yourself with the University’s student and PGR guidance on using GenAI tools ethically for work. This guidance provides you with the risks and limitations of using Generative AI tools in your academic study\nIt is not permitted to use Generative AI tools in assessment unless explicitly stated on the assessment page. It is your responsibility to use these tools in an appropriate way.\n\n\n\n\nHere are some questions with Copilot’s answers to help you decide how and when to use Copilot. Note that Copilot may respond differently if you ask identical prompts.\n\n\n\nStarting to learn R with the help of Copilot can be a smooth and enjoyable process. Here are some steps and tips to get you started:\n\nBegin by asking Copilot to explain fundamental concepts in R. This will give you a solid foundation to build on. - Example Questions: - “What are the basic data types in R?” - “How do I create a vector in R?”\n\nRequest detailed guides for common tasks. This helps you understand the process and logic behind the code. - Example Questions: - “Can you show me how to read a CSV file into R?” - “How do I create a simple plot using ggplot2?”\n\nReinforce your learning by practicing with beginner exercises. This will help you apply what you’ve learned. - Example Questions: - “Can you give me a beginner exercise for data manipulation in R?” - “What are some basic exercises to practice using loops in R?”\n\nLearn about important R packages that can simplify your work. Copilot can introduce you to these packages and show you how to use them. - Example Questions: - “What is the dplyr package and how do I use it?” - “Can you explain how to use the tidyverse package for data analysis?”\n\nWhen you encounter errors, ask Copilot for help in understanding and fixing them. This will improve your debugging skills. - Example Questions: - “I’m getting an error with my R code, can you help me debug it?” - “What does this error message mean in R?”\n\nAsk about best practices for writing clean and efficient R code. This will help you develop good &gt; coding habits. - Example Questions: - “What are some best practices for writing R code?” - “How can I make my R scripts more efficient?”\n\nSeeing examples can be very helpful. Ask for code snippets to understand how to implement different &gt; functions and methods. - Example Questions: - “Can you show me an example of how to use the print function in R?” - “How do I perform a linear regression in R?”\n\nDon’t hesitate to ask about anything you’re curious about, even if it seems basic. Copilot is here to help you learn at your own pace. - Example Questions: - “What are some good resources for learning R?” - “How do I create a histogram in R?”\n\n\n“How do I extract specific rows from a data frame in R?”\n“What are some common data visualization techniques in R?”\n\nBy leveraging Copilot’s capabilities, you can get personalized assistance and resources tailored to your learning needs. Remember, learning a new language takes time and practice, so be patient with yourself and keep experimenting. Happy coding! If you have any specific questions or need further guidance, feel free to ask!\n\n\n\n\n\nCode generated by Copilot can be a great starting point, but it’s important to review and test it thoroughly. Here are a few tips to ensure the code meets your needs:\n\nReview for Accuracy: Check the logic and syntax to make sure it aligns with your requirements.\nTest Thoroughly: Run tests to ensure the code works as expected in different scenarios.\nSecurity Considerations: Be mindful of potential security vulnerabilities and address them as needed.\nCustomization: Adapt and refine the code to better fit your specific use case.\n\nUsing these practices can help you make the most of Copilot’s assistance while ensuring the quality and reliability of your code. If you have any specific coding questions or need help with a particular piece of code, feel free to ask! 😊",
    "crumbs": [
      "Home",
      "Support",
      "Using Copilot at UoB"
    ]
  },
  {
    "objectID": "support/copilot.html#copilot-giving-advice-about-how-to-use-copilot",
    "href": "support/copilot.html#copilot-giving-advice-about-how-to-use-copilot",
    "title": "Using Copilot at University of Birmingham",
    "section": "",
    "text": "Here are some questions with Copilot’s answers to help you decide how and when to use Copilot. Note that Copilot may respond differently if you ask identical prompts.\n\n\n\nStarting to learn R with the help of Copilot can be a smooth and enjoyable process. Here are some steps and tips to get you started:\n\nBegin by asking Copilot to explain fundamental concepts in R. This will give you a solid foundation to build on. - Example Questions: - “What are the basic data types in R?” - “How do I create a vector in R?”\n\nRequest detailed guides for common tasks. This helps you understand the process and logic behind the code. - Example Questions: - “Can you show me how to read a CSV file into R?” - “How do I create a simple plot using ggplot2?”\n\nReinforce your learning by practicing with beginner exercises. This will help you apply what you’ve learned. - Example Questions: - “Can you give me a beginner exercise for data manipulation in R?” - “What are some basic exercises to practice using loops in R?”\n\nLearn about important R packages that can simplify your work. Copilot can introduce you to these packages and show you how to use them. - Example Questions: - “What is the dplyr package and how do I use it?” - “Can you explain how to use the tidyverse package for data analysis?”\n\nWhen you encounter errors, ask Copilot for help in understanding and fixing them. This will improve your debugging skills. - Example Questions: - “I’m getting an error with my R code, can you help me debug it?” - “What does this error message mean in R?”\n\nAsk about best practices for writing clean and efficient R code. This will help you develop good &gt; coding habits. - Example Questions: - “What are some best practices for writing R code?” - “How can I make my R scripts more efficient?”\n\nSeeing examples can be very helpful. Ask for code snippets to understand how to implement different &gt; functions and methods. - Example Questions: - “Can you show me an example of how to use the print function in R?” - “How do I perform a linear regression in R?”\n\nDon’t hesitate to ask about anything you’re curious about, even if it seems basic. Copilot is here to help you learn at your own pace. - Example Questions: - “What are some good resources for learning R?” - “How do I create a histogram in R?”\n\n\n“How do I extract specific rows from a data frame in R?”\n“What are some common data visualization techniques in R?”\n\nBy leveraging Copilot’s capabilities, you can get personalized assistance and resources tailored to your learning needs. Remember, learning a new language takes time and practice, so be patient with yourself and keep experimenting. Happy coding! If you have any specific questions or need further guidance, feel free to ask!\n\n\n\n\n\nCode generated by Copilot can be a great starting point, but it’s important to review and test it thoroughly. Here are a few tips to ensure the code meets your needs:\n\nReview for Accuracy: Check the logic and syntax to make sure it aligns with your requirements.\nTest Thoroughly: Run tests to ensure the code works as expected in different scenarios.\nSecurity Considerations: Be mindful of potential security vulnerabilities and address them as needed.\nCustomization: Adapt and refine the code to better fit your specific use case.\n\nUsing these practices can help you make the most of Copilot’s assistance while ensuring the quality and reliability of your code. If you have any specific coding questions or need help with a particular piece of code, feel free to ask! 😊",
    "crumbs": [
      "Home",
      "Support",
      "Using Copilot at UoB"
    ]
  },
  {
    "objectID": "support/frequently-asked-questions.html",
    "href": "support/frequently-asked-questions.html",
    "title": "Getting started with Jamovi",
    "section": "",
    "text": "Here are instructions and links about how to install Jamovi on your personal/home computer. It should already be installed in our computer rooms.\n\nGo to the Jamovi website\nClick on ‘Jamovi desktop’:\n\n\n\nClick the relevant version recommended for your device (the version number may not exactly match the image as new versions are released regularly - but should still say recommended)\n\n\n\nDownload the file and follow the installation instructions depending on your device.\n\n\n\n\nFirst check the start menu, if you can find both Jamovi and Rstudio then just open both from there and get started. Any programme that is not in the start menu can be opened using AppsAnywhere. This is available in the start menu of every campus computer – general information about AppsAnywhere can be found on this IT services page\n\n\n\nThis should be available as an icon on the desktop, or from within the start menu. If not, they you can try the web interface - https://apps.bham.ac.uk/ . The website may ask you to confirm that AppsAnywhere is installed on your device – you can click ‘I have it already’ if using a campus computer.\n\n\n\nOnce you have access to AppsAnywhere, you may have to log-in using your UoB credentials. The homepage of AppsAnywhere contains a search bar and will look something like this:\n\nYou can search for Jamovi and Rstudio in the search bar and click ‘Launch’ to run them. If this is the first time you’re running the app it may take a few minutes to get started. This is as AppsAnywhere is downloading and installing the programme on your computer.\n\n\n\nAppyAnywhere needs to be able to link to your desktop computer to run properly, the validation process ensures that this is working properly. Normally all you need to do is click to start validation and answer ‘Open AppsAnywhere Launcher’ when prompted by the browser.",
    "crumbs": [
      "Home",
      "Getting Started with Jamovi",
      "Getting Started with Jamovi"
    ]
  },
  {
    "objectID": "support/frequently-asked-questions.html#installing-jamovi-on-your-personal-computer",
    "href": "support/frequently-asked-questions.html#installing-jamovi-on-your-personal-computer",
    "title": "Getting started with Jamovi",
    "section": "",
    "text": "Here are instructions and links about how to install Jamovi on your personal/home computer. It should already be installed in our computer rooms.\n\nGo to the Jamovi website\nClick on ‘Jamovi desktop’:\n\n\n\nClick the relevant version recommended for your device (the version number may not exactly match the image as new versions are released regularly - but should still say recommended)\n\n\n\nDownload the file and follow the installation instructions depending on your device.",
    "crumbs": [
      "Home",
      "Getting Started with Jamovi",
      "Getting Started with Jamovi"
    ]
  },
  {
    "objectID": "support/frequently-asked-questions.html#i-cant-find-jamovirstudio-on-my-campus-computer.",
    "href": "support/frequently-asked-questions.html#i-cant-find-jamovirstudio-on-my-campus-computer.",
    "title": "Getting started with Jamovi",
    "section": "",
    "text": "First check the start menu, if you can find both Jamovi and Rstudio then just open both from there and get started. Any programme that is not in the start menu can be opened using AppsAnywhere. This is available in the start menu of every campus computer – general information about AppsAnywhere can be found on this IT services page",
    "crumbs": [
      "Home",
      "Getting Started with Jamovi",
      "Getting Started with Jamovi"
    ]
  },
  {
    "objectID": "support/frequently-asked-questions.html#i-cant-find-appsanywhere-on-my-campus-computer.",
    "href": "support/frequently-asked-questions.html#i-cant-find-appsanywhere-on-my-campus-computer.",
    "title": "Getting started with Jamovi",
    "section": "",
    "text": "This should be available as an icon on the desktop, or from within the start menu. If not, they you can try the web interface - https://apps.bham.ac.uk/ . The website may ask you to confirm that AppsAnywhere is installed on your device – you can click ‘I have it already’ if using a campus computer.",
    "crumbs": [
      "Home",
      "Getting Started with Jamovi",
      "Getting Started with Jamovi"
    ]
  },
  {
    "objectID": "support/frequently-asked-questions.html#how-do-i-use-appsanywhere",
    "href": "support/frequently-asked-questions.html#how-do-i-use-appsanywhere",
    "title": "Getting started with Jamovi",
    "section": "",
    "text": "Once you have access to AppsAnywhere, you may have to log-in using your UoB credentials. The homepage of AppsAnywhere contains a search bar and will look something like this:\n\nYou can search for Jamovi and Rstudio in the search bar and click ‘Launch’ to run them. If this is the first time you’re running the app it may take a few minutes to get started. This is as AppsAnywhere is downloading and installing the programme on your computer.",
    "crumbs": [
      "Home",
      "Getting Started with Jamovi",
      "Getting Started with Jamovi"
    ]
  },
  {
    "objectID": "support/frequently-asked-questions.html#appsanywhere-keeps-asking-me-about-validation.",
    "href": "support/frequently-asked-questions.html#appsanywhere-keeps-asking-me-about-validation.",
    "title": "Getting started with Jamovi",
    "section": "",
    "text": "AppyAnywhere needs to be able to link to your desktop computer to run properly, the validation process ensures that this is working properly. Normally all you need to do is click to start validation and answer ‘Open AppsAnywhere Launcher’ when prompted by the browser.",
    "crumbs": [
      "Home",
      "Getting Started with Jamovi",
      "Getting Started with Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-3_more-anovas-with-jamovi.html",
    "href": "tutorials/rmc_week-3_more-anovas-with-jamovi.html",
    "title": "Week 3 : More ANOVAs with Jamovi",
    "section": "",
    "text": "Quantitative Methods\n\n\n\n\n\nOne-way within participants ANOVA in Jamovi\n\n\n\nKruskal-Wallis Test (non-parametric between-participants ANOVA)\n\n\n\nFriedman Test (non-parametric within-participants ANOVA)\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Jamovi editor\n\n\n\nSetting up data files for between- and within-participants designs\n\n\n\nComputing descriptive statistics in Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with openly available research data\n\n\n\n\n\n\n\n\n\nNoteToday’s session\n\n\n\nWorkshop 3 is about how to carry out one-way ANOVAs when the same participants have taken part in three or more conditions of a study. It also introduces the non-parametric equivalents of one-way ANOVAs in Jamovi. We can use these tests for data that do not meet parametric assumptions.\nAs with a one-way between-participants ANOVA, one-way within-participants ANOVAs have a single factor (IV) which has more than two levels (conditions), and a single DV.\n\n\nAs stated in the previous workshop, one-way ANOVAs are carried out differently in Jamovi depending on whether the IV is a within- or between-participants factor. The following section shows how to perform a one-way within-participants ANOVA.\n\n\n\n\n\n\nThe next example is very similar to the experiment described for the one-way between-participants ANOVA in Walk-Through Example 5. For the within-participants example, the experiment has been conducted as a repeated measures experiment: the same participants carried out the tasks for all three conditions.\nThere is still one DV (response time) and one factor (type of priming), with 3 levels (surface, deep, no prime). However, now the factor is within-participants (or a ‘repeated measure’).\nThe hypotheses are (almost) the same as well but the data need to be entered differently into Jamovi, as in the next table. Each participant’s data must be entered on a single row, so where a participant provides three data points, these need to be entered next to each other along the same row.\n\n\n\n\n\n\n\n\n\nSurface Priming RT (scale variable) [surface_data]\nDeep Priming RT (scale variable) [deep_data]\nNo-Priming RT (scale variable) [no_prime_data]\n\n\n\n\nParticipant 1 data for surface\nParticipant 1 data for deep\nParticipant 1 data for no_prime\n\n\nParticipant 2 data for surface\nParticipant 2 data for deep\nParticipant 2 data for no_prime\n\n\nParticipant 3 data for surface\nParticipant 3 data for deep\nParticipant 3 data for no_prime\n\n\n\n\nThe hypotheses tested were:\n\nParticipants will categorise words which they have previously processed at a deep level more quickly than words they have processed at a surface level or in the no prime condition.\nIf surface processing has any effect on later categorization, the response times in the surface condition should be faster than in the no prime condition.\n\n\nWalk-Through Example 1 “The effect of priming on a categorization task (repeated measures)”\n\n\n\nDownload and open the RMC_NM1_1way_within_ANOVA.sav dataset from the Canvas folder.\nClick on the “Data” tab and check the data looks something like below…\n\n\n\n\n\n\nGo to Analyses -&gt; Exploration -&gt; Descriptives. In the dialog box, enter all repetitions of the DV into the ‘Variables’ box.\nGo to ‘Plots’. Check ‘Histogram’ and ‘Box Plot’ Check whether the data are normally distributed (a parametric assumption that needs to be met for ANOVA).\n\n\nIn this example all the data are positively skewed. This is the usual pattern in response time data and there has been some controversy around how this should be dealt with when analysing such data.\nFor the purposes of Level 2 research methods, we will treat response time data as ‘approximately normal’ where the long tail of the distribution does not extend beyond 3 standard deviations above the mean.\n\n\n\n\n\nGo to Analyses -&gt; ANOVA -&gt; Repeated Measures ANOVA\n\n\n\nIn the dialog box, enter a name for the IV into the ‘RM Factor 1’ box (e.g. “Priming”). Change the name ‘Level 1’ to the name of the first condition. Continue with the other 2 conditions (change ‘Level 2’ to the second and change ‘Level 3’ into the third).\nMove the conditions into their respective cells in the ‘Repeated Measures Cells’ box.\nClick the box ‘Partial η2’ under Effect Size.\nGo to ‘Estimated Marginal Means’. Enter the IV into the ‘Term 1’ box. Select ‘Marginal mean table’.\nGo to ‘Assumption Checks’ and check ‘Sphericity tests’\n\n\n\n\nGot to ‘Post Hoc Tests’. Place ‘Priming’ into the right-side box. Check ‘Bonferroni’ under Corrections.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipJamovi Output One-way ANOVA (within) questions\n\n\n\n\nQuestion 1\n\n\nWhich (if any) condition(s) is significantly different from “no prime”? surface and deepsurface onlydeep onlyneither are significant\nWhich (if any) condition(s) is significantly different from “deep”? surface and no primesurface onlyno prime onlyneither are significant\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipOne-way ANOVA (within) Write Up\n\n\n\n\nFill in the blanks\n\n\n(Any data preparation, e.g. how the response times were calculated or how the data were trimmed or if and how many outliers were removed, should be presented in the section of the method about data preparation and data analysis.)\nIn a report, an ANOVA results section could look like this:\nResults\nAlthough the mean categorization times were lowest in the deep priming condition as predicted, the categorization times in the no priming condition were very similar to those in the surface priming condition. A one-way repeated measures ANOVA found a small but significant effect of priming condition on categorization times (see Table 1).\nTable 1 Means (Standard Deviations in Parentheses), and One-Way Analysis of Variance Statistics for Categorization Times (ms) for the Priming Conditions.\n\n\n\n\n\n\n\n\n\n\n\nMeasure\n\nPriming Condition\n\nF(2,[])\nη2p\n\n\n\n\n\nSurface\nDeep\nNo-prime\n\n\n\n\nCategorisation time\n (388)\n1701 (384)\n1751 ()\n3.6*\n.03\n\n\n\n*p &lt;0.05\nUsing Bonferroni-corrected pairwise comparisons to carry out post-hoc analyses, none of the difference between the conditions were found to be significant. The difference between the surface priming and  conditions (mean difference =  ms, p = ) came closest to significance. The difference between  and no priming (mean difference = 50ms, p = .077) also approached significance. There was no significant difference between surface and  conditions. These findings supportpartially supportdo not support the hypothesis.\n\n\n\n\n\n\n\n\n\n\n\nIf the data do not meet parametric assumptions, we should take a more conservative approach to avoid making errors in the conclusions we draw.\nWhen we are dealing with data that are strongly skewed and/or have unequal variances, we should perform non-parametric tests to explore our effects. Non-parametric tests are also useful for ordinal data, as they work with ranks rather than absolute values.\n\nNon-parametric equivalents\n\nMost of the tests you have covered so far have a direct non-parametric equivalent. The table below shows how they map:\n\n\n\n\nParametric Test\nNon-Parametric Equivalent\n\n\n\n\nPearson’s correlation*\nSpearman’s correlation*\n\n\nIndependent samples t-test\nMann-Whitney\n\n\nPaired samples t-test\nWilcoxon\n\n\nBetween-participants one-way ANOVA\nKruskal-Wallis\n\n\nRepeated-measures one-way ANOVA\nFriedman\n\n\n\n*More on correlations in RMD\n\n\n\nFor a one-way ANOVA design with more than two independent groups, where the data do not meet parametric assumptions, we can use the Kruskal-Wallis test. We might also use this with very small or unequal sample sizes.\nIn this example, we want to know whether the day of the week (Monday, Tuesday, Thursday, or Friday) on which a group of students attends workshops for a research methods module affects how much they look forward to the module (rated on a 5-point scale).\nThe hypothesis tested was:\n\nThere are differences in self-rated looking forward score for taking part in the research methods module depending on the day of the week on which participants attend the workshop.\n\nNote – this is a two-sided hypothesis as no direction for the effect of day of attendance is specified.\n\n\n\nWalk-Through Example 2 “The effect of day of the week on looking forward to a research methods module.”\n\nDownload and open the RMC_NM1_lookfwd_Kruskal-Wallis.sav dataset\nClick Analyses -&gt; Exploration -&gt; Descriptives. In the dialog box, enter the DV into the ‘Variables’ box. Enter the IV into the ‘Split by’ box.\nGo to ‘Plots’. Check ‘Histograms’.\n\nRecord the medians and minimum and maximum scores from the Explore output [which is not presented here].\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure\n\n\n\nDay of the Week\n\n\n\n\n\n\n\n\n\nMonday\n\nTuesday\n\nThursday\n\nFriday\n\n\n\n\nMdn\nMin-Max\nMdn\nMin-Max\nMdn\nMin-Max\nMdn\nMin-Max\n\n\nLooking forward score\n3.0\n2-4\n3.0\n2-3\n4.0\n3-4\n3.0\n1-3\n\n\n\n\nGo to Analyses -&gt; ANOVA -&gt; One-Way ANOVA Kruskal-Wallis (under non-parametric)\nEnter the DV into the ‘Dependent Variables’ box. Enter the between-participants factor into the ‘Grouping Variable’ box\nTick the ‘Effect size’ box\n\n\n\n\n\n\n\nSince the Kruskal-Wallis test is significant, we need to follow it up with a post-hoc test to determine where the significant differences between the groups occur.\nWe could do this by performing Mann-Whitney tests (non-parametric comparison between two independent groups) to compare the different groups (levels) of the IV.\n\nTo avoid family-wise error, adjust the significance level depending on the number of paired comparisons performed using Bonferroni’s correction.\nThe correction is worked out by dividing the p-value acceptable for significance by the number of comparisons to be performed:\n\nE.g. if we accept p &lt; .05 as a cut-off for significance, then for 4 comparisons, the Bonferroni adjusted p-value =.05 / 4 = .0125. This means that any p-value greater than .0125 would not be considered significant.\n\n\nHowever, there is no simple way of doing this on Jamovi, unless you create new data sheets (or new variables) with only two subgroups included for each analysis.\nJamovi offers an option for pairwise comparisons called the ‘Dwass-Steel-Crithclow-Fligner pairwise comparions’ or ‘DSCF pairwise comparisons’. There’s a box you can select when running the Kruskal-Wallis test.\n\nThis test is two-sided and provides protection for family-wise error rates, like the Bonferroni correction does.\n\n\n\n\n\n\n\n\nNoteReminder\n\n\n\nWe need to look up the medians and minimum and maximum values for each condition from the Exploration analysis and report those.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipKruskal-Wallis Write Up\n\n\n\n\nFill in the blanks\n\n\n(Any data preparation, e.g. how the response times were calculated or how the data were trimmed or if and how many outliers were removed, should be presented in the section of the method about data preparation and data analysis.)\nIn a report, a Kruskal-Wallis results section could look like this:\nResults\nBecause the participant numbers were small and unequal across conditions and the score for how much participants looked forward to the module was ordinal and not normally distributed, a Kruskal-Wallis test was performed to examine the effect of day of the week upon the mean look-forward score (out of 5). It was found that day of the week had a significant effect on look-forward ratings, chi-sq2(3) = , p = , ε2 = .379.\nTable 1 shows the median look-forward scores for each day and paired comparisons (DSCF pairwise comparisons). Only the difference between look-forward scores for Tuesday compared with  was statistically significant. The hypothesis that the day of study affects how much students look forward to their research methods module was supportedpartially supportednot supported.\nTable 1 Median look forward scores (out of 5) by day of the week and including significance of paired comparisons.\n\n\n\n\n\n\n\n\n\n\n\nDay\nMedian look forward score\nPaired comparison significance\n\n\n\n\n\n\n\n\n\nMonday\nTuesday\nThursday\nFriday\n\n\nMonday\n\n-\n-\n-\n-\n\n\nTuesday\n3\np - .470\n-\n-\n-\n\n\nThursday\n\np - .194\np - .045*\n-\n-\n\n\nFriday\n3\np - .370\np - .978\np - .051\n-\n\n\n\n*p &lt;0.05\n\n\n\n\n\n\n\n\n\n\nWith a related (within-participants or repeated measures) design, where the data are non-parametric, we can use the non-parametric equivalent of the repeated measures one-way ANOVA: the Friedman test.\nIn this example, we test if there is a difference in how much the group of students from the previous example looks forward to their workshops on the research methods module (rated on a 5-point scale) depending on how long they have been attending the module (at the start, after 2 weeks, after 5 weeks).\nThe hypothesis tested was:\n\nThere are differences in self-rated looking forward score for taking part in the research methods module depending on how long the participants have been attending the workshops.\n\n\nWalk-Through Example 3 “The effect of length of attendance on looking forward to a module”\n\nDownload and open the RMC_NM1_lookfwd_Friedman.sav dataset\nGo to Analyses -&gt; Exploration -&gt; Descriptives. In the dialog box, enter all repetitions of the DV into the ‘Variables’ box.\nGo to ‘Plots’. Check ‘Histograms’.\n\nRecord the medians and other key descriptive statistics (e.g. maximum and minimum scores) from the Explore output [which is not presented here].\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure\n\nTime spent studying the module\n\n\n\n\n\n\n\n\n\nAt Start\n\nAfter 2 Weeks\n\nAfter 5 Weeks\n\n\n\n\nMdn\nMin-Max\nMdn\nMin-Max\nMdn\nMin-Max\n\n\nLooking forward score\n3.0\n2-5\n3.0\n1-4\n3.0\n1-4\n\n\n\n\nGo to Analyses -&gt; ANOVA -&gt; Repeated Measures ANOVA Friedman (under non-parametric)\nEnter all levels of the DV into the ‘Measures’ box\n\n\n\n\n\n\nSince the Friedman test is not significant, we do not need to carry out a post-hoc analysis.\nIf we did need to carry out post-hocs, we would use Wilcoxon tests (non-parametric comparison between two related conditions) to compare pairs of conditions.\n\nTo avoid family-wise error, adjust the significance level depending on the number of paired comparisons performed using Bonferroni’s correction, i.e. divide the p-value acceptable for significance by the number of comparisons to be performed:\n\nE.g. for 3 comparisons, the Bonferroni adjusted p-value = 0.05 / 3 = 0.0167.\n\nThis is can be done easily on Jamovi. It should look something like this.\n\n\n\n\n\n\n\nAlternatively, the Friedman test dialogue on Jamovi also offers Durbin-Conover Pairwise Comparisons, however, this test does not correct for family-wise error.\n\nIf you use Durbin-Conover comparisons, you should use a correction against the p-value you use for the comparisons.\n\nE.g. for 3 comparisons, the Bonferroni adjusted p-value = 0.05 / 3 = 0.0167.\nIn a results section, state that you used the Bonferroni correction and what the new criterion for significance was against which you compare the individual p-values for the comparisons.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteReminder\n\n\n\nWe need to look up the medians and minimum and maximum values for each condition from the Exploration analysis and report those.\n\n\n\n\n\n\n\n\n\n\n\nTipFriedman Write Up\n\n\n\n\nFill in the blanks\n\n\n(Any data preparation, e.g. how the response times were calculated or how the data were trimmed or if and how many outliers were removed, should be presented in the section of the method about data preparation and data analysis.)\nIn a report, a Friedman results section could look like this*:\nResults\nHistograms for the three conditions were inspected separately. As the data were skewed and the look forward scores were measured on an ordinal scale, a Friedman test was used. There was no significant effect of time spent studying the module upon look forward scores, chi-sq2() = 2.86, p = . In fact, the median score for all three conditions was identical at , and the hypothesis that time spent studying on the module would affect how much people looked forward to the workshop sessions was supportedpartially supportednot supported.\n*Note that the post-hocs are not reported here, as the main analysis was non-significant.\n\n\n\n\n\n\n\n\n\n\n\nNoteNOTE\n\n\n\nDownload and open the RMC_NM1_SPSS_attitude.sav dataset and answer the following questions. Before you start, recode the marks data into a new grouping variable called “grade_class” so that participants are allocated to either first_class (70-100), upper_second (60-69) or lower_second_or_below (0-59) groups.\n\n\n\n\n\n\n\n\nTipActivity Questions\n\n\n\n\nQuestion 1Question 1 NotesQuestion 2Question 3Question 4Question 5\n\n\nWhich type of statistical analysis would you need to carry out to test the hypothesis that “People with first class grades, upper second class grades, and lower second class or below grades on the first practical vary on their total score for the attitudes towards SPSS* (where a high score indicates a positive attitude towards SPSS)”? Independent ANOVARepeated measures ANOVAKruskal-WallisFriedman\nThink about you reasoning for choosing this test before looking at the question 1 notes.\n*This is the stats package we taught before we shifted to teaching Jamovi.\n\n\nThe wording of the hypothesis suggests that there are different groups of people being studied. This means that an independent (between-participants) design was being used.\nThere is a single factor which could be called grade-class or something similar. This means that the design is a one-way design.\nIf you Explore the data using SPSS, you can see that the data on the SPSS attitudes (the DV) are normally distributed - either if you look at the DV as a whole or if you look at the different grade-range groups.\nSample sizes are also similar and the range of variation in each group is similar.\nBased on this consideration, it is appropriate to us an ANOVA to test the hypothesis.\nA Kruskal-Wallis analysis would also be correct, if parametric assumptions are not met.\n\n\nUsing the appropriate walk-through example (from Workshops 2 or 3), carry out this test and inspect the output. How many participants are there in the “lower_second_or_below” group?\nN = \n\n\nWhat is the mean score and standard deviation for the “first_class” group?\nMean = \nSD = \n\n\nWhat is the appropriate test statistic (including the appropriate degrees of freedom)? What was the value of the test statistic (if it is a Greek letter type the name e.g. ‘chi-square’) and what was the associated p-value?\nTest statistic  = ; p = \nWhat was the measure of effect size (type its name, e.g. ‘partial eta squared’ or ‘epsilon squared’) and what was the value of the effect size?\nEffect size  = \n\n\nIf the test was significant, between which groups do the significant differences occur? The ‘first class’ group significantly differed from the ‘lower second class or below’ group ONLYThe ‘upper second class’ group significantly differed from the ‘lower second class or below’ group ONLYThe ‘first class’ group significantly differed from the ‘upper second class’ group ONLYThe ‘first class’ group significantly differed from the ‘lower second class or below’ group AND the ‘upper second class’ group significantly differed from the ‘lower second class or below’ groupThe ‘first class’ group significantly differed from the ‘lower second class or below’ group AND the ‘first class’ group significantly differed from the ‘upper second class’ groupThe ‘upper second class’ group significantly differed from the ‘lower second class or below’ group AND the ‘first class’ group significantly differed from the ‘upper second class’ groupAll pairs were significantly different from each otherNo pairs were significantly different",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 3 - More ANOVAs with Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-3_more-anovas-with-jamovi.html#comparing-more-than-2-conditions-on-jamovi",
    "href": "tutorials/rmc_week-3_more-anovas-with-jamovi.html#comparing-more-than-2-conditions-on-jamovi",
    "title": "Week 3 : More ANOVAs with Jamovi",
    "section": "",
    "text": "The next example is very similar to the experiment described for the one-way between-participants ANOVA in Walk-Through Example 5. For the within-participants example, the experiment has been conducted as a repeated measures experiment: the same participants carried out the tasks for all three conditions.\nThere is still one DV (response time) and one factor (type of priming), with 3 levels (surface, deep, no prime). However, now the factor is within-participants (or a ‘repeated measure’).\nThe hypotheses are (almost) the same as well but the data need to be entered differently into Jamovi, as in the next table. Each participant’s data must be entered on a single row, so where a participant provides three data points, these need to be entered next to each other along the same row.\n\n\n\n\n\n\n\n\n\nSurface Priming RT (scale variable) [surface_data]\nDeep Priming RT (scale variable) [deep_data]\nNo-Priming RT (scale variable) [no_prime_data]\n\n\n\n\nParticipant 1 data for surface\nParticipant 1 data for deep\nParticipant 1 data for no_prime\n\n\nParticipant 2 data for surface\nParticipant 2 data for deep\nParticipant 2 data for no_prime\n\n\nParticipant 3 data for surface\nParticipant 3 data for deep\nParticipant 3 data for no_prime\n\n\n\n\nThe hypotheses tested were:\n\nParticipants will categorise words which they have previously processed at a deep level more quickly than words they have processed at a surface level or in the no prime condition.\nIf surface processing has any effect on later categorization, the response times in the surface condition should be faster than in the no prime condition.\n\n\nWalk-Through Example 1 “The effect of priming on a categorization task (repeated measures)”\n\n\n\nDownload and open the RMC_NM1_1way_within_ANOVA.sav dataset from the Canvas folder.\nClick on the “Data” tab and check the data looks something like below…\n\n\n\n\n\n\nGo to Analyses -&gt; Exploration -&gt; Descriptives. In the dialog box, enter all repetitions of the DV into the ‘Variables’ box.\nGo to ‘Plots’. Check ‘Histogram’ and ‘Box Plot’ Check whether the data are normally distributed (a parametric assumption that needs to be met for ANOVA).\n\n\nIn this example all the data are positively skewed. This is the usual pattern in response time data and there has been some controversy around how this should be dealt with when analysing such data.\nFor the purposes of Level 2 research methods, we will treat response time data as ‘approximately normal’ where the long tail of the distribution does not extend beyond 3 standard deviations above the mean.\n\n\n\n\n\nGo to Analyses -&gt; ANOVA -&gt; Repeated Measures ANOVA\n\n\n\nIn the dialog box, enter a name for the IV into the ‘RM Factor 1’ box (e.g. “Priming”). Change the name ‘Level 1’ to the name of the first condition. Continue with the other 2 conditions (change ‘Level 2’ to the second and change ‘Level 3’ into the third).\nMove the conditions into their respective cells in the ‘Repeated Measures Cells’ box.\nClick the box ‘Partial η2’ under Effect Size.\nGo to ‘Estimated Marginal Means’. Enter the IV into the ‘Term 1’ box. Select ‘Marginal mean table’.\nGo to ‘Assumption Checks’ and check ‘Sphericity tests’\n\n\n\n\nGot to ‘Post Hoc Tests’. Place ‘Priming’ into the right-side box. Check ‘Bonferroni’ under Corrections.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipJamovi Output One-way ANOVA (within) questions\n\n\n\n\nQuestion 1\n\n\nWhich (if any) condition(s) is significantly different from “no prime”? surface and deepsurface onlydeep onlyneither are significant\nWhich (if any) condition(s) is significantly different from “deep”? surface and no primesurface onlyno prime onlyneither are significant\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipOne-way ANOVA (within) Write Up\n\n\n\n\nFill in the blanks\n\n\n(Any data preparation, e.g. how the response times were calculated or how the data were trimmed or if and how many outliers were removed, should be presented in the section of the method about data preparation and data analysis.)\nIn a report, an ANOVA results section could look like this:\nResults\nAlthough the mean categorization times were lowest in the deep priming condition as predicted, the categorization times in the no priming condition were very similar to those in the surface priming condition. A one-way repeated measures ANOVA found a small but significant effect of priming condition on categorization times (see Table 1).\nTable 1 Means (Standard Deviations in Parentheses), and One-Way Analysis of Variance Statistics for Categorization Times (ms) for the Priming Conditions.\n\n\n\n\n\n\n\n\n\n\n\nMeasure\n\nPriming Condition\n\nF(2,[])\nη2p\n\n\n\n\n\nSurface\nDeep\nNo-prime\n\n\n\n\nCategorisation time\n (388)\n1701 (384)\n1751 ()\n3.6*\n.03\n\n\n\n*p &lt;0.05\nUsing Bonferroni-corrected pairwise comparisons to carry out post-hoc analyses, none of the difference between the conditions were found to be significant. The difference between the surface priming and  conditions (mean difference =  ms, p = ) came closest to significance. The difference between  and no priming (mean difference = 50ms, p = .077) also approached significance. There was no significant difference between surface and  conditions. These findings supportpartially supportdo not support the hypothesis.",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 3 - More ANOVAs with Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-3_more-anovas-with-jamovi.html#comparing-more-than-2-conditions-when-parametric-assumptions-are-not-met",
    "href": "tutorials/rmc_week-3_more-anovas-with-jamovi.html#comparing-more-than-2-conditions-when-parametric-assumptions-are-not-met",
    "title": "Week 3 : More ANOVAs with Jamovi",
    "section": "",
    "text": "If the data do not meet parametric assumptions, we should take a more conservative approach to avoid making errors in the conclusions we draw.\nWhen we are dealing with data that are strongly skewed and/or have unequal variances, we should perform non-parametric tests to explore our effects. Non-parametric tests are also useful for ordinal data, as they work with ranks rather than absolute values.\n\nNon-parametric equivalents\n\nMost of the tests you have covered so far have a direct non-parametric equivalent. The table below shows how they map:\n\n\n\n\nParametric Test\nNon-Parametric Equivalent\n\n\n\n\nPearson’s correlation*\nSpearman’s correlation*\n\n\nIndependent samples t-test\nMann-Whitney\n\n\nPaired samples t-test\nWilcoxon\n\n\nBetween-participants one-way ANOVA\nKruskal-Wallis\n\n\nRepeated-measures one-way ANOVA\nFriedman\n\n\n\n*More on correlations in RMD\n\n\n\nFor a one-way ANOVA design with more than two independent groups, where the data do not meet parametric assumptions, we can use the Kruskal-Wallis test. We might also use this with very small or unequal sample sizes.\nIn this example, we want to know whether the day of the week (Monday, Tuesday, Thursday, or Friday) on which a group of students attends workshops for a research methods module affects how much they look forward to the module (rated on a 5-point scale).\nThe hypothesis tested was:\n\nThere are differences in self-rated looking forward score for taking part in the research methods module depending on the day of the week on which participants attend the workshop.\n\nNote – this is a two-sided hypothesis as no direction for the effect of day of attendance is specified.\n\n\n\nWalk-Through Example 2 “The effect of day of the week on looking forward to a research methods module.”\n\nDownload and open the RMC_NM1_lookfwd_Kruskal-Wallis.sav dataset\nClick Analyses -&gt; Exploration -&gt; Descriptives. In the dialog box, enter the DV into the ‘Variables’ box. Enter the IV into the ‘Split by’ box.\nGo to ‘Plots’. Check ‘Histograms’.\n\nRecord the medians and minimum and maximum scores from the Explore output [which is not presented here].\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure\n\n\n\nDay of the Week\n\n\n\n\n\n\n\n\n\nMonday\n\nTuesday\n\nThursday\n\nFriday\n\n\n\n\nMdn\nMin-Max\nMdn\nMin-Max\nMdn\nMin-Max\nMdn\nMin-Max\n\n\nLooking forward score\n3.0\n2-4\n3.0\n2-3\n4.0\n3-4\n3.0\n1-3\n\n\n\n\nGo to Analyses -&gt; ANOVA -&gt; One-Way ANOVA Kruskal-Wallis (under non-parametric)\nEnter the DV into the ‘Dependent Variables’ box. Enter the between-participants factor into the ‘Grouping Variable’ box\nTick the ‘Effect size’ box\n\n\n\n\n\n\n\nSince the Kruskal-Wallis test is significant, we need to follow it up with a post-hoc test to determine where the significant differences between the groups occur.\nWe could do this by performing Mann-Whitney tests (non-parametric comparison between two independent groups) to compare the different groups (levels) of the IV.\n\nTo avoid family-wise error, adjust the significance level depending on the number of paired comparisons performed using Bonferroni’s correction.\nThe correction is worked out by dividing the p-value acceptable for significance by the number of comparisons to be performed:\n\nE.g. if we accept p &lt; .05 as a cut-off for significance, then for 4 comparisons, the Bonferroni adjusted p-value =.05 / 4 = .0125. This means that any p-value greater than .0125 would not be considered significant.\n\n\nHowever, there is no simple way of doing this on Jamovi, unless you create new data sheets (or new variables) with only two subgroups included for each analysis.\nJamovi offers an option for pairwise comparisons called the ‘Dwass-Steel-Crithclow-Fligner pairwise comparions’ or ‘DSCF pairwise comparisons’. There’s a box you can select when running the Kruskal-Wallis test.\n\nThis test is two-sided and provides protection for family-wise error rates, like the Bonferroni correction does.\n\n\n\n\n\n\n\n\nNoteReminder\n\n\n\nWe need to look up the medians and minimum and maximum values for each condition from the Exploration analysis and report those.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipKruskal-Wallis Write Up\n\n\n\n\nFill in the blanks\n\n\n(Any data preparation, e.g. how the response times were calculated or how the data were trimmed or if and how many outliers were removed, should be presented in the section of the method about data preparation and data analysis.)\nIn a report, a Kruskal-Wallis results section could look like this:\nResults\nBecause the participant numbers were small and unequal across conditions and the score for how much participants looked forward to the module was ordinal and not normally distributed, a Kruskal-Wallis test was performed to examine the effect of day of the week upon the mean look-forward score (out of 5). It was found that day of the week had a significant effect on look-forward ratings, chi-sq2(3) = , p = , ε2 = .379.\nTable 1 shows the median look-forward scores for each day and paired comparisons (DSCF pairwise comparisons). Only the difference between look-forward scores for Tuesday compared with  was statistically significant. The hypothesis that the day of study affects how much students look forward to their research methods module was supportedpartially supportednot supported.\nTable 1 Median look forward scores (out of 5) by day of the week and including significance of paired comparisons.\n\n\n\n\n\n\n\n\n\n\n\nDay\nMedian look forward score\nPaired comparison significance\n\n\n\n\n\n\n\n\n\nMonday\nTuesday\nThursday\nFriday\n\n\nMonday\n\n-\n-\n-\n-\n\n\nTuesday\n3\np - .470\n-\n-\n-\n\n\nThursday\n\np - .194\np - .045*\n-\n-\n\n\nFriday\n3\np - .370\np - .978\np - .051\n-\n\n\n\n*p &lt;0.05\n\n\n\n\n\n\n\n\n\n\nWith a related (within-participants or repeated measures) design, where the data are non-parametric, we can use the non-parametric equivalent of the repeated measures one-way ANOVA: the Friedman test.\nIn this example, we test if there is a difference in how much the group of students from the previous example looks forward to their workshops on the research methods module (rated on a 5-point scale) depending on how long they have been attending the module (at the start, after 2 weeks, after 5 weeks).\nThe hypothesis tested was:\n\nThere are differences in self-rated looking forward score for taking part in the research methods module depending on how long the participants have been attending the workshops.\n\n\nWalk-Through Example 3 “The effect of length of attendance on looking forward to a module”\n\nDownload and open the RMC_NM1_lookfwd_Friedman.sav dataset\nGo to Analyses -&gt; Exploration -&gt; Descriptives. In the dialog box, enter all repetitions of the DV into the ‘Variables’ box.\nGo to ‘Plots’. Check ‘Histograms’.\n\nRecord the medians and other key descriptive statistics (e.g. maximum and minimum scores) from the Explore output [which is not presented here].\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure\n\nTime spent studying the module\n\n\n\n\n\n\n\n\n\nAt Start\n\nAfter 2 Weeks\n\nAfter 5 Weeks\n\n\n\n\nMdn\nMin-Max\nMdn\nMin-Max\nMdn\nMin-Max\n\n\nLooking forward score\n3.0\n2-5\n3.0\n1-4\n3.0\n1-4\n\n\n\n\nGo to Analyses -&gt; ANOVA -&gt; Repeated Measures ANOVA Friedman (under non-parametric)\nEnter all levels of the DV into the ‘Measures’ box\n\n\n\n\n\n\nSince the Friedman test is not significant, we do not need to carry out a post-hoc analysis.\nIf we did need to carry out post-hocs, we would use Wilcoxon tests (non-parametric comparison between two related conditions) to compare pairs of conditions.\n\nTo avoid family-wise error, adjust the significance level depending on the number of paired comparisons performed using Bonferroni’s correction, i.e. divide the p-value acceptable for significance by the number of comparisons to be performed:\n\nE.g. for 3 comparisons, the Bonferroni adjusted p-value = 0.05 / 3 = 0.0167.\n\nThis is can be done easily on Jamovi. It should look something like this.\n\n\n\n\n\n\n\nAlternatively, the Friedman test dialogue on Jamovi also offers Durbin-Conover Pairwise Comparisons, however, this test does not correct for family-wise error.\n\nIf you use Durbin-Conover comparisons, you should use a correction against the p-value you use for the comparisons.\n\nE.g. for 3 comparisons, the Bonferroni adjusted p-value = 0.05 / 3 = 0.0167.\nIn a results section, state that you used the Bonferroni correction and what the new criterion for significance was against which you compare the individual p-values for the comparisons.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteReminder\n\n\n\nWe need to look up the medians and minimum and maximum values for each condition from the Exploration analysis and report those.\n\n\n\n\n\n\n\n\n\n\n\nTipFriedman Write Up\n\n\n\n\nFill in the blanks\n\n\n(Any data preparation, e.g. how the response times were calculated or how the data were trimmed or if and how many outliers were removed, should be presented in the section of the method about data preparation and data analysis.)\nIn a report, a Friedman results section could look like this*:\nResults\nHistograms for the three conditions were inspected separately. As the data were skewed and the look forward scores were measured on an ordinal scale, a Friedman test was used. There was no significant effect of time spent studying the module upon look forward scores, chi-sq2() = 2.86, p = . In fact, the median score for all three conditions was identical at , and the hypothesis that time spent studying on the module would affect how much people looked forward to the workshop sessions was supportedpartially supportednot supported.\n*Note that the post-hocs are not reported here, as the main analysis was non-significant.\n\n\n\n\n\n\n\n\n\n\n\nNoteNOTE\n\n\n\nDownload and open the RMC_NM1_SPSS_attitude.sav dataset and answer the following questions. Before you start, recode the marks data into a new grouping variable called “grade_class” so that participants are allocated to either first_class (70-100), upper_second (60-69) or lower_second_or_below (0-59) groups.\n\n\n\n\n\n\n\n\nTipActivity Questions\n\n\n\n\nQuestion 1Question 1 NotesQuestion 2Question 3Question 4Question 5\n\n\nWhich type of statistical analysis would you need to carry out to test the hypothesis that “People with first class grades, upper second class grades, and lower second class or below grades on the first practical vary on their total score for the attitudes towards SPSS* (where a high score indicates a positive attitude towards SPSS)”? Independent ANOVARepeated measures ANOVAKruskal-WallisFriedman\nThink about you reasoning for choosing this test before looking at the question 1 notes.\n*This is the stats package we taught before we shifted to teaching Jamovi.\n\n\nThe wording of the hypothesis suggests that there are different groups of people being studied. This means that an independent (between-participants) design was being used.\nThere is a single factor which could be called grade-class or something similar. This means that the design is a one-way design.\nIf you Explore the data using SPSS, you can see that the data on the SPSS attitudes (the DV) are normally distributed - either if you look at the DV as a whole or if you look at the different grade-range groups.\nSample sizes are also similar and the range of variation in each group is similar.\nBased on this consideration, it is appropriate to us an ANOVA to test the hypothesis.\nA Kruskal-Wallis analysis would also be correct, if parametric assumptions are not met.\n\n\nUsing the appropriate walk-through example (from Workshops 2 or 3), carry out this test and inspect the output. How many participants are there in the “lower_second_or_below” group?\nN = \n\n\nWhat is the mean score and standard deviation for the “first_class” group?\nMean = \nSD = \n\n\nWhat is the appropriate test statistic (including the appropriate degrees of freedom)? What was the value of the test statistic (if it is a Greek letter type the name e.g. ‘chi-square’) and what was the associated p-value?\nTest statistic  = ; p = \nWhat was the measure of effect size (type its name, e.g. ‘partial eta squared’ or ‘epsilon squared’) and what was the value of the effect size?\nEffect size  = \n\n\nIf the test was significant, between which groups do the significant differences occur? The ‘first class’ group significantly differed from the ‘lower second class or below’ group ONLYThe ‘upper second class’ group significantly differed from the ‘lower second class or below’ group ONLYThe ‘first class’ group significantly differed from the ‘upper second class’ group ONLYThe ‘first class’ group significantly differed from the ‘lower second class or below’ group AND the ‘upper second class’ group significantly differed from the ‘lower second class or below’ groupThe ‘first class’ group significantly differed from the ‘lower second class or below’ group AND the ‘first class’ group significantly differed from the ‘upper second class’ groupThe ‘upper second class’ group significantly differed from the ‘lower second class or below’ group AND the ‘first class’ group significantly differed from the ‘upper second class’ groupAll pairs were significantly different from each otherNo pairs were significantly different",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 3 - More ANOVAs with Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "This week we will explore how to use the jmv package in R to test hypotheses about a dataset using t-tests and to compute effect sizes corresponding to those tests. We will use those effect sizes to help interpret the sensitivity and power of our experiments.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nCohen’s d effect size\n\n\n\nPower analysis\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nUse mutate() in R to compute a new variable\n\n\n\nCompute t-tests using the jmv R package\n\n\n\nRun a power analysis in Rj\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nUnderstand and validate code written by someone else\n\n\n\nUse power analysis to recommend a sample size for a replication\n\n\n\n\n\n\n\n\n\n\n\nNoteDataset\n\n\n\nPlease note, this is the same dataset as we used in week 4 - but make sure that you load in the rmb-week-3_lecture-quiz-data_ai-faces-fixed.omv file from this week and NOT the week 4 file. It contains a small fix that we need for this week.\n\n\nWe’ll be working with the dataset we collected after the break during the lecture in Week 3. This is a partial replication of a study published in the journal Psychological Science (Miller et al. 2023). The original paper found the following results summarised in the abstract.\n\nRecent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 (N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces—a phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 (N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI.\n\nWe ran a quiz that students in the lecture could join on their phones/laptops. There were two parts to the quiz:\nThe faces section included 12 faces that were either photographs or real people or AI generated images of people. The stimuli were taked from the materials released by (Miller et al. 2023) on the Open Science Framework. Responses were either ‘Real person’ or ‘AI generated’.\n\nThe confidence section asked participants to rate their confidence in their ability to do the following three things.\n\nDistinguish real photos of people from AI generated images (Experimental condition)\nDistinguish photos of happy people from photos of sad people (Emotional control condition)\nDistinguish photos of people you used to know in primary school from strangers (Memory control condition)\n\nScores were recorded on a scale from 1 (Completely confidence) to 10 (Not at all confident). The confidence section was repeated before and after the faces section to see if participants confidence changed as a result of doing the AI faces task\n\n\n\nThis week we will explore some factors that influence t-stats and p-values whilst introducing the concept of loops in R. We will finish by using effect sizes to compute a power analysis.\n\n\n\nLast week, we used Jamovi to create several new variables that allowed us to run our analyses. We can do the same in a more transparent and reproducible way using R code.\nWe need to create both our proportion of correct faces across the 12 stimuli in one variable and a grouping variable which indicates whether each participant was confident in their ability to tell the difference between AI faces and real faces.\nWe can do both with the mutate() from the dplyr library, this provides functionality that lets us create, modify, and delete columns within a dataset. Take a look at the official documentation for mutate for more information.\nWhen computing a new varaible with mutate() we three pieces of information.\n\nThe dataset to work on\nThe name of the new variable to be created\nThe definition of how to compute the new variable for existing variables.\n\n\n\nLet’s start with our grouping variable. Our three pieces of information are\n\nThe dataset to work on - is data, which refers to the original datasheet loaded into Jamovi\nThe name of the new variable - is ConfidentBefore, the same as we used last week\nThe definition - is AIConfidenceBefore&lt;4, again the same as we used last week\n\nWe can combine these into a single line to create the variable. Note that we save the result into a new variable named my_data, we’ll use this from now on to avoid confusion with the original datasheet.\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\nWe can check that this has done what we expected by running head to see the first few rows of the dataset.\nhead(my_data)\nThe result should look like this:\n\nWe can see on in the results on the right hand side that the additional colume ConfidentBefore now appears with TRUE and FALSE values for each participant.\n\n\n\nLet’s do the same for total proportion of correctly identified faces. We can use the same principle as we used for the grouping variable and use mutate() along with the variable definition and name to create our new column.\nThis definition is pretty long as we have 12 different faces to work with! You can copy the whole line using the copy icon in the right hand side of the code box. Take a moment to look through and understand each part: which part is the Dataset, the Name and the Definition?\nmy_data &lt;- mutate(my_data, PropFacesCorrect = (Face1_Real + Face2_Real + Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face7_Real + Face8_Real + Face9_Real + Face10_Real + Face11_AI + Face12_AI) / 12)\nand validate the overall result using head() to make sure that my_data now has the column we expected.\n\nThe final column of the dataset is now PropFacesCorrect and contains the proportion of face trials that participant got right.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute variables containing the proportion of correct responses for photos of real people and AI generated faces separately?\nStore the results in PropRealCorrectFaces for real faces and PropAICorrectFaces for AI generated faces.\n\n\nWe can use the code we wrote to compute the proportion of correct responses for all faces as a starting point.\nThink about how you could modify this line to compute the result for either real or AI faces on their own? What would you need to change in the code?\n\n\nThe following code will compute the variables\nmy_data &lt;- mutate(my_data, PropRealFacesCorrect = (Face1_Real + Face2_Real + Face7_Real + Face8_Real + Face9_Real + Face10_Real) / 6)\nmy_data &lt;- mutate(my_data, PropAIFacesCorrect = (Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face11_AI + Face12_AI) / 6)\nWe’ve separated out the columns to sum all the real or AI faces together and changed the division to divide the results by 6 rather than 12.\nChecking the results with head() should produce the following outputs with four additional columns!\n\n\n\n\n\n\n\n\n\n\n\nWe have the ingredients for our hypothesis test. Let’s use R to explore the following hypothesis (Hypothesis 2 from week 4).\n\nConfident people are better at distinguishing AI faces from real faces\n\nWe can compute this test using the following code that calls ttestIS() - this is the function that computes independent samples t-tests. Read more about it on the ttestIS documentation page.\nYou can click the numbers by the definitions at the bottom to highlight the corresponding part of the code.\n1jmv::ttestIS(\n2    formula = PropFacesCorrect ~ ConfidentBefore,\n3    data = my_data,\n4    effectSize = TRUE)\n\n1\n\njmv::ttestIS is the name of the R function that Jamovi uses to compute independent samples t-tests\n\n2\n\nThis tells ttestIS the formula that defines the test we want to run.\n\n3\n\nThis tells ttestIS which dataset we want to analyse\n\n4\n\nThis adds an additional effect size computation to the results\n\n\nMost of this will be familiar from previous weeks, but let’s think about the formula in a little more detail.\nIn R, the tilde (~) is used in formula definitions to specify the relationship between variables, particularly in statistical modeling and data analysis. Here, the tilde separates the dependent variable (response) from the independent variables (predictors). In our example, PropFacesCorrect is the dependent variable and ConfidentBefore is our independent variable (grouping variable) - so this formula\nPropFacesCorrect ~ ConfidentBefore\nis essentially telling ttestIS() to “Compute the ttest on PropFacesCorrect using ConfidentBefore as our groups”.\nWe can now run all our code to get the results of the ttest - the code clearly tells the reader how all the relevant variables were computed and what hypothesis test has been run all in one screen.\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nRead through the ttestIS documentation page. How could you change your code to add the following assumption checks\n\nTest for homogeneity of variance\nTest for normal distribution\n\n\n\nThe documentation page contains a list of all the possible information that we can pass as an input to our ttestIS() function call. Each item in the list corresponds to the options available in the Jamovi dialogue box.\nHave a look at the norm and eqv definitions. What would you need to add to the function call to run these additional checks?\n\n\nThe following code will compute all the checks in addition to the t-test.\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE,\n    norm = TRUE,\n    eqv = TRUE)                             \nThe results should look like this:\n\nDo the assumptions for Student’s t-test hold for this data?\n\n\n\n\n\n\nYour script might be getting quite long at this point! It is always a good idea to keep code neat and tidy where possible so that other people are able to read it, and so that we can read it if we come back to the analysis in the future.\nThere are many ways to keep things organised in a script. Here are two good hints, code comments can organise your script without changing the output and print() statements can help organise your code and the outputs.\n\n\nAny line of R code that starts with a hashtag is called a ‘comment’. The writing on this line is there for our information and will not be run by the computer. Adding code comments is a useful way to annotate your code indicating what each line or each section is doing. For example:\n# This is code comment\n\n# Compute which participants were more confident than the median\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\n\nThis will make it easier to understand what the coding is doing in future.\n\n\n\nWe can use the print() function to help organise our code as well. The text within the call to print() will not be executed by the computer but will simply be printed into the output console. This can be useful to break the output of your code into sections and to inlcude additional information about the analysis next to the outputs themselves.\nFor example:\nprint('Hypothesis 1')\nprint('Confident people are better at distinguishing AI faces from real faces')\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)\nThis code will print out information about the hypothesis next to the outputs.\nA full cleaned and commented version of our code might look like this.\n# Compute which participants were more confident than the median\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\n\n# Compute the proportion of all faces each participant got right\nmy_data &lt;- mutate(my_data, PropFacesCorrect = (Face1_Real + Face2_Real + Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face7_Real + Face8_Real + Face9_Real + Face10_Real + Face11_AI + Face12_AI) / 12)\n\n# Compute the proportion of REAL faces each participant got right\nmy_data &lt;- mutate(my_data, PropRealFacesCorrect = (Face1_Real + Face2_Real + Face7_Real + Face8_Real + Face9_Real + Face10_Real) / 6)\n\n# Compute the proportion of AI faces each participant got right\nmy_data &lt;- mutate(my_data, PropAIFacesCorrect = (Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face11_AI + Face12_AI) / 6)\n\n# Hypothesis test\nprint('Hypothesis 1')\nprint('Confident people are better at distinguishing AI faces from real faces')\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)\nand running this code produces the following outputs including the test from our print statements\n\nNice and clear what is happening at each stage! There is no perfect or ‘correct’ way to tidy up your code (though some people can get opinionated about this…). Choose a mix of comments and print statements that makes sense to you.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nExtend your code to test a second hypothesis.\n\nConfident participants are more accurate than unconfident participants when identifying photos of real faces.\n\nInclude some code comments and print statements\n\n\nThis will require a second indenpendent samples t-test asking whether the mean proportion correct value for only real faces is different between our confident and not-confident groups.\nUse the previous t-test as a starting point, can you copy this and modify it to do what we need?\n\n\nThe following code will test the new hypothesis with some comments and print statements\n# Hypothesis test\nprint('Hypothesis 2')\nprint('Confident participants are more accurate than unconfident participants when identifying photos of real faces.')\njmv::ttestIS(                                       \n    formula = PropRealFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)                       \nThe output of the whole script should now look like this.\n\nNote how the print statements help to separate the results into interpretable chunks.\n\n\n\n\n\n\nNow we’re going to explore how the size of the data sample changes the t-values that w\nIntroduce slice_sample Introduce simple loops\n\n\n\n\nImagine that of your friends has been running some analyses on this dataset and would like you to check over their work. They send you the following R code that they have put together.\n# Compute some new variables\nmy_data &lt;- mutate(data, ChangeAIConfidence = AIConfidenceBefore - AIConfidenceAfter)\nmy_data &lt;- mutate(my_data, ChangeEmoConfidence = EmoConfidenceBefore - EmoConfidenceAfter)\nmy_data &lt;- mutate(my_data, ChangeMemoryConfidence = MemoryConfidenceBefore - MemoryConfidenceAfter)\n\n# Run the tests\njmv::ttestOneS(\n    data = my_data,\n    vars = vars(ChangeAIConfidence, ChangeEmoConfidence, ChangeMemoryConfidence),\n    testValue = 0.0,\n    wilcoxon = TRUE,\n    desc = TRUE,\n    norm = TRUE)\nLet’s think about this code a bit, your friend didn’t really explain the plan…\n\n\n\n\n\n\nCaution Data Skills - what variables are created?\n\n\n\n\n\nThe code creates three new variables, ChangeAIConfidence, ChangeEmoConfidence and ChangeMemoryConfidence - each is computed by subtracting the confidence values after the face task from the confidence values before the task.\nThese variables represent the change in confidence from completing the task.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - what statistical test is performed?\n\n\n\n\n\nThe code runs a one sample t-test comparing the data mean to 0.0 using the jamovi function jmv::ttestOneS(). The code runs the test on all three variables. We can see that the input vars is passed a list of all three variable names.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - what additional computations are performed?\n\n\n\n\n\nIn addition to the t-tests, the code runs a Wilcoxon Rank test, a Shapiro-Wilk test for normality and a descriptive statistics table.\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nOpen a new Rj window and run this code before going any further! Take a moment to read through the code and decide what it is doing.\n\n\n\n\n\n\nCautionClick here if you want to check that you have the right outputs.\n\n\n\n\n\nThe results should look like this:\n\n\n\n\n\n\nThey have written the following about the results.\n\nA one sample t-test found that the change in participant confidence in distinguishing AI faces from real faces was significantly reduced after performing the task. The difference in confidence before and after the task (M=-1.1, SD=2.7) was significantly below zero, t(76) = 3.538, p = 0.0007. In contrast there was no change after the task confidence in emotion recognition (M=-.017, SD=1.341), t(76) = -1.012, p = 0.313 and face memory (M=-0.02985, SD=1.044), t(76) = -0.234, p = 0.816.\n\nLet’s think about whether this is accurate.\n\n\n\n\n\n\nCaution Data Skills - are the reported statistics correct? take your time to read carefully and compare to the outputs\n\n\n\n\n\nNo! there are a number of mistakes throughout this writing. How many of the following mistakes did you spot?\n\nA one sample t-test found that the change in participant confidence in distinguishing AI faces from real faces was significantly reduced after performing the task. The difference in confidence before and after the task (M=-1.1, SD=2.7) was significantly below zero, t(76) = 3.538, p = 0.0007. In contrast there was no change after the task confidence in emotion recognition (M=-.017, SD=1.341), t(76) = -1.012, p = 0.313 and face memory (M=-0.02985, SD=1.044), t(76) = -0.234, p = 0.816.\n\n\n3.538 -&gt; the sign of the t-values on this test is wrong. Take a look a the output table, this should be -3.538.\n76 -&gt; the degrees of freedom for the second and third tests are reported incorrectly (perhaps copied from the first test?). These should be 64 and 66 respectively\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - was the correct test reported?\n\n\n\n\n\nAgain, no! there are mistakes here as well. The friend has done the right thing by computing the Shapiro-Wilk statistics but appears to have ignored them in the writing.\nShapiro-Wilk is significant for all three variables - we should strongly consider reporting the Wilcoxon Rank Sum test rather than Student’s t.\nThey could have run this analysis as a paired t-test, but the one sample t-test of the differences is also a valid approach.\n\n\n\nComputing statistics and accurately reporting the results takes a lot of precision! take your time when writing code and results sections - make sure to check and double check your work as you go…\n\n\n\nLet’s think more about effect sizes - computing effect sizes in Jamovi and R is really easy. Simply click the check box to add ‘Effect size’ under the ‘Addtional Statistics’ section of the Jamovi window - or add effectSize = TRUE to the function call in R (we’ve already been doing this above!).\n\n\n\n\n\n\nNoteKey step\n\n\n\nUpdate your friend’s code to include computation of effect sizes - the results should appear in the t-test results table on the right.\n\n\n\nThere are effect sizes computed for every t-test and its non-parametric alternatives.\n\nCohen’s d is the parametric effect size corresponding to Student’s t-test\nRank Biserial Correlation is the non-parametric effect size corresponding to Wilcoxon’s test.\n\nThough they have methodological differences, these effect sizes can be interpreted in the same way as a measure of the magnitude of an effect.\nRemember that this is different to the t-value which is a measure of evidence against the null hypothesis. The important difference is that we can have strong evidence against the null either as the difference is large or if we have measured a small difference very precisely. The effect size only cares about the size of the difference between conditions.\nNote that Jamovi/R provide a ‘signed’ effect size indicating the direction of the effect in the same way that a t-statistic does. For the following work we can ignore this sign and focus on the magnitudes only. In other words, we’ll consider an effect size of +0.5 or -0.5 to indicate the same magitude of effect.\nThe creator of many of our classic effect size measures provided a guideline for what might be considered a ‘small’ or a ‘large’ effect.\n\n\n\nEffect Size\nCohen’s d\nInterpretation\n\n\n\n\nSmall\n0.2\nSmall effect\n\n\nMedium\n0.5\nMedium effect\n\n\nLarge\n0.8\nLarge effect\n\n\n\nThese can be useful guidelines - through they have been criticised strongly for both the arbitrary nature of the thresholds and for over-simplifying interpretation of effect sizes. We can still use them as an informal indicator to help us quickly interpret our results.\n\n\n\n\n\n\nCaution Data Skills - Is the effect size for ChangeAIConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.403 is just below the thresold to be a ‘medium’ effect (ignoring the sign of the effect size). The effect size for the Rank Biserial correlation is similar.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Is the effect size for ChangeEmoConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.126 is just below the thresold to be a ‘small’ effect (ignoring the sign of the effect size). The effect size for the Rank Biserial correlation is larger but still closer to small than medium.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Is the effect size for ChangeMemoryConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.0286 is tiny! the Rank Biserial correlation is even smaller…\n\n\n\n\n\n\nOne core application for effect sizes is that we can use the results to inform our sample size planning for future studies. we can do this with four linked pieces of information\n\nEffect Size this is our effect size, usually Cohen’s d for analyses in RM-B\nSignificance Thresold - this is the ‘alpha’ level at which we would consider a result significant, usually 5% or 0.05.\nPower - this is probability of correctly detecting an effect if one exists in the data, we usually want 80% power.\nNumber of datapoints (N) - this is our sample size.\n\nThese four pieces of information are linked such that if we have any three of these values we can always compute the fourth.\nThis lets us do some very useful calculations to see how well our dataset is enabling us to answer our hypothesis and to plan for future studies. For examples\n\nIf we know the effect size, significance threshold and number of participants in a study, we can compute the power to tell us the probability that we can correctly detect an effect. This is known as ‘sensitivity’.\n\nor\n\nIf we want to compute how many participants we’ll need in a future study and we already know the effect size from previous literature, we can specify the power and significance threshold we want and compute the number of participants that we’ll need.\n\nVery useful! We can compute these different values using a Jamovi module known as ‘jpower’\n\n\n\n\n\n\nNoteKey step - Install the jpower addon in Jamovi\n\n\n\nClick through the ‘Modules’ button on the top right of the Jamovi ribbon. Click to the list of ‘Available’ modules and install ‘jpower’\n\nClick through to open the jpower menu and select a ‘one-sample t-test’ from the options. You should see the following dialogue box\n\n\n\nLet’s take a moment to understand the options.\nFirst - notice that we have the four pieces of information listed near the top of the options.\n\n\n\njpower menu\nInterpretation\n\n\n\n\nMinimally interesting effect size (δ)\nEffect size\n\n\nMinimum desired power\nPower\n\n\nN\nNumber of participants\n\n\nα (type 1 error rate)\nSignificance threshold\n\n\n\nBy default, one of these options is grayed out and not modifible. This is as jpower works to compute one value at a time whilst asking you to specify the other three. By default, we’re computing N as specified by the ‘Calculate’ drop down menu at the top of the dialogue box. Try changing this to calculate one of the other values and notice that the greyed out box changes each time.\n\n\n\n\n\n\nNoteKey step - Install the jpower addon in Jamovi\n\n\n\nLet’s run through an example. Say that we want to answer the following question:\n\nWhat is the probability the ChangeAIConfidence has been correctly detected in our previous analysis.\n\nWe should run through the following steps.\n\nSelect that we want to Calcuate ‘Power’ in the top drop down menu.\nSet the ‘Minimally interesting effect size (δ)’ to our observed effect size from before. This is 0.429 (ignoring the sign)\nSet the ‘N’ to 77 - this is the number of participants in the test (degrees of freedom are 76)\nSet ‘α (type 1 error rate)’ to 0.05 - this is usually the default\n\nThe results should appear automatically.\n\n\n\njpower gives us some really useful explanatory text along with the results. Read this carefully to understand what is going on. Firstly,\n\nThe purpose of a power analysis is to evaluate the sensitivity of a design and test. You have chosen to calculate the sensitivity of the chosen design for detecting the specified effect size.\n\nThis clarifies the type of analyses we have run - we want to compute the sensitivity of our analysis given particular data and effect size. Next the results are summarised\n\nA design with a sample size of 77 can detect effect sizes of δ ≥ 0.429 with a probability of at least 0.96, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α= 0.05.\n\nThis is the critical information. This tells us that the power is 0.96 for our analysis, indicating that we have a 96% chance of correctly detecting an effect size at least as large as the one from our test. Great news!\nFinally, the ‘Power by Effect size’ table tells us the power we would have for different effect sizes. For example, in this case if we have an effect size of 0.3 - we would have power between 50% and 80%. If the effect size were 0.2 we would have less than 50% power.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat is the sensitivity (power) of the effect size estimated by Cohen’s d for the ChangeEmoConfidence variable?\n\n\nTake a look at our previous results. Cohen’d d for ChangeEmoConfidence was -0.126.\n\n\nOur estimated power for the ChangeEmoConfidence is only 19.4%.\n\nA design with a sample size of 77 can detect effect sizes of δ ≥ 0.126 with a probability of at least0.194, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α = 0.05.\n\n\nThis is the practical consequence of effect sizes! The smaller effect for ChangeEmoConfidence is harder to detect than the larger value for ChangeAIConfidence so we have a much lower chance of getting the right answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nIt might be the case that our face recognition really does change participants confidence in recognising emotional faces, and our study simply doesn’t have enough participants to detect the change with statistical confidence. Perhaps we should consider running a larger study?\nIf we assume that our value for Cohen’s d is accurate and we assume that there really is an effect to be detected (these are pretty brave assumptions!), how many participants would a future study need to have 80% chance of correctly detecting the effect?\n\n\nWe’ll need to change our analysis to calculate ‘N’ rather than ‘Power’ using the drop down menu at the top. Can you specify the power?\n\n\nWe would need at least 497 participants to reject the null hypothesis if the estimated effect size for the change in emotional face recognition before and after our task is accurate! that is a lot….\n\nWe would need a sample size of 497 to reliably (with probability greater than 0.8) detect an effect size of δ ≥ 0.126, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α = 0.05.\n\n\nThis information suggests that this looking for differences in emotional face confidence is probably not worth pursuing futher. Our experiment wasn’t perfect but this effect is so small that it is likely to be of little to no practical significance.\n\n\n\n\n\n\n\n\nWe have explored some new methods for creating new variables and running t-tests with effect sizes using R code. The R code we wrote is a really clear way to specify how variables were manipulated and which tests were run. Quite complex analyses with several stages can be clearly expressed this way.\nFinally, we used effect sizes to explore the power and sensitivity of our experiment and explore the possiblity of a follow up study.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#the-dataset",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#the-dataset",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "NoteDataset\n\n\n\nPlease note, this is the same dataset as we used in week 4 - but make sure that you load in the rmb-week-3_lecture-quiz-data_ai-faces-fixed.omv file from this week and NOT the week 4 file. It contains a small fix that we need for this week.\n\n\nWe’ll be working with the dataset we collected after the break during the lecture in Week 3. This is a partial replication of a study published in the journal Psychological Science (Miller et al. 2023). The original paper found the following results summarised in the abstract.\n\nRecent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 (N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces—a phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 (N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI.\n\nWe ran a quiz that students in the lecture could join on their phones/laptops. There were two parts to the quiz:\nThe faces section included 12 faces that were either photographs or real people or AI generated images of people. The stimuli were taked from the materials released by (Miller et al. 2023) on the Open Science Framework. Responses were either ‘Real person’ or ‘AI generated’.\n\nThe confidence section asked participants to rate their confidence in their ability to do the following three things.\n\nDistinguish real photos of people from AI generated images (Experimental condition)\nDistinguish photos of happy people from photos of sad people (Emotional control condition)\nDistinguish photos of people you used to know in primary school from strangers (Memory control condition)\n\nScores were recorded on a scale from 1 (Completely confidence) to 10 (Not at all confident). The confidence section was repeated before and after the faces section to see if participants confidence changed as a result of doing the AI faces task",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#the-challenge",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#the-challenge",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "This week we will explore some factors that influence t-stats and p-values whilst introducing the concept of loops in R. We will finish by using effect sizes to compute a power analysis.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-new-variables-with-mutate",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-new-variables-with-mutate",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Last week, we used Jamovi to create several new variables that allowed us to run our analyses. We can do the same in a more transparent and reproducible way using R code.\nWe need to create both our proportion of correct faces across the 12 stimuli in one variable and a grouping variable which indicates whether each participant was confident in their ability to tell the difference between AI faces and real faces.\nWe can do both with the mutate() from the dplyr library, this provides functionality that lets us create, modify, and delete columns within a dataset. Take a look at the official documentation for mutate for more information.\nWhen computing a new varaible with mutate() we three pieces of information.\n\nThe dataset to work on\nThe name of the new variable to be created\nThe definition of how to compute the new variable for existing variables.\n\n\n\nLet’s start with our grouping variable. Our three pieces of information are\n\nThe dataset to work on - is data, which refers to the original datasheet loaded into Jamovi\nThe name of the new variable - is ConfidentBefore, the same as we used last week\nThe definition - is AIConfidenceBefore&lt;4, again the same as we used last week\n\nWe can combine these into a single line to create the variable. Note that we save the result into a new variable named my_data, we’ll use this from now on to avoid confusion with the original datasheet.\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\nWe can check that this has done what we expected by running head to see the first few rows of the dataset.\nhead(my_data)\nThe result should look like this:\n\nWe can see on in the results on the right hand side that the additional colume ConfidentBefore now appears with TRUE and FALSE values for each participant.\n\n\n\nLet’s do the same for total proportion of correctly identified faces. We can use the same principle as we used for the grouping variable and use mutate() along with the variable definition and name to create our new column.\nThis definition is pretty long as we have 12 different faces to work with! You can copy the whole line using the copy icon in the right hand side of the code box. Take a moment to look through and understand each part: which part is the Dataset, the Name and the Definition?\nmy_data &lt;- mutate(my_data, PropFacesCorrect = (Face1_Real + Face2_Real + Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face7_Real + Face8_Real + Face9_Real + Face10_Real + Face11_AI + Face12_AI) / 12)\nand validate the overall result using head() to make sure that my_data now has the column we expected.\n\nThe final column of the dataset is now PropFacesCorrect and contains the proportion of face trials that participant got right.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute variables containing the proportion of correct responses for photos of real people and AI generated faces separately?\nStore the results in PropRealCorrectFaces for real faces and PropAICorrectFaces for AI generated faces.\n\n\nWe can use the code we wrote to compute the proportion of correct responses for all faces as a starting point.\nThink about how you could modify this line to compute the result for either real or AI faces on their own? What would you need to change in the code?\n\n\nThe following code will compute the variables\nmy_data &lt;- mutate(my_data, PropRealFacesCorrect = (Face1_Real + Face2_Real + Face7_Real + Face8_Real + Face9_Real + Face10_Real) / 6)\nmy_data &lt;- mutate(my_data, PropAIFacesCorrect = (Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face11_AI + Face12_AI) / 6)\nWe’ve separated out the columns to sum all the real or AI faces together and changed the division to divide the results by 6 rather than 12.\nChecking the results with head() should produce the following outputs with four additional columns!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-a-t-test-using-r",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-a-t-test-using-r",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "We have the ingredients for our hypothesis test. Let’s use R to explore the following hypothesis (Hypothesis 2 from week 4).\n\nConfident people are better at distinguishing AI faces from real faces\n\nWe can compute this test using the following code that calls ttestIS() - this is the function that computes independent samples t-tests. Read more about it on the ttestIS documentation page.\nYou can click the numbers by the definitions at the bottom to highlight the corresponding part of the code.\n1jmv::ttestIS(\n2    formula = PropFacesCorrect ~ ConfidentBefore,\n3    data = my_data,\n4    effectSize = TRUE)\n\n1\n\njmv::ttestIS is the name of the R function that Jamovi uses to compute independent samples t-tests\n\n2\n\nThis tells ttestIS the formula that defines the test we want to run.\n\n3\n\nThis tells ttestIS which dataset we want to analyse\n\n4\n\nThis adds an additional effect size computation to the results\n\n\nMost of this will be familiar from previous weeks, but let’s think about the formula in a little more detail.\nIn R, the tilde (~) is used in formula definitions to specify the relationship between variables, particularly in statistical modeling and data analysis. Here, the tilde separates the dependent variable (response) from the independent variables (predictors). In our example, PropFacesCorrect is the dependent variable and ConfidentBefore is our independent variable (grouping variable) - so this formula\nPropFacesCorrect ~ ConfidentBefore\nis essentially telling ttestIS() to “Compute the ttest on PropFacesCorrect using ConfidentBefore as our groups”.\nWe can now run all our code to get the results of the ttest - the code clearly tells the reader how all the relevant variables were computed and what hypothesis test has been run all in one screen.\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nRead through the ttestIS documentation page. How could you change your code to add the following assumption checks\n\nTest for homogeneity of variance\nTest for normal distribution\n\n\n\nThe documentation page contains a list of all the possible information that we can pass as an input to our ttestIS() function call. Each item in the list corresponds to the options available in the Jamovi dialogue box.\nHave a look at the norm and eqv definitions. What would you need to add to the function call to run these additional checks?\n\n\nThe following code will compute all the checks in addition to the t-test.\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE,\n    norm = TRUE,\n    eqv = TRUE)                             \nThe results should look like this:\n\nDo the assumptions for Student’s t-test hold for this data?\n\n\n\n\n\n\nYour script might be getting quite long at this point! It is always a good idea to keep code neat and tidy where possible so that other people are able to read it, and so that we can read it if we come back to the analysis in the future.\nThere are many ways to keep things organised in a script. Here are two good hints, code comments can organise your script without changing the output and print() statements can help organise your code and the outputs.\n\n\nAny line of R code that starts with a hashtag is called a ‘comment’. The writing on this line is there for our information and will not be run by the computer. Adding code comments is a useful way to annotate your code indicating what each line or each section is doing. For example:\n# This is code comment\n\n# Compute which participants were more confident than the median\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\n\nThis will make it easier to understand what the coding is doing in future.\n\n\n\nWe can use the print() function to help organise our code as well. The text within the call to print() will not be executed by the computer but will simply be printed into the output console. This can be useful to break the output of your code into sections and to inlcude additional information about the analysis next to the outputs themselves.\nFor example:\nprint('Hypothesis 1')\nprint('Confident people are better at distinguishing AI faces from real faces')\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)\nThis code will print out information about the hypothesis next to the outputs.\nA full cleaned and commented version of our code might look like this.\n# Compute which participants were more confident than the median\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\n\n# Compute the proportion of all faces each participant got right\nmy_data &lt;- mutate(my_data, PropFacesCorrect = (Face1_Real + Face2_Real + Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face7_Real + Face8_Real + Face9_Real + Face10_Real + Face11_AI + Face12_AI) / 12)\n\n# Compute the proportion of REAL faces each participant got right\nmy_data &lt;- mutate(my_data, PropRealFacesCorrect = (Face1_Real + Face2_Real + Face7_Real + Face8_Real + Face9_Real + Face10_Real) / 6)\n\n# Compute the proportion of AI faces each participant got right\nmy_data &lt;- mutate(my_data, PropAIFacesCorrect = (Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face11_AI + Face12_AI) / 6)\n\n# Hypothesis test\nprint('Hypothesis 1')\nprint('Confident people are better at distinguishing AI faces from real faces')\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)\nand running this code produces the following outputs including the test from our print statements\n\nNice and clear what is happening at each stage! There is no perfect or ‘correct’ way to tidy up your code (though some people can get opinionated about this…). Choose a mix of comments and print statements that makes sense to you.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nExtend your code to test a second hypothesis.\n\nConfident participants are more accurate than unconfident participants when identifying photos of real faces.\n\nInclude some code comments and print statements\n\n\nThis will require a second indenpendent samples t-test asking whether the mean proportion correct value for only real faces is different between our confident and not-confident groups.\nUse the previous t-test as a starting point, can you copy this and modify it to do what we need?\n\n\nThe following code will test the new hypothesis with some comments and print statements\n# Hypothesis test\nprint('Hypothesis 2')\nprint('Confident participants are more accurate than unconfident participants when identifying photos of real faces.')\njmv::ttestIS(                                       \n    formula = PropRealFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)                       \nThe output of the whole script should now look like this.\n\nNote how the print statements help to separate the results into interpretable chunks.\n\n\n\n\n\n\nNow we’re going to explore how the size of the data sample changes the t-values that w\nIntroduce slice_sample Introduce simple loops",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#analysis-from-a-friend",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#analysis-from-a-friend",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Imagine that of your friends has been running some analyses on this dataset and would like you to check over their work. They send you the following R code that they have put together.\n# Compute some new variables\nmy_data &lt;- mutate(data, ChangeAIConfidence = AIConfidenceBefore - AIConfidenceAfter)\nmy_data &lt;- mutate(my_data, ChangeEmoConfidence = EmoConfidenceBefore - EmoConfidenceAfter)\nmy_data &lt;- mutate(my_data, ChangeMemoryConfidence = MemoryConfidenceBefore - MemoryConfidenceAfter)\n\n# Run the tests\njmv::ttestOneS(\n    data = my_data,\n    vars = vars(ChangeAIConfidence, ChangeEmoConfidence, ChangeMemoryConfidence),\n    testValue = 0.0,\n    wilcoxon = TRUE,\n    desc = TRUE,\n    norm = TRUE)\nLet’s think about this code a bit, your friend didn’t really explain the plan…\n\n\n\n\n\n\nCaution Data Skills - what variables are created?\n\n\n\n\n\nThe code creates three new variables, ChangeAIConfidence, ChangeEmoConfidence and ChangeMemoryConfidence - each is computed by subtracting the confidence values after the face task from the confidence values before the task.\nThese variables represent the change in confidence from completing the task.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - what statistical test is performed?\n\n\n\n\n\nThe code runs a one sample t-test comparing the data mean to 0.0 using the jamovi function jmv::ttestOneS(). The code runs the test on all three variables. We can see that the input vars is passed a list of all three variable names.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - what additional computations are performed?\n\n\n\n\n\nIn addition to the t-tests, the code runs a Wilcoxon Rank test, a Shapiro-Wilk test for normality and a descriptive statistics table.\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nOpen a new Rj window and run this code before going any further! Take a moment to read through the code and decide what it is doing.\n\n\n\n\n\n\nCautionClick here if you want to check that you have the right outputs.\n\n\n\n\n\nThe results should look like this:\n\n\n\n\n\n\nThey have written the following about the results.\n\nA one sample t-test found that the change in participant confidence in distinguishing AI faces from real faces was significantly reduced after performing the task. The difference in confidence before and after the task (M=-1.1, SD=2.7) was significantly below zero, t(76) = 3.538, p = 0.0007. In contrast there was no change after the task confidence in emotion recognition (M=-.017, SD=1.341), t(76) = -1.012, p = 0.313 and face memory (M=-0.02985, SD=1.044), t(76) = -0.234, p = 0.816.\n\nLet’s think about whether this is accurate.\n\n\n\n\n\n\nCaution Data Skills - are the reported statistics correct? take your time to read carefully and compare to the outputs\n\n\n\n\n\nNo! there are a number of mistakes throughout this writing. How many of the following mistakes did you spot?\n\nA one sample t-test found that the change in participant confidence in distinguishing AI faces from real faces was significantly reduced after performing the task. The difference in confidence before and after the task (M=-1.1, SD=2.7) was significantly below zero, t(76) = 3.538, p = 0.0007. In contrast there was no change after the task confidence in emotion recognition (M=-.017, SD=1.341), t(76) = -1.012, p = 0.313 and face memory (M=-0.02985, SD=1.044), t(76) = -0.234, p = 0.816.\n\n\n3.538 -&gt; the sign of the t-values on this test is wrong. Take a look a the output table, this should be -3.538.\n76 -&gt; the degrees of freedom for the second and third tests are reported incorrectly (perhaps copied from the first test?). These should be 64 and 66 respectively\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - was the correct test reported?\n\n\n\n\n\nAgain, no! there are mistakes here as well. The friend has done the right thing by computing the Shapiro-Wilk statistics but appears to have ignored them in the writing.\nShapiro-Wilk is significant for all three variables - we should strongly consider reporting the Wilcoxon Rank Sum test rather than Student’s t.\nThey could have run this analysis as a paired t-test, but the one sample t-test of the differences is also a valid approach.\n\n\n\nComputing statistics and accurately reporting the results takes a lot of precision! take your time when writing code and results sections - make sure to check and double check your work as you go…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-effect-sizes-for-t-tests",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-effect-sizes-for-t-tests",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Let’s think more about effect sizes - computing effect sizes in Jamovi and R is really easy. Simply click the check box to add ‘Effect size’ under the ‘Addtional Statistics’ section of the Jamovi window - or add effectSize = TRUE to the function call in R (we’ve already been doing this above!).\n\n\n\n\n\n\nNoteKey step\n\n\n\nUpdate your friend’s code to include computation of effect sizes - the results should appear in the t-test results table on the right.\n\n\n\nThere are effect sizes computed for every t-test and its non-parametric alternatives.\n\nCohen’s d is the parametric effect size corresponding to Student’s t-test\nRank Biserial Correlation is the non-parametric effect size corresponding to Wilcoxon’s test.\n\nThough they have methodological differences, these effect sizes can be interpreted in the same way as a measure of the magnitude of an effect.\nRemember that this is different to the t-value which is a measure of evidence against the null hypothesis. The important difference is that we can have strong evidence against the null either as the difference is large or if we have measured a small difference very precisely. The effect size only cares about the size of the difference between conditions.\nNote that Jamovi/R provide a ‘signed’ effect size indicating the direction of the effect in the same way that a t-statistic does. For the following work we can ignore this sign and focus on the magnitudes only. In other words, we’ll consider an effect size of +0.5 or -0.5 to indicate the same magitude of effect.\nThe creator of many of our classic effect size measures provided a guideline for what might be considered a ‘small’ or a ‘large’ effect.\n\n\n\nEffect Size\nCohen’s d\nInterpretation\n\n\n\n\nSmall\n0.2\nSmall effect\n\n\nMedium\n0.5\nMedium effect\n\n\nLarge\n0.8\nLarge effect\n\n\n\nThese can be useful guidelines - through they have been criticised strongly for both the arbitrary nature of the thresholds and for over-simplifying interpretation of effect sizes. We can still use them as an informal indicator to help us quickly interpret our results.\n\n\n\n\n\n\nCaution Data Skills - Is the effect size for ChangeAIConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.403 is just below the thresold to be a ‘medium’ effect (ignoring the sign of the effect size). The effect size for the Rank Biserial correlation is similar.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Is the effect size for ChangeEmoConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.126 is just below the thresold to be a ‘small’ effect (ignoring the sign of the effect size). The effect size for the Rank Biserial correlation is larger but still closer to small than medium.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - Is the effect size for ChangeMemoryConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.0286 is tiny! the Rank Biserial correlation is even smaller…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#use-an-effect-size-for-a-sensitivity-power-analysis",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#use-an-effect-size-for-a-sensitivity-power-analysis",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "One core application for effect sizes is that we can use the results to inform our sample size planning for future studies. we can do this with four linked pieces of information\n\nEffect Size this is our effect size, usually Cohen’s d for analyses in RM-B\nSignificance Thresold - this is the ‘alpha’ level at which we would consider a result significant, usually 5% or 0.05.\nPower - this is probability of correctly detecting an effect if one exists in the data, we usually want 80% power.\nNumber of datapoints (N) - this is our sample size.\n\nThese four pieces of information are linked such that if we have any three of these values we can always compute the fourth.\nThis lets us do some very useful calculations to see how well our dataset is enabling us to answer our hypothesis and to plan for future studies. For examples\n\nIf we know the effect size, significance threshold and number of participants in a study, we can compute the power to tell us the probability that we can correctly detect an effect. This is known as ‘sensitivity’.\n\nor\n\nIf we want to compute how many participants we’ll need in a future study and we already know the effect size from previous literature, we can specify the power and significance threshold we want and compute the number of participants that we’ll need.\n\nVery useful! We can compute these different values using a Jamovi module known as ‘jpower’\n\n\n\n\n\n\nNoteKey step - Install the jpower addon in Jamovi\n\n\n\nClick through the ‘Modules’ button on the top right of the Jamovi ribbon. Click to the list of ‘Available’ modules and install ‘jpower’\n\nClick through to open the jpower menu and select a ‘one-sample t-test’ from the options. You should see the following dialogue box\n\n\n\nLet’s take a moment to understand the options.\nFirst - notice that we have the four pieces of information listed near the top of the options.\n\n\n\njpower menu\nInterpretation\n\n\n\n\nMinimally interesting effect size (δ)\nEffect size\n\n\nMinimum desired power\nPower\n\n\nN\nNumber of participants\n\n\nα (type 1 error rate)\nSignificance threshold\n\n\n\nBy default, one of these options is grayed out and not modifible. This is as jpower works to compute one value at a time whilst asking you to specify the other three. By default, we’re computing N as specified by the ‘Calculate’ drop down menu at the top of the dialogue box. Try changing this to calculate one of the other values and notice that the greyed out box changes each time.\n\n\n\n\n\n\nNoteKey step - Install the jpower addon in Jamovi\n\n\n\nLet’s run through an example. Say that we want to answer the following question:\n\nWhat is the probability the ChangeAIConfidence has been correctly detected in our previous analysis.\n\nWe should run through the following steps.\n\nSelect that we want to Calcuate ‘Power’ in the top drop down menu.\nSet the ‘Minimally interesting effect size (δ)’ to our observed effect size from before. This is 0.429 (ignoring the sign)\nSet the ‘N’ to 77 - this is the number of participants in the test (degrees of freedom are 76)\nSet ‘α (type 1 error rate)’ to 0.05 - this is usually the default\n\nThe results should appear automatically.\n\n\n\njpower gives us some really useful explanatory text along with the results. Read this carefully to understand what is going on. Firstly,\n\nThe purpose of a power analysis is to evaluate the sensitivity of a design and test. You have chosen to calculate the sensitivity of the chosen design for detecting the specified effect size.\n\nThis clarifies the type of analyses we have run - we want to compute the sensitivity of our analysis given particular data and effect size. Next the results are summarised\n\nA design with a sample size of 77 can detect effect sizes of δ ≥ 0.429 with a probability of at least 0.96, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α= 0.05.\n\nThis is the critical information. This tells us that the power is 0.96 for our analysis, indicating that we have a 96% chance of correctly detecting an effect size at least as large as the one from our test. Great news!\nFinally, the ‘Power by Effect size’ table tells us the power we would have for different effect sizes. For example, in this case if we have an effect size of 0.3 - we would have power between 50% and 80%. If the effect size were 0.2 we would have less than 50% power.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat is the sensitivity (power) of the effect size estimated by Cohen’s d for the ChangeEmoConfidence variable?\n\n\nTake a look at our previous results. Cohen’d d for ChangeEmoConfidence was -0.126.\n\n\nOur estimated power for the ChangeEmoConfidence is only 19.4%.\n\nA design with a sample size of 77 can detect effect sizes of δ ≥ 0.126 with a probability of at least0.194, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α = 0.05.\n\n\nThis is the practical consequence of effect sizes! The smaller effect for ChangeEmoConfidence is harder to detect than the larger value for ChangeAIConfidence so we have a much lower chance of getting the right answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nIt might be the case that our face recognition really does change participants confidence in recognising emotional faces, and our study simply doesn’t have enough participants to detect the change with statistical confidence. Perhaps we should consider running a larger study?\nIf we assume that our value for Cohen’s d is accurate and we assume that there really is an effect to be detected (these are pretty brave assumptions!), how many participants would a future study need to have 80% chance of correctly detecting the effect?\n\n\nWe’ll need to change our analysis to calculate ‘N’ rather than ‘Power’ using the drop down menu at the top. Can you specify the power?\n\n\nWe would need at least 497 participants to reject the null hypothesis if the estimated effect size for the change in emotional face recognition before and after our task is accurate! that is a lot….\n\nWe would need a sample size of 497 to reliably (with probability greater than 0.8) detect an effect size of δ ≥ 0.126, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α = 0.05.\n\n\nThis information suggests that this looking for differences in emotional face confidence is probably not worth pursuing futher. Our experiment wasn’t perfect but this effect is so small that it is likely to be of little to no practical significance.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#summary",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#summary",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "We have explored some new methods for creating new variables and running t-tests with effect sizes using R code. The R code we wrote is a really clear way to specify how variables were manipulated and which tests were run. Quite complex analyses with several stages can be clearly expressed this way.\nFinally, we used effect sizes to explore the power and sensitivity of our experiment and explore the possiblity of a follow up study.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "This week we will explore how to use Jamovi to compute and visualise descriptive statistics from a dataset. This will include some revision on Jamovi from last semester and an introduction on how we can use the R coding language to simplify complex data processing.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nMeasures of central tendency\n\n\n\nMeasures of dispersion and spread\n\n\n\nMeasures of skew and normality\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Rj editor to use R code within Jamovi\n\n\n\nLearn the basics about functions and variables in R\n\n\n\nComputing descriptive statistics using R\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nValidating and working with openly available research data\n\n\n\nCreate a reproducible data analysis script\n\n\n\n\n\n\n\n\n\n\n\nNoteDataset\n\n\n\nNote that this is the same dataset as in week 1\n\n\nWe will be exploring data that was collected as part of the ‘Many Labs 3’ project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‘Big Five’ personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs. extravagant/careless)\nAgreeableness (friendly/compassionate vs. critical/rational)\nNeuroticism (sensitive/nervous vs. resilient/confident)\nOpenness to experience (inventive/curious vs. consistent/cautious)\nExtraversion (outgoing/energetic vs. solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.\n\n\n\nLast week, we looked at how we can spend some time understanding a new dataset and compute some descriptive statistics from it.\nNow, we’re going to learn how to use Jamovi and R to make this process more repeatable and more transparent. This will make it much easier for us to come back to the analysis and understand what steps were taken at a later date.\n\n\n\n\n\n\nWarningOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.\n\n\n\n\n\nWe’ll be working with the Rj Add-On for Jamovi this week.\n\n\n\n\n\n\nNoteKey step\n\n\n\nBefore continuing, install the Rj add-on for jamovi following the guide for installing Jamovi modules. Once installed you should have a new button on the Ribbon menu at the top of Jamovi…\n\n\n\nClick on this and select ‘Rj Editor’ to open a window in which we can write R code! Click on the small cog icon and make sure that ‘Output’ is set to ‘Show code and output’.\n\nWe can use the left-hand window, named Rj editor, to write commands, using the R language. These are the straightforward text commands that tell the computer a series of steps that we wanted to carry out. These commands can be edited and rerun as many times as we need, they can even be shared with collaborators and included as part of scientific publication.\nThis is a very powerful tool once we have a written set of instructions to work with rather than individually clicking through instructions. Though it takes a bit longer to write the commands, our analysis is much more transparent and reproducible once it is done.\nThe outputs of the analysis defined in the Rj editor appear on the right-hand side of the Jamovi window as usual. To check this out, let’s explore using or as a calculator.\n\n\n\n\n\n\nTipData Skills - R as a calculator\n\n\n\nTry typing a set of sums or equations into the Rj editor. Include one sum on each line and include as many as you like. When you’re ready, you can tell RJ to run the commands by clicking on the green triangle in the top right corner of the RJ editor. This will automatically compute. All of the sums you to find and print the results out in the right hand side window. see the example below.\nYou can use the following symbols to define your sums.\n\n\n\nSymbol\nOperation\n\n\n\n\n+\nAdd\n\n\n-\nSubtract\n\n\n*\nMultiply\n\n\n/\nDivide\n\n\n\n\n\n\n\n\n\n\nOften, we don’t just want to see the result of a calculation, we want to store the result to use again later. For this, we want to use variables. For example, here we store the result of a calculator sum in a variable named ‘y’\ny &lt;- 2 + 2\nIf you run this in Rj, you’ll notice that we no longer get the number 4 printed to the screen. This is as the output is now caught and stored rather than printed out. If we still want to see what is contained in y, we can use another helpful function named print.\ny &lt;- 2 + 2\nprint(y)\nWill now also display the content of y.\n\n\n\n\n\n\nNoteDefinition of a variable\n\n\n\nA variable is an object that provides a labelled space to store information. The &lt;- or = symbols indicate that we’re assigning some information to a variable (the two symbols behave the same way, you could see either).\nCrucially, the information on the right hand side is assigned to a variable with the name from the left hand side.\n\n\n\n\n\n\n\n\nTipCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about variables in R. We suggest you try the following prompts to get started.\n\n“Write a simple description of variables in R for a beginner”\n“How can I choose a name for my variable in R”\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\nNotice that the one of the equations in my example is a bit different to the others. The final example uses a function called sqrt as a shortcut to compute the square root of the number nine.\nThe sqrt function calculates the square root of a given number. In R code, this looks something like this:\nsqrt(x)\nwith the ‘x’ replaced by any number. For example, we could compute the square root of 36 using the following command.\nsqrt(36)\nAdd this line to your Rj editor and check the result..\nMany functions, take inputs contained in parentheses written after the name of the function. These inputs are sometimes known as the arguments of a function. In the last example, 36 is an argument for the function sqrt.\nSometimes functions take several arguments, and the arguments themselves can have specific names. For example.\nround(12.345, digits = 0)\nwill round the value x to a whole number with zero digits after the decimal point. Where as\nround(12.345, digits = 2)\nwould round the same number to preserve 2 digits after the decimal point. Try running these examples in Rj with a range of different numbers to get a sense of how it works.\n\n\n\n\n\n\nNoteDefinition of a function\n\n\n\nFunctions are used to perform specific, predefined tasks, such as calculations, data transformations, or visualization. You can think of it as a “black box” that takes in a value, performs some internal calculation, and returns the result.\n\n\n\n\n\n\n\n\nTipCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about functions in R. We suggest you try the following prompts to get started.\n\n“Write a simple description of functions in R for a beginner”\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\nA single analysis might have several lines of code that create, store and reuse a several different variables with several different functions. For example:\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat does this R code do? what is the value of x, y and z?\n x &lt;- 10 + 10\n y &lt;- sqrt(16)\n z &lt;- x / y\n\n\nTry copying out and running the code in Rj.\nYou can add some print functions to see the value of the intermediate variables x and y as well. Try adding print(x) on the line immediately after x is defined.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final answers:\nx is 20 and y is 4, so z = 20 / 4 which is 5.\n\n\n\n\n\n\n\nRj has a special feature that the data loaded into our spreadsheet is always contained in a special variable named data. We can refer to this data variable to view and run analyses on our dataset.\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nTake a moment to load the data file into Jamovi and compute the descriptive statistics for the big five personality factors again.\n\n\n\nThe data we have loaded into our Jamovi session can be accessed in Rj with the data variable. Jamovi automatically makes the data available for us in the R environment.\nWe can view and interact with the data here as well. For example, to view the first few rows of the dataset you can run the head function and pass in data as an argument.\nhead(data)\nand to view the last few rows, you can do the same with the tail function.\ntail(data)\nOften, we’ll only want to work with a subset of the columns in a data frame at any given time. We can access individual columns using the dollar operator. For example, this code will access the age column within the loaded dataset.\ndata$age\nLet’s break this command down into its pieces:\n\n\n\n\n\n\n\nR code\nMeaning\n\n\n\n\ndata\nthis is the name of the whole data array we have loaded in\n\n\n$\nthis is a separator that R uses to help us access subsections of datasets\n\n\nage\nThis is the name of a single column we want to access\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you access the column of Mood scores from the same dataset?\n\n\nYou can use the head function to see the list of all available columns in the dataset. The example above accesses the ‘age’ column, you can tweak this to extract the mood data instead.\n\n\nThe final code should look like this\ndata$Mood\n\n\n\n\n\nWe can also apply functions to these variables. R includes a wide range of standard mathmatical functions that we can use for common computations. This code computes the mean of the age data in our dataset.\nmean(data$age)\nTry running this in Rj. You should get the answer 19.3 - this matches the value we computed using the descriptives analysis last week.\nSimilarly, the following function computes the standard deviation\nsd(data$age)\nIf you run this, you should get the answer 3.808.\n\n\n\nOk, back to statistics! How can we use our new R skills to compute statistics?\nThe trick is to understand that all the analyses in Jamovi are actually carried out by functions in R. Each analysis we can select from the top ribbon in Jamovi corresponds to a function that can be called in the code. Similarly, all of the options that we select in the analysis, dialogue boxes, correspond to arguments for that function. Let’s take a look in practice.\nJamovi provides a collection of R functions that correspond to each analysis dialogue box in Jamovi. This package of functions is named jmv. Thejmv function that computes descriptive statistics is called:\njmv::descriptives\nHere we first name the jmv package specify the descriptives function within it (this link is made with the two colons).\nWe can compute the descriptive statistics for age by passing the data into the function and specifying that we want to use the age variable. Try running the following:\njmv::descriptives(data, vars='age')\nand you should generate the following result table.\n\n\n\n\n\n\n\nNoteNote!\n\n\n\nYou might notice that we still have the unusual ages in this analysis, we’ll discuss how to remove them in the next section\n\n\nJamovi provide a lot of documentation on the descriptives function. This lists every single option that can be passed into jmv::descriptives. There is an option here for every single checkbox and selector in the main Jamovi dialogue.\nThe following code additionally computes a histogram by adding the hist argument.\njmv::descriptives(data, vars='age', hist=TRUE)\n\nWe can end up specifying a lot of these options. Sometimes it can be a good idea to split the function across multiple lines. This is fine as long as the parentheses are in the right place and there are commas in between each option.\nThis code computes descriptive statistics including the skewness, interquartile range and mode.\njmv::descriptives(data,\n  vars='age',\n  skew=TRUE,\n  iqr=TRUE,\n  mode=TRUE,\n  hist=TRUE\n  )\n\nOf course, we could run these analyses by hand. The main advantage here is that we can automate the selection of critical options and reduce the possiblity of mistakes when communicating analyses within teams. As long as we have this line saved, we can rerun it on new datasets without having to recheck every box by hand.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse the examples above and the JMV documentation on the descriptives function to write a function definition that does the following.\nCompute the core descriptive statistics for the Openness variable, make sure to include the range and a boxplot.\n\n\nTake a look at the ‘range’ and ‘box’ options in the documentation page.\nBe careful with the spelling of each argument!\n\n\nThe final code should be:\njmv::descriptives(data,\n  vars='Openness',\n  range=TRUE,\n  box=TRUE\n  )\n\n\n\n\n\n\n\n\n\n\nRemember that last week we very carefully removed several participants who had entered unrealisic or impossible ages into the datasets. This may have been an accident or mistake but either way we should not use this data. Our examples so far have just used all the data.\nR and Rj make it very easy to clean data without the boring, manual work we did last week. R can do the hard work and we can avoid scrolling through very large datasets by hand. Sounds good!\nThe first thing we can do is investigate what range of responses that people actually gave when asked about their age. To do this we can use a new function called unique(). This takes a column of a dataset as an input and returns a list of all the unique responses that it contains.\n\n\n\n\n\n\nNoteKey step\n\n\n\nLoad up an Rj Editor window (or continue in your old one) and run unique() on the age data.\nThe result should look like this:\n\n\n\nMost of these responses are fine but, as we saw last week, there are a lot of strange ones. Most importantly, we can find the extreme values that are very likely errors during data collection (-2 and 149)\nWe need to remove these from our analysis to get the most accurate results.\nTo do this we’re going to use an R library named dplyr (short for ‘Data Pliers’, see their documentation here). This is an extremely powerful library that is very widely used across the world.\nTo use dplyr, we first need to load the library into Rj by adding the following line to our code.\nlibrary(dplyr)\nThis tells Rj that we want to use the functions from this library.\nThe key function we need today is filter() - this extracts rows from a dataset based on logical conditions that we specify in the code. We are going to specify two arguments, firstly we will pass in the data and secondly we will specify the condition.\nIn this example we extract only the rows in which participants have an age above 20.\nlibrary(dplyr)\n\nfilter(data, age&gt;20)\n\nNotice that the dataset printed in the right hand window now only has values from the participants who are older than 20.\nWe can even specify multiple options - here we extract rows with participants older than 20 but still 23 or younger.\nlibrary(dplyr)\n\nfilter(data, age&gt;20, age&lt;=23)\nNext, to recreate our data tidying form last week, we need to extract the rows of participants who are older than -2 and younger than 149.\nThese values might be too specific and might not apply to every dataset. We can apply some common sense and suggest some broad, but much more sensible boundaries for a student population. We also save the result into a variable we can reuse later.\ntidy_data &lt;- filter(data, age &gt; 15, age &lt; 60)\nAnd finally, we can combine this with our call to jmv::descriptives to both filter our data to only the rows we want to use and compute the descriptive statistics on the result.\ntidy_data &lt;- filter(data, age &gt; 15, age &lt; 60)\n\njmv::descriptives(tidy_data, vars='age')\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nPractice changing the age conditions to a few different options to see the difference it makes to the descriptives analysis. What happens when you set a condition that results in zero valid participants?\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse the information in this tutorial to compute descriptive statistics for the Openness variable from Introverted participants. In other words - what is the mean Openness score for participants who are more Introverted than Extraverted.\nCompute the core descriptive statistics for the Openness variable, make sure to include the median and a histogram\n\n\nTo extract the data rows for Introverted participants we want to extract rows where the Extraversion score is less than 3.5\n\n\nThe final code should be:\nlibrary(dplyr)\n\ntidy_data &lt;- filter(data, Extraversion &lt; 3.5)\n\njmv::descriptives(tidy_data, vars='Openness')\n\n\n\n\n\n\n\n\n\n\nIt probably seems like a small difference so far. After all, we same results for our analysis using the check, boxes in Jamovi or the code in R. The Jamovi analysis is certainly quicker to run for the first time and it’s more convenient when exploring your data.\nHowever the R code has several crucial advantages.\n\nThe R code is transparent about what is actually being run to generate the results and by sharing this code, we enable others to repeat our analysis. This makes it really straightforward to run and re-run identical analyses in situations where we need to be really sure that everybody has done the same thing. For example, if we want to be certain that two data sets have been analysed in an identical way then that our code would be the best way to do this.\nR code is the best way to share what we’ve done with the rest of the world. It is common practice for modern academic public it to include the code that was used to generate the results in the paper. This way everybody who wants to check can make sure they fully understand what the authors of the paper have done.\n\nFinally, over the course of the semester we’ll see that R allows us to do much more than what is possible in Jamovi. We can define blocks of R code that can tidy-up and arrange our data, run multiple analyses and generate tables & figures all in one go. Much easier that clicking through all the menus each time we want to rerun an analysis!\n\n\n\n\n\nBlock, Jack. 1995. “A Contrarian View of the Five-Factor Approach to Personality Description.” Psychological Bulletin 117 (2): 187–215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. “Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.” Journal of Experimental Social Psychology 67: 68–82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. “A Very Brief Measure of the Big-Five Personality Domains.” Journal of Research in Personality 37 (6): 504–28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#the-dataset",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#the-dataset",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "NoteDataset\n\n\n\nNote that this is the same dataset as in week 1\n\n\nWe will be exploring data that was collected as part of the ‘Many Labs 3’ project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‘Big Five’ personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs. extravagant/careless)\nAgreeableness (friendly/compassionate vs. critical/rational)\nNeuroticism (sensitive/nervous vs. resilient/confident)\nOpenness to experience (inventive/curious vs. consistent/cautious)\nExtraversion (outgoing/energetic vs. solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#the-challenge",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#the-challenge",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Last week, we looked at how we can spend some time understanding a new dataset and compute some descriptive statistics from it.\nNow, we’re going to learn how to use Jamovi and R to make this process more repeatable and more transparent. This will make it much easier for us to come back to the analysis and understand what steps were taken at a later date.\n\n\n\n\n\n\nWarningOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#install-the-rj-add-on-in-jamovi",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#install-the-rj-add-on-in-jamovi",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "We’ll be working with the Rj Add-On for Jamovi this week.\n\n\n\n\n\n\nNoteKey step\n\n\n\nBefore continuing, install the Rj add-on for jamovi following the guide for installing Jamovi modules. Once installed you should have a new button on the Ribbon menu at the top of Jamovi…\n\n\n\nClick on this and select ‘Rj Editor’ to open a window in which we can write R code! Click on the small cog icon and make sure that ‘Output’ is set to ‘Show code and output’.\n\nWe can use the left-hand window, named Rj editor, to write commands, using the R language. These are the straightforward text commands that tell the computer a series of steps that we wanted to carry out. These commands can be edited and rerun as many times as we need, they can even be shared with collaborators and included as part of scientific publication.\nThis is a very powerful tool once we have a written set of instructions to work with rather than individually clicking through instructions. Though it takes a bit longer to write the commands, our analysis is much more transparent and reproducible once it is done.\nThe outputs of the analysis defined in the Rj editor appear on the right-hand side of the Jamovi window as usual. To check this out, let’s explore using or as a calculator.\n\n\n\n\n\n\nTipData Skills - R as a calculator\n\n\n\nTry typing a set of sums or equations into the Rj editor. Include one sum on each line and include as many as you like. When you’re ready, you can tell RJ to run the commands by clicking on the green triangle in the top right corner of the RJ editor. This will automatically compute. All of the sums you to find and print the results out in the right hand side window. see the example below.\nYou can use the following symbols to define your sums.\n\n\n\nSymbol\nOperation\n\n\n\n\n+\nAdd\n\n\n-\nSubtract\n\n\n*\nMultiply\n\n\n/\nDivide",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#variables-and-functions",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#variables-and-functions",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Often, we don’t just want to see the result of a calculation, we want to store the result to use again later. For this, we want to use variables. For example, here we store the result of a calculator sum in a variable named ‘y’\ny &lt;- 2 + 2\nIf you run this in Rj, you’ll notice that we no longer get the number 4 printed to the screen. This is as the output is now caught and stored rather than printed out. If we still want to see what is contained in y, we can use another helpful function named print.\ny &lt;- 2 + 2\nprint(y)\nWill now also display the content of y.\n\n\n\n\n\n\nNoteDefinition of a variable\n\n\n\nA variable is an object that provides a labelled space to store information. The &lt;- or = symbols indicate that we’re assigning some information to a variable (the two symbols behave the same way, you could see either).\nCrucially, the information on the right hand side is assigned to a variable with the name from the left hand side.\n\n\n\n\n\n\n\n\nTipCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about variables in R. We suggest you try the following prompts to get started.\n\n“Write a simple description of variables in R for a beginner”\n“How can I choose a name for my variable in R”\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\nNotice that the one of the equations in my example is a bit different to the others. The final example uses a function called sqrt as a shortcut to compute the square root of the number nine.\nThe sqrt function calculates the square root of a given number. In R code, this looks something like this:\nsqrt(x)\nwith the ‘x’ replaced by any number. For example, we could compute the square root of 36 using the following command.\nsqrt(36)\nAdd this line to your Rj editor and check the result..\nMany functions, take inputs contained in parentheses written after the name of the function. These inputs are sometimes known as the arguments of a function. In the last example, 36 is an argument for the function sqrt.\nSometimes functions take several arguments, and the arguments themselves can have specific names. For example.\nround(12.345, digits = 0)\nwill round the value x to a whole number with zero digits after the decimal point. Where as\nround(12.345, digits = 2)\nwould round the same number to preserve 2 digits after the decimal point. Try running these examples in Rj with a range of different numbers to get a sense of how it works.\n\n\n\n\n\n\nNoteDefinition of a function\n\n\n\nFunctions are used to perform specific, predefined tasks, such as calculations, data transformations, or visualization. You can think of it as a “black box” that takes in a value, performs some internal calculation, and returns the result.\n\n\n\n\n\n\n\n\nTipCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about functions in R. We suggest you try the following prompts to get started.\n\n“Write a simple description of functions in R for a beginner”\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\nA single analysis might have several lines of code that create, store and reuse a several different variables with several different functions. For example:\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat does this R code do? what is the value of x, y and z?\n x &lt;- 10 + 10\n y &lt;- sqrt(16)\n z &lt;- x / y\n\n\nTry copying out and running the code in Rj.\nYou can add some print functions to see the value of the intermediate variables x and y as well. Try adding print(x) on the line immediately after x is defined.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final answers:\nx is 20 and y is 4, so z = 20 / 4 which is 5.\n\n\n\n\n\n\n\nRj has a special feature that the data loaded into our spreadsheet is always contained in a special variable named data. We can refer to this data variable to view and run analyses on our dataset.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#accessing-the-data-table-in-rj",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#accessing-the-data-table-in-rj",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "NoteKey step\n\n\n\nTake a moment to load the data file into Jamovi and compute the descriptive statistics for the big five personality factors again.\n\n\n\nThe data we have loaded into our Jamovi session can be accessed in Rj with the data variable. Jamovi automatically makes the data available for us in the R environment.\nWe can view and interact with the data here as well. For example, to view the first few rows of the dataset you can run the head function and pass in data as an argument.\nhead(data)\nand to view the last few rows, you can do the same with the tail function.\ntail(data)\nOften, we’ll only want to work with a subset of the columns in a data frame at any given time. We can access individual columns using the dollar operator. For example, this code will access the age column within the loaded dataset.\ndata$age\nLet’s break this command down into its pieces:\n\n\n\n\n\n\n\nR code\nMeaning\n\n\n\n\ndata\nthis is the name of the whole data array we have loaded in\n\n\n$\nthis is a separator that R uses to help us access subsections of datasets\n\n\nage\nThis is the name of a single column we want to access\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you access the column of Mood scores from the same dataset?\n\n\nYou can use the head function to see the list of all available columns in the dataset. The example above accesses the ‘age’ column, you can tweak this to extract the mood data instead.\n\n\nThe final code should look like this\ndata$Mood\n\n\n\n\n\nWe can also apply functions to these variables. R includes a wide range of standard mathmatical functions that we can use for common computations. This code computes the mean of the age data in our dataset.\nmean(data$age)\nTry running this in Rj. You should get the answer 19.3 - this matches the value we computed using the descriptives analysis last week.\nSimilarly, the following function computes the standard deviation\nsd(data$age)\nIf you run this, you should get the answer 3.808.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#running-desriptive-statistics-using-rj",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#running-desriptive-statistics-using-rj",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Ok, back to statistics! How can we use our new R skills to compute statistics?\nThe trick is to understand that all the analyses in Jamovi are actually carried out by functions in R. Each analysis we can select from the top ribbon in Jamovi corresponds to a function that can be called in the code. Similarly, all of the options that we select in the analysis, dialogue boxes, correspond to arguments for that function. Let’s take a look in practice.\nJamovi provides a collection of R functions that correspond to each analysis dialogue box in Jamovi. This package of functions is named jmv. Thejmv function that computes descriptive statistics is called:\njmv::descriptives\nHere we first name the jmv package specify the descriptives function within it (this link is made with the two colons).\nWe can compute the descriptive statistics for age by passing the data into the function and specifying that we want to use the age variable. Try running the following:\njmv::descriptives(data, vars='age')\nand you should generate the following result table.\n\n\n\n\n\n\n\nNoteNote!\n\n\n\nYou might notice that we still have the unusual ages in this analysis, we’ll discuss how to remove them in the next section\n\n\nJamovi provide a lot of documentation on the descriptives function. This lists every single option that can be passed into jmv::descriptives. There is an option here for every single checkbox and selector in the main Jamovi dialogue.\nThe following code additionally computes a histogram by adding the hist argument.\njmv::descriptives(data, vars='age', hist=TRUE)\n\nWe can end up specifying a lot of these options. Sometimes it can be a good idea to split the function across multiple lines. This is fine as long as the parentheses are in the right place and there are commas in between each option.\nThis code computes descriptive statistics including the skewness, interquartile range and mode.\njmv::descriptives(data,\n  vars='age',\n  skew=TRUE,\n  iqr=TRUE,\n  mode=TRUE,\n  hist=TRUE\n  )\n\nOf course, we could run these analyses by hand. The main advantage here is that we can automate the selection of critical options and reduce the possiblity of mistakes when communicating analyses within teams. As long as we have this line saved, we can rerun it on new datasets without having to recheck every box by hand.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse the examples above and the JMV documentation on the descriptives function to write a function definition that does the following.\nCompute the core descriptive statistics for the Openness variable, make sure to include the range and a boxplot.\n\n\nTake a look at the ‘range’ and ‘box’ options in the documentation page.\nBe careful with the spelling of each argument!\n\n\nThe final code should be:\njmv::descriptives(data,\n  vars='Openness',\n  range=TRUE,\n  box=TRUE\n  )",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#selecting-rows-of-data-using-the-filter-function",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#selecting-rows-of-data-using-the-filter-function",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Remember that last week we very carefully removed several participants who had entered unrealisic or impossible ages into the datasets. This may have been an accident or mistake but either way we should not use this data. Our examples so far have just used all the data.\nR and Rj make it very easy to clean data without the boring, manual work we did last week. R can do the hard work and we can avoid scrolling through very large datasets by hand. Sounds good!\nThe first thing we can do is investigate what range of responses that people actually gave when asked about their age. To do this we can use a new function called unique(). This takes a column of a dataset as an input and returns a list of all the unique responses that it contains.\n\n\n\n\n\n\nNoteKey step\n\n\n\nLoad up an Rj Editor window (or continue in your old one) and run unique() on the age data.\nThe result should look like this:\n\n\n\nMost of these responses are fine but, as we saw last week, there are a lot of strange ones. Most importantly, we can find the extreme values that are very likely errors during data collection (-2 and 149)\nWe need to remove these from our analysis to get the most accurate results.\nTo do this we’re going to use an R library named dplyr (short for ‘Data Pliers’, see their documentation here). This is an extremely powerful library that is very widely used across the world.\nTo use dplyr, we first need to load the library into Rj by adding the following line to our code.\nlibrary(dplyr)\nThis tells Rj that we want to use the functions from this library.\nThe key function we need today is filter() - this extracts rows from a dataset based on logical conditions that we specify in the code. We are going to specify two arguments, firstly we will pass in the data and secondly we will specify the condition.\nIn this example we extract only the rows in which participants have an age above 20.\nlibrary(dplyr)\n\nfilter(data, age&gt;20)\n\nNotice that the dataset printed in the right hand window now only has values from the participants who are older than 20.\nWe can even specify multiple options - here we extract rows with participants older than 20 but still 23 or younger.\nlibrary(dplyr)\n\nfilter(data, age&gt;20, age&lt;=23)\nNext, to recreate our data tidying form last week, we need to extract the rows of participants who are older than -2 and younger than 149.\nThese values might be too specific and might not apply to every dataset. We can apply some common sense and suggest some broad, but much more sensible boundaries for a student population. We also save the result into a variable we can reuse later.\ntidy_data &lt;- filter(data, age &gt; 15, age &lt; 60)\nAnd finally, we can combine this with our call to jmv::descriptives to both filter our data to only the rows we want to use and compute the descriptive statistics on the result.\ntidy_data &lt;- filter(data, age &gt; 15, age &lt; 60)\n\njmv::descriptives(tidy_data, vars='age')\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nPractice changing the age conditions to a few different options to see the difference it makes to the descriptives analysis. What happens when you set a condition that results in zero valid participants?\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse the information in this tutorial to compute descriptive statistics for the Openness variable from Introverted participants. In other words - what is the mean Openness score for participants who are more Introverted than Extraverted.\nCompute the core descriptive statistics for the Openness variable, make sure to include the median and a histogram\n\n\nTo extract the data rows for Introverted participants we want to extract rows where the Extraversion score is less than 3.5\n\n\nThe final code should be:\nlibrary(dplyr)\n\ntidy_data &lt;- filter(data, Extraversion &lt; 3.5)\n\njmv::descriptives(tidy_data, vars='Openness')",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#summary",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#summary",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "It probably seems like a small difference so far. After all, we same results for our analysis using the check, boxes in Jamovi or the code in R. The Jamovi analysis is certainly quicker to run for the first time and it’s more convenient when exploring your data.\nHowever the R code has several crucial advantages.\n\nThe R code is transparent about what is actually being run to generate the results and by sharing this code, we enable others to repeat our analysis. This makes it really straightforward to run and re-run identical analyses in situations where we need to be really sure that everybody has done the same thing. For example, if we want to be certain that two data sets have been analysed in an identical way then that our code would be the best way to do this.\nR code is the best way to share what we’ve done with the rest of the world. It is common practice for modern academic public it to include the code that was used to generate the results in the paper. This way everybody who wants to check can make sure they fully understand what the authors of the paper have done.\n\nFinally, over the course of the semester we’ll see that R allows us to do much more than what is possible in Jamovi. We can define blocks of R code that can tidy-up and arrange our data, run multiple analyses and generate tables & figures all in one go. Much easier that clicking through all the menus each time we want to rerun an analysis!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#references",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#references",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Block, Jack. 1995. “A Contrarian View of the Five-Factor Approach to Personality Description.” Psychological Bulletin 117 (2): 187–215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. “Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.” Journal of Experimental Social Psychology 67: 68–82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. “A Very Brief Measure of the Big-Five Personality Domains.” Journal of Research in Personality 37 (6): 504–28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "This week we will explore how to use Jamovi to test hypotheses about a dataset using t-tests. We will touch on some revision from previous weeks so please do jump back to past computer practicals for revision if required\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nIndependent samples t-tests\n\n\n\nPaired samples t-tests\n\n\n\nAssumptions of parametric tests\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nComputing a new variable from existing data\n\n\n\nComputing checks for normality and homogeneity of variance\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nExploring and understanding new datasets\n\n\n\n\n\nWe’ll be working with the dataset we collected after the break during the lecture in Week 3. This is a partial replication of a study published in the journal Psychological Science (Miller et al. 2023). The original paper found the following results summarised in the abstract.\n\nRecent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 (N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces—a phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 (N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI.\n\nWe ran a quiz that students in the lecture could join on their phones/laptops. There were two parts to the quiz:\nThe faces section included 12 faces that were either photographs or real people or AI generated images of people. The stimuli were taked from the materials released by (Miller et al. 2023) on the Open Science Framework. Responses were either ‘Real person’ or ‘AI generated’.\n\nThe confidence section asked participants to rate their confidence in their ability to do the following three things.\n\nDistinguish real photos of people from AI generated images (Experimental condition)\nDistinguish photos of happy people from photos of sad people (Emotional control condition)\nDistinguish photos of people you used to know in primary school from strangers (Memory control condition)\n\nScores were recorded on a scale from 1 (Completely confidence) to 10 (Not at all confident). The confidence section was repeated before and after the faces section to see if participants confidence changed as a result of doing the AI faces task\n\n\n\nThis week we will use both one sample and two sample t-tests to explore the following hypotheses.\n\nPeople are able to distinguish AI generated faces from real photos of humans.\nConfident people are better at distinguishing AI faces from real faces.\nPeople’s confidence in distinguishing AI generated faces will reduce after performing the task, but their confidence about emotion perception and memory will not change.\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nTake a moment to think about these hypotheses. Which statistical test is most appropriate for each? Do they call for a one-tailed or a two-tailed test?\n\n\n\n\n\nIt is critical to take some time to understand the data we work with before running critical hypothesis tests. Here we’ll take a look through the dataset to understand what information is present and if we’re happy to proceed with the analysis. This is similar to what we did in week 1 - you can refer back to the week 1 materials for additional guidance if you need it.\n\n\n\n\n\n\nNoteKey step\n\n\n\nBefore going any further, the data file rmb-week-3_lecture-quiz-data_ai-faces.csv into a new Jamovi session.\n\n\nTake a read through the data columns. We have 26 in total with the following information.\n\n\n\n\n\n\n\nColumn Names\nDescription\n\n\n\n\nFirst Name\nParticipant ID - always ‘Anonymous’\n\n\nDataUse\nParticipant response to data re-use question\n\n\nAIConfidenceBefore\nConfidence in distinguishing AI faces from real BEFORE the task : 1 (Completely confident) to 10 (Not at all confident)\n\n\nEmoConfidenceBefore\nConfidence in distinguishing happy from sad faces BEFORE the task (Emotional control) : 1 (Completely confidence) to 10 (Not at all confident)\n\n\nMemoryConfidenceBefore\nConfidence in recognising a face from a long time ago BEFORE the task (Memory control) : 1 (Completely confidence) to 10 (Not at all confident)\n\n\nFace1_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace2_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace3_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace4_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace5_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace6_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace7_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace8_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace9_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace10_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace11_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace12_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nQuiz1\nResponse for revision quiz question\n\n\nQuiz2\nResponse for revision quiz question\n\n\nQuiz3\nResponse for revision quiz question\n\n\nAIConfidenceAfter\nConfidence in distinguishing AI faces from real AFTER the task\n\n\nEmoConfidenceAfter\nConfidence in distinguishing happy from sad faces AFTER the task (Emotional control)\n\n\nMemoryConfidenceAfter\nConfidence in recognising a face from a long time ago AFTER the task (Memory control)\n\n\n\nWork through the following questions, try to get an answer yourself before clicking to see the result. Data exploration is a critical skill that you’ll need whenever looking a new data throughout your degree.\n\n\n\n\n\n\nCaution Data Skills - how many participants took part in the quiz?\n\n\n\n\n\nWe have 124 rows of data observations the spreadsheet.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - did everyone consent to have their data included in this practical?\n\n\n\n\n\nYes, the responses in the DataUse column are always positive - “Yes, I’m happy for my data to be included”. We removed the data with negative responses before sharing the data here.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - proportion of responsese would we expect to be accurate if participants responded randomly in the face questions.?\n\n\n\n\n\nWe would expect 50% accuracy if participants answered randomly. There were only two response options ‘Real person’ or ‘AI generated’.\n\n\n\nYou can use Descriptive Statistics to answer the following questions.\n\n\n\n\n\n\nCaution Data Skills - which face did participants identify most accurately?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nFace 11 was most accurately identified as AI generated with 91.9% accuracy!\nFace 4 (AI) and Face 1 (Real) were close behind.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - which face did participants identify least accurately?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nFace 9 was least accurately indified as a real human with 31.8% accuracy. Face 5 (AI was second least accurate)\n\n\n\n\n\n\n\n\n\nCaution Data Skills - did we get complete data from all participants in the face task?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nNo, quite a few participants dropped in and out during the task. We’re missing between 6 (Face 2 and 3) and 21 (Face 12) participants on each question.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - were participants most confident in their AI discrimination, emotion recognition or memory before the face task?\n\n\n\n\n\nCompute desriptive statistics for all AIConfidenceBefore, EmoConfidenceBefore and MemoryConfidenceBefore.\nParticipants were most confident in their ability to distinguish happy from sad faces in the Emotional face control condition with a score of 2.51. Participants were least confident in their ability to distinguish AI faces from real faces with a score of 4.54.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - are the confidence scores before face task normally distributed?\n\n\n\n\n\nCompute desriptive statistics for all AIConfidenceBefore, EmoConfidenceBefore and MemoryConfidenceBefore. Add the ‘Shapiro-Wilk’ statistic to the table.\nIt looks like none of these variabiles are normally distributied… The W statistics for AIConfidenceBefore is much higher than the other two, but the p-values indicate that all three show a departure from a normal distribution.\nAdd a Histogram to your descriptive plots - we can see that AI confidence looks close to normally distributed but there is a very large skew in both Emotional confidence and Memory confidence - some participants had very low confidence scores in these conditions!\n\n\n\n\n\n\nThe descriptive statistics gave us a good overview of the dataset and we can start working towards testing our hypotheses.\nOne critical piece of information is missing though! we have accuracy for each individual face but not an overall score for each participant. We’ll need to compute this new variable ourselves from the average accuracy of all twelve faces.\n\n\n\n\n\n\nTipData Skills - computing a variable from other columns\n\n\n\nWe can define our own variables in Jamovi using the ‘Compute’ function in the ‘Variables’ or the ‘Data’ tabs. Open a new Transformed variable.\nThis will open a menu with an option to give the new variable an name and description. Name the variable ‘PropFacesCorrect’ to indicate that it contatins the proportion of faces that the participant responded correctly on. You can add a description if you like though this is optional.\nThe variable is defined within the formula box below the name definitions. We want to compute the average accuracy across all 12 faces so we can add the formula to compute that into the box.\n\nThe formula should add all the columns together and divide the result by 12 (the total number of faces). Make sure that all the additions are grouped by parentheses! otherwise Jamovi will only divide the final value by 12 and add it to the others. This is an example of BODMAS - Brackets, Of, Division/Multiplication, Addition/Subtraction that you might have covered in maths in school. talk to your tutor to make sure that this step makes sense.\nThe formula should look something like this, I’ve removed some faces to simplify the visualisation. You should include them all.\n(Face1_Real + Face2_Real + ... + Face11_AI + Face12_AI) / 12\nOnce this is complete, you should be able to find your new column of values.\n\n\nNow, let’s take a look at our new variable. Compute some descritive statistics!\n\n\n\n\n\n\nCaution Data Skills - how many participants gave a response to all 12 faces?\n\n\n\n\n\nWe have an N of 86 with 38 participants not responding to one or more of the faces.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - are the average accuracies normally distributed?\n\n\n\n\n\nTake a look at the Shaprio-Wilk statistic and the histogram of PropFacesCorrect, this looks like a normally distributed data variable.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - what proportion correct did the most and least accurate participants get?\n\n\n\n\n\nTake a look at the maxiumum and minumim of hte descriptive statistics, and perhaps add the table of ‘Most extreme’ data values in the Outiers section.\nTwo individuals managed to get all 12 faces correct! Two individuals were at 25% accurate corresponding to 3 out of 12 faces correct.\n\n\n\n\n\n\nOk, we’re ready to test the first hypothesis. Use the information you know about the dataset and try to find an answer!\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nTest the following hypothesis:\n\nPeople are able to distinguish AI generated faces from real photos of humans\n\nWhat sort of hypothesis is this and what is the most approprate statistical test?\nCompute the statistics, do the data support the experimental or the null hypothesis?\n\n\nWe could write a statistical version of this hypothesis as something like this\n\nPeople are able to distinguish AI generated faces from real photos of faces at an accuracy greater than chance level.\n\nWhat sort of test do you need to run?\n\n\nWe need to run a one sample t-test that compares the PropCorrectFaces variable to a chance level of 0.5 (corresponding to 50%). The results could be reported as follows\n\nA one sample t-test comparing the group average proportion of correctly identified faces (M = 0.634, SD=0.164) to chance level (proportion correct = 0.5) showed a significant effect, t(85) = 7.52, p&lt;0.001. Participants were on average more accurate than chance at distinguishing AI generated face from photos of real faces.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - do the results support hypothesis 1?\n\n\n\n\n\nYes, the statistical test allows us to reject the null hypothesis that participant performance was no different to chance level on this task and accept the experimental hypothesis that participants are able to distinguish AI generated faces from real photos of humans.\n\n\n\n\n\n\nNow the second hypothesis. We don’t have everything we need to test this hypothesis yet. We’ll need some way to split our participants into two groups - one with high confidence in AI face detection and one with low confidence. Time to compute another variable.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCompute a new variable named ConfidentBefore that separates the groups based on the median AI face detection confidence before the task.\n\n\nYou’ll need the median value for AIConfidenceBefore - you can compute this from descriptive statistics.\nThe computed variable will need some logical condition (using operators like ‘&gt;’, ‘&lt;’ or ‘==’) that separates participants with confidence above and below the median.\n\n\nThe median value for AIConfidenceBefore is 4, so our computed variable definition will look like this.\n\nThe values in ConfidentBefore will now be ‘True’ for people with high confidence and ‘False’ for people with low confidence. It doesn’t matter if you’ve done this the other way around - the tests will still work but the results will be flipped in the other direction (multiplied by -1)\n\n\n\n\n\nWith our new variable, we have what we need to run an independent samples t-test. This is very straightforward following the analyses we’ve run previously in the module.\nOpen the ‘Independent Sample t-test’ menu under ‘t-tests’. To run the analysis, drag PropFacesCorrect across as our dependent variable and our new ConfidentBefore variable as the grouping variable. The results should appear on the right automatically.\nOnce you have computed the core test - add the following options:\n\nDescriptives\nHomogeneity Test\nNormality Test\n\n\nLet’s think through the results\n\n\n\n\n\n\nCaution Data Skills - are the data normally distributed?\n\n\n\n\n\nThe Shairo-Wilk statistic is not significant, indicating that the data are likely to be normally distributed\n\n\n\n\n\n\n\n\n\nCaution Data Skills - do we have homogeneity of variance?\n\n\n\n\n\nLevene’s statistic is not significant, indicating that the variance of the two groups is comparable. We can confirm this by looking at the standard deviations in the descriptives table. 0.171 and 0.158 are fairly similar so the test report makes intuitive sense.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - which test should we report? Student’s, Welch’s or Mann-Whitney U?\n\n\n\n\n\nBoth assumptions of the standard Student’s t-test we looked at above are met by the data - we can proceed with Student’s test.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - were confident partiticpants or unconfident participants more accurate?\n\n\n\n\n\nConfident participants were more accurate! participants in this study are good at judging their ability.\nWe can see this by looking at the means in the descriptive statistics and the descriptives plot if you have run it.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - do the results support hypothesis 2?\n\n\n\n\n\nYes, overall it seems like we can reject the null for hypothesis 2 and accept the experimental hypothesis that more confident participants are better at distinguishing AI faces from real human faces.\n\n\n\n\n\n\nNext we want to explore whether performing the face decision task changes peoples confidences in their abililty to detect AI generated faces. Remember that all participants categorised 12 faces with immediate correct/incorrect feedback and made confidence ratings at the start and end of the task.\nOpen the ‘Paired Sample t-test’ menu under ‘t-tests’. To run the analysis, drag AIConfidenceBefore and AIConfidenceAfter across as our pair of dependent variables. This is the format for running paired samples t-tests, the rest of the options should be familiar from our previous analyses.\nOnce you have computed the core test, do the same for the EmoConfidence and MemoryConfidence, and add the following options:\n\nDescriptives\nNormality Test\n\nThe results should appear on the right automatically.\n\nLet’s think through the results…\n\n\n\n\n\n\nCaution Data Skills - does confidence in distinguishing AI faces from real faces change after completing the face task?\n\n\n\n\n\nYes, the results indicate that there is a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to distinguish AI faces from real faces before (M=4.48, SD=2.20) and after (M=5.66, SD=2.57) the face perception task. t(76) = -3.769, p&lt;0.001. Confidence was significantly reduced after completing the task.\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - does confidence in distinguishing sad faces from happy faces change after completing the face task?\n\n\n\n\n\nNo, the results indicate that there is not a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to distinguish sad faces from happy faces before (M=2.57, SD=1.49) and after (M=2.74, SD=1.37) the face perception task. t(64) = -1.1017, p=0.313. Confidence was unchanged before and after the task.\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - does confidence in recognising a face you haven’t seen for a long time change after completing the face task?\n\n\n\n\n\nNo, the results indicate that there is not a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to recognise faces of people they haven’t seen for a long time before (M=3.67, SD=1.89) and after (M=3.70, SD=1.86) the face perception task. t(66) = -0.234, p=0.816. Confidence was unchanged before and after the task.\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - What test should we report for these comparisons? Are the parametric assumptions met?\n\n\n\n\n\nCheck the results of hte normality tests. The Shapiro-Wilk statistic is significant for all three tests! We should consider the non-parametric alternative test - add the ‘Wilcoxon Rank’ test to your analysis.\nWe should report the Wilcoxon Rank test along with the medians for non-parametric data. For example:\n\nA Wilcoxon Rank test showed a significant difference in participants confidence in their ability to distinguish AI faces from real faces before (Median=4) and after (Median=5) the face perception task. W=680, p&lt;0.001. Confidence was significantly reduced after completing the task.\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - do the results support hypothesis 3?\n\n\n\n\n\nYes, overall it seems like we can reject the null for hypothesis 3 and accept the experimental hypothesis. Participants were less confident in their ability to detect AI generated faces after completing the task but their confidence in detecting emotions and remembering faces from a long time ago remains unchanged.\n\n\n\n\n\n\nLet’s use our new skills from this practical to answer one last question.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse a t-test to test this hypothesis:\n\nPeople are more accurate at identifying photos of real people compared to AI generated photos\n\nYou’ll several of the skills from this session to answer the question… think through what sort of variables you’ll need and what sort of test you’ll need.\n\n\nWe need 2 new variables to answer this question. We have already computed PropCorrectFaces in an earlier section, but we now need to make separate versions of this for AI faces and real faces…\nThe variable transforms will look something like this:\n \nWhat sort of t-test will you need?\n\n\nWe need to compute a paired samples t-test to answer the question as each participant contributes to both the AI face and real face conditions. Compute the test along with some descriptive statistics, we can report the test as follows\n\nA paired samples t-test showed a significant difference in the correct identification of AI faces (M=0.702, SD=0.198) compared to real faces (M=0.566, SD=0.229). t(85) = -4.60, p&lt;0.001. AI faces were identified more accurately than real faces.\n\n\n\n\n\n\n\n\n\n\nWe’ve computed a range of tests to statistically assess our hypotheses today! One experiment can often yield enough data to run a wide range of analyses. It is always a good idea to start with your hypotheses and predictions to break the analysis down into manageable chunks.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#the-dataset",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#the-dataset",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "We’ll be working with the dataset we collected after the break during the lecture in Week 3. This is a partial replication of a study published in the journal Psychological Science (Miller et al. 2023). The original paper found the following results summarised in the abstract.\n\nRecent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 (N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces—a phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 (N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI.\n\nWe ran a quiz that students in the lecture could join on their phones/laptops. There were two parts to the quiz:\nThe faces section included 12 faces that were either photographs or real people or AI generated images of people. The stimuli were taked from the materials released by (Miller et al. 2023) on the Open Science Framework. Responses were either ‘Real person’ or ‘AI generated’.\n\nThe confidence section asked participants to rate their confidence in their ability to do the following three things.\n\nDistinguish real photos of people from AI generated images (Experimental condition)\nDistinguish photos of happy people from photos of sad people (Emotional control condition)\nDistinguish photos of people you used to know in primary school from strangers (Memory control condition)\n\nScores were recorded on a scale from 1 (Completely confidence) to 10 (Not at all confident). The confidence section was repeated before and after the faces section to see if participants confidence changed as a result of doing the AI faces task",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#the-challenge",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#the-challenge",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "This week we will use both one sample and two sample t-tests to explore the following hypotheses.\n\nPeople are able to distinguish AI generated faces from real photos of humans.\nConfident people are better at distinguishing AI faces from real faces.\nPeople’s confidence in distinguishing AI generated faces will reduce after performing the task, but their confidence about emotion perception and memory will not change.\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nTake a moment to think about these hypotheses. Which statistical test is most appropriate for each? Do they call for a one-tailed or a two-tailed test?",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#exploring-the-data",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#exploring-the-data",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "It is critical to take some time to understand the data we work with before running critical hypothesis tests. Here we’ll take a look through the dataset to understand what information is present and if we’re happy to proceed with the analysis. This is similar to what we did in week 1 - you can refer back to the week 1 materials for additional guidance if you need it.\n\n\n\n\n\n\nNoteKey step\n\n\n\nBefore going any further, the data file rmb-week-3_lecture-quiz-data_ai-faces.csv into a new Jamovi session.\n\n\nTake a read through the data columns. We have 26 in total with the following information.\n\n\n\n\n\n\n\nColumn Names\nDescription\n\n\n\n\nFirst Name\nParticipant ID - always ‘Anonymous’\n\n\nDataUse\nParticipant response to data re-use question\n\n\nAIConfidenceBefore\nConfidence in distinguishing AI faces from real BEFORE the task : 1 (Completely confident) to 10 (Not at all confident)\n\n\nEmoConfidenceBefore\nConfidence in distinguishing happy from sad faces BEFORE the task (Emotional control) : 1 (Completely confidence) to 10 (Not at all confident)\n\n\nMemoryConfidenceBefore\nConfidence in recognising a face from a long time ago BEFORE the task (Memory control) : 1 (Completely confidence) to 10 (Not at all confident)\n\n\nFace1_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace2_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace3_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace4_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace5_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace6_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace7_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace8_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace9_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace10_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace11_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace12_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nQuiz1\nResponse for revision quiz question\n\n\nQuiz2\nResponse for revision quiz question\n\n\nQuiz3\nResponse for revision quiz question\n\n\nAIConfidenceAfter\nConfidence in distinguishing AI faces from real AFTER the task\n\n\nEmoConfidenceAfter\nConfidence in distinguishing happy from sad faces AFTER the task (Emotional control)\n\n\nMemoryConfidenceAfter\nConfidence in recognising a face from a long time ago AFTER the task (Memory control)\n\n\n\nWork through the following questions, try to get an answer yourself before clicking to see the result. Data exploration is a critical skill that you’ll need whenever looking a new data throughout your degree.\n\n\n\n\n\n\nCaution Data Skills - how many participants took part in the quiz?\n\n\n\n\n\nWe have 124 rows of data observations the spreadsheet.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - did everyone consent to have their data included in this practical?\n\n\n\n\n\nYes, the responses in the DataUse column are always positive - “Yes, I’m happy for my data to be included”. We removed the data with negative responses before sharing the data here.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - proportion of responsese would we expect to be accurate if participants responded randomly in the face questions.?\n\n\n\n\n\nWe would expect 50% accuracy if participants answered randomly. There were only two response options ‘Real person’ or ‘AI generated’.\n\n\n\nYou can use Descriptive Statistics to answer the following questions.\n\n\n\n\n\n\nCaution Data Skills - which face did participants identify most accurately?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nFace 11 was most accurately identified as AI generated with 91.9% accuracy!\nFace 4 (AI) and Face 1 (Real) were close behind.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - which face did participants identify least accurately?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nFace 9 was least accurately indified as a real human with 31.8% accuracy. Face 5 (AI was second least accurate)\n\n\n\n\n\n\n\n\n\nCaution Data Skills - did we get complete data from all participants in the face task?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nNo, quite a few participants dropped in and out during the task. We’re missing between 6 (Face 2 and 3) and 21 (Face 12) participants on each question.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - were participants most confident in their AI discrimination, emotion recognition or memory before the face task?\n\n\n\n\n\nCompute desriptive statistics for all AIConfidenceBefore, EmoConfidenceBefore and MemoryConfidenceBefore.\nParticipants were most confident in their ability to distinguish happy from sad faces in the Emotional face control condition with a score of 2.51. Participants were least confident in their ability to distinguish AI faces from real faces with a score of 4.54.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - are the confidence scores before face task normally distributed?\n\n\n\n\n\nCompute desriptive statistics for all AIConfidenceBefore, EmoConfidenceBefore and MemoryConfidenceBefore. Add the ‘Shapiro-Wilk’ statistic to the table.\nIt looks like none of these variabiles are normally distributied… The W statistics for AIConfidenceBefore is much higher than the other two, but the p-values indicate that all three show a departure from a normal distribution.\nAdd a Histogram to your descriptive plots - we can see that AI confidence looks close to normally distributed but there is a very large skew in both Emotional confidence and Memory confidence - some participants had very low confidence scores in these conditions!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#computing-overall-accuracy",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#computing-overall-accuracy",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "The descriptive statistics gave us a good overview of the dataset and we can start working towards testing our hypotheses.\nOne critical piece of information is missing though! we have accuracy for each individual face but not an overall score for each participant. We’ll need to compute this new variable ourselves from the average accuracy of all twelve faces.\n\n\n\n\n\n\nTipData Skills - computing a variable from other columns\n\n\n\nWe can define our own variables in Jamovi using the ‘Compute’ function in the ‘Variables’ or the ‘Data’ tabs. Open a new Transformed variable.\nThis will open a menu with an option to give the new variable an name and description. Name the variable ‘PropFacesCorrect’ to indicate that it contatins the proportion of faces that the participant responded correctly on. You can add a description if you like though this is optional.\nThe variable is defined within the formula box below the name definitions. We want to compute the average accuracy across all 12 faces so we can add the formula to compute that into the box.\n\nThe formula should add all the columns together and divide the result by 12 (the total number of faces). Make sure that all the additions are grouped by parentheses! otherwise Jamovi will only divide the final value by 12 and add it to the others. This is an example of BODMAS - Brackets, Of, Division/Multiplication, Addition/Subtraction that you might have covered in maths in school. talk to your tutor to make sure that this step makes sense.\nThe formula should look something like this, I’ve removed some faces to simplify the visualisation. You should include them all.\n(Face1_Real + Face2_Real + ... + Face11_AI + Face12_AI) / 12\nOnce this is complete, you should be able to find your new column of values.\n\n\nNow, let’s take a look at our new variable. Compute some descritive statistics!\n\n\n\n\n\n\nCaution Data Skills - how many participants gave a response to all 12 faces?\n\n\n\n\n\nWe have an N of 86 with 38 participants not responding to one or more of the faces.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - are the average accuracies normally distributed?\n\n\n\n\n\nTake a look at the Shaprio-Wilk statistic and the histogram of PropFacesCorrect, this looks like a normally distributed data variable.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - what proportion correct did the most and least accurate participants get?\n\n\n\n\n\nTake a look at the maxiumum and minumim of hte descriptive statistics, and perhaps add the table of ‘Most extreme’ data values in the Outiers section.\nTwo individuals managed to get all 12 faces correct! Two individuals were at 25% accurate corresponding to 3 out of 12 faces correct.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-1---people-are-able-to-distinguish-ai-generated-faces-from-real-photos-of-humans",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-1---people-are-able-to-distinguish-ai-generated-faces-from-real-photos-of-humans",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "Ok, we’re ready to test the first hypothesis. Use the information you know about the dataset and try to find an answer!\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nTest the following hypothesis:\n\nPeople are able to distinguish AI generated faces from real photos of humans\n\nWhat sort of hypothesis is this and what is the most approprate statistical test?\nCompute the statistics, do the data support the experimental or the null hypothesis?\n\n\nWe could write a statistical version of this hypothesis as something like this\n\nPeople are able to distinguish AI generated faces from real photos of faces at an accuracy greater than chance level.\n\nWhat sort of test do you need to run?\n\n\nWe need to run a one sample t-test that compares the PropCorrectFaces variable to a chance level of 0.5 (corresponding to 50%). The results could be reported as follows\n\nA one sample t-test comparing the group average proportion of correctly identified faces (M = 0.634, SD=0.164) to chance level (proportion correct = 0.5) showed a significant effect, t(85) = 7.52, p&lt;0.001. Participants were on average more accurate than chance at distinguishing AI generated face from photos of real faces.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - do the results support hypothesis 1?\n\n\n\n\n\nYes, the statistical test allows us to reject the null hypothesis that participant performance was no different to chance level on this task and accept the experimental hypothesis that participants are able to distinguish AI generated faces from real photos of humans.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-2---confident-people-are-better-at-distinguishing-ai-faces-from-real-faces.",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-2---confident-people-are-better-at-distinguishing-ai-faces-from-real-faces.",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "Now the second hypothesis. We don’t have everything we need to test this hypothesis yet. We’ll need some way to split our participants into two groups - one with high confidence in AI face detection and one with low confidence. Time to compute another variable.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCompute a new variable named ConfidentBefore that separates the groups based on the median AI face detection confidence before the task.\n\n\nYou’ll need the median value for AIConfidenceBefore - you can compute this from descriptive statistics.\nThe computed variable will need some logical condition (using operators like ‘&gt;’, ‘&lt;’ or ‘==’) that separates participants with confidence above and below the median.\n\n\nThe median value for AIConfidenceBefore is 4, so our computed variable definition will look like this.\n\nThe values in ConfidentBefore will now be ‘True’ for people with high confidence and ‘False’ for people with low confidence. It doesn’t matter if you’ve done this the other way around - the tests will still work but the results will be flipped in the other direction (multiplied by -1)\n\n\n\n\n\nWith our new variable, we have what we need to run an independent samples t-test. This is very straightforward following the analyses we’ve run previously in the module.\nOpen the ‘Independent Sample t-test’ menu under ‘t-tests’. To run the analysis, drag PropFacesCorrect across as our dependent variable and our new ConfidentBefore variable as the grouping variable. The results should appear on the right automatically.\nOnce you have computed the core test - add the following options:\n\nDescriptives\nHomogeneity Test\nNormality Test\n\n\nLet’s think through the results\n\n\n\n\n\n\nCaution Data Skills - are the data normally distributed?\n\n\n\n\n\nThe Shairo-Wilk statistic is not significant, indicating that the data are likely to be normally distributed\n\n\n\n\n\n\n\n\n\nCaution Data Skills - do we have homogeneity of variance?\n\n\n\n\n\nLevene’s statistic is not significant, indicating that the variance of the two groups is comparable. We can confirm this by looking at the standard deviations in the descriptives table. 0.171 and 0.158 are fairly similar so the test report makes intuitive sense.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - which test should we report? Student’s, Welch’s or Mann-Whitney U?\n\n\n\n\n\nBoth assumptions of the standard Student’s t-test we looked at above are met by the data - we can proceed with Student’s test.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - were confident partiticpants or unconfident participants more accurate?\n\n\n\n\n\nConfident participants were more accurate! participants in this study are good at judging their ability.\nWe can see this by looking at the means in the descriptive statistics and the descriptives plot if you have run it.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - do the results support hypothesis 2?\n\n\n\n\n\nYes, overall it seems like we can reject the null for hypothesis 2 and accept the experimental hypothesis that more confident participants are better at distinguishing AI faces from real human faces.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-3---peoples-confidence-in-their-ability-to-distinguish-ai-generated-faces-will-reduce-after-performing-the-task",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-3---peoples-confidence-in-their-ability-to-distinguish-ai-generated-faces-will-reduce-after-performing-the-task",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "Next we want to explore whether performing the face decision task changes peoples confidences in their abililty to detect AI generated faces. Remember that all participants categorised 12 faces with immediate correct/incorrect feedback and made confidence ratings at the start and end of the task.\nOpen the ‘Paired Sample t-test’ menu under ‘t-tests’. To run the analysis, drag AIConfidenceBefore and AIConfidenceAfter across as our pair of dependent variables. This is the format for running paired samples t-tests, the rest of the options should be familiar from our previous analyses.\nOnce you have computed the core test, do the same for the EmoConfidence and MemoryConfidence, and add the following options:\n\nDescriptives\nNormality Test\n\nThe results should appear on the right automatically.\n\nLet’s think through the results…\n\n\n\n\n\n\nCaution Data Skills - does confidence in distinguishing AI faces from real faces change after completing the face task?\n\n\n\n\n\nYes, the results indicate that there is a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to distinguish AI faces from real faces before (M=4.48, SD=2.20) and after (M=5.66, SD=2.57) the face perception task. t(76) = -3.769, p&lt;0.001. Confidence was significantly reduced after completing the task.\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - does confidence in distinguishing sad faces from happy faces change after completing the face task?\n\n\n\n\n\nNo, the results indicate that there is not a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to distinguish sad faces from happy faces before (M=2.57, SD=1.49) and after (M=2.74, SD=1.37) the face perception task. t(64) = -1.1017, p=0.313. Confidence was unchanged before and after the task.\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - does confidence in recognising a face you haven’t seen for a long time change after completing the face task?\n\n\n\n\n\nNo, the results indicate that there is not a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to recognise faces of people they haven’t seen for a long time before (M=3.67, SD=1.89) and after (M=3.70, SD=1.86) the face perception task. t(66) = -0.234, p=0.816. Confidence was unchanged before and after the task.\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - What test should we report for these comparisons? Are the parametric assumptions met?\n\n\n\n\n\nCheck the results of hte normality tests. The Shapiro-Wilk statistic is significant for all three tests! We should consider the non-parametric alternative test - add the ‘Wilcoxon Rank’ test to your analysis.\nWe should report the Wilcoxon Rank test along with the medians for non-parametric data. For example:\n\nA Wilcoxon Rank test showed a significant difference in participants confidence in their ability to distinguish AI faces from real faces before (Median=4) and after (Median=5) the face perception task. W=680, p&lt;0.001. Confidence was significantly reduced after completing the task.\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - do the results support hypothesis 3?\n\n\n\n\n\nYes, overall it seems like we can reject the null for hypothesis 3 and accept the experimental hypothesis. Participants were less confident in their ability to detect AI generated faces after completing the task but their confidence in detecting emotions and remembering faces from a long time ago remains unchanged.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#bonus-hypothesis-people-are-more-accurate-at-identifying-photos-of-real-people-compared-to-ai-generated-photos.",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#bonus-hypothesis-people-are-more-accurate-at-identifying-photos-of-real-people-compared-to-ai-generated-photos.",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "Let’s use our new skills from this practical to answer one last question.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse a t-test to test this hypothesis:\n\nPeople are more accurate at identifying photos of real people compared to AI generated photos\n\nYou’ll several of the skills from this session to answer the question… think through what sort of variables you’ll need and what sort of test you’ll need.\n\n\nWe need 2 new variables to answer this question. We have already computed PropCorrectFaces in an earlier section, but we now need to make separate versions of this for AI faces and real faces…\nThe variable transforms will look something like this:\n \nWhat sort of t-test will you need?\n\n\nWe need to compute a paired samples t-test to answer the question as each participant contributes to both the AI face and real face conditions. Compute the test along with some descriptive statistics, we can report the test as follows\n\nA paired samples t-test showed a significant difference in the correct identification of AI faces (M=0.702, SD=0.198) compared to real faces (M=0.566, SD=0.229). t(85) = -4.60, p&lt;0.001. AI faces were identified more accurately than real faces.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#summary",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#summary",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "We’ve computed a range of tests to statistically assess our hypotheses today! One experiment can often yield enough data to run a wide range of analyses. It is always a good idea to start with your hypotheses and predictions to break the analysis down into manageable chunks.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-4_factorial-anova-with-jamovi.html",
    "href": "tutorials/rmc_week-4_factorial-anova-with-jamovi.html",
    "title": "Week 4 : Factorial ANOVA with Jamovi",
    "section": "",
    "text": "Quantitative Methods\n\n\n\n\n\nUnderstand and interpret ANOVA outputs\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nTwo-way between-participants ANOVA\n\n\n\nTwo-way repeated measures ANOVA\n\n\n\nTwo-way mixed ANOVA\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with openly available research data\n\n\n\n\n\n\n\n\n\nNoteToday’s session\n\n\n\nIn today’s session we will explore more complicated ANOVA designs. The real power of ANOVA is that it allows us to examine interactions between two or more variables so we can examine how different factor combinations affect our participants.\n\n\n\nThere are three ways in which we can set up a study to explore two factors with two levels each:\n\nA different group of participants takes part in each condition (independent/between-participants 2x2 design).\nThe same group of participants takes part in all conditions (repeated measures/within-participants 2x2 design).\nTwo different groups of participants take part in two conditions each (mixed 2x2 design).\n\nIn lectures and computer practicals, we will also talk about how:\n\nA factorial ANOVA can have more than two conditions on each of its factors – just as a one-way ANOVA has three or more conditions on one factor.\n\nE.g. a two-way ANOVA might have one factor with three conditions and one with four. This would be called a ‘3x4’ design.\nIf a factor with more than 2 conditions has a significant main effect, we need to follow up using a post-hoc comparison as we would do for a one-way ANOVA.\n\n\nA factorial ANOVA can have more than two factors. This means that there are more interactions to consider and interpret. Usually our research questions and hypotheses play an important role in interpreting these interactions.\n\nE.g. a three-way ANOVA might have three factors with two conditions each – this is called a ‘2x2x2’ design.\nDesigns can be extended by using more conditions (e.g. 2x2x3 designs) or by adding more factors (e.g. 2x2x2x2 designs). The same principles apply:\n\nIf there are more than two conditions and a significant effect, a post-hoc is needed to identify which conditions are different to each other.\nIf there more factors, there will be more interactions to interpret.\n\n\n\n\n\n\n\nIn this example, we want to test if using the word “certain”, with either “It is” or “I am” as a prefix, influences someone’s decision making (whether they intend to make a bet). This factor is called communication mode, with two levels: internal (“I am”) vs. external (“It is”).\nWe also want to test if the speaker’s expertise (novice vs. expert) affects intention to bet and if expertise interacts with the effect of the communication mode.\nHypotheses:\n\nWe predict that internal mode of communication will have a greater influence on intention to bet than external mode of communication, and that a recommendation from an expert will have a greater effect upon intention to bet than a recommendation from a novice.\nWe expect this difference between experts and novices to be particularly pronounced for the internal mode of communication, such that an expert saying ‘I am certain’ will be particularly influential for intention to bet. [&lt;- this part predicts an interaction.]\n\nDifferent participants should take part in each condition of this experiment to avoid confounding of decision-making from previous experiences with the task.\nThe data need to be entered like this on Jamovi:\n\n\n\n\n\n\n\n\n\nCommunication mode (nominal variable with values 1 and 2) [grouping variable 1]\nExpertise (nominal variable with values 0 and 1) [grouping variable 2]\nIntention to bet (scale variable; for intention to bet, each participant’s individual score is recorded)\n\n\n\n\n1 (where 1 = external)\n0 (where 0 = novice)\nData for group 1. Each row has the same value for communication mode (i.e. 1) and for expertise (i.e. 0)\n\n\n1 (where 1 = external)\n1 (where 1 = expert)\nData for group 2. Each row has the same value for communication mode (i.e. 1) and for expertise (i.e. 1)\n\n\n2 (where 2 = internal)\n0 (where 0 = novice)\nData for group 3. Each row has the same value for communication mode (i.e. 2) and for expertise (i.e. 0)\n\n\n2 (where 2 = internal)\n1 (where 1 = expert)\nData for group 4. Each row has the same value for communication mode (i.e. 2) and for expertise (i.e. 1)\n\n\n\nWalk-Through Example 9 “The effects of communication mode and speaker expertise upon intention to bet”\n\n\n\nDownload and open the RMC_NM1_uncertainties_2x2_between.sav dataset\nWith two independent factors, you need two grouping variables, one for each factor, and a scale variable for the DV. The condition a participant took part in is determined by the combination of the two grouping variables.\n\n\n\n\n\n\nGo to Analyses -&gt; Exploration -&gt; Descriptives\nIn the dialog box, enter the DV into the ‘Variables’ box. Enter all IVs into the ‘Split by’ box. Go to ‘Plots’. Check ‘Histograms’.\n\n\n\n\n\n\n\nNoteNote\n\n\n\nIn this analysis, the individual conditions are skewed. This is in part due to the DV being measured on a very restricted scale (out of 5). However, all four conditions are showing some variability. So given that there is no non-parametric equivalent for 2 x 2 ANOVA, we would still run the ANOVA here.\n\n\n\n\n\n\nTo get the descriptive statistics for the main effects in Jamovi you need to run the analysis separately.\nGo to Analyses -&gt; Exploration -&gt; Descriptives\nIn the dialogue box, enter the DV into the ‘Variables’ box. Enter the IVs into the ‘Split By’ box.\n\nIf you want to calculate the means for each IV separately, do this: In the dialogue box, enter the DV into the ‘Variables’ box. Enter just one IV into the ‘Split By’ box. Repeat steps f and g for the other IV.\n\n\n\n\n\n\nGo to Analyses -&gt; ANOVA -&gt; ANOVA\n\n\n\nIn the dialog box, enter the DV into the ‘Dependent Variable’ box. Enter the two IVs in the ‘Fixed Factors’ box.\n\n\nCheck the box next to ‘partial η2’\n\n\n\nGo to ‘Estimated Marginal Means’. Enter the first IV in the Term 1 box. Click ‘Add New Term’. Enter the second IV into the Term 2 box. Click ‘Add New Term’. Enter the IV that you want to be displayed on the horizontal axis first in the Term 3 box (if one of your IVs is about time, e.g. ‘before’ and ‘after’ conditions, put this on the horizontal axis). Enter the IV you want to be displayed as separate lines second in the box Term 3 box. Change the error bars to ‘Standard Error’. Check the box next to ‘Marginal means plots’ and ‘Marginal means tables’.\n\n\n\nGo to ‘Post Hoc Tests’. Put the interaction into the box on the right. Select ‘Bonferroni’ from the Correction list. Check ‘Cohen’s d’ under Effect Size. This will allow you to identify where differences between conditions are, as with a one-way ANOVA.\n\n\n\n\n\n\nThe three rows labelled “communication_mode”, “expertise” and **“communication_mode*expertise” are most important in interpreting this output. (We also need to look at the “Residuals”** row, for the error df term.)\n\nThe “communication_mode” row shows that there is a significant main effect of communication mode upon the intention to bet, F(1, 95) = 68.50, p &lt; .001, η2p = .419.\n\nThe estimated marginal means table for communication_mode provides information about the direction of the difference. Here intention to bet (decision) is greater for internal mode communications than for external mode.\nPartial eta squared (η2p) tell us that this effect explains 42% of the variance in the data. This is a medium to large effect size.\n\nThe “expertise” row shows that there is also a significant main effect of expertise upon the intention to bet, F(1, 95) = 6.15, p = .015, η2p = .061.\n\nSee the estimated marginal means table for expertise for details of the difference. Here expert advice leads to a slightly higher intention to bet than novice advice.\nAn effect size (η2p) of 6% is small. In terms of discussing the findings, this means that speaker expertise plays a less important role when considered on its own than communication mode.\n\nFinally, the **“communication_mode*expertise” [pronounced ‘communication mode by expertise’] row shows that the interaction between communication mode and expertise is not significant, F(1, 95) = 3.60, p = .061, η2p = .037.** [Note – the effect size is not always reported when an effect is not significant.]\n\nAs the interaction is nearly significant, we might comment on this in a report.\n\n\n\nIf an interaction is significant, we would look at the interaction plot to examine it. Jamovi presents this near the end of the output, but looking at this before working through all the other tables Jamovi provides can be helpful for interpreting the findings.\n\n\n\n\n\n\n\n\n\n\nThe post hoc comparisons sections of the output explores the interaction by looking at differences in pairs of conditions, thus providing more detail about the nature of the observed effects\n\nThese tables show the results of t-tests between the different pairs of variables\n\n\n\n\n\n\n\n\n\n\n\n\nTip2x2 independent ANOVA Write Up\n\n\n\n\nFill in the blanks\n\n\n(Any data preparation, e.g. how the response times were calculated or how the data were trimmed or if and how many outliers were removed, should be presented in the section of the method about data preparation and data analysis.)\nIn a report, an ANOVA results section could look like this:\nResults\nThe intention to bet for each group was analysed using an independent 2 (communication mode: external vs. ) x 2 (level of expertise:  vs. expert) analysis of variance. The ANOVA found significant main effects on intention to bet for communication mode, F(1, ) = , p &lt; .001, η2p = , and for level of expertise, F (1, 95) = 6.15, p = , η2p = .061. The interaction between communication mode and level of expertise was approaching significance, F(1, 95) = , p = .061, η2p = .037.\nThe results broadly supported the hypothesis in that expert recommendations were more effective than novice ones, and across both levels of expertise the external mode of communication was less effective than internal mode.\nAlthough the interaction effect did not reach significance, the results supportpartially supportdo not support the prediction that the effect of expertise would be particularly noticeable in the internal mode of communication (see Table 1).\nTable 1. Mean intention to bet (out of 5) for external and internal communication mode depending on level of expertise of speaker (standard deviation in parentheses).\n\n\n\n\n\n\n\n\nCommunication mode\nLevel of expertise\n\n\n\n\n\n\nNovice\nExpert\n\n\nExternal\n (1.06)\n2.27 (1.12)\n\n\nInternal\n3.48 (1.16)\n4.39 ()\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this example, we want to test if the preference for positive chance phrases (e.g., “it is likely”) over negative chance phrases (“there is little chance”) is affected by the likelihood of a described event (low chance vs high chance), and if this is in turn affected by the time pressure (limited time vs unlimited time) under which participants respond. The proportion of times that a participant indicated a preference for a positive chance phrase over a negative chance phrase was recorded.\nHypothesis:\n\nPreferences will be expressed more strongly when time is limited than when it is unlimited, and this effect will be particularly strong when likelihood of an event occurring is high.\n\nTo minimise error due to people’s individual differences, the same participants could take part in all conditions. The data need to be entered into Jamovi as follows:\n\n\n\n\n\n\n\n\n\n\nLow likelihood, limited time (scale variable)\nLow likelihood, unlimited time (scale variable)\nHigh likelihood, limited time (scale variable)\nHigh likelihood, unlimited time (scale variable)\n\n\n\n\nData for Cond. 1, Ppt 1\nData for Cond. 2, Ppt 1\nData for Cond. 3, Ppt 1\nData for Cond. 4, Ppt 1\n\n\nData for Cond. 1, Ppt 2\nData for Cond. 2, Ppt 2\nData for Cond. 3, Ppt 2\nData for Cond. 4, Ppt 2\n\n\nData for Cond. 1, Ppt 3\nData for Cond. 2, Ppt 3\nData for Cond. 3, Ppt 3\nData for Cond. 4, Ppt 3\n\n\n…\n…\n…\n…\n\n\n\n\n\n\n\n\n\nNoteNote\n\n\n\nAlthough this looks like there are four DVs, this is not the case: only one thing was measured – the preference for positive phrases under four different circumstances. Notice that there is no grouping variable since all participants completed all of the different conditions and so were part of the same group.\n\n\nWalk-Through Example 10 “The effects of likelihood and time pressure on preference for positive chance phrases”\n\n\n\nDownload and open the RMC_NM1_directionality_2x2_within.sav dataset\nWith a repeated measures design, you do not have grouping variables, but you need 1 column for each data point you collect from the same participant.\n\nIn a 2x2 repeated measures design that means 4 columns, 1 for each condition.\n\n\n\n\n\n\n\nGo to Analyses -&gt; Exploration -&gt; Descriptives\nIn the dialog box, enter all columns of the DV into the ‘Variables’ box. Go to ‘Plots’. Check ‘Histogram’.\n\n\n\n\n\n\nGo to Analyses -&gt; ANOVA -&gt; Repeated Measures ANOVA\n\n\n\nIn the RM Factor 1 box, type a meaningful name for the first IV. Fill in the boxes with meaningful labels for the levels. e.g. ‘Likelihood’ for the IV overall, and ‘Low’ and ‘High’ for the levels of the IV.\nIn the RM Factor 2 box, type a meaningful name for the second IV. Fill in the boxes with meaningful labels for the levels. e.g. ‘Time Pressure’ for the IV overall, and ‘Limited’ and ‘Unlimited’ for the levels of the IV.\nAdd the variables in the correct ‘Repeated Measures Cells’ (match up the names of your columns with the labels in each cell. Check that the combinations are correct.\nCheck the box next to ‘partial η2’ under Effect Size and change the Dependent Variable Label to something meaningful. e.g. ‘Proportion’.\nGo to ‘Estimated Marginal Means’. Enter the first IV in the Term 1 box. Click ‘Add New Term’. Enter the second IV into the Term 2 box. Click ‘Add New Term’. Enter the IV that you want to be displayed on the horizontal axis first in the Term 3 box (if one of your IVs is about time, e.g. ‘before’ and ‘after’ conditions, put this on the horizontal axis). Enter the IV you want to be displayed as separate lines second in the box Term 3 box. Change the error bars to ‘Standard Error’. Check the box next to ‘Marginal means plots’ and ‘Marginal means tables’.\nGo to ‘Post Hoc Tests’. Put the interaction into the box on the right. Select ‘Bonferroni’ from the Correction list.\n\n\n\n\n\n\n\n\n\n\n\nFrom the next part of the output, only the most relevant tables are reproduced here: the estimated marginal means for likelihood and for time_pressure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip2x2 repeated measures ANOVA Write Up\n\n\n\n\nFill in the blanks\n\n\n(Any data preparation, e.g. how the response times were calculated or how the data were trimmed or if and how many outliers were removed, should be presented in the section of the method about data preparation and data analysis.)\nIn a report, an ANOVA results section could look like this:\nResults\nThe effects of likelihood (low vs. high) and time pressure (limited time vs. unlimited time) upon preference for positive chance phrases expressed as a proportion was tested with a 2x2 repeated measures ANOVA. As predicted there was a significant main effect of likelihood, F(1,19) = 33.8; p &lt; .001; η2p = , and a significant main effect of time pressure, F(1,19) = ; p ; η2p = .945. The interaction between likelihood and time pressure was also significant, F(, ) = 59.7; p &lt; .001; η2p = .\nPreferences for positive chance phrases were expressed less strongly for low likelihood scenarios compared to high likelihood, while time pressure led to more strongly expressed preferences than when time was unlimited. In keeping with predictions, the interaction showed that the effect of time pressure was particularly strong for high likelihood scenarios compared to low likelihood scenarios (see Table 1).\nTable 1. Mean preference for positive chance phrases (as a proportion) depending on likelihood of scenario and time pressure (standard deviation in parentheses).\n\n\n\n\n\n\n\n\nLikelihood of scenario\nTime allowed\n\n\n\n\n\n\nLimited\nUnlimited\n\n\nLow\n.67 ()\n.52 (0.08)\n\n\nHigh\n (.05)\n.51 ()\n\n\n\n(Report values to 2 d.p.)\n\n\n\n\n\n\n\n\n\n\nIn this example, an experiment was conducted to look at the effect of sentence type (question vs. statement-to-verify) on the rate of semantic illusions.\nSince all participants are taking part in both conditions – they see question stimuli and statement stimuli, the order in which the sentence types are processed has been counterbalanced. We can therefore also look at whether presentation order (questions first or statements first) affects the rate of semantic illusions.\nThe sentence type is therefore the within-participants (repeated measures) factor, and the presentation order is the between-participants factor in this design.\nHypothesis:\n\nQuestions will lead to a higher semantic illusion rate than statements, regardless of presentation order.\n\nThe data for a mixed design need to be entered into Jamovi like this:\n\n\n\n\n\n\n\n\n\nOrder (qu1st1) (a nominal variable with values 1 and 2) [grouping variable 1]\nRate of semantic illusions with questions (qusirate) (scale variable)\nRate of semantic illusions with statements (stsirate) (scale variable)\n\n\n\n\n1 (where 1 = questions first)\nqusirate for Ppt 1 who saw questions first\nstsirate for Ppt 1 who saw questions first\n\n\n\nqusirate for Ppt 2 who saw questions first\nstsirate for Ppt 2 who saw questions first\n\n\n\n…\n…\n\n\n2 (where 2 = statements first)\nqusirate for Ppt 1 who saw statements first\nstsirate for Ppt 1 who saw statements first\n\n\n\nqusirate for Ppt 2 who saw statements first\nstsirate for Ppt 2 who saw statements first\n\n\n\n…\n…\n\n\n\nWalk-Through Example 11 “The effects sentence type and presentation order upon semantic illusion rate”\n\n\n\nDownload and open the RMC_NM1_statements-questions_2x2_mixed.sav dataset\nWith a mixed design, you need one grouping variable and you need one column for each data point you collected from the same participant (or each level of the within-participants factor).\nIn a 2x2 mixed design that means three columns, one for each condition on the within-participants (repeated) measure and one for the grouping variable.\n\n\n\n\n\n\nDescriptive Statistics. Go to Analyses -&gt; Exploration -&gt; Descriptives. In the dialog box, enter the two DVs (repeated measures) into the ‘Variables’ box.\nEnter the grouping variable (between-participants IV) into the ‘Split By’ box. Go to ‘Plots’. Check ‘Histogram’.\n\n\n\n\n\nGo to Analyses -&gt; ANOVA -&gt; Repeated Measures ANOVA\n\n\n\nIn the RM Factor 1 box, type a meaningful name for the IV (e.g. ‘Sentence Type’). Fill in the boxes with meaningful labels for the levels (e.g. ‘Question’ and ‘Statement to verify’).\nAdd the variables in the correct ‘Repeated Measures Cells’ (match up the names of your columns with the labels in each cell).\nEnter the between-participants IV into ‘Between-Subjects Factor’ box.\nCheck the box next to ‘partial η2’ under Effect Size and change the Dependent Variable Label to something meaningful (e.g. ‘Rate of semantic illusions’).\n\n\n\nGo to ‘Estimated Marginal Means’. Enter the first IV in the Term 1 box. Click ‘Add New Term’. Enter the second IV into the Term 2 box. Click ‘Add New Term’. Enter the IV that you want to be displayed on the horizontal axis first in the Term 3 box (if one of your IVs is about time, e.g. ‘before’ and ‘after’ conditions, put this on the horizontal axis, e.g. in this example, it makes sense to have presentation order (qu1st1) on the horizontal axis). Enter the IV you want to be displayed as separate lines second in the box Term 3 box. Change the error bars to ‘Standard Error’. Check the box next to ‘Marginal means plots’ and ‘Marginal means tables’.\nGo to ‘Post Hoc Tests’. Put the interaction into the box on the right. Select ‘Bonferroni’ from the Correction list.\n\n\n\n\n\n\n\n\n\n\n\nFrom the next part of the output, only the most relevant tables are reproduced here: the estimated marginal means for presentation order and for sentence type.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteNote - including charts\n\n\n\nWe do not usually include an interaction plot, unless the interaction is significant, and then we tend to use bar charts instead of line charts as in the Jamovi output.\n\n\n\n\n\n\n\n\n\n\n\nTip2x2 mixed ANOVA Write Up\n\n\n\n\nFill in the blanks\n\n\n(Any data preparation, e.g. how the response times were calculated or how the data were trimmed or if and how many outliers were removed, should be presented in the section of the method about data preparation and data analysis.)\nIn a report, an ANOVA results section could look like this:\nResults (Report values to 3 d.p.)\nThe rate of semantic illusions for each condition was analysed using a mixed ANOVA with one within-participants factor of sentence type (question vs. statement) and one between-participants factor of presentation order (questions first vs. statements first).\nThere was a significant effect of sentence type on semantic illusion rate, F() = 53.013, p &lt; .001, η2p = , but not for the presentation order, F (1, 132) = , p = .346, or for the interaction between sentence type and presentation order, F(1, 132) = 0.835, p = .*\nAs predicted, the results showed that more semantic illusions occurred when the sentence was a question than when the sentence was a statement. The presentation order did not have any influence on semantic illusion rate. See Table 1 for means and standard deviations.\n*Here partial eta squared is only reported for the significant effects in the analysis\nTable 1. Mean semantic illusion rates (%) for questions compared to statements in both presentation orders (standard deviation in parentheses).\n\n\n\n\n\n\n\n\nPresentation order\nSentence Type\n\n\n\n\n\n\nQuestions\nStatements\n\n\nQuestions first\n43.9 ()\n29.9 (17.1)\n\n\nStatements first\n (19.0)\n29.1 ()\n\n\n\n(Report values to 1 d.p.)",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 4 - Factorial ANOVA with Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-4_factorial-anova-with-jamovi.html#two-between-participants-ivs-2x2-independent-anova",
    "href": "tutorials/rmc_week-4_factorial-anova-with-jamovi.html#two-between-participants-ivs-2x2-independent-anova",
    "title": "Week 4 : Factorial ANOVA with Jamovi",
    "section": "",
    "text": "In this example, we want to test if using the word “certain”, with either “It is” or “I am” as a prefix, influences someone’s decision making (whether they intend to make a bet). This factor is called communication mode, with two levels: internal (“I am”) vs. external (“It is”).\nWe also want to test if the speaker’s expertise (novice vs. expert) affects intention to bet and if expertise interacts with the effect of the communication mode.\nHypotheses:\n\nWe predict that internal mode of communication will have a greater influence on intention to bet than external mode of communication, and that a recommendation from an expert will have a greater effect upon intention to bet than a recommendation from a novice.\nWe expect this difference between experts and novices to be particularly pronounced for the internal mode of communication, such that an expert saying ‘I am certain’ will be particularly influential for intention to bet. [&lt;- this part predicts an interaction.]\n\nDifferent participants should take part in each condition of this experiment to avoid confounding of decision-making from previous experiences with the task.\nThe data need to be entered like this on Jamovi:\n\n\n\n\n\n\n\n\n\nCommunication mode (nominal variable with values 1 and 2) [grouping variable 1]\nExpertise (nominal variable with values 0 and 1) [grouping variable 2]\nIntention to bet (scale variable; for intention to bet, each participant’s individual score is recorded)\n\n\n\n\n1 (where 1 = external)\n0 (where 0 = novice)\nData for group 1. Each row has the same value for communication mode (i.e. 1) and for expertise (i.e. 0)\n\n\n1 (where 1 = external)\n1 (where 1 = expert)\nData for group 2. Each row has the same value for communication mode (i.e. 1) and for expertise (i.e. 1)\n\n\n2 (where 2 = internal)\n0 (where 0 = novice)\nData for group 3. Each row has the same value for communication mode (i.e. 2) and for expertise (i.e. 0)\n\n\n2 (where 2 = internal)\n1 (where 1 = expert)\nData for group 4. Each row has the same value for communication mode (i.e. 2) and for expertise (i.e. 1)\n\n\n\nWalk-Through Example 9 “The effects of communication mode and speaker expertise upon intention to bet”\n\n\n\nDownload and open the RMC_NM1_uncertainties_2x2_between.sav dataset\nWith two independent factors, you need two grouping variables, one for each factor, and a scale variable for the DV. The condition a participant took part in is determined by the combination of the two grouping variables.\n\n\n\n\n\n\nGo to Analyses -&gt; Exploration -&gt; Descriptives\nIn the dialog box, enter the DV into the ‘Variables’ box. Enter all IVs into the ‘Split by’ box. Go to ‘Plots’. Check ‘Histograms’.\n\n\n\n\n\n\n\nNoteNote\n\n\n\nIn this analysis, the individual conditions are skewed. This is in part due to the DV being measured on a very restricted scale (out of 5). However, all four conditions are showing some variability. So given that there is no non-parametric equivalent for 2 x 2 ANOVA, we would still run the ANOVA here.\n\n\n\n\n\n\nTo get the descriptive statistics for the main effects in Jamovi you need to run the analysis separately.\nGo to Analyses -&gt; Exploration -&gt; Descriptives\nIn the dialogue box, enter the DV into the ‘Variables’ box. Enter the IVs into the ‘Split By’ box.\n\nIf you want to calculate the means for each IV separately, do this: In the dialogue box, enter the DV into the ‘Variables’ box. Enter just one IV into the ‘Split By’ box. Repeat steps f and g for the other IV.\n\n\n\n\n\n\nGo to Analyses -&gt; ANOVA -&gt; ANOVA\n\n\n\nIn the dialog box, enter the DV into the ‘Dependent Variable’ box. Enter the two IVs in the ‘Fixed Factors’ box.\n\n\nCheck the box next to ‘partial η2’\n\n\n\nGo to ‘Estimated Marginal Means’. Enter the first IV in the Term 1 box. Click ‘Add New Term’. Enter the second IV into the Term 2 box. Click ‘Add New Term’. Enter the IV that you want to be displayed on the horizontal axis first in the Term 3 box (if one of your IVs is about time, e.g. ‘before’ and ‘after’ conditions, put this on the horizontal axis). Enter the IV you want to be displayed as separate lines second in the box Term 3 box. Change the error bars to ‘Standard Error’. Check the box next to ‘Marginal means plots’ and ‘Marginal means tables’.\n\n\n\nGo to ‘Post Hoc Tests’. Put the interaction into the box on the right. Select ‘Bonferroni’ from the Correction list. Check ‘Cohen’s d’ under Effect Size. This will allow you to identify where differences between conditions are, as with a one-way ANOVA.\n\n\n\n\n\n\nThe three rows labelled “communication_mode”, “expertise” and **“communication_mode*expertise” are most important in interpreting this output. (We also need to look at the “Residuals”** row, for the error df term.)\n\nThe “communication_mode” row shows that there is a significant main effect of communication mode upon the intention to bet, F(1, 95) = 68.50, p &lt; .001, η2p = .419.\n\nThe estimated marginal means table for communication_mode provides information about the direction of the difference. Here intention to bet (decision) is greater for internal mode communications than for external mode.\nPartial eta squared (η2p) tell us that this effect explains 42% of the variance in the data. This is a medium to large effect size.\n\nThe “expertise” row shows that there is also a significant main effect of expertise upon the intention to bet, F(1, 95) = 6.15, p = .015, η2p = .061.\n\nSee the estimated marginal means table for expertise for details of the difference. Here expert advice leads to a slightly higher intention to bet than novice advice.\nAn effect size (η2p) of 6% is small. In terms of discussing the findings, this means that speaker expertise plays a less important role when considered on its own than communication mode.\n\nFinally, the **“communication_mode*expertise” [pronounced ‘communication mode by expertise’] row shows that the interaction between communication mode and expertise is not significant, F(1, 95) = 3.60, p = .061, η2p = .037.** [Note – the effect size is not always reported when an effect is not significant.]\n\nAs the interaction is nearly significant, we might comment on this in a report.\n\n\n\nIf an interaction is significant, we would look at the interaction plot to examine it. Jamovi presents this near the end of the output, but looking at this before working through all the other tables Jamovi provides can be helpful for interpreting the findings.\n\n\n\n\n\n\n\n\n\n\nThe post hoc comparisons sections of the output explores the interaction by looking at differences in pairs of conditions, thus providing more detail about the nature of the observed effects\n\nThese tables show the results of t-tests between the different pairs of variables\n\n\n\n\n\n\n\n\n\n\n\n\nTip2x2 independent ANOVA Write Up\n\n\n\n\nFill in the blanks\n\n\n(Any data preparation, e.g. how the response times were calculated or how the data were trimmed or if and how many outliers were removed, should be presented in the section of the method about data preparation and data analysis.)\nIn a report, an ANOVA results section could look like this:\nResults\nThe intention to bet for each group was analysed using an independent 2 (communication mode: external vs. ) x 2 (level of expertise:  vs. expert) analysis of variance. The ANOVA found significant main effects on intention to bet for communication mode, F(1, ) = , p &lt; .001, η2p = , and for level of expertise, F (1, 95) = 6.15, p = , η2p = .061. The interaction between communication mode and level of expertise was approaching significance, F(1, 95) = , p = .061, η2p = .037.\nThe results broadly supported the hypothesis in that expert recommendations were more effective than novice ones, and across both levels of expertise the external mode of communication was less effective than internal mode.\nAlthough the interaction effect did not reach significance, the results supportpartially supportdo not support the prediction that the effect of expertise would be particularly noticeable in the internal mode of communication (see Table 1).\nTable 1. Mean intention to bet (out of 5) for external and internal communication mode depending on level of expertise of speaker (standard deviation in parentheses).\n\n\n\n\n\n\n\n\nCommunication mode\nLevel of expertise\n\n\n\n\n\n\nNovice\nExpert\n\n\nExternal\n (1.06)\n2.27 (1.12)\n\n\nInternal\n3.48 (1.16)\n4.39 ()",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 4 - Factorial ANOVA with Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-4_factorial-anova-with-jamovi.html#two-within-participants-ivs-2x2-repeated-measures-anova",
    "href": "tutorials/rmc_week-4_factorial-anova-with-jamovi.html#two-within-participants-ivs-2x2-repeated-measures-anova",
    "title": "Week 4 : Factorial ANOVA with Jamovi",
    "section": "",
    "text": "In this example, we want to test if the preference for positive chance phrases (e.g., “it is likely”) over negative chance phrases (“there is little chance”) is affected by the likelihood of a described event (low chance vs high chance), and if this is in turn affected by the time pressure (limited time vs unlimited time) under which participants respond. The proportion of times that a participant indicated a preference for a positive chance phrase over a negative chance phrase was recorded.\nHypothesis:\n\nPreferences will be expressed more strongly when time is limited than when it is unlimited, and this effect will be particularly strong when likelihood of an event occurring is high.\n\nTo minimise error due to people’s individual differences, the same participants could take part in all conditions. The data need to be entered into Jamovi as follows:\n\n\n\n\n\n\n\n\n\n\nLow likelihood, limited time (scale variable)\nLow likelihood, unlimited time (scale variable)\nHigh likelihood, limited time (scale variable)\nHigh likelihood, unlimited time (scale variable)\n\n\n\n\nData for Cond. 1, Ppt 1\nData for Cond. 2, Ppt 1\nData for Cond. 3, Ppt 1\nData for Cond. 4, Ppt 1\n\n\nData for Cond. 1, Ppt 2\nData for Cond. 2, Ppt 2\nData for Cond. 3, Ppt 2\nData for Cond. 4, Ppt 2\n\n\nData for Cond. 1, Ppt 3\nData for Cond. 2, Ppt 3\nData for Cond. 3, Ppt 3\nData for Cond. 4, Ppt 3\n\n\n…\n…\n…\n…\n\n\n\n\n\n\n\n\n\nNoteNote\n\n\n\nAlthough this looks like there are four DVs, this is not the case: only one thing was measured – the preference for positive phrases under four different circumstances. Notice that there is no grouping variable since all participants completed all of the different conditions and so were part of the same group.\n\n\nWalk-Through Example 10 “The effects of likelihood and time pressure on preference for positive chance phrases”\n\n\n\nDownload and open the RMC_NM1_directionality_2x2_within.sav dataset\nWith a repeated measures design, you do not have grouping variables, but you need 1 column for each data point you collect from the same participant.\n\nIn a 2x2 repeated measures design that means 4 columns, 1 for each condition.\n\n\n\n\n\n\n\nGo to Analyses -&gt; Exploration -&gt; Descriptives\nIn the dialog box, enter all columns of the DV into the ‘Variables’ box. Go to ‘Plots’. Check ‘Histogram’.\n\n\n\n\n\n\nGo to Analyses -&gt; ANOVA -&gt; Repeated Measures ANOVA\n\n\n\nIn the RM Factor 1 box, type a meaningful name for the first IV. Fill in the boxes with meaningful labels for the levels. e.g. ‘Likelihood’ for the IV overall, and ‘Low’ and ‘High’ for the levels of the IV.\nIn the RM Factor 2 box, type a meaningful name for the second IV. Fill in the boxes with meaningful labels for the levels. e.g. ‘Time Pressure’ for the IV overall, and ‘Limited’ and ‘Unlimited’ for the levels of the IV.\nAdd the variables in the correct ‘Repeated Measures Cells’ (match up the names of your columns with the labels in each cell. Check that the combinations are correct.\nCheck the box next to ‘partial η2’ under Effect Size and change the Dependent Variable Label to something meaningful. e.g. ‘Proportion’.\nGo to ‘Estimated Marginal Means’. Enter the first IV in the Term 1 box. Click ‘Add New Term’. Enter the second IV into the Term 2 box. Click ‘Add New Term’. Enter the IV that you want to be displayed on the horizontal axis first in the Term 3 box (if one of your IVs is about time, e.g. ‘before’ and ‘after’ conditions, put this on the horizontal axis). Enter the IV you want to be displayed as separate lines second in the box Term 3 box. Change the error bars to ‘Standard Error’. Check the box next to ‘Marginal means plots’ and ‘Marginal means tables’.\nGo to ‘Post Hoc Tests’. Put the interaction into the box on the right. Select ‘Bonferroni’ from the Correction list.\n\n\n\n\n\n\n\n\n\n\n\nFrom the next part of the output, only the most relevant tables are reproduced here: the estimated marginal means for likelihood and for time_pressure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip2x2 repeated measures ANOVA Write Up\n\n\n\n\nFill in the blanks\n\n\n(Any data preparation, e.g. how the response times were calculated or how the data were trimmed or if and how many outliers were removed, should be presented in the section of the method about data preparation and data analysis.)\nIn a report, an ANOVA results section could look like this:\nResults\nThe effects of likelihood (low vs. high) and time pressure (limited time vs. unlimited time) upon preference for positive chance phrases expressed as a proportion was tested with a 2x2 repeated measures ANOVA. As predicted there was a significant main effect of likelihood, F(1,19) = 33.8; p &lt; .001; η2p = , and a significant main effect of time pressure, F(1,19) = ; p ; η2p = .945. The interaction between likelihood and time pressure was also significant, F(, ) = 59.7; p &lt; .001; η2p = .\nPreferences for positive chance phrases were expressed less strongly for low likelihood scenarios compared to high likelihood, while time pressure led to more strongly expressed preferences than when time was unlimited. In keeping with predictions, the interaction showed that the effect of time pressure was particularly strong for high likelihood scenarios compared to low likelihood scenarios (see Table 1).\nTable 1. Mean preference for positive chance phrases (as a proportion) depending on likelihood of scenario and time pressure (standard deviation in parentheses).\n\n\n\n\n\n\n\n\nLikelihood of scenario\nTime allowed\n\n\n\n\n\n\nLimited\nUnlimited\n\n\nLow\n.67 ()\n.52 (0.08)\n\n\nHigh\n (.05)\n.51 ()\n\n\n\n(Report values to 2 d.p.)",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 4 - Factorial ANOVA with Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-4_factorial-anova-with-jamovi.html#one-between-participants-iv-and-one-within-participants-iv-2x2-mixed-anova",
    "href": "tutorials/rmc_week-4_factorial-anova-with-jamovi.html#one-between-participants-iv-and-one-within-participants-iv-2x2-mixed-anova",
    "title": "Week 4 : Factorial ANOVA with Jamovi",
    "section": "",
    "text": "In this example, an experiment was conducted to look at the effect of sentence type (question vs. statement-to-verify) on the rate of semantic illusions.\nSince all participants are taking part in both conditions – they see question stimuli and statement stimuli, the order in which the sentence types are processed has been counterbalanced. We can therefore also look at whether presentation order (questions first or statements first) affects the rate of semantic illusions.\nThe sentence type is therefore the within-participants (repeated measures) factor, and the presentation order is the between-participants factor in this design.\nHypothesis:\n\nQuestions will lead to a higher semantic illusion rate than statements, regardless of presentation order.\n\nThe data for a mixed design need to be entered into Jamovi like this:\n\n\n\n\n\n\n\n\n\nOrder (qu1st1) (a nominal variable with values 1 and 2) [grouping variable 1]\nRate of semantic illusions with questions (qusirate) (scale variable)\nRate of semantic illusions with statements (stsirate) (scale variable)\n\n\n\n\n1 (where 1 = questions first)\nqusirate for Ppt 1 who saw questions first\nstsirate for Ppt 1 who saw questions first\n\n\n\nqusirate for Ppt 2 who saw questions first\nstsirate for Ppt 2 who saw questions first\n\n\n\n…\n…\n\n\n2 (where 2 = statements first)\nqusirate for Ppt 1 who saw statements first\nstsirate for Ppt 1 who saw statements first\n\n\n\nqusirate for Ppt 2 who saw statements first\nstsirate for Ppt 2 who saw statements first\n\n\n\n…\n…\n\n\n\nWalk-Through Example 11 “The effects sentence type and presentation order upon semantic illusion rate”\n\n\n\nDownload and open the RMC_NM1_statements-questions_2x2_mixed.sav dataset\nWith a mixed design, you need one grouping variable and you need one column for each data point you collected from the same participant (or each level of the within-participants factor).\nIn a 2x2 mixed design that means three columns, one for each condition on the within-participants (repeated) measure and one for the grouping variable.\n\n\n\n\n\n\nDescriptive Statistics. Go to Analyses -&gt; Exploration -&gt; Descriptives. In the dialog box, enter the two DVs (repeated measures) into the ‘Variables’ box.\nEnter the grouping variable (between-participants IV) into the ‘Split By’ box. Go to ‘Plots’. Check ‘Histogram’.\n\n\n\n\n\nGo to Analyses -&gt; ANOVA -&gt; Repeated Measures ANOVA\n\n\n\nIn the RM Factor 1 box, type a meaningful name for the IV (e.g. ‘Sentence Type’). Fill in the boxes with meaningful labels for the levels (e.g. ‘Question’ and ‘Statement to verify’).\nAdd the variables in the correct ‘Repeated Measures Cells’ (match up the names of your columns with the labels in each cell).\nEnter the between-participants IV into ‘Between-Subjects Factor’ box.\nCheck the box next to ‘partial η2’ under Effect Size and change the Dependent Variable Label to something meaningful (e.g. ‘Rate of semantic illusions’).\n\n\n\nGo to ‘Estimated Marginal Means’. Enter the first IV in the Term 1 box. Click ‘Add New Term’. Enter the second IV into the Term 2 box. Click ‘Add New Term’. Enter the IV that you want to be displayed on the horizontal axis first in the Term 3 box (if one of your IVs is about time, e.g. ‘before’ and ‘after’ conditions, put this on the horizontal axis, e.g. in this example, it makes sense to have presentation order (qu1st1) on the horizontal axis). Enter the IV you want to be displayed as separate lines second in the box Term 3 box. Change the error bars to ‘Standard Error’. Check the box next to ‘Marginal means plots’ and ‘Marginal means tables’.\nGo to ‘Post Hoc Tests’. Put the interaction into the box on the right. Select ‘Bonferroni’ from the Correction list.\n\n\n\n\n\n\n\n\n\n\n\nFrom the next part of the output, only the most relevant tables are reproduced here: the estimated marginal means for presentation order and for sentence type.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteNote - including charts\n\n\n\nWe do not usually include an interaction plot, unless the interaction is significant, and then we tend to use bar charts instead of line charts as in the Jamovi output.\n\n\n\n\n\n\n\n\n\n\n\nTip2x2 mixed ANOVA Write Up\n\n\n\n\nFill in the blanks\n\n\n(Any data preparation, e.g. how the response times were calculated or how the data were trimmed or if and how many outliers were removed, should be presented in the section of the method about data preparation and data analysis.)\nIn a report, an ANOVA results section could look like this:\nResults (Report values to 3 d.p.)\nThe rate of semantic illusions for each condition was analysed using a mixed ANOVA with one within-participants factor of sentence type (question vs. statement) and one between-participants factor of presentation order (questions first vs. statements first).\nThere was a significant effect of sentence type on semantic illusion rate, F() = 53.013, p &lt; .001, η2p = , but not for the presentation order, F (1, 132) = , p = .346, or for the interaction between sentence type and presentation order, F(1, 132) = 0.835, p = .*\nAs predicted, the results showed that more semantic illusions occurred when the sentence was a question than when the sentence was a statement. The presentation order did not have any influence on semantic illusion rate. See Table 1 for means and standard deviations.\n*Here partial eta squared is only reported for the significant effects in the analysis\nTable 1. Mean semantic illusion rates (%) for questions compared to statements in both presentation orders (standard deviation in parentheses).\n\n\n\n\n\n\n\n\nPresentation order\nSentence Type\n\n\n\n\n\n\nQuestions\nStatements\n\n\nQuestions first\n43.9 ()\n29.9 (17.1)\n\n\nStatements first\n (19.0)\n29.1 ()\n\n\n\n(Report values to 1 d.p.)",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 4 - Factorial ANOVA with Jamovi"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "The sampling distribution is central to how we can move from t-values to p-values - yet it is one of the trickier parts of this course. The lectures and pre-lecture materials frequently deal with computer simulations to show the properties of the tests we’re using, so this week you will write your own simulation to help understand the sampling distribution!\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nSampling Distributions\n\n\n\nt-values\n\n\n\np-values\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nComputing a one sample t-test in R\n\n\n\nSimulating data in R\n\n\n\nWriting loops in R\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nCreating shareable code to demonstrate a statistical concept\n\n\n\n\n\nTLDR: There is no data to download! we’re going to make the data ourselves.\nThe dataset this week is very simple - we’re going to create it ourselves using R. At this point, we know a lot about the types of questions that we can ask with t-tests and we know a lot about the data processing around it. We can check our understanding of different aspects of these statistical tests by simulating data samples that have known properties (eg mean and standard deviation) and checking if the outcome of the t-test is what we would expect.\nThis way we can numerically validate the tests we’re running and take a look under the hood and see what sort of processes Jamovi and R are using to compute the tests for us.\n\n\n\n\n\n\nNoteContent Info\n\n\n\nThere is a lot of code in this session - don’t be afraid to ask for help from your tutors!\n\n\n\n\n\nToday, we’ll first introduce some concepts and functions in R to simulate a data sample and compute our own one sample t-test. With that in hand, we will set up code to run thousands of tests in one go to help us to better understand sampling distributions.\n\n\n\nThe first thing we’ll need is to compute a t value for a one sample t-test given a dataset. We can work from this definition you’ll remember from the lectures.\n\\[\nt = \\frac{\\text{The sample mean} - \\text{Comparison Value}}{\\text{The standard error of the mean}}\n\\]\nand\n\\[\n\\text{The standard error of the mean} = \\frac{\\text{The sample standard deviation}}{\\sqrt{n}}\n\\]\nLet’s think about what we’ll need to do to compute our t-test. Reading through the definitions, we’ll need some basic arithmatic (+, - and /) need to use the following R functions that we have worked with in previous weeks.\n\n\n\n\n\n\n\nFunction\nDefinition\n\n\n\n\nmean()\nCompute the average of the numbers in a data variable\n\n\nsd()\nCompute the standard deviation of the numbers in a data variable\n\n\nsqrt()\nCompute the square root of a number\n\n\n\nWith these components, we can compute the t-value for\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nMatch the components of the one sample t-test to the R code that computes them. We can assume that our data is in a variable named sample and the number of data points is in a variable named n. The comparison value that we want to compare our mean to is in a variable named comparison_value.\n\n\n\n\n\n\n\nAssumption\nDefinition\n\n\n\n\nsample_mean =\nsample_standard_deviation / sqrt(n)mean(sample)(sample_mean - comparison_value) / sample_standard_errorsd(sample)\n\n\nsample_standard_deviation =\nsample_standard_deviation / sqrt(n)mean(sample)(sample_mean - comparison_value) / sample_standard_errorsd(sample)\n\n\nsample_standard_error =\nsample_standard_deviation / sqrt(n)mean(sample)(sample_mean - comparison_value) / sample_standard_errorsd(sample)\n\n\nt_value =\nsample_standard_deviation / sqrt(n)mean(sample)(sample_mean - comparison_value) / sample_standard_errorsd(sample)\n\n\n\n\n\nCarefully compare the options to the equations for a t-test and R function definitions above.\n\n\n\n\n\nNow we have our ingredients, let’s run a t-test!\n\n\n\n\n\n\nNoteKey step\n\n\n\nOpen Jamovi, install the Rj add-on and open a new Rj window before we go any further\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nTake this code template and copy it across to an Rj window. Can you complete the code to compute a one-sample t-test to quantify how different the mean of sample is from the comparison_value of zero.\nCopy this code into your Rj window (you can overwrite the previous code if you like), and fill in the blanks indicated by __YOUR_CODE_HERE__ (you should delete __YOUR_CODE_HERE__ and replace it with your answer).\n\n# Number of participants\nn = 25\n\n# Generate a random sample\nsample = rnorm(n, mean=0, sd=1)\n\n# Calculate the sample mean and standard deviation\nsample_mean = __YOUR_CODE_HERE__\nsample_sd = __YOUR_CODE_HERE__\n\n# Calculate the sample error of the mean \nsample_standard_error = __YOUR_CODE_HERE__\n\n# Define comparison value\ncomparison_value = 0\n\n# Compute t-value\nt_value = __YOUR_CODE_HERE__\n\nprint(t_value)\n\n\nAll the answers are in previous sections, pay particular attention to the code examples in the previous ‘Check your understanding’ exercise.\nChat with one of your tutors if you get stuck.\n\n\nThe final code should look like this\n\n# Number of participants\nn = 25\n\n# Generate a random sample\nsample = rnorm(n, mean=0, sd=1)\n\n# Calculate the sample mean and standard deviation\nsample_mean = mean(sample)\nsample_sd = sd(sample)\n\n# Calculate the sample error of the mean \nsample_standard_error = sample_sd / sqrt(n)\n\n# Define comparison value\ncomparison_value = 0\n\n# Compute t-value\nt_value = (sample_mean - comparison_value) / sample_standard_error\n\nprint(t_value)\nTry running the code to make sure that it prints out a t-value. If you get some red text, it means something has gone wrong. Check your answers and chat with one of the tutors.\n\n\n\n\n\nSo, now we have some working code to compute our own t-values. This is going to be very useful in this session.\nWe have checked that the code runs, but it would be good to make sure that we’re all actually getting the same answers from the code. This is difficult at the moment as our random number generator will be producing different numbers every time (as it should!). This is normally a good thing but in a teaching setting it can make things difficult.\nWe can solve this issue by setting a “Random Seed” in our code. A random seed is a starting point for generating a sequence of random numbers. Think of it as the initial value that sets the stage for randomness. When you set a random seed, you ensure that the sequence of random numbers generated can be generated the same way each time. This is really critical for Reproducibility and Consistency when checking code..\nIn R, you can set a random seed using the set.seed() function with a number that defines the ‘initial conditions’ for the random numbers that we will generate. For example, we could use\nset.seed(123)\nTo tell R to generate random numbers using a specific set of initial conditions. We’ll get the same sequence of numbers each time we generate a data sample. This means that if we all add the same random seed to the code, we should get the same t-value every time.\nLet’s try adding the random seed to the code.\n\nWe should all get the value -0.176 as the answer. Now we can be certain that we’re all getting the right answers.\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestions\n\n\nAdapt your code to compute the t-value for the following conditions. Use a random seed of 123 throughout.\n\nA data sample of 100 participants with mean of zero and standard deviation of one with a comparison value of zero: t = \nA data sample of 100 participants with mean of three and standard deviation of one with a comparison value of zero: t = \nA data sample of 10 participants with mean of three and standard deviation of one with a comparison value of two: t = \nA data sample of 10 participants with mean of three and standard deviation of three with a comparison value of two: t = \n\n\n\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nAfter completing the exercise, reset your code to the following conditions\n\nn = 20\nrnorm(n, mean=0, sd=1)\ncomparison_value = 0\n\n\n\n\n\n\nNote that we’re simulating data with a mean of zero and using a comparison_value of zero as well. This is our Null Model - the t-values we get from this code are ones that might appear if there is no true effect in the data.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 0, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, main = \"Sampling Distribution of t-values\", xlim=c(-4, 4),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n    \n\nAfter this exercise, you can now compute the null model for a one sample t-test yourself. With this code you can run an analysis to explore how likely it is to observe different t-values when we assume the null hypothesis is true.\nThink about the following questions and tweak your code to find the answers.\n\n\n\n\n\n\nCaution Data Skills - how does the sampling distribution change when we have large samples? Use n = 250 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 25:\n    # Number of participants\n    n = 250\n\nRunning this a few times, you should see that the sampling distribution gets narrower compared to what we saw for n = 20. We commonly see values between -2 and 2 but almost never see values of -3 or +3.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - how does the sampling distribution change when we have very small samples? Use n = 4 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 5:\n    # Number of participants\n    n = 4\n\nRunning this a few times, you should see that the sampling distribution gets broader than for smaller samples. We see more values close to 4 and -4 than we did with a dataset of 20 or 250.\n\n\n\nIn all of these analyses we are absolutely assuming that the null hypothesis of no difference is true. With this assumptions, our sampling distribution shows that we get more extreme t-values (around +/- 4) by chance when working with smaller data samples. By contrast for larger data samples these same values occur extremely rarely.\nWhen data are perfectly normally distributed (parametric assumptions are met), Jamovi and R can compute these null distributions for us using a shortcut without simulating lots of data samples but the result is the same.\n\n\n\nWe introduced the idea of a null model in the lectures. This is a model of what the outcome of our statistics would look like if the data have a random and unbiased structure. The null model is closely linked to the null hypothesis - it describes the probability to observing a particular outcome from our statistics under the assumption that the null hypothesis is true.\nThe sampling distributions we have computed above are exactly null models. We have\n\nDefined population parameters in which we know for sure that the mean is zero\nSampled data observations from that population\nComputed the one sample t-test between the data sample and zero\nStored the t-value for every single resampling\nVisualised the distribution of t-values with a histrogram\n\nThe critical point here is that the null distribution is very stable. We can make confident predictions about what the null distribution of t-values will look like when there is no effect. Let’s try some examples:\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nHow does the distribution of t-values change when we make these changes? Importantly, we know that there is no effect (no difference between the population mean and the comparison value) in all of these examples.\nRemember that we are using random sampling so run each example a few times to get a sense of the result.\nHow does the t-value distribution change when:\n\n…we increase the standard deviation of the population parameters? The mean increasesthe variance increasesit stays pretty much the samethe variance decreasesthe mean decreases\n…we decrease the standard deviation of the population parameters? The mean increasesthe variance increasesit stays pretty much the samethe variance decreasesthe mean decreases\n…we change the mean and comparison value (keeping both values the same)? The mean increasesthe variance increasesit stays pretty much the samethe variance decreasesthe mean decreases\n\n\n\nRead through your code carefully to find the component that you should change.\nTalk with your tutor if you’re stuck.\n(We’ll look at changing the means more in a later section…)\n\n\n\n\n\nSo, the null model for our t-tests is the sampling distribution of t-values when we know there is no effect. Critically this null module is super stable! We can make a lot of changes to the population parameters and precise values being compared and the distribution remains largely the same. We saw in the previous section that sample size can change the tails of the distribution a bit, but most of the rest doesn’t make a big difference.\nThis is a critical point that enables us to complete our statistical assessment. Now that we can be really confident about the probability of observing a particular t-value under the assumption that there is no effect - we’re ready to compute a p-value for our hypothesis test.\n\n\n\nRemember the definition of the p-value from the lectures, this is a bit fiddely but will hopefully make some more sense now that you’re more familiar with the null model. Take a look at the lecture materials from week 5 if you need a refresher.\n\nThe p-value tells us the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct.\n\nWe now have all the ingredients to compute this value. We need\n\nA null model distribution telling us how likely we are to observe particular t-values when we assume the null hypothesis is correct\nThe t-value from a particular data sample\nA statistical significance threshold\n\nTo conduct a hypothesis test, we need to compare an individual t-value from a test to the null distribution of t-value that we have just computed. Remember that the null distrbution is very stable - if we know the sample size then we can use the same null distribution for any t-test!\nLet’s put this into practice.\n\n\n\n\n\n\nNoteKey step\n\n\n\nBefore going any further, reset your code to the following conditions\n\nn = 20\nrnorm(n, mean=0, sd=1)\ncomparison_value = 0\n\n\n\nWe can visualise where one single t-value is on a histogram by using a convenient R function named abline() which can be used to plot straight lines on a figure. We can use abline() as follows:\nabline(v = 1, col = \"red\", lwd = 2)\nThis has specified three input arguments.\n\n\n\n\n\n\n\nInput\nDescription\n\n\n\n\nv\nA value at which to plot a Vertical line\n\n\ncol\nOptional input to choose a colour\n\n\nlwd\nOptional input to specify the width of the line lwd=1 is a thin line and lwd=5 would be very thick.\n\n\n\nWe can add this to the bottom of our code to see where a t-value of 1 (defined by v=1 in the function) would appear on the null distribution. Try changing the value of v a few times to see the difference.\n\n\n\n\n\n\n\nWarningJamovi Issue\n\n\n\nIt is likely that Jamovi will present two plots when you run this code rather than one. If this happens you will have one plot for the histogram and a second for the same histogram with the vertical line (example in the image above).\nI’m not sure why this happens but it is a mostly harmless quirk of Rj - please ignore the first plot!\n\n\nThe p-value describes the probabilty of observing a t-value as least as large as the one from our test. This is equivalent to ‘cutting’ the null distribution at the specific t-value and calculating how likely it is to see a result that is more extreme than the point of the cut.\nFor example, from the lecture notes, here we have a parametric null distribution of t-values (parametric meaning that it wasn’t computed from simulations) that is cut at a position that means we have a 5% chance of observing a value at least as extreme as the cut.\n\nWe can do the same to compute the probability of observing a value to the left or to the right of the vertical line we’ve added to our plot. We can use some simple maths to compute exactly this value\n# Compute the percentile of the value 1\nobserved_value = 1\npercentile = sum(t_values &gt;= observed_value) / length(t_values)\nprint(percentile)\nThis code does the following steps:\n\nsum(t_values &gt;= observed_t_value) counts how many values in the data are larger than or equal to the specified value.\nlength(t_values) gives the total number of values in the data.\nDividing the count by the total number of values converts the result into a decimal indicating the proportion of values in t_values that are larger than or equal to observed_t_value.\n\nThe final value of percentile is then our p-value, which exactly matches the definition from the lecture.\n\nThe p-value tells us the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct.\n\nLet’s put all of this together in our code.\nset.seed(123)\n\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 2, sd = 4)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 2\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\nobserved_t_value = 2.3\npercentile &lt;- sum(t_values &gt;= observed_t_value) / length(t_values)\n\nprint('My t-value')\nprint(observed_t_value)\nprint('My p-value')\nprint(percentile)\n\n\n# Plot the sampling distribution of t-values\nhist(t_values, breaks=24, main = \"Sampling Distribution of t-values\", xlim=c(-10, 10),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n     \nabline(v = observed_t_value, col = \"red\", lwd=5)\nHere, we’ve added some code to specify a t-value from some data that we need to compute a p-value for (observed_t_value = 2.3), we compute and print the associated p-value and create the histogram with a red line indicating the position of observed_t_value.\nThe result should look like this (ignoring this odd double plot…)\n\nIf you have set.seed(123) at the top of your script you should have exactly the same values that I do.\nThe results are telling us that we have a p-value of 0.019. This is less than our conventional threshold of 0.05 so we would consider a t-value of 2.3 to be statistically significant.\nOur code is only checking the right sided tail of the null distribution. This is ok as we can assume that the null distribution of t-values is symmetrical. This is a key point that means we can simplify some things.\n\nThis is a One-Tailed test, as we are only considering one side of the distribution in our p-value calculation. A two-tailed test is simple though, simply divide the p-value by 2 as the lower tail should have an identical shape. For example if our code says that a result has a one-tailed p-value of 0.08 then the two-tailed p-value is 0.04.\nOur code is only checking the high end of the distribution (values to the right of the red line). p-value for a negative t-value is simple to compute though. It is just one with the computed p-value subtracted from it. eg if we have a t-value of -1, our code will say that 0.837 (or 83.7%) of the null values are larger - we can simply subtract it from one to get the correct p-value 1 - 0.837 = 0.163.\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHintSolution\n\n\nWhat is the p-value for the following values of observed_t_value? Make sure you have set.seed(123) set at the top of your script or the values will differ from mine!\n\n\n\n\n\n\n\n\nobserved_t_value\nTail of test\np-value\n\n\n\n\n1\nOne-Tailed\n\n\n\n3\nOne-Tailed\n\n\n\n2.5\nTwo-Tailed\n\n\n\n-1.4\nOne-Tailed\n\n\n\n-0.5\nTwo-Tailed\n\n\n\n\n\n\nCarefully read the key points from the bullet points above about converting values. Talk with your tutor if you get stuck.\n\n\n\n\n\n\n\n\n\n\n\nobserved_t_value\nTail of test\np-value\nExplanation\n\n\n\n\n1\nOne-Tailed\n0.151\nTake the result directly from the script\n\n\n3\nOne-Tailed\n0.001\nTake the result directly from the script\n\n\n2.5\nTwo-Tailed\n0.01 / 2 = 0.005\nDivide the result from the script by 2.\n\n\n-1.4\nOne-Tailed\n1 - 0.915 = 0.085\nSubtract the result from 1\n\n\n-0.5\nTwo-Tailed\n(1 - 0.685) / 2 = 0.1575\nSubtract the result from 1 AND then divide by 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nTake a moment to feel good about yourself if you have made it this far. These are not straightforward concepts and take time to get used to. Seriously well done!\nIf you’re here but still finding it tough, don’t worry - speak with one of your workshop tutors and they can help to explain things to you.\n\n\n\n\n\nFinally, let’s consider why we test against the null distribution of no effect. This is a weird concept but is fairly straightforward to see why we have to do it based onfrom the code we have written so far.\nWe can absolutely compute a sampling distribution for a case where there is a real difference between the mean and the comparison value. For example, this code simulates data with a mean of 1 and compares that to a comparison value of 0. Note that the rnorm() function inputs have changed.\nset.seed(123)\n\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 1, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\nobserved_t_value = 2.3\npercentile &lt;- sum(t_values &gt;= observed_t_value) / length(t_values)\n\nprint('My t-value')\nprint(observed_t_value)\nprint('My p-value')\nprint(percentile)\n\n# Plot the sampling distribution of t-values\nhist(t_values, breaks=24, main = \"Sampling Distribution of t-values\", xlim=c(-10, 10),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n     \nabline(v = observed_t_value, col = \"red\", lwd=5)\nThe result is a sampling distribution which is not centred around mean.\n\nThis distribution tells us that with 20 participants, a mean of 1 and a standard deviation of 1 - we are mostly likely to get a t-value of 5 but that t-value could be as small as 2 or as large as 9 for extreme data samples.\nThis is very useful, but the issue that is we have had to assume what the real effect is. We get radically different sampling distributions if we change that effect around. Take a look at these examples.\n\n\n\n\n\n\n\n\n\n\n\nThe distributions move around so much that the percentile value of the same t-value would be completely incomparable. A t-value of 5 is hugely unlikey in some cases and very frequent in others. If we have to assume that there is a specific effect the result would be extremely senstive to exactly what that assumption is. In contrast there is only one distribution to work with if we assume there is no effect.\nThere are an infinite number of possible sampling distributions when there is a real effect! It isn’t possible to know which distribution we should be comparing with without making some massive assumptions about the exact size, variability and direction of effect that we expect to see. This simply isn’t possible in practice.\nIn contrast, the null case is very closely controlled and easier to work with. The interpretation is trickier but rejecting the null hypothesis is a tractable problem that we can do a good job of solving.\n\n\n\n\nIt is worth repeating - well done for completing this session. It can be hard work developing the code to explore sampling distributions, t-values and p-values but it is a fantastic way to gain a more intuitive understanding of how and why we do things the way we do.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#the-dataset",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#the-dataset",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "TLDR: There is no data to download! we’re going to make the data ourselves.\nThe dataset this week is very simple - we’re going to create it ourselves using R. At this point, we know a lot about the types of questions that we can ask with t-tests and we know a lot about the data processing around it. We can check our understanding of different aspects of these statistical tests by simulating data samples that have known properties (eg mean and standard deviation) and checking if the outcome of the t-test is what we would expect.\nThis way we can numerically validate the tests we’re running and take a look under the hood and see what sort of processes Jamovi and R are using to compute the tests for us.\n\n\n\n\n\n\nNoteContent Info\n\n\n\nThere is a lot of code in this session - don’t be afraid to ask for help from your tutors!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#the-challenge",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#the-challenge",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "Today, we’ll first introduce some concepts and functions in R to simulate a data sample and compute our own one sample t-test. With that in hand, we will set up code to run thousands of tests in one go to help us to better understand sampling distributions.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#code-for-a-one-sample-t-test",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#code-for-a-one-sample-t-test",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "The first thing we’ll need is to compute a t value for a one sample t-test given a dataset. We can work from this definition you’ll remember from the lectures.\n\\[\nt = \\frac{\\text{The sample mean} - \\text{Comparison Value}}{\\text{The standard error of the mean}}\n\\]\nand\n\\[\n\\text{The standard error of the mean} = \\frac{\\text{The sample standard deviation}}{\\sqrt{n}}\n\\]\nLet’s think about what we’ll need to do to compute our t-test. Reading through the definitions, we’ll need some basic arithmatic (+, - and /) need to use the following R functions that we have worked with in previous weeks.\n\n\n\n\n\n\n\nFunction\nDefinition\n\n\n\n\nmean()\nCompute the average of the numbers in a data variable\n\n\nsd()\nCompute the standard deviation of the numbers in a data variable\n\n\nsqrt()\nCompute the square root of a number\n\n\n\nWith these components, we can compute the t-value for\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nMatch the components of the one sample t-test to the R code that computes them. We can assume that our data is in a variable named sample and the number of data points is in a variable named n. The comparison value that we want to compare our mean to is in a variable named comparison_value.\n\n\n\n\n\n\n\nAssumption\nDefinition\n\n\n\n\nsample_mean =\nsample_standard_deviation / sqrt(n)mean(sample)(sample_mean - comparison_value) / sample_standard_errorsd(sample)\n\n\nsample_standard_deviation =\nsample_standard_deviation / sqrt(n)mean(sample)(sample_mean - comparison_value) / sample_standard_errorsd(sample)\n\n\nsample_standard_error =\nsample_standard_deviation / sqrt(n)mean(sample)(sample_mean - comparison_value) / sample_standard_errorsd(sample)\n\n\nt_value =\nsample_standard_deviation / sqrt(n)mean(sample)(sample_mean - comparison_value) / sample_standard_errorsd(sample)\n\n\n\n\n\nCarefully compare the options to the equations for a t-test and R function definitions above.\n\n\n\n\n\nNow we have our ingredients, let’s run a t-test!\n\n\n\n\n\n\nNoteKey step\n\n\n\nOpen Jamovi, install the Rj add-on and open a new Rj window before we go any further\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nTake this code template and copy it across to an Rj window. Can you complete the code to compute a one-sample t-test to quantify how different the mean of sample is from the comparison_value of zero.\nCopy this code into your Rj window (you can overwrite the previous code if you like), and fill in the blanks indicated by __YOUR_CODE_HERE__ (you should delete __YOUR_CODE_HERE__ and replace it with your answer).\n\n# Number of participants\nn = 25\n\n# Generate a random sample\nsample = rnorm(n, mean=0, sd=1)\n\n# Calculate the sample mean and standard deviation\nsample_mean = __YOUR_CODE_HERE__\nsample_sd = __YOUR_CODE_HERE__\n\n# Calculate the sample error of the mean \nsample_standard_error = __YOUR_CODE_HERE__\n\n# Define comparison value\ncomparison_value = 0\n\n# Compute t-value\nt_value = __YOUR_CODE_HERE__\n\nprint(t_value)\n\n\nAll the answers are in previous sections, pay particular attention to the code examples in the previous ‘Check your understanding’ exercise.\nChat with one of your tutors if you get stuck.\n\n\nThe final code should look like this\n\n# Number of participants\nn = 25\n\n# Generate a random sample\nsample = rnorm(n, mean=0, sd=1)\n\n# Calculate the sample mean and standard deviation\nsample_mean = mean(sample)\nsample_sd = sd(sample)\n\n# Calculate the sample error of the mean \nsample_standard_error = sample_sd / sqrt(n)\n\n# Define comparison value\ncomparison_value = 0\n\n# Compute t-value\nt_value = (sample_mean - comparison_value) / sample_standard_error\n\nprint(t_value)\nTry running the code to make sure that it prints out a t-value. If you get some red text, it means something has gone wrong. Check your answers and chat with one of the tutors.\n\n\n\n\n\nSo, now we have some working code to compute our own t-values. This is going to be very useful in this session.\nWe have checked that the code runs, but it would be good to make sure that we’re all actually getting the same answers from the code. This is difficult at the moment as our random number generator will be producing different numbers every time (as it should!). This is normally a good thing but in a teaching setting it can make things difficult.\nWe can solve this issue by setting a “Random Seed” in our code. A random seed is a starting point for generating a sequence of random numbers. Think of it as the initial value that sets the stage for randomness. When you set a random seed, you ensure that the sequence of random numbers generated can be generated the same way each time. This is really critical for Reproducibility and Consistency when checking code..\nIn R, you can set a random seed using the set.seed() function with a number that defines the ‘initial conditions’ for the random numbers that we will generate. For example, we could use\nset.seed(123)\nTo tell R to generate random numbers using a specific set of initial conditions. We’ll get the same sequence of numbers each time we generate a data sample. This means that if we all add the same random seed to the code, we should get the same t-value every time.\nLet’s try adding the random seed to the code.\n\nWe should all get the value -0.176 as the answer. Now we can be certain that we’re all getting the right answers.\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestions\n\n\nAdapt your code to compute the t-value for the following conditions. Use a random seed of 123 throughout.\n\nA data sample of 100 participants with mean of zero and standard deviation of one with a comparison value of zero: t = \nA data sample of 100 participants with mean of three and standard deviation of one with a comparison value of zero: t = \nA data sample of 10 participants with mean of three and standard deviation of one with a comparison value of two: t = \nA data sample of 10 participants with mean of three and standard deviation of three with a comparison value of two: t = \n\n\n\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nAfter completing the exercise, reset your code to the following conditions\n\nn = 20\nrnorm(n, mean=0, sd=1)\ncomparison_value = 0",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#sampling-distributions-for-one-sample-t-tests",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#sampling-distributions-for-one-sample-t-tests",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "Note that we’re simulating data with a mean of zero and using a comparison_value of zero as well. This is our Null Model - the t-values we get from this code are ones that might appear if there is no true effect in the data.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 0, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, main = \"Sampling Distribution of t-values\", xlim=c(-4, 4),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n    \n\nAfter this exercise, you can now compute the null model for a one sample t-test yourself. With this code you can run an analysis to explore how likely it is to observe different t-values when we assume the null hypothesis is true.\nThink about the following questions and tweak your code to find the answers.\n\n\n\n\n\n\nCaution Data Skills - how does the sampling distribution change when we have large samples? Use n = 250 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 25:\n    # Number of participants\n    n = 250\n\nRunning this a few times, you should see that the sampling distribution gets narrower compared to what we saw for n = 20. We commonly see values between -2 and 2 but almost never see values of -3 or +3.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - how does the sampling distribution change when we have very small samples? Use n = 4 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 5:\n    # Number of participants\n    n = 4\n\nRunning this a few times, you should see that the sampling distribution gets broader than for smaller samples. We see more values close to 4 and -4 than we did with a dataset of 20 or 250.\n\n\n\nIn all of these analyses we are absolutely assuming that the null hypothesis of no difference is true. With this assumptions, our sampling distribution shows that we get more extreme t-values (around +/- 4) by chance when working with smaller data samples. By contrast for larger data samples these same values occur extremely rarely.\nWhen data are perfectly normally distributed (parametric assumptions are met), Jamovi and R can compute these null distributions for us using a shortcut without simulating lots of data samples but the result is the same.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#the-null-model",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#the-null-model",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "We introduced the idea of a null model in the lectures. This is a model of what the outcome of our statistics would look like if the data have a random and unbiased structure. The null model is closely linked to the null hypothesis - it describes the probability to observing a particular outcome from our statistics under the assumption that the null hypothesis is true.\nThe sampling distributions we have computed above are exactly null models. We have\n\nDefined population parameters in which we know for sure that the mean is zero\nSampled data observations from that population\nComputed the one sample t-test between the data sample and zero\nStored the t-value for every single resampling\nVisualised the distribution of t-values with a histrogram\n\nThe critical point here is that the null distribution is very stable. We can make confident predictions about what the null distribution of t-values will look like when there is no effect. Let’s try some examples:\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nHow does the distribution of t-values change when we make these changes? Importantly, we know that there is no effect (no difference between the population mean and the comparison value) in all of these examples.\nRemember that we are using random sampling so run each example a few times to get a sense of the result.\nHow does the t-value distribution change when:\n\n…we increase the standard deviation of the population parameters? The mean increasesthe variance increasesit stays pretty much the samethe variance decreasesthe mean decreases\n…we decrease the standard deviation of the population parameters? The mean increasesthe variance increasesit stays pretty much the samethe variance decreasesthe mean decreases\n…we change the mean and comparison value (keeping both values the same)? The mean increasesthe variance increasesit stays pretty much the samethe variance decreasesthe mean decreases\n\n\n\nRead through your code carefully to find the component that you should change.\nTalk with your tutor if you’re stuck.\n(We’ll look at changing the means more in a later section…)\n\n\n\n\n\nSo, the null model for our t-tests is the sampling distribution of t-values when we know there is no effect. Critically this null module is super stable! We can make a lot of changes to the population parameters and precise values being compared and the distribution remains largely the same. We saw in the previous section that sample size can change the tails of the distribution a bit, but most of the rest doesn’t make a big difference.\nThis is a critical point that enables us to complete our statistical assessment. Now that we can be really confident about the probability of observing a particular t-value under the assumption that there is no effect - we’re ready to compute a p-value for our hypothesis test.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#computing-a-p-value",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#computing-a-p-value",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "Remember the definition of the p-value from the lectures, this is a bit fiddely but will hopefully make some more sense now that you’re more familiar with the null model. Take a look at the lecture materials from week 5 if you need a refresher.\n\nThe p-value tells us the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct.\n\nWe now have all the ingredients to compute this value. We need\n\nA null model distribution telling us how likely we are to observe particular t-values when we assume the null hypothesis is correct\nThe t-value from a particular data sample\nA statistical significance threshold\n\nTo conduct a hypothesis test, we need to compare an individual t-value from a test to the null distribution of t-value that we have just computed. Remember that the null distrbution is very stable - if we know the sample size then we can use the same null distribution for any t-test!\nLet’s put this into practice.\n\n\n\n\n\n\nNoteKey step\n\n\n\nBefore going any further, reset your code to the following conditions\n\nn = 20\nrnorm(n, mean=0, sd=1)\ncomparison_value = 0\n\n\n\nWe can visualise where one single t-value is on a histogram by using a convenient R function named abline() which can be used to plot straight lines on a figure. We can use abline() as follows:\nabline(v = 1, col = \"red\", lwd = 2)\nThis has specified three input arguments.\n\n\n\n\n\n\n\nInput\nDescription\n\n\n\n\nv\nA value at which to plot a Vertical line\n\n\ncol\nOptional input to choose a colour\n\n\nlwd\nOptional input to specify the width of the line lwd=1 is a thin line and lwd=5 would be very thick.\n\n\n\nWe can add this to the bottom of our code to see where a t-value of 1 (defined by v=1 in the function) would appear on the null distribution. Try changing the value of v a few times to see the difference.\n\n\n\n\n\n\n\nWarningJamovi Issue\n\n\n\nIt is likely that Jamovi will present two plots when you run this code rather than one. If this happens you will have one plot for the histogram and a second for the same histogram with the vertical line (example in the image above).\nI’m not sure why this happens but it is a mostly harmless quirk of Rj - please ignore the first plot!\n\n\nThe p-value describes the probabilty of observing a t-value as least as large as the one from our test. This is equivalent to ‘cutting’ the null distribution at the specific t-value and calculating how likely it is to see a result that is more extreme than the point of the cut.\nFor example, from the lecture notes, here we have a parametric null distribution of t-values (parametric meaning that it wasn’t computed from simulations) that is cut at a position that means we have a 5% chance of observing a value at least as extreme as the cut.\n\nWe can do the same to compute the probability of observing a value to the left or to the right of the vertical line we’ve added to our plot. We can use some simple maths to compute exactly this value\n# Compute the percentile of the value 1\nobserved_value = 1\npercentile = sum(t_values &gt;= observed_value) / length(t_values)\nprint(percentile)\nThis code does the following steps:\n\nsum(t_values &gt;= observed_t_value) counts how many values in the data are larger than or equal to the specified value.\nlength(t_values) gives the total number of values in the data.\nDividing the count by the total number of values converts the result into a decimal indicating the proportion of values in t_values that are larger than or equal to observed_t_value.\n\nThe final value of percentile is then our p-value, which exactly matches the definition from the lecture.\n\nThe p-value tells us the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct.\n\nLet’s put all of this together in our code.\nset.seed(123)\n\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 2, sd = 4)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 2\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\nobserved_t_value = 2.3\npercentile &lt;- sum(t_values &gt;= observed_t_value) / length(t_values)\n\nprint('My t-value')\nprint(observed_t_value)\nprint('My p-value')\nprint(percentile)\n\n\n# Plot the sampling distribution of t-values\nhist(t_values, breaks=24, main = \"Sampling Distribution of t-values\", xlim=c(-10, 10),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n     \nabline(v = observed_t_value, col = \"red\", lwd=5)\nHere, we’ve added some code to specify a t-value from some data that we need to compute a p-value for (observed_t_value = 2.3), we compute and print the associated p-value and create the histogram with a red line indicating the position of observed_t_value.\nThe result should look like this (ignoring this odd double plot…)\n\nIf you have set.seed(123) at the top of your script you should have exactly the same values that I do.\nThe results are telling us that we have a p-value of 0.019. This is less than our conventional threshold of 0.05 so we would consider a t-value of 2.3 to be statistically significant.\nOur code is only checking the right sided tail of the null distribution. This is ok as we can assume that the null distribution of t-values is symmetrical. This is a key point that means we can simplify some things.\n\nThis is a One-Tailed test, as we are only considering one side of the distribution in our p-value calculation. A two-tailed test is simple though, simply divide the p-value by 2 as the lower tail should have an identical shape. For example if our code says that a result has a one-tailed p-value of 0.08 then the two-tailed p-value is 0.04.\nOur code is only checking the high end of the distribution (values to the right of the red line). p-value for a negative t-value is simple to compute though. It is just one with the computed p-value subtracted from it. eg if we have a t-value of -1, our code will say that 0.837 (or 83.7%) of the null values are larger - we can simply subtract it from one to get the correct p-value 1 - 0.837 = 0.163.\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHintSolution\n\n\nWhat is the p-value for the following values of observed_t_value? Make sure you have set.seed(123) set at the top of your script or the values will differ from mine!\n\n\n\n\n\n\n\n\nobserved_t_value\nTail of test\np-value\n\n\n\n\n1\nOne-Tailed\n\n\n\n3\nOne-Tailed\n\n\n\n2.5\nTwo-Tailed\n\n\n\n-1.4\nOne-Tailed\n\n\n\n-0.5\nTwo-Tailed\n\n\n\n\n\n\nCarefully read the key points from the bullet points above about converting values. Talk with your tutor if you get stuck.\n\n\n\n\n\n\n\n\n\n\n\nobserved_t_value\nTail of test\np-value\nExplanation\n\n\n\n\n1\nOne-Tailed\n0.151\nTake the result directly from the script\n\n\n3\nOne-Tailed\n0.001\nTake the result directly from the script\n\n\n2.5\nTwo-Tailed\n0.01 / 2 = 0.005\nDivide the result from the script by 2.\n\n\n-1.4\nOne-Tailed\n1 - 0.915 = 0.085\nSubtract the result from 1\n\n\n-0.5\nTwo-Tailed\n(1 - 0.685) / 2 = 0.1575\nSubtract the result from 1 AND then divide by 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nTake a moment to feel good about yourself if you have made it this far. These are not straightforward concepts and take time to get used to. Seriously well done!\nIf you’re here but still finding it tough, don’t worry - speak with one of your workshop tutors and they can help to explain things to you.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#optional-why-do-we-test-the-null-hypothesis",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#optional-why-do-we-test-the-null-hypothesis",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "Finally, let’s consider why we test against the null distribution of no effect. This is a weird concept but is fairly straightforward to see why we have to do it based onfrom the code we have written so far.\nWe can absolutely compute a sampling distribution for a case where there is a real difference between the mean and the comparison value. For example, this code simulates data with a mean of 1 and compares that to a comparison value of 0. Note that the rnorm() function inputs have changed.\nset.seed(123)\n\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 1, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\nobserved_t_value = 2.3\npercentile &lt;- sum(t_values &gt;= observed_t_value) / length(t_values)\n\nprint('My t-value')\nprint(observed_t_value)\nprint('My p-value')\nprint(percentile)\n\n# Plot the sampling distribution of t-values\nhist(t_values, breaks=24, main = \"Sampling Distribution of t-values\", xlim=c(-10, 10),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n     \nabline(v = observed_t_value, col = \"red\", lwd=5)\nThe result is a sampling distribution which is not centred around mean.\n\nThis distribution tells us that with 20 participants, a mean of 1 and a standard deviation of 1 - we are mostly likely to get a t-value of 5 but that t-value could be as small as 2 or as large as 9 for extreme data samples.\nThis is very useful, but the issue that is we have had to assume what the real effect is. We get radically different sampling distributions if we change that effect around. Take a look at these examples.\n\n\n\n\n\n\n\n\n\n\n\nThe distributions move around so much that the percentile value of the same t-value would be completely incomparable. A t-value of 5 is hugely unlikey in some cases and very frequent in others. If we have to assume that there is a specific effect the result would be extremely senstive to exactly what that assumption is. In contrast there is only one distribution to work with if we assume there is no effect.\nThere are an infinite number of possible sampling distributions when there is a real effect! It isn’t possible to know which distribution we should be comparing with without making some massive assumptions about the exact size, variability and direction of effect that we expect to see. This simply isn’t possible in practice.\nIn contrast, the null case is very closely controlled and easier to work with. The interpretation is trickier but rejecting the null hypothesis is a tractable problem that we can do a good job of solving.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#summary",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#summary",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "It is worth repeating - well done for completing this session. It can be hard work developing the code to explore sampling distributions, t-values and p-values but it is a fantastic way to gain a more intuitive understanding of how and why we do things the way we do.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-1_data-handling.html",
    "href": "tutorials/rmc_week-1_data-handling.html",
    "title": "Week 1 : Data Handling",
    "section": "",
    "text": "This week will serve as a refresher on Jamovi. You will use Jamovi throughout your degree for statistical analysis, so it is very important to refresh your memory on this!\nA guide to installing Jamovi on your personal computer can be found here\n\n\n\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nRecoding scores into groups\n\n\n\nCalculating total scores\n\n\n\nCalculating means\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Jamovi editor\n\n\n\nSetting up data files for between- and within-participants designs\n\n\n\nComputing descriptive statistics in Jamovi\n\n\n\n\n\n\n\n\n\nNoteToday’s session\n\n\n\nToday you will be re-familiarising yourself with the Jamovi interface and its layout / basic functions as well as learning how to set up data for different statistical designs.\n\n\n\n\n\n\nData are entered into Jamovi using a ‘spreadsheet’ format of rows and columns. When in the Data tab you can view the data. The rows are cases (i.e. participants – each participant gets their own row), the columns are variables (i.e. whatever was measured for each participant gets a column). In the Variable tab rows are the details for each variable (e.g. one row = one variable).\nEntering the data in the Data tab is straightforward: you simply type a value in the appropriate box, or ‘cell’.\n\n\n\nThe data files for use in the computer practicals are available on the RMC/NM1 Canvas ‘Course Pack’ module. In the text of these pages, the data files will be referred to by file name. You can open different types of data file directly in Jamovi (e.g. .sav or .csv). You can import files directly into Jamovi. There is more information in the Jamovi Textbook\n\n\n\n\n\nIn a within-participants (or ‘repeated measures’) design the dependent variable (DV) is measured more than once under different conditions or at different times. Therefore, one participant will have two or more scores (e.g. before and after an intervention). This means we need more than one column to record the relevant scores/measurements of the DV.\n\nFor example, in the Data tab: column 1 could be their participant number, column 2 could be their first score and column 3 could be their second score. In the Variables tab you would therefore need three rows to define these variables.\nParticipants could provide more than two scores in a repeated measures design.\n\nIn a between-participants design each participant is assigned to a single condition or group. Scores/measurements from two different people can NEVER go in the same row because one row = one person. This means that to identify which condition or group a participant belongs to, we need to assign a label to that participant, which identifies them by their group membership. This is called a grouping variable.\n\nFor example, in the Data tab: column 1 could be the participant number, column 2 could encode which group they were in (e.g. 1 = Experimental Condition OR 2 = Control Group) and column 3 could be their score. In this example, we would need to set up three variables in the Variable tab and for the second row (the group they were in) you would need to define the grouping variable so that you later know what the numbers refer to (e.g. 1 = experimental group, 2 = control group). You could add this information into the description of a variable in the Variables tab.\nParticipants could be categorised as belonging to more than on group, e.g. they could be classified according to whether they are studying psychology or neuroscience AND whether they feel confident about using Jamovi or not.\n\nIn some studies, participants belonging to different groups provide more than one score each. Such designs are called mixed designs.\n\nFor example, we might wish to study if a particular intervention helps students feel more confident with their use of Jamovi. This study might record Jamovi-confidence before the intervention and after the intervention. But we might also want to compare the size of the impact of the intervention for students who started off with high confidence to those who started off with low confidence.\n\n\n\n\n\nDownload and open RMC_NM1 Wksh1_Activity1.sav from the Canvas module for the Course Pack\nLoad the data into Jamovi\nOpen Jamovi and click the three horizontal lines in the top left\n\n\n\nClick open - browse\n\n\n\nNavigate to where you saved the data and click to open. Note: when you download the file from the Canvas page it will automatically be saved into your ‘Downloads’ folder so look here if you are struggling to find it.\n\n\n\nLook at the raw data (Data tab) and the variable labels (Variables tab) and answer the following questions.\n\n\n\n\n\n\n\nTipActivity 1 Questions\n\n\n\n\nCheck Your Understanding - Activity 1 QuestionsHint\n\n\nWhat is the repeated (within-subjects) variable and how many times is it measured?\nThe repeated (within-subjects) variable is breathing timetype of video and is measured  times.\n\n\nLook at the description in the Variables tab\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 1 Questions\n\n\n\n\nQuestion 2Hint\n\n\nWhat is the grouping (between-subjects) variable and what are the different groups?\nThe grouping (between-subjects) variable is type of videobreathing time and the groups are planttreedogcat and cattreedogplant.\n\n\nLook at the description in the Data tab\n\n\n\n\n\n\n\n\nOnce the data are entered, we need to look at what they can tell us. Jamovi has several tools we can use to better understand the data.\n\nBefore doing any statistical analysis, we can look at the data which gives us an idea of what we are dealing with: e.g. How many participants? How many variables? What is the range of scores? Are there any values that seem out of line with the rest?\nWhen satisfied that the raw data look okay, we continue our inspection of the data by using descriptive statistics, such as frequency counts, and measures of central tendency (such as mean, median and mode) and measures of spread (such as range or standard deviation).\n\n\n\n\n\nDownload and open the RMC_NCM_example_output_Explore.sav in Jamovi\nLook at the raw data (Data tab) and information about the variables (Variables tab) and answer the following questions.\n\n\n\n\n\n\n\nCaution Activity 2 Q1 - How many participants and how many variables were there?\n\n\n\n\n\n177 participants and 2 variables\n\n\n\n\n\n\n\n\n\nCaution Activity 2 Q2 - Are there any missing data or any odd-looking cells?\n\n\n\n\n\nYes\n\n\n\n\n\n\n\n\n\nCaution Activity 2 Q3 - Give an example (identify the example by the Jamovi row number)\n\n\n\n\n\nRows 48 and 175\n\n\n\n\n\n\n\n\n\nCaution Activity 2 Q4 - What did you notice about these rows of data? Describe it in a few words.\n\n\n\n\n\nOne of the rows is blank and the other has 999 in grey inside it\n\n\n\n\n\n\n\n\nFor nominal and ordinal data, the next step in getting to know the data is to determine the frequency counts for each possible score/value/option, i.e. how often each value occurs (e.g. the number of people who answered “yes” to the question “Did you take Psychology at A-Level?”).\nFor example, in the dataset RMC_NM1_example_output_Explore.sav, the variable about whether or not a participant took Psychology ‘A’-Level can be examined using this method.\n\n\n\n\nGo to the Analyses menu and choose Exploration\nFrom the submenu, choose Descriptives\n\n\n\nIn the descriptives box, choose the variable for which you want to calculate frequency counts and enter it into the box on the right-hand side. Click the tick box next to ‘Frequency tables’ to display the frequency table.\n\n\n\nAnswer the following questions based on the Jamovi output\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 3a Questions\n\n\n\n\nQuestion 1Hint\n\n\nHow many valid cases were processed? Was this correct?\n cases were processed.\n\n\nCompare cases to your answer to Activity 2, Q1\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 3a Questions\n\n\n\n\nQuestion 2\n\n\nLook at the number of times each option occurred. Which was the most common outcome? What percentage of the valid data did this account for?\nThe most common outcome was yesno and this accounted for % of the data.\n\n\n\n\n\n\n\n\n\n\nSometimes we want to look at combinations of categories, e.g. how many people studied Psychology at ‘A’-level, how many studied Biology at ‘A’-level, how many studied both, and how many studied neither subject.\nFor example, in the dataset RMC_NM1_example_output_Crosstabs.sav, the variable about whether or not a participant took Psychology ‘A’-Level and the variable about whether or not a participant took Biology ‘A’-level can be examined using this method.\n\n\n\n\nUsing RMC_NM1_example_output_Crosstabs.sav in Jamovi:\n\nGo to the Analyses menu and choose Frequencies.\nFrom the submenu, choose Independent Samples (this is under contingency tables).\n\n\n\n\nIn the Contingency Tables box, put one categorical variable into the Rows box and the other into the Columns box. (If you have more categorical variables that you want to examine, you add a new Layer for each further variable added).\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 3b Questions\n\n\n\n\nQuestion 1\n\n\nHow many students studied both Psychology and Biology at A-level? \n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 3b Questions\n\n\n\n\nQuestion 2\n\n\nWhich was the most common combination of subjects studied?\nThe most common combination of subjects studied was psychology without biology A-levelboth psychology and biology A-levelbiology without psychology A-levelneither psychology or biology A-level\n\n\n\n\n\n\n\n\n\n\nScale data (interval or ratio data) are not always suitable for analysis by frequency counts. Instead, we want to look at measures of central tendency (e.g. means) and measures of spread (e.g. standard deviation). A good way to examine scale data is to ‘explore’ them.\nThe Exploration module in Jamovi gives a range of statistics including charts to help us see whether the data are normally distributed, whether there are any outliers, and also to get an idea of the spread of the data and their typical value.\nWhile we should always carry out these explorative analyses, we tend not to report them all.\n\nFor example, we do not usually include histograms or stem-and-leaf plots in a results write-up. A good rule of thumb in terms of what we need to report is to think of the exploratory data analyses as being a way in which to check whether our data meet the assumptions underlying successful statistical analysis.\nIf they do not meet the assumptions, we need to report how they fail to meet them and state what this means in terms of how we then analyse the data.\n\nThe main assumptions which should be met for all parametric tests are:\n\nThe data are at least interval level of measurement.\nThe data are normally distributed.\nThe variance is homogeneous (similar) between different groups or conditions.\n\nWe always have to describe what the data are like in order to carry out a successful statistical analysis. Hence, we routinely report the means and standard deviations of scale data when we carry out statistical tests. (In RMC/NM1, this will be referred to as ‘reporting the descriptives’).\n\n\n\n\nUsing RMC_NM1_example_output_Explore.sav in Jamovi:\n\nGo to the Analyses menu and choose the Exploration module.\nFrom the submenu, choose Descriptives.\n\n\n\n\nIn the Descriptives box, choose the variable(s) you want to explore and enter it (them) into the Variables box on the right-hand side.\n\nIf you want to compare the score on one variable for two or more groups, enter the variable that represents your independent or grouping variable into the Split by box.\n\n\n\n\nClick on the Plots menu and select ‘histogram’ and ‘box plot’. Make sure that ‘label outliers’ is selected.\n\n\n\nAnswer the questions below based on the results\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 4 Questions\n\n\n\n\nQuestion 1\n\n\nAre the data normally distributed? (Base your judgement on the histograms.)\nThe data areare not normally distributed.\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 4 Questions\n\n\n\n\nQuestion 2\n\n\nAre there any outliers? These will be visible on the boxplots as circles with a number next to them (these numbers indicate which row(s) contains the outlier). Asterisks (*) denote ‘extreme’ values. Write down the row number for any extreme outliers.\nThere are no outliersThere is at least one outlier\nIf there are any outliers, write the row number for the outlier here \n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 4 Questions\n\n\n\n\nQuestion 3\n\n\nRemove the outlier from the data set by deleting the row with the outlier. Rerun the analysis. Are the data normally distributed?\nThe data areare not normally distributed.\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 4 Questions\n\n\n\n\nQuestion 4\n\n\nRerun the analysis on the data, but this time look at the two groups in comparison (see box above). Does it look as though there are differences between the groups?\nYes - the data look quite different in both the boxplots and box plots No - the data look quite similar in both the boxplots and box plots \n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 4 Questions\n\n\n\n\nQuestion 5\n\n\nReport the means and standard deviations for self-reported expertise for the two groups to 1 decimal place.\nPsychology A-level yes: M = , SD = \nPsychology A-level no: M = , SD = \n\n\n\n\n\n\n\n\n\nThe first part of the descriptives table shows how many cases were processed and if there is anything missing. Look at this to see if the correct number of cases has been processed.\n\nThis can be a useful way to spot if a mistake has been made at the data entry stage and a value has been missed out or added. If the ‘N’ in this table does not match the number of cases expected, double-check the data entry.\n\n\n[The output below used ‘research_expertise’ in the Variables box and ‘psych_A_level’ in the Split by box before any extreme outliers were removed.]\n\nThe descriptives table shows number of cases (N), number of missing values (Missing), mean, median, standard deviation, minimum, and maximum. You can change which values you want to display within the statistics drop down menu.\n\n\n\nThe output includes two graphical representations. The data from the example look like this as a histogram (left graph) and as a box and whisker plot (right plot). The boxplot gives us less information about the shape of the distribution but provides clear information about the outliers and which scores they are.\n\n\n\nAfter you remove the outlier, the histograms and box and whisker plots should look like this:\n\n\n\n\n\n\n\nWhen we enter data from questionnaires or have raw data collected from programmes such as Qualtrics or Open Sesame, we often have to prepare the data before analysing it. We might need to:\n\nCalculate total or mean scores for several questions, e.g. for the score on a questionnaire.\nCalculate a difference between two scores from the same participant, e.g. when checking if there has been a change from a baseline score.\nCombine groups of responses into meaningful categories, e.g. when we want to look at participants with high or low scores on a variable of interest.\n\nWe can create these new ‘transformed’ or ‘computed’ variables using Jamovi.\n\n\n\nThis activity demonstrates how to calculate the total and mean scores for a set of variables using the “compute” function in Jamovi.\n\n\n\n\nUsing RMC_NM1_datagen2.sav in Jamovi\n\nClick on the column heading of the empty column on the far right of the screen (next to att5badgrade). Go to the Data tab. Choose the Compute option. This will create a new column.\n\n\n\n\n\nDouble click the column heading to open the variable editor.\nIn the computed variable box, enter a meaningful name for the total score variable (this is the new variable you are calculating).\n\nGiven the nature of the items on this data sheet – look at the labels in Variable View – a name like ‘Attitude_to_Jamovi_total’ would be appropriate to describe the new score.\n\n\n\n\nClick the fx box which shows all variables and functions, click the first variable that contributes to your total score to add it to the equation (by double-clicking on the variable name).\nType the ‘+’ sign in the equation box and move the next contributing variable into the function box. Repeat this process, until you have entered all the variables which contribute to the total score.\nBack on the data sheet, you should now have a new variable with the computed total score.\n\n\n\nIn the Variables tab, describe the new variable appropriately, e.g. enter a descriptive, meaningful label into the description column and check that the level of measurement is correct (scale in this example).\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 5 Questions\n\n\n\n\nQuestion 1\n\n\nWhich participants had the highest score?\nParticipant ; total score = \n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 5 Questions\n\n\n\n\nQuestion 2\n\n\nWhich participants had the lowest total score?\nParticipant ; total score = \n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 5 Questions\n\n\n\n\nQuestion 3\n\n\nUsing the skills from the ‘Understanding the data’ section, decide whether men or women had higher total scores.\nFemalesMales had a higher total score (mean females = ; mean males = )\n\n\n\n\n\n\n\n\n\n\n\n\nUsing RMC_NM1_datagen2.sav in Jamovi, repeat the process with a new target variable name for the mean score variable.\n\nGo to the Compute module in the Data tab again.\nEnter a meaningful name for the mean score variable into the Computed Variable box.\n\nGiven the nature of the items on this data sheet – look at the labels in Variable View – a name like ‘Jamovi_attitude_mean’ would be appropriate to describe the new score.\n\nClick on the fx drop down\nIn the Functions window, select ‘Mean’ (you will have to scroll down to it). Double click to select it.\nInto the Variables window, select the first variable you want in the mean and type a comma next to it. Repeat until all variables are included in the list.\n\n\n\n\nBack on the data sheet, you should now have a second variable with the computed mean score.\n\n\n\nIn the Variable tab define and label the second new variable appropriately.\n\n\n\n\n\n\n\nCaution Activity 6 Q1 - Using the skills from the ‘Understanding the data’ section, which participant(s) have the second highest mean scores?\n\n\n\n\n\nParticipant 30, 49 and 63\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 6 Questions\n\n\n\n\nQuestion 2\n\n\nWhat is the overall mean (grand mean) for the whole sample (rounded to 1 d.p.)?\nThe overall mean (grand mean) for the whole sample is  (rounded to 1 d.p.)\n\n\n\n\n\n\n\n\n\n\n\nUsing RMC_NM1_Bloodsugar06-07_for_baseline_calc.sav in Jamovi, calculate a new variable for the change in bloodsugar level from baseline (before) to after a drink has been consumed (after).\n\nGo to the Variables tab. Choose the Compute option.\n\n\n\nEnter a meaningful name for the difference score variable into the Computed Variable box (this is the new variable you are calculating).\n\nHere ‘change_in_bloodsugar’ is a meaningful label.\n\nIn the fx box double click the ‘after’ variable to move it into the formula box (by double-clicking it). Type the ‘-’ sign on the formula box and move the ‘before’ variable into the formula box. When you calculate the new variable, a positive/greater-than-zero value in the difference variable will mean an increase in the ‘after’ condition compared to ‘before’, and a negative value will mean a decrease.\nBack on the data sheet, you should now have a new variable with the computed change in bloodsugar.\n\n\n\nIn the Variable View, define and label the new variable appropriately.\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 7 Questions\n\n\n\n\nQuestion 1\n\n\nVisually inspect the raw data in the change variable. Pay attention to the ‘drink’ variable group. On average does it look as if there is a difference in how bloodsugar changes for the two drink groups?\nYes it looks like there is a difference in how bloodsugar changes between the two different drinksNo it does not look like there is a difference in how bloodsugar changes between the two different drinks\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 7 Questions\n\n\n\n\nQuestion 2\n\n\nWhat did drink group 1 drink? Did their bloodsugar level increase or decrease?\nGroup 1 drank glucosesaccharin and their blood sugar increaseddecreased\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 7 Questions\n\n\n\n\nQuestion 3\n\n\nWhat did drink group 2 drink? Did their bloodsugar level increase or decrease?\nGroup 2 drank glucosesaccharin and their blood sugar increaseddecreased\n\n\n\n\n\n\n\n\n\nSometimes a continuous variable needs to be recoded to perform a particular analysis, e.g. if you wanted to turn a range of scores from 1 to 9 into three categories, ‘High’, ‘Middle’ and ‘Low’, this is what you need to do on Jamovi.\n\n\nUsing RMC_NM1_datagen3.sav in Jamovi, recode the ‘which_row’ score into a ‘distance_category’ (near, mid, far).\n\nGo to Data &gt; Transform…\n\n\n\nIn the drop down next to ‘source variable’, click ‘which_row’. Type a name (e.g. ‘distance_category’) in the ‘transformed variable’ box and describe the variable (e.g. ‘Distance from front’) in the description box.\nClick on the drop down next to ‘using transform’ and click ‘Create New Transform…’\nA new window will appear called “Transform”. Give the transformation at name (e.g. ‘Recode rows’).\n\nRECODING lowest values into ‘near’:\nClick ‘+ Add recode condition’\nThis will add a new box at the top which will say ‘if $source … use …’\nIn this box type the number you want to use as the upper limit for the ‘near’ condition (e.g. if you want to code 1, 2, and 3 as ‘near’, enter ‘&lt; 4’ into the dialog box as this will use all the numbers below, and not including, 4.)\nAfter the word ‘use’, type the value you want to recode the variable to (e.g. Use ‘near’ for the new variable. You need to include the quote marks around the new label.)\n\n\n\n\nRECODING middle values into ‘mid’:\n\nClick ‘+ Add recode condition’\nThis will add a new box which will say ‘if $source … use …’\nIn this box type the number you want to use as the upper limit for the ‘mid’ condition (e.g. if you want to code 4, 5 and 6 enter ‘&lt; 7’ into the dialog box as this will use all the numbers below, and not including, 7. Please note that the conditions are executed in order and because we have already set values below 4 to recode to 1, this formula will code values between 4 and 6 as 2. This may take some time to get your head around and this website may help https://blog.jamovi.org/2018/10/23/transforming-variables.html)\nAfter the word ‘use’, type the value you want to recode the variable to (e.g. Use ‘mid’ for the new variable. Don’t forget the quote marks around ‘mid’.)\n\n\n\n\nRECODING highest values into ‘far’:\n\nThe bottom formula box should say ‘else use’. In this box type ‘far’ to recode the remaining values (7. 8, and 9) as ‘far’.\n\n\n\n\nBack on the data sheet, you should now have a new variable with the recoded data.\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 8 Questions\n\n\n\n\nQuestion 1\n\n\nUsing the ‘Understanding the Data’ skills, work out how many participants fall into each category\n\nNear: \nMid: \nFar: \n\n\n\n\n\n\n\n\n\n\n\n\nWarningFuture work\n\n\n\nThese calculations of overall scores and recoding techniques will be useful in Research Methods D also",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 1 - Data handling"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-1_data-handling.html#setting-up-data-files-and-entering-data",
    "href": "tutorials/rmc_week-1_data-handling.html#setting-up-data-files-and-entering-data",
    "title": "Week 1 : Data Handling",
    "section": "",
    "text": "Data are entered into Jamovi using a ‘spreadsheet’ format of rows and columns. When in the Data tab you can view the data. The rows are cases (i.e. participants – each participant gets their own row), the columns are variables (i.e. whatever was measured for each participant gets a column). In the Variable tab rows are the details for each variable (e.g. one row = one variable).\nEntering the data in the Data tab is straightforward: you simply type a value in the appropriate box, or ‘cell’.\n\n\n\nThe data files for use in the computer practicals are available on the RMC/NM1 Canvas ‘Course Pack’ module. In the text of these pages, the data files will be referred to by file name. You can open different types of data file directly in Jamovi (e.g. .sav or .csv). You can import files directly into Jamovi. There is more information in the Jamovi Textbook",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 1 - Data handling"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-1_data-handling.html#data-entry-for-within--and-between-participants-or-mixed-designs",
    "href": "tutorials/rmc_week-1_data-handling.html#data-entry-for-within--and-between-participants-or-mixed-designs",
    "title": "Week 1 : Data Handling",
    "section": "",
    "text": "In a within-participants (or ‘repeated measures’) design the dependent variable (DV) is measured more than once under different conditions or at different times. Therefore, one participant will have two or more scores (e.g. before and after an intervention). This means we need more than one column to record the relevant scores/measurements of the DV.\n\nFor example, in the Data tab: column 1 could be their participant number, column 2 could be their first score and column 3 could be their second score. In the Variables tab you would therefore need three rows to define these variables.\nParticipants could provide more than two scores in a repeated measures design.\n\nIn a between-participants design each participant is assigned to a single condition or group. Scores/measurements from two different people can NEVER go in the same row because one row = one person. This means that to identify which condition or group a participant belongs to, we need to assign a label to that participant, which identifies them by their group membership. This is called a grouping variable.\n\nFor example, in the Data tab: column 1 could be the participant number, column 2 could encode which group they were in (e.g. 1 = Experimental Condition OR 2 = Control Group) and column 3 could be their score. In this example, we would need to set up three variables in the Variable tab and for the second row (the group they were in) you would need to define the grouping variable so that you later know what the numbers refer to (e.g. 1 = experimental group, 2 = control group). You could add this information into the description of a variable in the Variables tab.\nParticipants could be categorised as belonging to more than on group, e.g. they could be classified according to whether they are studying psychology or neuroscience AND whether they feel confident about using Jamovi or not.\n\nIn some studies, participants belonging to different groups provide more than one score each. Such designs are called mixed designs.\n\nFor example, we might wish to study if a particular intervention helps students feel more confident with their use of Jamovi. This study might record Jamovi-confidence before the intervention and after the intervention. But we might also want to compare the size of the impact of the intervention for students who started off with high confidence to those who started off with low confidence.\n\n\n\n\n\nDownload and open RMC_NM1 Wksh1_Activity1.sav from the Canvas module for the Course Pack\nLoad the data into Jamovi\nOpen Jamovi and click the three horizontal lines in the top left\n\n\n\nClick open - browse\n\n\n\nNavigate to where you saved the data and click to open. Note: when you download the file from the Canvas page it will automatically be saved into your ‘Downloads’ folder so look here if you are struggling to find it.\n\n\n\nLook at the raw data (Data tab) and the variable labels (Variables tab) and answer the following questions.\n\n\n\n\n\n\n\nTipActivity 1 Questions\n\n\n\n\nCheck Your Understanding - Activity 1 QuestionsHint\n\n\nWhat is the repeated (within-subjects) variable and how many times is it measured?\nThe repeated (within-subjects) variable is breathing timetype of video and is measured  times.\n\n\nLook at the description in the Variables tab\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 1 Questions\n\n\n\n\nQuestion 2Hint\n\n\nWhat is the grouping (between-subjects) variable and what are the different groups?\nThe grouping (between-subjects) variable is type of videobreathing time and the groups are planttreedogcat and cattreedogplant.\n\n\nLook at the description in the Data tab\n\n\n\n\n\n\n\n\nOnce the data are entered, we need to look at what they can tell us. Jamovi has several tools we can use to better understand the data.\n\nBefore doing any statistical analysis, we can look at the data which gives us an idea of what we are dealing with: e.g. How many participants? How many variables? What is the range of scores? Are there any values that seem out of line with the rest?\nWhen satisfied that the raw data look okay, we continue our inspection of the data by using descriptive statistics, such as frequency counts, and measures of central tendency (such as mean, median and mode) and measures of spread (such as range or standard deviation).\n\n\n\n\n\nDownload and open the RMC_NCM_example_output_Explore.sav in Jamovi\nLook at the raw data (Data tab) and information about the variables (Variables tab) and answer the following questions.\n\n\n\n\n\n\n\nCaution Activity 2 Q1 - How many participants and how many variables were there?\n\n\n\n\n\n177 participants and 2 variables\n\n\n\n\n\n\n\n\n\nCaution Activity 2 Q2 - Are there any missing data or any odd-looking cells?\n\n\n\n\n\nYes\n\n\n\n\n\n\n\n\n\nCaution Activity 2 Q3 - Give an example (identify the example by the Jamovi row number)\n\n\n\n\n\nRows 48 and 175\n\n\n\n\n\n\n\n\n\nCaution Activity 2 Q4 - What did you notice about these rows of data? Describe it in a few words.\n\n\n\n\n\nOne of the rows is blank and the other has 999 in grey inside it",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 1 - Data handling"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-1_data-handling.html#frequency-counts",
    "href": "tutorials/rmc_week-1_data-handling.html#frequency-counts",
    "title": "Week 1 : Data Handling",
    "section": "",
    "text": "For nominal and ordinal data, the next step in getting to know the data is to determine the frequency counts for each possible score/value/option, i.e. how often each value occurs (e.g. the number of people who answered “yes” to the question “Did you take Psychology at A-Level?”).\nFor example, in the dataset RMC_NM1_example_output_Explore.sav, the variable about whether or not a participant took Psychology ‘A’-Level can be examined using this method.\n\n\n\n\nGo to the Analyses menu and choose Exploration\nFrom the submenu, choose Descriptives\n\n\n\nIn the descriptives box, choose the variable for which you want to calculate frequency counts and enter it into the box on the right-hand side. Click the tick box next to ‘Frequency tables’ to display the frequency table.\n\n\n\nAnswer the following questions based on the Jamovi output\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 3a Questions\n\n\n\n\nQuestion 1Hint\n\n\nHow many valid cases were processed? Was this correct?\n cases were processed.\n\n\nCompare cases to your answer to Activity 2, Q1\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 3a Questions\n\n\n\n\nQuestion 2\n\n\nLook at the number of times each option occurred. Which was the most common outcome? What percentage of the valid data did this account for?\nThe most common outcome was yesno and this accounted for % of the data.",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 1 - Data handling"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-1_data-handling.html#crosstabs-looking-at-frequencies-in-combinations-of-groups",
    "href": "tutorials/rmc_week-1_data-handling.html#crosstabs-looking-at-frequencies-in-combinations-of-groups",
    "title": "Week 1 : Data Handling",
    "section": "",
    "text": "Sometimes we want to look at combinations of categories, e.g. how many people studied Psychology at ‘A’-level, how many studied Biology at ‘A’-level, how many studied both, and how many studied neither subject.\nFor example, in the dataset RMC_NM1_example_output_Crosstabs.sav, the variable about whether or not a participant took Psychology ‘A’-Level and the variable about whether or not a participant took Biology ‘A’-level can be examined using this method.\n\n\n\n\nUsing RMC_NM1_example_output_Crosstabs.sav in Jamovi:\n\nGo to the Analyses menu and choose Frequencies.\nFrom the submenu, choose Independent Samples (this is under contingency tables).\n\n\n\n\nIn the Contingency Tables box, put one categorical variable into the Rows box and the other into the Columns box. (If you have more categorical variables that you want to examine, you add a new Layer for each further variable added).\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 3b Questions\n\n\n\n\nQuestion 1\n\n\nHow many students studied both Psychology and Biology at A-level? \n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 3b Questions\n\n\n\n\nQuestion 2\n\n\nWhich was the most common combination of subjects studied?\nThe most common combination of subjects studied was psychology without biology A-levelboth psychology and biology A-levelbiology without psychology A-levelneither psychology or biology A-level",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 1 - Data handling"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-1_data-handling.html#exploring-the-data",
    "href": "tutorials/rmc_week-1_data-handling.html#exploring-the-data",
    "title": "Week 1 : Data Handling",
    "section": "",
    "text": "Scale data (interval or ratio data) are not always suitable for analysis by frequency counts. Instead, we want to look at measures of central tendency (e.g. means) and measures of spread (e.g. standard deviation). A good way to examine scale data is to ‘explore’ them.\nThe Exploration module in Jamovi gives a range of statistics including charts to help us see whether the data are normally distributed, whether there are any outliers, and also to get an idea of the spread of the data and their typical value.\nWhile we should always carry out these explorative analyses, we tend not to report them all.\n\nFor example, we do not usually include histograms or stem-and-leaf plots in a results write-up. A good rule of thumb in terms of what we need to report is to think of the exploratory data analyses as being a way in which to check whether our data meet the assumptions underlying successful statistical analysis.\nIf they do not meet the assumptions, we need to report how they fail to meet them and state what this means in terms of how we then analyse the data.\n\nThe main assumptions which should be met for all parametric tests are:\n\nThe data are at least interval level of measurement.\nThe data are normally distributed.\nThe variance is homogeneous (similar) between different groups or conditions.\n\nWe always have to describe what the data are like in order to carry out a successful statistical analysis. Hence, we routinely report the means and standard deviations of scale data when we carry out statistical tests. (In RMC/NM1, this will be referred to as ‘reporting the descriptives’).\n\n\n\n\nUsing RMC_NM1_example_output_Explore.sav in Jamovi:\n\nGo to the Analyses menu and choose the Exploration module.\nFrom the submenu, choose Descriptives.\n\n\n\n\nIn the Descriptives box, choose the variable(s) you want to explore and enter it (them) into the Variables box on the right-hand side.\n\nIf you want to compare the score on one variable for two or more groups, enter the variable that represents your independent or grouping variable into the Split by box.\n\n\n\n\nClick on the Plots menu and select ‘histogram’ and ‘box plot’. Make sure that ‘label outliers’ is selected.\n\n\n\nAnswer the questions below based on the results\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 4 Questions\n\n\n\n\nQuestion 1\n\n\nAre the data normally distributed? (Base your judgement on the histograms.)\nThe data areare not normally distributed.\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 4 Questions\n\n\n\n\nQuestion 2\n\n\nAre there any outliers? These will be visible on the boxplots as circles with a number next to them (these numbers indicate which row(s) contains the outlier). Asterisks (*) denote ‘extreme’ values. Write down the row number for any extreme outliers.\nThere are no outliersThere is at least one outlier\nIf there are any outliers, write the row number for the outlier here \n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 4 Questions\n\n\n\n\nQuestion 3\n\n\nRemove the outlier from the data set by deleting the row with the outlier. Rerun the analysis. Are the data normally distributed?\nThe data areare not normally distributed.\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 4 Questions\n\n\n\n\nQuestion 4\n\n\nRerun the analysis on the data, but this time look at the two groups in comparison (see box above). Does it look as though there are differences between the groups?\nYes - the data look quite different in both the boxplots and box plots No - the data look quite similar in both the boxplots and box plots \n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 4 Questions\n\n\n\n\nQuestion 5\n\n\nReport the means and standard deviations for self-reported expertise for the two groups to 1 decimal place.\nPsychology A-level yes: M = , SD = \nPsychology A-level no: M = , SD = \n\n\n\n\n\n\n\n\n\nThe first part of the descriptives table shows how many cases were processed and if there is anything missing. Look at this to see if the correct number of cases has been processed.\n\nThis can be a useful way to spot if a mistake has been made at the data entry stage and a value has been missed out or added. If the ‘N’ in this table does not match the number of cases expected, double-check the data entry.\n\n\n[The output below used ‘research_expertise’ in the Variables box and ‘psych_A_level’ in the Split by box before any extreme outliers were removed.]\n\nThe descriptives table shows number of cases (N), number of missing values (Missing), mean, median, standard deviation, minimum, and maximum. You can change which values you want to display within the statistics drop down menu.\n\n\n\nThe output includes two graphical representations. The data from the example look like this as a histogram (left graph) and as a box and whisker plot (right plot). The boxplot gives us less information about the shape of the distribution but provides clear information about the outliers and which scores they are.\n\n\n\nAfter you remove the outlier, the histograms and box and whisker plots should look like this:",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 1 - Data handling"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-1_data-handling.html#using-jamovi-to-preparetransform-data-for-analysis",
    "href": "tutorials/rmc_week-1_data-handling.html#using-jamovi-to-preparetransform-data-for-analysis",
    "title": "Week 1 : Data Handling",
    "section": "",
    "text": "When we enter data from questionnaires or have raw data collected from programmes such as Qualtrics or Open Sesame, we often have to prepare the data before analysing it. We might need to:\n\nCalculate total or mean scores for several questions, e.g. for the score on a questionnaire.\nCalculate a difference between two scores from the same participant, e.g. when checking if there has been a change from a baseline score.\nCombine groups of responses into meaningful categories, e.g. when we want to look at participants with high or low scores on a variable of interest.\n\nWe can create these new ‘transformed’ or ‘computed’ variables using Jamovi.\n\n\n\nThis activity demonstrates how to calculate the total and mean scores for a set of variables using the “compute” function in Jamovi.\n\n\n\n\nUsing RMC_NM1_datagen2.sav in Jamovi\n\nClick on the column heading of the empty column on the far right of the screen (next to att5badgrade). Go to the Data tab. Choose the Compute option. This will create a new column.\n\n\n\n\n\nDouble click the column heading to open the variable editor.\nIn the computed variable box, enter a meaningful name for the total score variable (this is the new variable you are calculating).\n\nGiven the nature of the items on this data sheet – look at the labels in Variable View – a name like ‘Attitude_to_Jamovi_total’ would be appropriate to describe the new score.\n\n\n\n\nClick the fx box which shows all variables and functions, click the first variable that contributes to your total score to add it to the equation (by double-clicking on the variable name).\nType the ‘+’ sign in the equation box and move the next contributing variable into the function box. Repeat this process, until you have entered all the variables which contribute to the total score.\nBack on the data sheet, you should now have a new variable with the computed total score.\n\n\n\nIn the Variables tab, describe the new variable appropriately, e.g. enter a descriptive, meaningful label into the description column and check that the level of measurement is correct (scale in this example).\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 5 Questions\n\n\n\n\nQuestion 1\n\n\nWhich participants had the highest score?\nParticipant ; total score = \n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 5 Questions\n\n\n\n\nQuestion 2\n\n\nWhich participants had the lowest total score?\nParticipant ; total score = \n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 5 Questions\n\n\n\n\nQuestion 3\n\n\nUsing the skills from the ‘Understanding the data’ section, decide whether men or women had higher total scores.\nFemalesMales had a higher total score (mean females = ; mean males = )",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 1 - Data handling"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-1_data-handling.html#calculate-mean-scores-for-several-related-variables",
    "href": "tutorials/rmc_week-1_data-handling.html#calculate-mean-scores-for-several-related-variables",
    "title": "Week 1 : Data Handling",
    "section": "",
    "text": "Using RMC_NM1_datagen2.sav in Jamovi, repeat the process with a new target variable name for the mean score variable.\n\nGo to the Compute module in the Data tab again.\nEnter a meaningful name for the mean score variable into the Computed Variable box.\n\nGiven the nature of the items on this data sheet – look at the labels in Variable View – a name like ‘Jamovi_attitude_mean’ would be appropriate to describe the new score.\n\nClick on the fx drop down\nIn the Functions window, select ‘Mean’ (you will have to scroll down to it). Double click to select it.\nInto the Variables window, select the first variable you want in the mean and type a comma next to it. Repeat until all variables are included in the list.\n\n\n\n\nBack on the data sheet, you should now have a second variable with the computed mean score.\n\n\n\nIn the Variable tab define and label the second new variable appropriately.\n\n\n\n\n\n\n\nCaution Activity 6 Q1 - Using the skills from the ‘Understanding the data’ section, which participant(s) have the second highest mean scores?\n\n\n\n\n\nParticipant 30, 49 and 63\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 6 Questions\n\n\n\n\nQuestion 2\n\n\nWhat is the overall mean (grand mean) for the whole sample (rounded to 1 d.p.)?\nThe overall mean (grand mean) for the whole sample is  (rounded to 1 d.p.)",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 1 - Data handling"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-1_data-handling.html#calculate-a-difference-between-2-scores-from-the-same-participant-change-from-baseline",
    "href": "tutorials/rmc_week-1_data-handling.html#calculate-a-difference-between-2-scores-from-the-same-participant-change-from-baseline",
    "title": "Week 1 : Data Handling",
    "section": "",
    "text": "Using RMC_NM1_Bloodsugar06-07_for_baseline_calc.sav in Jamovi, calculate a new variable for the change in bloodsugar level from baseline (before) to after a drink has been consumed (after).\n\nGo to the Variables tab. Choose the Compute option.\n\n\n\nEnter a meaningful name for the difference score variable into the Computed Variable box (this is the new variable you are calculating).\n\nHere ‘change_in_bloodsugar’ is a meaningful label.\n\nIn the fx box double click the ‘after’ variable to move it into the formula box (by double-clicking it). Type the ‘-’ sign on the formula box and move the ‘before’ variable into the formula box. When you calculate the new variable, a positive/greater-than-zero value in the difference variable will mean an increase in the ‘after’ condition compared to ‘before’, and a negative value will mean a decrease.\nBack on the data sheet, you should now have a new variable with the computed change in bloodsugar.\n\n\n\nIn the Variable View, define and label the new variable appropriately.\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 7 Questions\n\n\n\n\nQuestion 1\n\n\nVisually inspect the raw data in the change variable. Pay attention to the ‘drink’ variable group. On average does it look as if there is a difference in how bloodsugar changes for the two drink groups?\nYes it looks like there is a difference in how bloodsugar changes between the two different drinksNo it does not look like there is a difference in how bloodsugar changes between the two different drinks\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 7 Questions\n\n\n\n\nQuestion 2\n\n\nWhat did drink group 1 drink? Did their bloodsugar level increase or decrease?\nGroup 1 drank glucosesaccharin and their blood sugar increaseddecreased\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 7 Questions\n\n\n\n\nQuestion 3\n\n\nWhat did drink group 2 drink? Did their bloodsugar level increase or decrease?\nGroup 2 drank glucosesaccharin and their blood sugar increaseddecreased",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 1 - Data handling"
    ]
  },
  {
    "objectID": "tutorials/rmc_week-1_data-handling.html#recode-into-different-variables-to-create-categories-from-a-continuous-variable",
    "href": "tutorials/rmc_week-1_data-handling.html#recode-into-different-variables-to-create-categories-from-a-continuous-variable",
    "title": "Week 1 : Data Handling",
    "section": "",
    "text": "Sometimes a continuous variable needs to be recoded to perform a particular analysis, e.g. if you wanted to turn a range of scores from 1 to 9 into three categories, ‘High’, ‘Middle’ and ‘Low’, this is what you need to do on Jamovi.\n\n\nUsing RMC_NM1_datagen3.sav in Jamovi, recode the ‘which_row’ score into a ‘distance_category’ (near, mid, far).\n\nGo to Data &gt; Transform…\n\n\n\nIn the drop down next to ‘source variable’, click ‘which_row’. Type a name (e.g. ‘distance_category’) in the ‘transformed variable’ box and describe the variable (e.g. ‘Distance from front’) in the description box.\nClick on the drop down next to ‘using transform’ and click ‘Create New Transform…’\nA new window will appear called “Transform”. Give the transformation at name (e.g. ‘Recode rows’).\n\nRECODING lowest values into ‘near’:\nClick ‘+ Add recode condition’\nThis will add a new box at the top which will say ‘if $source … use …’\nIn this box type the number you want to use as the upper limit for the ‘near’ condition (e.g. if you want to code 1, 2, and 3 as ‘near’, enter ‘&lt; 4’ into the dialog box as this will use all the numbers below, and not including, 4.)\nAfter the word ‘use’, type the value you want to recode the variable to (e.g. Use ‘near’ for the new variable. You need to include the quote marks around the new label.)\n\n\n\n\nRECODING middle values into ‘mid’:\n\nClick ‘+ Add recode condition’\nThis will add a new box which will say ‘if $source … use …’\nIn this box type the number you want to use as the upper limit for the ‘mid’ condition (e.g. if you want to code 4, 5 and 6 enter ‘&lt; 7’ into the dialog box as this will use all the numbers below, and not including, 7. Please note that the conditions are executed in order and because we have already set values below 4 to recode to 1, this formula will code values between 4 and 6 as 2. This may take some time to get your head around and this website may help https://blog.jamovi.org/2018/10/23/transforming-variables.html)\nAfter the word ‘use’, type the value you want to recode the variable to (e.g. Use ‘mid’ for the new variable. Don’t forget the quote marks around ‘mid’.)\n\n\n\n\nRECODING highest values into ‘far’:\n\nThe bottom formula box should say ‘else use’. In this box type ‘far’ to recode the remaining values (7. 8, and 9) as ‘far’.\n\n\n\n\nBack on the data sheet, you should now have a new variable with the recoded data.\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Activity 8 Questions\n\n\n\n\nQuestion 1\n\n\nUsing the ‘Understanding the Data’ skills, work out how many participants fall into each category\n\nNear: \nMid: \nFar: \n\n\n\n\n\n\n\n\n\n\n\n\nWarningFuture work\n\n\n\nThese calculations of overall scores and recoding techniques will be useful in Research Methods D also",
    "crumbs": [
      "Home",
      "Research Methods C",
      "Week 1 - Data handling"
    ]
  },
  {
    "objectID": "tutorials/rma_week-10_tests_of_difference_Chi-Squared.html",
    "href": "tutorials/rma_week-10_tests_of_difference_Chi-Squared.html",
    "title": "Week 10 : Tests of difference: Chi-Squared",
    "section": "",
    "text": "This week you will be learning about\n\n\n\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nHypothesis testing\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Jamovi editor\n\n\n\nPerforming a chi-squared test in Jamovi\n\n\n\nPlot basic graphs in Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with openly available research data\n\n\n\n\n\n\n\n\n\nNoteToday’s session\n\n\n\nToday you will just be familiarising yourself with the Chi-Squared test of difference. The Chi-Squared test is a non-parametric test of difference. It is used when…\n\nYou want to know about differences between groups\n\nAre there more females than males studying Psychology?\n\nYou want to know whether there is an association between two categorical variables\n\nIs there an association between gender and smoking?\n\nYour groups are independent\n\nEach observation only contributes to one cell of the analysis (e.g. gender)\n\nYou have categorical data (otherwise known as on the nominal level of measurement)\n\nFrequency or number of observations in a number of categories e.g. 10 male vs 50 females enrolled in Psychology\n\nYour data violates parametric assumptions\n\nYour data is not normally distributed\n\n\n\n\n\n\n\n\n\nThe table underneath contains information about smoking status distributed by gender. This information can be entered in to Jamovi providing that appropriate variables are constructed. In this example there are three variables, namely ‘smoking status’, ‘gender’, and ‘frequency’. In Point 2 illustrates these variables in Jamovi ‘Variable view’. Point 3 illustrates these variables and the associated data in Jamovi ‘Data view’\n\n\n\n\n\n\n\n\n\nSmokes\nDoes not smoke\n\n\n\n\nMale\n25\n            128\n\n\nFemale\n12\n            85\n\n\n\n\nTo learn how to perform a Chi-square test you may open the example data in this week’s Canvas folder and work through the steps detailed below – to open the file, simply double click on it. If you have Jamovi installed on your computer, then it will automatically open\nCheck that all relevant details are included in variable view – click on ‘variables’ in the top tool bar (it should look like the screen below). You should add your own labels here too. Also check that the values are specified for ‘gender’ and ‘smokes’.\n\n\n\nTo edit your variables, simply double tap on the variable of choice in the ‘variable view’ or tap on ‘edit’ in the tool bar\nYou will see all the details about the variable, which you can change.\n\n\n\n\nClick on ‘data’ and check that a row defines each cell of your frequency table (i.e. you should have 1 row for females who smoke, 1 for females who do not smoke, 1 for males who smoke and 1 for males who do not smoke). The total observed frequency of each category should be in the corresponding frequency column.\n\n\n\nNow you are ready to perform the Chi squared test. Click on: Frequencies &gt; Independent Samples\n\n\n\nAdd ‘Gender’ to the rows box, ‘Smokes’ to the Columns box, and ‘Frequency’ to the counts box\n\n\n\nClick on statistics and tick the Chi Squared test\n\n\n\nNext, select the cells button and select expected counts AND row, column and total\n\n\n\nYou can also play around with plots to visualise your data\n\n\n\nOn the right-hand side, you will see your results\n\n\n\n\n\n\nA Pearson Chi-Square was used to identify whether gender influenced smoking frequency. Results of the chi-square showed no significant difference in smoking frequency between genders (χ² (1) = .742, p = .389).\n\n\n\n\n\n\nNoteNote\n\n\n\nYou can find the χ symbol on word by going to Insert &gt; Symbol. You might need to look under More Symbols if you haven’t used this symbol before.\n\n\n\n\n\nA chi-square analysis was carried out to investigate the relationship between gender and smoking frequency, the Jamovi output showing the results of the analysis is shown below.\n\n\n\n\n\n\n\nCaution Data Skills - What are the observed frequencies for men and women who did and did not smoke?\n\n\n\n\n\nThe rows represent participants and the columns represent the different variables\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nWhat are the observed frequencies for males and females who did and did not smoke?\n\n\n\n\n\n\n\n\n\nDid smoke\nDid not smoke\n\n\n\n\nMale\n\n\n\n\nFemale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nHow would you report the results of the chi-square analysis in APA format?\nA Pearson Chi-Square was used to identify whether gendersmokingexercise influenced obestiysmokingexercise frequency. Results of the chi-square showed a significant differencedid not show a signficant difference in smoking frequency between genders (χ² () = , p  = .652&lt; .001= .036.\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nWhat do these results show?\nAs the results were statistically significantnot significant, there is anis no association between obestiysmokingexercise frequency and gendersmokingexercise, with malesfemales being more likely to be a smoker than malesfemales.\n\n\n\n\n\n\n\n\nThe table underneath shows how often men and women play football. Enter this data into Jamovi following the same format as in the previous example.\n\n\n\n\nPlay football\nDo not play football\n\n\n\n\nMale\n60\n70\n\n\nFemale\n45\n80\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionResults interpretationHint\n\n\nRun the chi-square and report the results\nA Pearson Chi-Square was used to identify whether there was an association between gender and whether or not you play football. Results of the chi-square showed a significant associationdid not show a signficant association between gender and whether or not you play football (χ² () = , p  &lt; .001= .100= .076.\n\n\nWhat do the results mean and what can you conclude?\nThe results mean that there is no significant associationa signficant association between gender and whether or not you play football. Men are significantlynot signficantly more likely to play football than women.\n\n\nTake a look at the Jamovi output",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 10 - Tests of Difference Chi-Squared"
    ]
  },
  {
    "objectID": "tutorials/rma_week-10_tests_of_difference_Chi-Squared.html#how-to-perform-a-chi-square-in-jamovi",
    "href": "tutorials/rma_week-10_tests_of_difference_Chi-Squared.html#how-to-perform-a-chi-square-in-jamovi",
    "title": "Week 10 : Tests of difference: Chi-Squared",
    "section": "",
    "text": "The table underneath contains information about smoking status distributed by gender. This information can be entered in to Jamovi providing that appropriate variables are constructed. In this example there are three variables, namely ‘smoking status’, ‘gender’, and ‘frequency’. In Point 2 illustrates these variables in Jamovi ‘Variable view’. Point 3 illustrates these variables and the associated data in Jamovi ‘Data view’\n\n\n\n\n\n\n\n\n\nSmokes\nDoes not smoke\n\n\n\n\nMale\n25\n            128\n\n\nFemale\n12\n            85\n\n\n\n\nTo learn how to perform a Chi-square test you may open the example data in this week’s Canvas folder and work through the steps detailed below – to open the file, simply double click on it. If you have Jamovi installed on your computer, then it will automatically open\nCheck that all relevant details are included in variable view – click on ‘variables’ in the top tool bar (it should look like the screen below). You should add your own labels here too. Also check that the values are specified for ‘gender’ and ‘smokes’.\n\n\n\nTo edit your variables, simply double tap on the variable of choice in the ‘variable view’ or tap on ‘edit’ in the tool bar\nYou will see all the details about the variable, which you can change.\n\n\n\n\nClick on ‘data’ and check that a row defines each cell of your frequency table (i.e. you should have 1 row for females who smoke, 1 for females who do not smoke, 1 for males who smoke and 1 for males who do not smoke). The total observed frequency of each category should be in the corresponding frequency column.\n\n\n\nNow you are ready to perform the Chi squared test. Click on: Frequencies &gt; Independent Samples\n\n\n\nAdd ‘Gender’ to the rows box, ‘Smokes’ to the Columns box, and ‘Frequency’ to the counts box\n\n\n\nClick on statistics and tick the Chi Squared test\n\n\n\nNext, select the cells button and select expected counts AND row, column and total\n\n\n\nYou can also play around with plots to visualise your data\n\n\n\nOn the right-hand side, you will see your results",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 10 - Tests of Difference Chi-Squared"
    ]
  },
  {
    "objectID": "tutorials/rma_week-10_tests_of_difference_Chi-Squared.html#how-to-report-these-results-in-apa-format",
    "href": "tutorials/rma_week-10_tests_of_difference_Chi-Squared.html#how-to-report-these-results-in-apa-format",
    "title": "Week 10 : Tests of difference: Chi-Squared",
    "section": "",
    "text": "A Pearson Chi-Square was used to identify whether gender influenced smoking frequency. Results of the chi-square showed no significant difference in smoking frequency between genders (χ² (1) = .742, p = .389).\n\n\n\n\n\n\nNoteNote\n\n\n\nYou can find the χ symbol on word by going to Insert &gt; Symbol. You might need to look under More Symbols if you haven’t used this symbol before.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 10 - Tests of Difference Chi-Squared"
    ]
  },
  {
    "objectID": "tutorials/rma_week-10_tests_of_difference_Chi-Squared.html#exercise-1",
    "href": "tutorials/rma_week-10_tests_of_difference_Chi-Squared.html#exercise-1",
    "title": "Week 10 : Tests of difference: Chi-Squared",
    "section": "",
    "text": "A chi-square analysis was carried out to investigate the relationship between gender and smoking frequency, the Jamovi output showing the results of the analysis is shown below.\n\n\n\n\n\n\n\nCaution Data Skills - What are the observed frequencies for men and women who did and did not smoke?\n\n\n\n\n\nThe rows represent participants and the columns represent the different variables\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nWhat are the observed frequencies for males and females who did and did not smoke?\n\n\n\n\n\n\n\n\n\nDid smoke\nDid not smoke\n\n\n\n\nMale\n\n\n\n\nFemale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nHow would you report the results of the chi-square analysis in APA format?\nA Pearson Chi-Square was used to identify whether gendersmokingexercise influenced obestiysmokingexercise frequency. Results of the chi-square showed a significant differencedid not show a signficant difference in smoking frequency between genders (χ² () = , p  = .652&lt; .001= .036.\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestion\n\n\nWhat do these results show?\nAs the results were statistically significantnot significant, there is anis no association between obestiysmokingexercise frequency and gendersmokingexercise, with malesfemales being more likely to be a smoker than malesfemales.",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 10 - Tests of Difference Chi-Squared"
    ]
  },
  {
    "objectID": "tutorials/rma_week-10_tests_of_difference_Chi-Squared.html#exercise-2",
    "href": "tutorials/rma_week-10_tests_of_difference_Chi-Squared.html#exercise-2",
    "title": "Week 10 : Tests of difference: Chi-Squared",
    "section": "",
    "text": "The table underneath shows how often men and women play football. Enter this data into Jamovi following the same format as in the previous example.\n\n\n\n\nPlay football\nDo not play football\n\n\n\n\nMale\n60\n70\n\n\nFemale\n45\n80\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionResults interpretationHint\n\n\nRun the chi-square and report the results\nA Pearson Chi-Square was used to identify whether there was an association between gender and whether or not you play football. Results of the chi-square showed a significant associationdid not show a signficant association between gender and whether or not you play football (χ² () = , p  &lt; .001= .100= .076.\n\n\nWhat do the results mean and what can you conclude?\nThe results mean that there is no significant associationa signficant association between gender and whether or not you play football. Men are significantlynot signficantly more likely to play football than women.\n\n\nTake a look at the Jamovi output",
    "crumbs": [
      "Home",
      "Research Methods A",
      "Week 10 - Tests of Difference Chi-Squared"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "The sampling distribution is central to how we can move from t-values to p-values - yet it is one of the trickier parts of this course. The lectures and pre-lecture materials frequently deal with computer simulations to show the properties of the tests we’re using, so this week you will write your own simulation to help understand the sampling distribution!\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nSampling Distributions\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nSimulating data in R\n\n\n\nWriting loops in R\n\n\n\nCreating histograms in R\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nCreating shareable code to demonstrate a statistical concept\n\n\n\n\n\nTLDR: There is no data to download! we’re going to make the data ourselves.\nThe dataset this week is very simple - we’re going to create it ourselves using R. At this point, we know a lot about the types of hypotheses that we can ask when comparing the averages of groups within a dataset. We can check our understanding of different aspects of these statistical tests by simulating data samples that have known properties (eg mean and standard deviation) and checking if the outcome of the t-test is what we would expect.\nThis way we can numerically validate the tests we’re running and take a look under the hood and see what sort of processes Jamovi and R are using to compute the tests for us.\n\n\n\n\n\n\nNoteContent Info\n\n\n\nThere is a lot of code in this session - don’t be afraid to ask for help from your tutors!\n\n\n\n\n\nToday, we’ll first introduce some concepts and functions in R to simulate a data sample and compute our own one sample t-test. With that in hand, we will set up code to run thousands of tests in one go to help us to better understand sampling distributions.\n\n\n\nThe rnorm() function in R is used to generate random numbers from a normal (Gaussian) distribution. A call to rnorm() looks like this:\nrnorm(n, mean = 0, sd = 1)\nWe can see that rnorm() takes three values as inputs\n\n\n\n\n\n\n\ninput\nDescription\n\n\n\n\nn\nThe number of random numbers you want to generate, we always need to specify this.\n\n\nmean\nThe mean (average) of the normal distribution. The default value is 0 if it isn’t specified\n\n\nsd\nThe standard deviation of the normal distribution. The default value is 1 if it isn’t specified\n\n\n\nSo we could tweak these input arguments to simulate any number of data points from any normal distribution that we wish.\n\n\n\n\n\n\nTipCheck Your Understanding - R code\n\n\n\nWhat code would we need to write to simulate the following?\n20 data points from a distribution with mean of 1 and standard deviation of 2\n\nR code here -&gt; .\n\n33 data points from a distribution with mean of -3 and standard deviation of 1.23\n\nR code here -&gt; .\n\nfive thousand data points from a distribution with mean of 1.789 and standard deviation of 18.2\n\nR code here -&gt; .\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nOpen Jamovi, install the Rj add-on and open a new Rj window before we go any further\n\n\nWe can use rnorm() within the Rj code window as usual. Here I’m simulating 20 data points from a distribution with a mean of 1 and a standard deviation of 2. Try running the same code in your window.\n\nThese 20 simulated numbers could now represent a data sample for a variable in an analysis. The difference is we have absolute confidence about the ‘truth’ of the population statistics underlying this data sample.\nYou may notice that you do get 20 numbers, but that you get a different 20 numbers than I did! This is as rnorm() works hard to generate random numbers that are different each time.\n\n\n\n\n\n\nNoteKey step\n\n\n\nYou can verify the random nature of the simulated data by running your code multiple times.\nEven though the code stays the same you will get a different data sample each time. Remember that the code and the underlying distribution remain the always the same but the data sample is changing.\n\n\nThis is actually a good thing for us. Real experiments and data collection always have an element of random luck and we want our simulations to reflect this as well.\n\n\n\n\n\n\nTipData skills - revision on sampling and populations\n\n\n\nSampling is a fundamental process in statistics where a subset of individuals from a population is selected to estimate characteristics of the whole population. This is built from the assumption that the subset of individuals in our sample is representative of that population.\nSome samples will be more representative than others. We can do a lot of practical work and planning to get the best possible sample for our research but there is always some random chance in who ends up in a data sample and who does not.\n\nThere are two main reasons why a sample might not be representative.\nBias if the data collection has included some bias (intentional or accidental) towards a subgroup of our population then it is very likely that this will lead to an unrepresentative sample. This is not something we can solve with statistics. Good reseach design, consideration of ethics and thorough planning is the only way to avoid systematic biases.\nLuck even if we have a unbiased data collection approach, two samples that draw from the sample population can still be different due to random chance. This will always be true when we are restricted to looking at a subset of the full population - fortunately, this is something we can quantify with statistics. We’ll explore how in this session.\nThe code we write this week will simulate drawing a subset of individuals from a population. Each iteration of the code could be considered as a separate replication of the same experiment.\n\n\n\n\n\nIn week 2 we discussed how sampling relatively small amount of data from a wider population can impact our estimates of values like the mean.\nTo recap, let’s say we’re interested in computing the average attainment for secondary school children on a particular vocabulary test. Our target population might be ‘all secondary school students in the UK’ but is it practically impossible to get data from every single student currently in secondard school. We have to make do with smaller data samples of a few tens or few hundreds of students (or even more, if we’re lucky).\nOur simulations do exactly this. The parameters we pass into rnorm() define ‘population level’ characteristics in the form of a normal distribution and each time we run the function we draw a ‘sample’ of a fixed number of data points from that population. We can think of this as rerunning our data sampling every time the function is executed. This would be massively complicated, time consuming and expensive in real life, but with a few assumptions a computer can simulate it in a fraction of a second.\nWe can now start to explore how sampling variability - AKA the random differences between data samples from the same population - can impact our estimates of the mean and standard deviation.\nUpdate your code to store the data sample into a variable and compute the mean with mean() and standard deviation with sd(). The result might look like this.\nsample = rnorm(20, mean=1, sd=2)\n\n# Compute the mean\nprint('Mean:')\nprint(mean(sample))\n\n# Compute the standard deviation\nprint('Standard Deviation:')\nprint(sd(sample))\n\nRunning this code will show the mean and standard deviation of each individual data sample, try running this a few times. Notice that the values are sometimes close to our ‘population’ estimates of mean = 1 and standard deviation = 2 but also sometimes quite far away.\nTry tweaking your code to answer these questions\n\n\n\n\n\n\nCaution Data Skills - is the estimate of the mean better or worse with a sample of 200?\n\n\n\n\n\nUpdate your code to use this data simulation\nsample = rnorm(200, mean=1, sd=2)\nRunning this a few times should show you mean values much closer to 1 and standard deviations much closer to 2 than we saw with a sample of 20 values. They still won’t be perfect though.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - is the estimate of the mean better or worse when the population standard deviation is 10?\n\n\n\n\n\nUpdate your code to use this data simulation\nsample = rnorm(200, mean=1, sd=10)\nRunning this a few times should make your estimates of the mean and standard deviation much more variable again. When variability in the population is very wide, large data samples won’t necessarily make up the difference.\n\n\n\nWe can get an informal sense of sampling variability by clicking to rerun this code a few times, but R code allows us to do much better, we can tweak the code to rerun this analysis hundreds or thousands of times to explicitly quantify the variability we see.\nTo do this, we’ll need a code element called a loop which looks something like this\nnum_simulations = 10\n\n# Loop to perform simulations\nfor (i in 1:num_simulations) {\n    print(i)\n}\nThere are two aspects to this code block. The first sets the number of simulated data samples we want to draw.\nnum_simulations = 10\nThis line sets the variable num_simulations to 10. This means you want to perform 10 iterations in the loop.\nThe second part is the loop itself:\nfor (i in 1:num_simulations) {\n    print(i)\n}\nThis is a for loop that will iterate from 1 to num_simulations (which is 10 in this case). Here’s what happens inside the loop:\n\nInitialization: The loop starts with i set to 1.\nCondition: The loop will continue to run as long as i is less than or equal to num_simulations.\nIncrement: After each iteration, i is incremented by 1.\n\nInside the loop, the print(i) statement is executed, which prints the current value of i to the console.\nThe output of this code will be the numbers from 1 to 10, each printed on a new line. The full output might look like this:\n\nPrinting the value of i is instructive but really we want our loop to include some interesting computations. Next, we should move our sampling code within the curly brackets {} of the loop.\nEverything within the those curly brackets will be repeated on every iteration of the loop. So if we includ the code to draw a sample and compute the mean, it will be repeated every time. Here is an example:\n# My loop\nnum_simulations = 10\n\nfor (i in 1:num_simulations) {\n    sample = rnorm(20, mean=1, sd=10)\n    print(mean(sample))\n}\n\nThe output of the loop is now the mean of each data sample, though the exact values you get will be slightly different to mine.\nWe can see that 20 samples drawn from a population with mean = 1, sd = 10 we have extremely variable estimates of the mean. Try reducing the standard deviation back down to 2 to see the result.\n# My loop\nnum_simulations = 10\n\nfor (i in 1:num_simulations) {\n    sample = rnorm(20, mean=1, sd=2)\n    print(mean(sample))\n}\n\nThese are much closer to our expected population value of 1.\n\n\n\nWe’re very close to completing our loop. There is one final issue to sort.\nAt the moment, the estimated sample is lost every time the code in the loop restarts - we need to store the whole set of mean values for every single simulation if we want to properly see what is going on.\nFor this we need to add two elements to the loop. Firstly we need to create a variable to store the means we estimate. We do this before the loop starts.\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nand secondly, within the loop, we need to add the estimated mean to our new mean_values variable.\n# Compute and store the mean of the data sample\nmean_values[i] = mean(sample)\nmean_values can store many numbers altogether in a list or vector. Each individual element can be accessed using the square bracket notation. For example mean_values[5] would access the fifth value in the list. The code above will store each value in to the ith position in the list so that we keep them all.\nThe final code should look like this:\n# My loop\nnum_simulations = 10\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(20, mean=1, sd=2)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\nmean_values now contains all of our simulated means that we can visualise using a histogram with the hist() function.\n\nWhich will produce a simple Histogram of the values for us. Note that, as ever, your histogram will be a little different to mine. Try rerunning the code a few times to get a sense of the variabity.\nWe can make our plot a bit more attractive and informative by specifying some additional inputs. These just change the visualisation of the histogram rather than the analysis.\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Means\", xlim=c(-2, 4),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\nThis has a lot of inputs, lets break down what they all mean\n\n\n\nInput\nDescription\n\n\n\n\nmean_values\nThe data sample to visualise\n\n\nmain\nThe main title\n\n\nxlim\nThe start and end points of the x-axis within a collection\n\n\nxlab\nThe label for the x-axis\n\n\ncol\nThe colour of the bars\n\n\nborder\nThe colour of the borders of the bars\n\n\n\nAdding these to the function call makes a much nicer and more informative image.\n\nYou can tweak the titles and axis labels to be sometime informative to you, and you can change the colours to be anything you like. Try customising the plot - update the labels and colours as you prefer them.\n\n\n\n\n\n\nNoteClick for names of colours in R\n\n\n\n\n\nYou can use the colour names defined here to customise your histogram.\n\n\n\n\n\n\n\nOnce your histogram is customised and ready to go - we can put our code to work and understand the core concepts.\nLet’s review what we have so far.\n\nWe have constructed a loop that draws data samples from a defined population\nWe compute and store the mean of that data sample\nWe use a histogram to visualise the distribution of means estimated from all of the data samples\nThis is the sampling distribution of the mean.\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nMake sure you have an organised script ready to go at this point, try adding comments to understand what each line does. You can delete some of the older parts of the code that aren’t included in the loop if you like. Your code should look something like this.:\n# My loop\nnum_simulations = 10\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(20, mean=1, sd=2)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Means\", xlim=c(-2, 4),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\n\nRecall that the variance of the sampling distribution is one of its key parameters - it tells us how precisely our estimated means map onto the known population mean. (Recall from lecture 3 that the standard error of the mean can be computed from this number).\nSampling distributions with wider variances indicate that we have less precisely estimated the mean, whereas tighter distributions indicate that we have more confidence. Let’s use our code to illustrate this concept.\n\n\n\n\n\n\nNoteKey step\n\n\n\nChange your code to repeat the simulation 1000 times rather than just 10, you should get a much more ‘normal’ distribution that changes less when you rerun the code.\n\n\n\nThis distribution is the sampling distribution of the mean for the simulated data. We can learn the following things:\n\nWe are most likely to estimate a value of the mean that is close to 1\nThe spread of the distribution indicates that values between 0.5 and 1.5 are still fairly common\nWe will very occasionally estimate a mean around 0 or around 2\n\nTry changing your code to answer these questions.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when each data sample has 100 data points rather than just 20?\n\n\nFor this question, you’ll need to update the first input to rnorm() to have 100 data points rather than 20.\n\n\nWith more data in each data sample - we can see that the spread in the sampling distribution has reduced. In other words, each sample has a similar estimate of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nWe will very occasionally estimate a mean around 0.5 or around 1.5, but these are rare\n\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when we increase the standard deviation to 10?\n\n\nFor this question, you’ll need to update the sd input to rnorm() to be 10 rather than 2.\n\n\nWith more variability in each data sample - we can see that the variance in the sampling distribution has increased. In other words, each sample can have extremely different estimates of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nIt is still common to get estimates of the mean of 0.5 or around 1.5.\nWith this much variability, some data samples have a mean estimate as far away as -1 or 3!\n\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nBack in week 4 we computed the following descriptives for participants confidence in a range of face processing tasks.\n\nCan you compute the sampling distribution of the mean for the AIConfidenceBefore variable?\n\n\nFor this question, you’ll need to the number of data samples, the mean and the standard deviation using the values in hte descriptive statistics.\nIts also a good idea to update the xlim of your histogram to something around c(2, 7).\n\n\nThe code and result should look something like this:\n\nBased on this, we can see that the most commonly estimated mean value is very close to our expected mean of 4.57 and that almost every data sample has a mean of between 4 and 5.\nThis is a useful way to visualise how accurate our sample mean estimates are likely to be.\n\n\n\n\n\n\n\n\nThe standard error of the mean is a very useful metric which tells us how our estimate of the mean is. We compute it using a formula most of the time if the parametric assumptions hold - but the best way to compute this is directly from the sampling distribution we have just created!\nOnce we have a sampling distribution for the mean, the standard error of the mean is simply the standard deviation of that samping distribution. Simple.\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nOpen a new Rj window and copy your code loop into it before starting this segment.\nWe will make large changes to the code and you may want to refer back to the previous parts for revision later.\n\n\nWe can compute sampling distributions for any summary statistic that we want to. This can be useful for quantifying sampling variability properties other than the mean. One straightforward example is the sampling distribution of the maximum.\nLet’s use the heights of human women as an example. According to Our World in Data, the height of women born between 1980 and 1994 is normally distributed with a mean values of 164.7cm and the standard deviation is 7.07cm.\nWe can use these parameters to define a population that we can use to simulate our data samples. This code will draw a data sample of the height of a single person.\nrnorm(1, mean=164.7, sd=7.07)\nWe can plug this into our code to compute the sampling distributions for height. The full loop will look like this - remember to update the xlim input to the histograms as well!\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(10, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\nThis is the sampling distribution of the mean for samples of 10 data points draw from the population distribution of women’s heights. The sampling distribution shows that the average height of a group of 10 women drawn randomly from the population will nearly always fall between 160cm and 170cm.\n\nWe can adapt this code to ask other questions - for example how tall is the tallest person in a group of 10 women likely to be?\nWe can adapt our code to compute this sampling distribution of maximums as well. Here is an exmaple - look out for sections with NEW CODE indicating a change.\nCritically, we use the max() function to return the maximum value of each data sample, in exactly the same way that we used mean() to compute the average.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nmax_values = numeric(num_simulations)  # &lt;- NEW CODE\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(10, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n    max_values[i] = max(sample)  # &lt;- NEW CODE\n}\n\n# Plot the sampling distribution\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\n# NEW CODE\nhist(max_values, main = \"Sampling Distribution of Maximums\", xlim=c(130, 200),\n     xlab = \"Sample Maximum\", col = \"skyblue\", border = \"white\")\n    \n\nThis shows us that the tallest person in a group of 10 women is likely to be between 170cm and 180cm.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute the sampling distribution of the minimum value to compute the height of the shortest person in groups of 10 women?\nUpdate the code that you have already written.\n\n\nTake a look at the changes highlighted in the previous section, can you repeat those changes to include an equivalent to the maximum?\n\n\nThe following code computes and displays the sampling distribution of the minimum value.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nmax_values = numeric(num_simulations)\nmin_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(4, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n    max_values[i] = max(sample)\n    min_values[i] = min(sample)\n    \n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\nhist(max_values, main = \"Sampling Distribution of Maximums\", xlim=c(130, 200),\n     xlab = \"Sample Maximum\", col = \"skyblue\", border = \"white\")\n    \nhist(min_values, main = \"Sampling Distribution of Minimums\", xlim=c(130, 200),\n     xlab = \"Sample Minimum\", col = \"skyblue\", border = \"white\")\n\n\n\n\n\n\n\n\n\nThroughout this course, we have seen many data variables that are not normally distributed. Examples include dice rolls, reaction times and the intrusive memories from last week. These are interesting datasets but don’t follow some of the nice assumptions that we can use with normally distributed data.\nNow that we have our loops to compute sampling distributions, we can explore how much of a problem non normally distributed data is for our estimates.\n\n\n\n\n\n\nNoteKey step\n\n\n\nOpen a new Rj window and copy your code loop from section 5 into it before starting this segment.\nWe will make large changes to the code and you may want to refer back to the previous parts for revision later.\n\n\nLet’s simulate some data for an experiment. Let’s simulate data from an experiment where participants indicate their confidence in performing some action on a scale from 1 to 10. The absolute truth of these data observations is that every participant has a equal probability of selecting any confidence from 1 to 10 - this is known as a uniform distribution and it is very different to a normal distribution.\nWe can simulate this data using runif() - note that this is an abbreviation of ‘random uniform’, not an indication to ‘run if’ something happens…\nTo use runif() we specify an n as usual in addition to the smallest and largest values that might occur.\nrunif(n, min = 1, max = 10)\nAs usual, we can slot this into our data loop to simulate our uniform distribution. Let’s start by drawing a single value during each repetition of the experiment. This will visualise our distribution.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = runif(1, min = 1, max = 10)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(0, 11),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\nYou can run this a few times to verify that the histogram shows a nearly uniform distribution where all outcomes are approximately equally likely to occur.\nNow, something odd happens when we increase the size of each data sample drawn from our population parameters. Rather than visualising the response of a single individual - lets take a look at the average response of 10 different individuals.\nUpdate the code to have an n of 10 rather than 1 in the call to runif().\n\nNow, though the raw data are uniformly distributed - the averages of uniform variables look like they are more normally distributed. Strange.\nThis is a property called the Central Limit Theorem. The Central Limit Theorem states that, given a sufficiently large sample size, the sampling distribution of the sample mean will be approximately normally distributed, regardless of the shape of the population distribution.\nThis is important as it tells us that the properties of data can change when we make computations from it. In our example, each individual response is uniformly distributed but the average response is approximately normally distributed.\nSecondly, this is why the normal distribution is so important. The Central Limit Theorem means that is weirdly likely that we will encounter normally distributed data variables - any variable that is an aggregate of other things becomes more and more normally distributed. We often deal with this sort of composite data in Psychology,\nThe learning statistics with jamovi textbook as an excellent description of central limit theorem if you want more information.\n\n\n\nWe’ve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#the-dataset",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#the-dataset",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "TLDR: There is no data to download! we’re going to make the data ourselves.\nThe dataset this week is very simple - we’re going to create it ourselves using R. At this point, we know a lot about the types of hypotheses that we can ask when comparing the averages of groups within a dataset. We can check our understanding of different aspects of these statistical tests by simulating data samples that have known properties (eg mean and standard deviation) and checking if the outcome of the t-test is what we would expect.\nThis way we can numerically validate the tests we’re running and take a look under the hood and see what sort of processes Jamovi and R are using to compute the tests for us.\n\n\n\n\n\n\nNoteContent Info\n\n\n\nThere is a lot of code in this session - don’t be afraid to ask for help from your tutors!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#the-challenge",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#the-challenge",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "Today, we’ll first introduce some concepts and functions in R to simulate a data sample and compute our own one sample t-test. With that in hand, we will set up code to run thousands of tests in one go to help us to better understand sampling distributions.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#simulating-data-samples-in-r",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#simulating-data-samples-in-r",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "The rnorm() function in R is used to generate random numbers from a normal (Gaussian) distribution. A call to rnorm() looks like this:\nrnorm(n, mean = 0, sd = 1)\nWe can see that rnorm() takes three values as inputs\n\n\n\n\n\n\n\ninput\nDescription\n\n\n\n\nn\nThe number of random numbers you want to generate, we always need to specify this.\n\n\nmean\nThe mean (average) of the normal distribution. The default value is 0 if it isn’t specified\n\n\nsd\nThe standard deviation of the normal distribution. The default value is 1 if it isn’t specified\n\n\n\nSo we could tweak these input arguments to simulate any number of data points from any normal distribution that we wish.\n\n\n\n\n\n\nTipCheck Your Understanding - R code\n\n\n\nWhat code would we need to write to simulate the following?\n20 data points from a distribution with mean of 1 and standard deviation of 2\n\nR code here -&gt; .\n\n33 data points from a distribution with mean of -3 and standard deviation of 1.23\n\nR code here -&gt; .\n\nfive thousand data points from a distribution with mean of 1.789 and standard deviation of 18.2\n\nR code here -&gt; .\n\n\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nOpen Jamovi, install the Rj add-on and open a new Rj window before we go any further\n\n\nWe can use rnorm() within the Rj code window as usual. Here I’m simulating 20 data points from a distribution with a mean of 1 and a standard deviation of 2. Try running the same code in your window.\n\nThese 20 simulated numbers could now represent a data sample for a variable in an analysis. The difference is we have absolute confidence about the ‘truth’ of the population statistics underlying this data sample.\nYou may notice that you do get 20 numbers, but that you get a different 20 numbers than I did! This is as rnorm() works hard to generate random numbers that are different each time.\n\n\n\n\n\n\nNoteKey step\n\n\n\nYou can verify the random nature of the simulated data by running your code multiple times.\nEven though the code stays the same you will get a different data sample each time. Remember that the code and the underlying distribution remain the always the same but the data sample is changing.\n\n\nThis is actually a good thing for us. Real experiments and data collection always have an element of random luck and we want our simulations to reflect this as well.\n\n\n\n\n\n\nTipData skills - revision on sampling and populations\n\n\n\nSampling is a fundamental process in statistics where a subset of individuals from a population is selected to estimate characteristics of the whole population. This is built from the assumption that the subset of individuals in our sample is representative of that population.\nSome samples will be more representative than others. We can do a lot of practical work and planning to get the best possible sample for our research but there is always some random chance in who ends up in a data sample and who does not.\n\nThere are two main reasons why a sample might not be representative.\nBias if the data collection has included some bias (intentional or accidental) towards a subgroup of our population then it is very likely that this will lead to an unrepresentative sample. This is not something we can solve with statistics. Good reseach design, consideration of ethics and thorough planning is the only way to avoid systematic biases.\nLuck even if we have a unbiased data collection approach, two samples that draw from the sample population can still be different due to random chance. This will always be true when we are restricted to looking at a subset of the full population - fortunately, this is something we can quantify with statistics. We’ll explore how in this session.\nThe code we write this week will simulate drawing a subset of individuals from a population. Each iteration of the code could be considered as a separate replication of the same experiment.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#estimating-the-samping-distribution-of-the-mean",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#estimating-the-samping-distribution-of-the-mean",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "In week 2 we discussed how sampling relatively small amount of data from a wider population can impact our estimates of values like the mean.\nTo recap, let’s say we’re interested in computing the average attainment for secondary school children on a particular vocabulary test. Our target population might be ‘all secondary school students in the UK’ but is it practically impossible to get data from every single student currently in secondard school. We have to make do with smaller data samples of a few tens or few hundreds of students (or even more, if we’re lucky).\nOur simulations do exactly this. The parameters we pass into rnorm() define ‘population level’ characteristics in the form of a normal distribution and each time we run the function we draw a ‘sample’ of a fixed number of data points from that population. We can think of this as rerunning our data sampling every time the function is executed. This would be massively complicated, time consuming and expensive in real life, but with a few assumptions a computer can simulate it in a fraction of a second.\nWe can now start to explore how sampling variability - AKA the random differences between data samples from the same population - can impact our estimates of the mean and standard deviation.\nUpdate your code to store the data sample into a variable and compute the mean with mean() and standard deviation with sd(). The result might look like this.\nsample = rnorm(20, mean=1, sd=2)\n\n# Compute the mean\nprint('Mean:')\nprint(mean(sample))\n\n# Compute the standard deviation\nprint('Standard Deviation:')\nprint(sd(sample))\n\nRunning this code will show the mean and standard deviation of each individual data sample, try running this a few times. Notice that the values are sometimes close to our ‘population’ estimates of mean = 1 and standard deviation = 2 but also sometimes quite far away.\nTry tweaking your code to answer these questions\n\n\n\n\n\n\nCaution Data Skills - is the estimate of the mean better or worse with a sample of 200?\n\n\n\n\n\nUpdate your code to use this data simulation\nsample = rnorm(200, mean=1, sd=2)\nRunning this a few times should show you mean values much closer to 1 and standard deviations much closer to 2 than we saw with a sample of 20 values. They still won’t be perfect though.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - is the estimate of the mean better or worse when the population standard deviation is 10?\n\n\n\n\n\nUpdate your code to use this data simulation\nsample = rnorm(200, mean=1, sd=10)\nRunning this a few times should make your estimates of the mean and standard deviation much more variable again. When variability in the population is very wide, large data samples won’t necessarily make up the difference.\n\n\n\nWe can get an informal sense of sampling variability by clicking to rerun this code a few times, but R code allows us to do much better, we can tweak the code to rerun this analysis hundreds or thousands of times to explicitly quantify the variability we see.\nTo do this, we’ll need a code element called a loop which looks something like this\nnum_simulations = 10\n\n# Loop to perform simulations\nfor (i in 1:num_simulations) {\n    print(i)\n}\nThere are two aspects to this code block. The first sets the number of simulated data samples we want to draw.\nnum_simulations = 10\nThis line sets the variable num_simulations to 10. This means you want to perform 10 iterations in the loop.\nThe second part is the loop itself:\nfor (i in 1:num_simulations) {\n    print(i)\n}\nThis is a for loop that will iterate from 1 to num_simulations (which is 10 in this case). Here’s what happens inside the loop:\n\nInitialization: The loop starts with i set to 1.\nCondition: The loop will continue to run as long as i is less than or equal to num_simulations.\nIncrement: After each iteration, i is incremented by 1.\n\nInside the loop, the print(i) statement is executed, which prints the current value of i to the console.\nThe output of this code will be the numbers from 1 to 10, each printed on a new line. The full output might look like this:\n\nPrinting the value of i is instructive but really we want our loop to include some interesting computations. Next, we should move our sampling code within the curly brackets {} of the loop.\nEverything within the those curly brackets will be repeated on every iteration of the loop. So if we includ the code to draw a sample and compute the mean, it will be repeated every time. Here is an example:\n# My loop\nnum_simulations = 10\n\nfor (i in 1:num_simulations) {\n    sample = rnorm(20, mean=1, sd=10)\n    print(mean(sample))\n}\n\nThe output of the loop is now the mean of each data sample, though the exact values you get will be slightly different to mine.\nWe can see that 20 samples drawn from a population with mean = 1, sd = 10 we have extremely variable estimates of the mean. Try reducing the standard deviation back down to 2 to see the result.\n# My loop\nnum_simulations = 10\n\nfor (i in 1:num_simulations) {\n    sample = rnorm(20, mean=1, sd=2)\n    print(mean(sample))\n}\n\nThese are much closer to our expected population value of 1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#visualising-the-samping-distribution-of-the-mean",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#visualising-the-samping-distribution-of-the-mean",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "We’re very close to completing our loop. There is one final issue to sort.\nAt the moment, the estimated sample is lost every time the code in the loop restarts - we need to store the whole set of mean values for every single simulation if we want to properly see what is going on.\nFor this we need to add two elements to the loop. Firstly we need to create a variable to store the means we estimate. We do this before the loop starts.\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nand secondly, within the loop, we need to add the estimated mean to our new mean_values variable.\n# Compute and store the mean of the data sample\nmean_values[i] = mean(sample)\nmean_values can store many numbers altogether in a list or vector. Each individual element can be accessed using the square bracket notation. For example mean_values[5] would access the fifth value in the list. The code above will store each value in to the ith position in the list so that we keep them all.\nThe final code should look like this:\n# My loop\nnum_simulations = 10\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(20, mean=1, sd=2)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\nmean_values now contains all of our simulated means that we can visualise using a histogram with the hist() function.\n\nWhich will produce a simple Histogram of the values for us. Note that, as ever, your histogram will be a little different to mine. Try rerunning the code a few times to get a sense of the variabity.\nWe can make our plot a bit more attractive and informative by specifying some additional inputs. These just change the visualisation of the histogram rather than the analysis.\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Means\", xlim=c(-2, 4),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\nThis has a lot of inputs, lets break down what they all mean\n\n\n\nInput\nDescription\n\n\n\n\nmean_values\nThe data sample to visualise\n\n\nmain\nThe main title\n\n\nxlim\nThe start and end points of the x-axis within a collection\n\n\nxlab\nThe label for the x-axis\n\n\ncol\nThe colour of the bars\n\n\nborder\nThe colour of the borders of the bars\n\n\n\nAdding these to the function call makes a much nicer and more informative image.\n\nYou can tweak the titles and axis labels to be sometime informative to you, and you can change the colours to be anything you like. Try customising the plot - update the labels and colours as you prefer them.\n\n\n\n\n\n\nNoteClick for names of colours in R\n\n\n\n\n\nYou can use the colour names defined here to customise your histogram.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#understanding-the-sampling-distribution-of-the-mean",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#understanding-the-sampling-distribution-of-the-mean",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "Once your histogram is customised and ready to go - we can put our code to work and understand the core concepts.\nLet’s review what we have so far.\n\nWe have constructed a loop that draws data samples from a defined population\nWe compute and store the mean of that data sample\nWe use a histogram to visualise the distribution of means estimated from all of the data samples\nThis is the sampling distribution of the mean.\n\n\n\n\n\n\n\nNoteKey step\n\n\n\nMake sure you have an organised script ready to go at this point, try adding comments to understand what each line does. You can delete some of the older parts of the code that aren’t included in the loop if you like. Your code should look something like this.:\n# My loop\nnum_simulations = 10\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(20, mean=1, sd=2)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Means\", xlim=c(-2, 4),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\n\nRecall that the variance of the sampling distribution is one of its key parameters - it tells us how precisely our estimated means map onto the known population mean. (Recall from lecture 3 that the standard error of the mean can be computed from this number).\nSampling distributions with wider variances indicate that we have less precisely estimated the mean, whereas tighter distributions indicate that we have more confidence. Let’s use our code to illustrate this concept.\n\n\n\n\n\n\nNoteKey step\n\n\n\nChange your code to repeat the simulation 1000 times rather than just 10, you should get a much more ‘normal’ distribution that changes less when you rerun the code.\n\n\n\nThis distribution is the sampling distribution of the mean for the simulated data. We can learn the following things:\n\nWe are most likely to estimate a value of the mean that is close to 1\nThe spread of the distribution indicates that values between 0.5 and 1.5 are still fairly common\nWe will very occasionally estimate a mean around 0 or around 2\n\nTry changing your code to answer these questions.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when each data sample has 100 data points rather than just 20?\n\n\nFor this question, you’ll need to update the first input to rnorm() to have 100 data points rather than 20.\n\n\nWith more data in each data sample - we can see that the spread in the sampling distribution has reduced. In other words, each sample has a similar estimate of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nWe will very occasionally estimate a mean around 0.5 or around 1.5, but these are rare\n\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when we increase the standard deviation to 10?\n\n\nFor this question, you’ll need to update the sd input to rnorm() to be 10 rather than 2.\n\n\nWith more variability in each data sample - we can see that the variance in the sampling distribution has increased. In other words, each sample can have extremely different estimates of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nIt is still common to get estimates of the mean of 0.5 or around 1.5.\nWith this much variability, some data samples have a mean estimate as far away as -1 or 3!\n\n\n\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nBack in week 4 we computed the following descriptives for participants confidence in a range of face processing tasks.\n\nCan you compute the sampling distribution of the mean for the AIConfidenceBefore variable?\n\n\nFor this question, you’ll need to the number of data samples, the mean and the standard deviation using the values in hte descriptive statistics.\nIts also a good idea to update the xlim of your histogram to something around c(2, 7).\n\n\nThe code and result should look something like this:\n\nBased on this, we can see that the most commonly estimated mean value is very close to our expected mean of 4.57 and that almost every data sample has a mean of between 4 and 5.\nThis is a useful way to visualise how accurate our sample mean estimates are likely to be.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#standard-error-of-the-mean",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#standard-error-of-the-mean",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "The standard error of the mean is a very useful metric which tells us how our estimate of the mean is. We compute it using a formula most of the time if the parametric assumptions hold - but the best way to compute this is directly from the sampling distribution we have just created!\nOnce we have a sampling distribution for the mean, the standard error of the mean is simply the standard deviation of that samping distribution. Simple.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#optional---sampling-distributions-of-the-maximum",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#optional---sampling-distributions-of-the-maximum",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "NoteKey step\n\n\n\nOpen a new Rj window and copy your code loop into it before starting this segment.\nWe will make large changes to the code and you may want to refer back to the previous parts for revision later.\n\n\nWe can compute sampling distributions for any summary statistic that we want to. This can be useful for quantifying sampling variability properties other than the mean. One straightforward example is the sampling distribution of the maximum.\nLet’s use the heights of human women as an example. According to Our World in Data, the height of women born between 1980 and 1994 is normally distributed with a mean values of 164.7cm and the standard deviation is 7.07cm.\nWe can use these parameters to define a population that we can use to simulate our data samples. This code will draw a data sample of the height of a single person.\nrnorm(1, mean=164.7, sd=7.07)\nWe can plug this into our code to compute the sampling distributions for height. The full loop will look like this - remember to update the xlim input to the histograms as well!\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(10, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\nThis is the sampling distribution of the mean for samples of 10 data points draw from the population distribution of women’s heights. The sampling distribution shows that the average height of a group of 10 women drawn randomly from the population will nearly always fall between 160cm and 170cm.\n\nWe can adapt this code to ask other questions - for example how tall is the tallest person in a group of 10 women likely to be?\nWe can adapt our code to compute this sampling distribution of maximums as well. Here is an exmaple - look out for sections with NEW CODE indicating a change.\nCritically, we use the max() function to return the maximum value of each data sample, in exactly the same way that we used mean() to compute the average.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nmax_values = numeric(num_simulations)  # &lt;- NEW CODE\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(10, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n    max_values[i] = max(sample)  # &lt;- NEW CODE\n}\n\n# Plot the sampling distribution\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\n# NEW CODE\nhist(max_values, main = \"Sampling Distribution of Maximums\", xlim=c(130, 200),\n     xlab = \"Sample Maximum\", col = \"skyblue\", border = \"white\")\n    \n\nThis shows us that the tallest person in a group of 10 women is likely to be between 170cm and 180cm.\n\n\n\n\n\n\n\nTipCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute the sampling distribution of the minimum value to compute the height of the shortest person in groups of 10 women?\nUpdate the code that you have already written.\n\n\nTake a look at the changes highlighted in the previous section, can you repeat those changes to include an equivalent to the maximum?\n\n\nThe following code computes and displays the sampling distribution of the minimum value.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nmax_values = numeric(num_simulations)\nmin_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(4, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n    max_values[i] = max(sample)\n    min_values[i] = min(sample)\n    \n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\nhist(max_values, main = \"Sampling Distribution of Maximums\", xlim=c(130, 200),\n     xlab = \"Sample Maximum\", col = \"skyblue\", border = \"white\")\n    \nhist(min_values, main = \"Sampling Distribution of Minimums\", xlim=c(130, 200),\n     xlab = \"Sample Minimum\", col = \"skyblue\", border = \"white\")",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#optional---sampling-distributions-from-non-normal-data",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#optional---sampling-distributions-from-non-normal-data",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "Throughout this course, we have seen many data variables that are not normally distributed. Examples include dice rolls, reaction times and the intrusive memories from last week. These are interesting datasets but don’t follow some of the nice assumptions that we can use with normally distributed data.\nNow that we have our loops to compute sampling distributions, we can explore how much of a problem non normally distributed data is for our estimates.\n\n\n\n\n\n\nNoteKey step\n\n\n\nOpen a new Rj window and copy your code loop from section 5 into it before starting this segment.\nWe will make large changes to the code and you may want to refer back to the previous parts for revision later.\n\n\nLet’s simulate some data for an experiment. Let’s simulate data from an experiment where participants indicate their confidence in performing some action on a scale from 1 to 10. The absolute truth of these data observations is that every participant has a equal probability of selecting any confidence from 1 to 10 - this is known as a uniform distribution and it is very different to a normal distribution.\nWe can simulate this data using runif() - note that this is an abbreviation of ‘random uniform’, not an indication to ‘run if’ something happens…\nTo use runif() we specify an n as usual in addition to the smallest and largest values that might occur.\nrunif(n, min = 1, max = 10)\nAs usual, we can slot this into our data loop to simulate our uniform distribution. Let’s start by drawing a single value during each repetition of the experiment. This will visualise our distribution.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = runif(1, min = 1, max = 10)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(0, 11),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\nYou can run this a few times to verify that the histogram shows a nearly uniform distribution where all outcomes are approximately equally likely to occur.\nNow, something odd happens when we increase the size of each data sample drawn from our population parameters. Rather than visualising the response of a single individual - lets take a look at the average response of 10 different individuals.\nUpdate the code to have an n of 10 rather than 1 in the call to runif().\n\nNow, though the raw data are uniformly distributed - the averages of uniform variables look like they are more normally distributed. Strange.\nThis is a property called the Central Limit Theorem. The Central Limit Theorem states that, given a sufficiently large sample size, the sampling distribution of the sample mean will be approximately normally distributed, regardless of the shape of the population distribution.\nThis is important as it tells us that the properties of data can change when we make computations from it. In our example, each individual response is uniformly distributed but the average response is approximately normally distributed.\nSecondly, this is why the normal distribution is so important. The Central Limit Theorem means that is weirdly likely that we will encounter normally distributed data variables - any variable that is an aggregate of other things becomes more and more normally distributed. We often deal with this sort of composite data in Psychology,\nThe learning statistics with jamovi textbook as an excellent description of central limit theorem if you want more information.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#summary",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#summary",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "We’ve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html",
    "href": "tutorials/rmb_week-7_non-parametrics.html",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "This week we will explore how to check the assumptions of hypothesis tests and make sure that we’re using the right tests. We will introduce the idea of transforming data to squeeze the data into the shape of a normal distribution.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nAssumptions of parametric tests\n\n\n\nLog transforms and Box-Cox transforms\n\n\n\nHistograms\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nCompute data transformations using Jamovi\n\n\n\nRun non-parametric alternative tests in Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nReplicate part of a published analysis\n\n\n\n\n\n\n\n\n\nNoteContent Info\n\n\n\nThis tutorial refers to a video in which participants watched unpleasant events to simulate a traumatic experience in the lab. The content of this video is not disclosed or dicussed in any more detail than this during this tutorial - those who are interested can find more information in methods section the published article. This tutorial does explore the emotional impact that the video has on participants.\n\n\n\n\nFlashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we’re going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind’s eye and cause distress. We investigated whether reconsolidation—the process during which memories become malleable when recalled—can be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group.\n\n\n\nThis week - we will explore two of the four conditions to test the following hypothesis:\n\nPlaying tetris will after a trumatic event will reduce number of subsequent intrusive memories\n\nAlong the way, we’ll take a detailed tour of our options when working with non-normally distributed data.\nThe theme this week is PRECISION! we need high precision when interpreting and analysing results, the exercises this week are designed to give you practice in paying close attention to the details of your data…\n\n\n\nLet’s start by getting familiar with the dataset. Load the file into Jamovi and take a look at the ‘Variables’ tab.\n\nThe creators of this dataset have helpfully included annotations for many of the variables they’ve measured. This is extremely useful for us and definitely worth a few moments for you to do the same when working with your own data.\n\n\n\n\n\n\nNoteKey step\n\n\n\nTake a moment to read through the variable descriptions. You’ll need to refer back to this list throughout the session to find relevant variables.\n\n\nNext - take a look at the data sheet and the Condition variable. These relate to the different conditions in the experiment - all participants watched the film containing traumatic events and the four conditions describe what participants did after watching the film on day 1 of the experiment.\n\n\n\n\n\n\n\nCondition\nDescription\n\n\n\n\nNo-Task Control\nParticipants sit quietly\n\n\nReactivation+Tetris\nParticipants rewatch the film and then play Tetris\n\n\nTetris Only\nParticipants play Tetris\n\n\nReactivation Only\nParticipants rewatch the film\n\n\n\nParticipants were asked to keep an diary of any intrusive thoughts that they had prior to the experiment (day zero) and during the week after the experiment (days one to seven).\n\n\n\n\n\n\nCaution Data Skills - which variable tracks the diary of intrusive memories?\n\n\n\n\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary contains the number of intrusive memories the participant recorded in the week after watching the film.\nDay_Zero_Image_Based_Intrusions_in_Intrusion_Diary is very similar - it contains the number of intrusive memories the participant recorded in the time prior to watching the film. This is a good measure to include so we can rule out differences in the baseline number of intrusive memories between our groups.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - what information do we have about participant’s performance when playing tetris?\n\n\n\n\n\nTetris_Total_Score contains the cumulative score of all tetris games the participant played - participants were allowed to restart their game if they finished before the time was up.\nWe also have Self_Rated_Tetris_Performance which measures how participants rated the difficulty of the tetris task and Tetris_Demand_Rating which measures how participants thought that playing tetris would affect their intrusive memories.\n\n\n\n\n\n\nLet’s start by taking a look at how participants in the conditions that played Tetris scored during their game time. Compute descriptive statistics on the Tetris_Total_Score variable and take a moment to get a sense of what it is telling you.\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the descriptive statistics of Tetris_Total_Score to answer the following questions in the spaces provided.\nThe average Tetris score was .\nThe most successful Tetris player scored  points.\nThe Tetris scores are positively skewednot skewednegatively skewed with a skewness of .\nThe Shapiro-Wilk statistic has a value of W =  and the p-value indicates that the scores are significantly non-normally distributednormally distributed\nWe should use parametricnon-parametric statistical approaches when analysing Tetris_Total_Score.\n\n\n\n\n\n\n\n\nLet’s use this data to test a hypothesis.\n\nPerformance whilst playing Tetris will be different at different times of day.\n\nWe can test this with an independent samples t-test as we’ve done in previous weeks. Set up the t-test and include the following:\n\nWelchs’ t-test\nMann-Whitney U\nEffect Size\nHomogeneity test\nNormality test\n\n\nLet’s unpack the results.\n\n\n\n\n\n\nCaution Data Skills - does the Normality assumption check confirm what we saw in the descriptives?\n\n\n\n\n\nYes, we get the same picture but with a little less detail. Shapiro-Wilk’s test is significant indicating a non-normal distribution, but we still need the descriptive stats to understand HOW the distribution is different from a normal distribution.\nImportantly, this confirms that we should report the non-parametric alternative test - the Mann-Whitney U.\nYou could add the descriptives to the t-test page - but notice that this only includes a handful of metrics that are specifically related to t-tests (mean, standard deviation etc). We need the full list on the ‘Descriptives’ tab to investigate distributions in detail.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - what does Levene’s test indicate and does this matter for us?\n\n\n\n\n\nLevene’s test explores homogeneity of variance, the result is not significant indicating that we are safe to assume homogeneity of variance between our groups.\nHowever, the data are not normally distributed - non-parametric tests don’t have an assumption of homogeneity of variance at all so, for this analysis, Levene’s test isn’t really relevant.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - what does the Mann-Whitney U test indicate about our hypothesis?\n\n\n\n\n\nThe Mann-Whitney U test is not significant - it seems like tetris scores are similar for participants who played in the morning and those who played in the afternoon.\n\n\n\n\n\n\nOn to our main hypothesis. (James et al. 2015) were most interested in the combination of the reactivation task and the tetris intervention and their effect on intrusive memories about the film. Let’s focus on only the two conditions we can compare with a t-test.\n\n\n\n\n\n\nNoteKey step\n\n\n\nSet up a Jamovi data filter to focus our analysis on only the two conditions “No-Task Control” and “Reactivation+Tetris” conditions. Take a look at week 3 section 4 to revise data filters if you need to.\nYou can use this code to select both conditions together:\nCondition == \"No-Task Control\" or Condition == \"Reactivation+Tetris\"\nFinally, compute an independent samples t-test to test the following:\n\nPerforming a Reactivation and Tetris task changes the number of intrusive memories in the week after watching a traumatic film.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThere are  participants in each group.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p &lt; \nLevene’s test for homogeneity of variance indicated that we cancan not assume homogeneity of variance between the No-Task and Reactivation+Tetris groups, F(1, 32) = , p = .\nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in the week after the film between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .\n\n\n\n\n\n\n\n\n\n\n\nOur hypothesis test gives statistical support to the idea that playing tetris reduces intrusive memories - but could there be other explanations? This experiment is likely not perfect and it is possible that some bad luck in the data collection could have impacted the results. In this section we’ll validate a few additional factors in the experimenta..\n\n\n(James et al. 2015) collected data variable to act as a control condition and rule out one possible confound in the experiment. Days_Zero_Image_Based_Intrusions_in_Intrusion_Diary indicates the number of intrusive memories each participant had in the time before the experiment.\nThere isn’t a huge number of participants in each condition, it is possible that one group of participants was having more or less intrusive memories to begin with - irrespective of the experimental procedure.\n\n\n\n\n\n\nNoteKey step\n\n\n\nCompute a t-test to test the following:\n\nThere are no differences in the number of instrusive memories between the two experimental group BEFORE the experiment started.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThe participant in the ‘No-Task Control’ group had mean of  intrusive memories prior to the experiment starting.\nThe participant in the ‘Reactivation+Tetris’ group had mean of  intrusive memories prior to the experiment starting.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p = \nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in day before between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .\n\n\n\n\n\n\n\n\n\n\n\nAnother important factor to check was that the film actually changed how the participants felt. If the film didn’t do anything to impact the participants emotions then it is possible that we don’t actually understand the source of the difference we saw in the main hypothesis.\nThe film is the main experimental manipulation that so it is critical to check that it worked as we expected.\n\n\n\n\n\n\nNoteKey step\n\n\n\nCompute paired samples t-test’s to check the following:\n\nWatching the traumatic film will change participants emotional state.\n\nCheck ALL of participants reported sadness, hopelessness, fear, horror and anxiousness.\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nTrue or False? The data variables for these factors are Normally Distributed according to Shaprio-Wilk statistics.\n\nSadness TRUEFALSE\nHopeless TRUEFALSE\nFear TRUEFALSE\nHorror TRUEFALSE\nAxious TRUEFALSE\n\nHow did the following factors change or not after watching the film?\n\nSadness increasedstayed the samedecreased after watching the film\nHopelessless increasedstayed the samedecreased after watching the film\nFear increasedstayed the samedecreased after watching the film\nHorror increasedstayed the samedecreased after watching the film\nAxiousness increasedstayed the samedecreased after watching the film\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - did the film manipulation work?\n\n\n\n\n\nYes, all factors apart from Hopelessless showed a statistically significant increase after watching the film. This indicates that the film had a strongly impact on participants negative emotions. It seems like the experimental manipulation worked.\n\n\n\n\n\n\n\nThere are times when we might not want to use a non-parametric test, for example if we specifically want to interpret the mean of the data rather than the median or if we’re running many similar tests and we don’t want to be switching back and forth between parametric and non-parametric tests.\nWe have another option when working with non-normal data. This is to apply a reversible transformation to the data observations in a variable to change the distribution to one that is closer to being normally distributed. When this process works, we are able to run the parametric test on the transformed data rather thans swapping to the non-parametric alternative.\nThis is valid when the transformation preserves the relationships in the dataset, that is it maintains the relative order of the observations. If data point A is larger than B before transformation then it should still be larger after the transformation. It is fine if the transformation changes the numbers themselves - we just have to be careful when making an interpretation.\nWe’ll use the LOG data transformation in this section\n\n\n\n\n\n\n\nTransform\nExplanation\n\n\n\n\nLog Transform\nThe logarithm function, typically base 10 (log), is applied to each data point.\n\n\n\nOne downside of the log transform is that it has a weird value when the datapoint being transformed is zero or negative. log(0) equals negative infinity! this will break a lot of analyses that happen afterwards but fortunately there is a straightforward solution.\nWhen your data contains zeros or negatives you can simply add an offset value to make your data positive and then apply the log transform. Again, this is valid as long as we are preserving the relationships within our data by adding exactly the same offset to every single point in the dataset.\nWe can easily transform variables using Jamovi and have done something very similar when computing variables in previous weeks.\n\n\n\n\n\n\nTipData Skills - transforming a variable.\n\n\n\nYou can read about transforming variables in detail on the Jamovi documentation page on transforming variables. There are a few different steps but they are all straightforward.\nFirst, head to the ‘Data’ view and right click on the variable you want to transform, in this case we want Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary\n\nThis will open a dialogue box where you can define the new variable. Give your new variable a descriptive name and make sure that the ‘Source variable’ is the one that you expect. Note that you might see a ‘Circular definition’ warning until you rename your variable.\n\nNext we need to specify the transformation that we want to use. Click into the ‘using transform’ drop-down box and select “Create New Transform”.\nHere you can define the transformation that you want to apply. You can define a single transform and apply it to multiple variables. Give the transformation a descriptive name and specify that you want the transfrom to apply the LOG10() function to the $source variable. ($source is a way to tell Jamovi to use ‘whichever variable is passed into the transform’ rather than defining the transform on only 1 variable here.).\nWe also include a small offset value of +1 here. This is as some participants had no intrusive memories at all and we want to avoid issues with computing log(0) in our analysis.\n\nYou can click on the Fx logo to select functions from a list if you prefer, there are lots of pre-defined options.\n\nOnce you’re happy with the transformation, you can close the transformation definition dialogue and specify that we want to use the ‘LogTransform’ that we just defined to transfrom Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary to make our new variable.\n\nThere are a lot of steps here - but each is fairly straightforward. Ask your tutor if you get stuck.\n\n\nNow that we have our new variable we can explore what it has done. Let’s compare the descriptive statistics for the original and the transformed variable.\n\n \n\n\n\n\n\n\nCaution Data Skills - has the log transform changed mean of the data?\n\n\n\n\n\nYes, the log transform has changed the mean by a long way. The original data shows that the average participant had 3.5 intrusive memories but that number has changed to 0.54 in the transformed data.\nThis does not mean that the number has really reduced! remember that the transformation is changing the absolute values of the observations but should be preserving the relative relationships between the data points.\nThis is fine, but we must not report the log-transformed values as the actual means. The participants still had an average of 3.5 intrusive memories whether we transformed the data or not.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - has the log transform changed the skewness and kurtosis of the data?\n\n\n\n\n\nYes, it has had a big impact. Both the skewness and kurtosis values are much closer to zero in the transformed data compared to the original. Both indicates that the transformed data are more normally distributed.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - is there a statistically significant difference from a normal distribution in the transformed data?\n\n\n\n\n\nNo, whereas the Shapiro-Wilk statistic showed a significant departure from a normal distribution for the original data (W = 0.8, p &lt; 0.001) it shows that the transformed data is now indistinguishable from a normal distribution (W = 0.953, p = 0.130)\n\n\n\nFinally - let’s see if we can use our transformed variables to compute a parametric t-test. Repeat the independent samples t-test from earlier but include all three variables in the ‘Dependent Variables’ box. Make sure to include\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary - original data\nDays_One_to_Seven_Transformed - Log transformed data\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nWith the original data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . We can proceed with Student’s t-testWelch’s t-testMann-Whitney U test.\nUsing the log-transformed data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . Levene’s test found that the we cancan not assume that the groups have homogeneity of variance, F(1, 34) = , p = . We can proceed with Student’s t-testWelch’s t-testMann-Whitney U test.\n\n\n\nNote that Welch’s t-test is valid whether or not we can assume homogeneity of variance…\n\n\n\n\n\n\n\n\nWe have explored an interesting dataset in some detail and worked with serveral different analysis approaches. Specifically, we have several options when working on checking the assumptions of our analyses. We have to be careful when computing and reporting hypothesis tests that we’re using the correct tests in a way that is supported by the data. Sometimes we can use a transform to continuous using parametric tests when the assumptions are not supported but this will not always work.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#the-dataset",
    "href": "tutorials/rmb_week-7_non-parametrics.html#the-dataset",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Flashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we’re going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind’s eye and cause distress. We investigated whether reconsolidation—the process during which memories become malleable when recalled—can be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#the-challenge",
    "href": "tutorials/rmb_week-7_non-parametrics.html#the-challenge",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "This week - we will explore two of the four conditions to test the following hypothesis:\n\nPlaying tetris will after a trumatic event will reduce number of subsequent intrusive memories\n\nAlong the way, we’ll take a detailed tour of our options when working with non-normally distributed data.\nThe theme this week is PRECISION! we need high precision when interpreting and analysing results, the exercises this week are designed to give you practice in paying close attention to the details of your data…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#dataset-description",
    "href": "tutorials/rmb_week-7_non-parametrics.html#dataset-description",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Let’s start by getting familiar with the dataset. Load the file into Jamovi and take a look at the ‘Variables’ tab.\n\nThe creators of this dataset have helpfully included annotations for many of the variables they’ve measured. This is extremely useful for us and definitely worth a few moments for you to do the same when working with your own data.\n\n\n\n\n\n\nNoteKey step\n\n\n\nTake a moment to read through the variable descriptions. You’ll need to refer back to this list throughout the session to find relevant variables.\n\n\nNext - take a look at the data sheet and the Condition variable. These relate to the different conditions in the experiment - all participants watched the film containing traumatic events and the four conditions describe what participants did after watching the film on day 1 of the experiment.\n\n\n\n\n\n\n\nCondition\nDescription\n\n\n\n\nNo-Task Control\nParticipants sit quietly\n\n\nReactivation+Tetris\nParticipants rewatch the film and then play Tetris\n\n\nTetris Only\nParticipants play Tetris\n\n\nReactivation Only\nParticipants rewatch the film\n\n\n\nParticipants were asked to keep an diary of any intrusive thoughts that they had prior to the experiment (day zero) and during the week after the experiment (days one to seven).\n\n\n\n\n\n\nCaution Data Skills - which variable tracks the diary of intrusive memories?\n\n\n\n\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary contains the number of intrusive memories the participant recorded in the week after watching the film.\nDay_Zero_Image_Based_Intrusions_in_Intrusion_Diary is very similar - it contains the number of intrusive memories the participant recorded in the time prior to watching the film. This is a good measure to include so we can rule out differences in the baseline number of intrusive memories between our groups.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - what information do we have about participant’s performance when playing tetris?\n\n\n\n\n\nTetris_Total_Score contains the cumulative score of all tetris games the participant played - participants were allowed to restart their game if they finished before the time was up.\nWe also have Self_Rated_Tetris_Performance which measures how participants rated the difficulty of the tetris task and Tetris_Demand_Rating which measures how participants thought that playing tetris would affect their intrusive memories.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#is-participant-performance-on-tetris-normally-distributed",
    "href": "tutorials/rmb_week-7_non-parametrics.html#is-participant-performance-on-tetris-normally-distributed",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Let’s start by taking a look at how participants in the conditions that played Tetris scored during their game time. Compute descriptive statistics on the Tetris_Total_Score variable and take a moment to get a sense of what it is telling you.\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the descriptive statistics of Tetris_Total_Score to answer the following questions in the spaces provided.\nThe average Tetris score was .\nThe most successful Tetris player scored  points.\nThe Tetris scores are positively skewednot skewednegatively skewed with a skewness of .\nThe Shapiro-Wilk statistic has a value of W =  and the p-value indicates that the scores are significantly non-normally distributednormally distributed\nWe should use parametricnon-parametric statistical approaches when analysing Tetris_Total_Score.\n\n\n\n\n\n\n\n\nLet’s use this data to test a hypothesis.\n\nPerformance whilst playing Tetris will be different at different times of day.\n\nWe can test this with an independent samples t-test as we’ve done in previous weeks. Set up the t-test and include the following:\n\nWelchs’ t-test\nMann-Whitney U\nEffect Size\nHomogeneity test\nNormality test\n\n\nLet’s unpack the results.\n\n\n\n\n\n\nCaution Data Skills - does the Normality assumption check confirm what we saw in the descriptives?\n\n\n\n\n\nYes, we get the same picture but with a little less detail. Shapiro-Wilk’s test is significant indicating a non-normal distribution, but we still need the descriptive stats to understand HOW the distribution is different from a normal distribution.\nImportantly, this confirms that we should report the non-parametric alternative test - the Mann-Whitney U.\nYou could add the descriptives to the t-test page - but notice that this only includes a handful of metrics that are specifically related to t-tests (mean, standard deviation etc). We need the full list on the ‘Descriptives’ tab to investigate distributions in detail.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - what does Levene’s test indicate and does this matter for us?\n\n\n\n\n\nLevene’s test explores homogeneity of variance, the result is not significant indicating that we are safe to assume homogeneity of variance between our groups.\nHowever, the data are not normally distributed - non-parametric tests don’t have an assumption of homogeneity of variance at all so, for this analysis, Levene’s test isn’t really relevant.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - what does the Mann-Whitney U test indicate about our hypothesis?\n\n\n\n\n\nThe Mann-Whitney U test is not significant - it seems like tetris scores are similar for participants who played in the morning and those who played in the afternoon.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#is-tetris-effective-at-reducing-intrusive-memories",
    "href": "tutorials/rmb_week-7_non-parametrics.html#is-tetris-effective-at-reducing-intrusive-memories",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "On to our main hypothesis. (James et al. 2015) were most interested in the combination of the reactivation task and the tetris intervention and their effect on intrusive memories about the film. Let’s focus on only the two conditions we can compare with a t-test.\n\n\n\n\n\n\nNoteKey step\n\n\n\nSet up a Jamovi data filter to focus our analysis on only the two conditions “No-Task Control” and “Reactivation+Tetris” conditions. Take a look at week 3 section 4 to revise data filters if you need to.\nYou can use this code to select both conditions together:\nCondition == \"No-Task Control\" or Condition == \"Reactivation+Tetris\"\nFinally, compute an independent samples t-test to test the following:\n\nPerforming a Reactivation and Tetris task changes the number of intrusive memories in the week after watching a traumatic film.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThere are  participants in each group.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p &lt; \nLevene’s test for homogeneity of variance indicated that we cancan not assume homogeneity of variance between the No-Task and Reactivation+Tetris groups, F(1, 32) = , p = .\nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in the week after the film between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#control-analyses-testing-for-potential-confounds.",
    "href": "tutorials/rmb_week-7_non-parametrics.html#control-analyses-testing-for-potential-confounds.",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Our hypothesis test gives statistical support to the idea that playing tetris reduces intrusive memories - but could there be other explanations? This experiment is likely not perfect and it is possible that some bad luck in the data collection could have impacted the results. In this section we’ll validate a few additional factors in the experimenta..\n\n\n(James et al. 2015) collected data variable to act as a control condition and rule out one possible confound in the experiment. Days_Zero_Image_Based_Intrusions_in_Intrusion_Diary indicates the number of intrusive memories each participant had in the time before the experiment.\nThere isn’t a huge number of participants in each condition, it is possible that one group of participants was having more or less intrusive memories to begin with - irrespective of the experimental procedure.\n\n\n\n\n\n\nNoteKey step\n\n\n\nCompute a t-test to test the following:\n\nThere are no differences in the number of instrusive memories between the two experimental group BEFORE the experiment started.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThe participant in the ‘No-Task Control’ group had mean of  intrusive memories prior to the experiment starting.\nThe participant in the ‘Reactivation+Tetris’ group had mean of  intrusive memories prior to the experiment starting.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p = \nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in day before between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .\n\n\n\n\n\n\n\n\n\n\n\nAnother important factor to check was that the film actually changed how the participants felt. If the film didn’t do anything to impact the participants emotions then it is possible that we don’t actually understand the source of the difference we saw in the main hypothesis.\nThe film is the main experimental manipulation that so it is critical to check that it worked as we expected.\n\n\n\n\n\n\nNoteKey step\n\n\n\nCompute paired samples t-test’s to check the following:\n\nWatching the traumatic film will change participants emotional state.\n\nCheck ALL of participants reported sadness, hopelessness, fear, horror and anxiousness.\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nTrue or False? The data variables for these factors are Normally Distributed according to Shaprio-Wilk statistics.\n\nSadness TRUEFALSE\nHopeless TRUEFALSE\nFear TRUEFALSE\nHorror TRUEFALSE\nAxious TRUEFALSE\n\nHow did the following factors change or not after watching the film?\n\nSadness increasedstayed the samedecreased after watching the film\nHopelessless increasedstayed the samedecreased after watching the film\nFear increasedstayed the samedecreased after watching the film\nHorror increasedstayed the samedecreased after watching the film\nAxiousness increasedstayed the samedecreased after watching the film\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution Data Skills - did the film manipulation work?\n\n\n\n\n\nYes, all factors apart from Hopelessless showed a statistically significant increase after watching the film. This indicates that the film had a strongly impact on participants negative emotions. It seems like the experimental manipulation worked.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#transforming-non-normally-distributed-data",
    "href": "tutorials/rmb_week-7_non-parametrics.html#transforming-non-normally-distributed-data",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "There are times when we might not want to use a non-parametric test, for example if we specifically want to interpret the mean of the data rather than the median or if we’re running many similar tests and we don’t want to be switching back and forth between parametric and non-parametric tests.\nWe have another option when working with non-normal data. This is to apply a reversible transformation to the data observations in a variable to change the distribution to one that is closer to being normally distributed. When this process works, we are able to run the parametric test on the transformed data rather thans swapping to the non-parametric alternative.\nThis is valid when the transformation preserves the relationships in the dataset, that is it maintains the relative order of the observations. If data point A is larger than B before transformation then it should still be larger after the transformation. It is fine if the transformation changes the numbers themselves - we just have to be careful when making an interpretation.\nWe’ll use the LOG data transformation in this section\n\n\n\n\n\n\n\nTransform\nExplanation\n\n\n\n\nLog Transform\nThe logarithm function, typically base 10 (log), is applied to each data point.\n\n\n\nOne downside of the log transform is that it has a weird value when the datapoint being transformed is zero or negative. log(0) equals negative infinity! this will break a lot of analyses that happen afterwards but fortunately there is a straightforward solution.\nWhen your data contains zeros or negatives you can simply add an offset value to make your data positive and then apply the log transform. Again, this is valid as long as we are preserving the relationships within our data by adding exactly the same offset to every single point in the dataset.\nWe can easily transform variables using Jamovi and have done something very similar when computing variables in previous weeks.\n\n\n\n\n\n\nTipData Skills - transforming a variable.\n\n\n\nYou can read about transforming variables in detail on the Jamovi documentation page on transforming variables. There are a few different steps but they are all straightforward.\nFirst, head to the ‘Data’ view and right click on the variable you want to transform, in this case we want Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary\n\nThis will open a dialogue box where you can define the new variable. Give your new variable a descriptive name and make sure that the ‘Source variable’ is the one that you expect. Note that you might see a ‘Circular definition’ warning until you rename your variable.\n\nNext we need to specify the transformation that we want to use. Click into the ‘using transform’ drop-down box and select “Create New Transform”.\nHere you can define the transformation that you want to apply. You can define a single transform and apply it to multiple variables. Give the transformation a descriptive name and specify that you want the transfrom to apply the LOG10() function to the $source variable. ($source is a way to tell Jamovi to use ‘whichever variable is passed into the transform’ rather than defining the transform on only 1 variable here.).\nWe also include a small offset value of +1 here. This is as some participants had no intrusive memories at all and we want to avoid issues with computing log(0) in our analysis.\n\nYou can click on the Fx logo to select functions from a list if you prefer, there are lots of pre-defined options.\n\nOnce you’re happy with the transformation, you can close the transformation definition dialogue and specify that we want to use the ‘LogTransform’ that we just defined to transfrom Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary to make our new variable.\n\nThere are a lot of steps here - but each is fairly straightforward. Ask your tutor if you get stuck.\n\n\nNow that we have our new variable we can explore what it has done. Let’s compare the descriptive statistics for the original and the transformed variable.\n\n \n\n\n\n\n\n\nCaution Data Skills - has the log transform changed mean of the data?\n\n\n\n\n\nYes, the log transform has changed the mean by a long way. The original data shows that the average participant had 3.5 intrusive memories but that number has changed to 0.54 in the transformed data.\nThis does not mean that the number has really reduced! remember that the transformation is changing the absolute values of the observations but should be preserving the relative relationships between the data points.\nThis is fine, but we must not report the log-transformed values as the actual means. The participants still had an average of 3.5 intrusive memories whether we transformed the data or not.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - has the log transform changed the skewness and kurtosis of the data?\n\n\n\n\n\nYes, it has had a big impact. Both the skewness and kurtosis values are much closer to zero in the transformed data compared to the original. Both indicates that the transformed data are more normally distributed.\n\n\n\n\n\n\n\n\n\nCaution Data Skills - is there a statistically significant difference from a normal distribution in the transformed data?\n\n\n\n\n\nNo, whereas the Shapiro-Wilk statistic showed a significant departure from a normal distribution for the original data (W = 0.8, p &lt; 0.001) it shows that the transformed data is now indistinguishable from a normal distribution (W = 0.953, p = 0.130)\n\n\n\nFinally - let’s see if we can use our transformed variables to compute a parametric t-test. Repeat the independent samples t-test from earlier but include all three variables in the ‘Dependent Variables’ box. Make sure to include\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary - original data\nDays_One_to_Seven_Transformed - Log transformed data\n\n\n\n\n\n\n\n\nTipCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nWith the original data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . We can proceed with Student’s t-testWelch’s t-testMann-Whitney U test.\nUsing the log-transformed data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . Levene’s test found that the we cancan not assume that the groups have homogeneity of variance, F(1, 34) = , p = . We can proceed with Student’s t-testWelch’s t-testMann-Whitney U test.\n\n\n\nNote that Welch’s t-test is valid whether or not we can assume homogeneity of variance…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#summary",
    "href": "tutorials/rmb_week-7_non-parametrics.html#summary",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "We have explored an interesting dataset in some detail and worked with serveral different analysis approaches. Specifically, we have several options when working on checking the assumptions of our analyses. We have to be careful when computing and reporting hypothesis tests that we’re using the correct tests in a way that is supported by the data. Sometimes we can use a transform to continuous using parametric tests when the assumptions are not supported but this will not always work.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  }
]