---
engine: knitr
---

```{r}
#| echo: false
library(webexercises)
```


# Week 10 : Simulating Sampling Distributions for t-values {.unnumbered}

The sampling distribution is central to how we can move from t-values to p-values - yet it is one of the trickier parts of this course. The lectures and pre-lecture materials frequently deal with computer simulations to show the properties of the tests we're using, so this week you will write your own simulation to help understand the sampling distribution!

|  ![](../images/logos/rm-logo_quantitative-methods.png){width=50px}  | Quantitative Methods          |
|-----|:-------------------------------------------|
|            | Sampling Distributions                  |
|            | t-values                  |
|            | p-values                           |

: Learning objectives for this session

|    ![](../images/logos/rm-logo_data-skills.png){width=50px}      | Data Skills       |
|-----|:-------------------------------------------|
|          | Computing a one sample t-test in R              |
|          | Simulating data in R              |
|          | Writing loops in R              |


|      ![](../images/logos/rm-logo_open-science.png){width=50px}      | Open Science                                               |
|-----|:-------------------------------------------|
|          | Creating shareable code to demonstrate a statistical concept       |


## 1. The Dataset {.unnumbered}

TLDR: There is no data to download! we're going to make the data ourselves.

The dataset this week is very simple - we're going to create it ourselves using R. At this point, we know a lot about the types of questions that we can ask with t-tests and we know a lot about the data processing around it. We can check our understanding of different aspects of these statistical tests by simulating data samples that have known properties (eg mean and standard deviation) and checking if the outcome of the t-test is what we would expect.

This way we can numerically validate the tests we're running and take a look under the hood and see what sort of processes Jamovi and R are using to compute the tests for us.

::: {.callout-note appearance="simple"}
# Content Info

There is a lot of code in this session - don't be afraid to ask for help from your tutors!

:::

## 2. The Challenge {.unnumbered}

Today, we'll first introduce some concepts and functions in R to simulate a data sample and compute our own one sample t-test. With that in hand, we will set up code to run thousands of tests in one go to help us to better understand sampling distributions.




## 6. Sampling distributions for one sample t-tests {.unnumbered}


So far we've looked at sampling distributions of the mean but we can do the same thing with t-tests to get a sense of how varaible a t statistic might be under different circumstances. This also gives us some important insights into how t and p values are related.

To do this we'll need to make a couple of additions to our code.



::: {.callout-note appearance="simple"}
# Key step

Open a new Rj window before starting this segment. The code changes will be large and you may want to refer back to the previous parts for revision later.

:::

The first thing we'll need is to compute a t value for a one sample t-test from our data vector from this definition:

$$
t = \frac{\text{The sample mean} - \text{Comparison Value}}{\text{The standard error of the mean}}
$$

We need to use this code to compute the three ingredients and the t-test itself. Copy this code into your new Rj window and take a moment to understand each line. 

```R
# Number of participants
n = 20

# Generate a random sample
sample = rnorm(n, mean = 0, sd = 1)

# Calculate the sample mean and standard deviation
sample_mean = mean(sample)
sample_sd = sd(sample)

# Calculate the sample error of the mean 
sample_standard_error = sample_sd / sqrt(n)

# Define comparison value
comparison_value = 0

# Compute t-value
t_value = (sample_mean - comparison_value) / sample_standard_error
print(t_value)
```

 Note that we're simulating data with a `mean` of zero and using a `comparison_value` of zero as well. This is our Null Model - the t-values we get from this code are ones that might appear if there is no true effect in the data.

![](../images/rmb-week-8_computer-practical/rmb-week-8_computer-practical_img15.png)


As before, you can run this code a few times to see the variability in the final t-value. Remember this this is a null model - most of the t-values are close to zero, but some are quite large...


<!--# #############################################  -->

:::: callout-tip
## Check Your Understanding

::: panel-tabset
## Question

For the final part of new code, we need to combine our t-test with the loop from the previous section.

Copy this code into your Rj window (you can overwrite the previous code if you like), and fill in the blanks indicated by  `__YOUR_CODE_HERE__` (you should delete `__YOUR_CODE_HERE__` and replace it with your answer).

If you get it right you will see a histogram of sampling distribution of t values.

```R
# My loop
num_simulations = 1000

# Create a numeric variable to store the means
t_values = numeric(num_simulations)

# Run the loop
for (i in 1:num_simulations) {
    # Number of participants
    n = __YOUR_CODE_HERE__

    # Generate a random sample
    sample = __YOUR_CODE_HERE__

    # Calculate the sample mean and standard deviation
    sample_mean = __YOUR_CODE_HERE__
    sample_sd = __YOUR_CODE_HERE__

    # Calculate the sample error of the mean 
    sample_standard_error = __YOUR_CODE_HERE__

    # Define comparison value
    comparison_value = __YOUR_CODE_HERE__

    # Compute t-value
    t_value[i] = __YOUR_CODE_HERE__
}

# Plot the sampling distribution of t-values
hist(t_values, main = "Sampling Distribution of t-values", xlim=c(-4, 4),
     xlab = "Sample t-value", col = "skyblue", border = "white")

```
## Hint

All the answers are in previous sections, pay particular attention to the code defining the t-test in section 5.

Chat with one of your tutors if you get stuck.

## Solution

Well done if you got this correct. The final code should look like this:

```R
# My loop
num_simulations = 1000

# Create a numeric variable to store the means
t_values = numeric(num_simulations)

# Run the loop
for (i in 1:num_simulations) {
    # Number of participants
    n = 20

    # Generate a random sample
    sample = rnorm(n, mean = 0, sd = 1)

    # Calculate the sample mean and standard deviation
    sample_mean = mean(sample)
    sample_sd = sd(sample)

    # Calculate the sample error of the mean 
    sample_standard_error = sample_sd / sqrt(n)

    # Define comparison value
    comparison_value = 0

    # Compute t-value
    t_values[i] = (sample_mean - comparison_value) / sample_standard_error
}

# Plot the sampling distribution of t-values
hist(t_values, main = "Sampling Distribution of t-values", xlim=c(-4, 4),
     xlab = "Sample t-value", col = "skyblue", border = "white")
    
```

![](../images/rmb-week-8_computer-practical/rmb-week-8_computer-practical_img16.png)


:::
::::

After this exercise, you can now compute the null model for a one sample t-test yourself. With this code you can run an analysis to explore how likely it is to observe different t-values when we assume the null hypothesis is true.

Think about the following questions and tweak your code to find the answers.


::: {.callout-caution icon="false" collapse="true"}
## <i class="bi bi-question-lg"></i> Data Skills - how does the sampling distribution change when we have large samples? Use n = 250 as an example.

Update your code to reduce the number of participants to 25:

```R
    # Number of participants
    n = 250
```

![](../images/rmb-week-8_computer-practical/rmb-week-8_computer-practical_img18.png)


Running this a few times, you should see that the sampling distribution gets *narrower* compared to what we saw for `n = 20`. We commonly see values between -2 and 2 but almost never see values of -3 or +3.

:::


::: {.callout-caution icon="false" collapse="true"}
## <i class="bi bi-question-lg"></i> Data Skills - how does the sampling distribution change when we have very small samples? Use n = 4 as an example.

Update your code to reduce the number of participants to 5:

```R
    # Number of participants
    n = 4
```

![](../images/rmb-week-8_computer-practical/rmb-week-8_computer-practical_img17.png)

Running this a few times, you should see that the sampling distribution gets *broader* than for smaller samples. We see more values close to 4 and -4 than we did with a dataset of 20 or 250.

:::


In all of these analyses we are absolutely assuming that the null hypothesis of no difference is true. With this assumptions, our sampling distribution shows that we get more extreme t-values (around +/- 4) by chance when working with smaller data samples. By contrast for larger data samples these same values occur extremely rarely.

When data are perfectly normally distributed (parametric assumptions are met), Jamovi and R can compute these null distributions for us using a shortcut without simulating lots of data samples but the result is the same.

To conduct a hypothesis test from real data, we would compute the t-statistic as normal and can then compare the observed value to this sampling distribution that assumes no difference and has the same sample size. If the t-value we've observed from the real data would be extremely unlikely to have occured if the null is true, then we can reject that hypothesis.



## 7. Why do we test the null distribution? {.unnumbered}


Finally, let's consider why we test against the null distribution of no effect. This is fairly straightforward from the code we have written so far.

We can absolutely compute a sampling distribution for a case where there is a real difference between the mean and the comparison value. For example, this code simulates data with a mean of 1 and compares that to a comparison value of 0. Note that the `rnorm()` function inputs have changed.

```R
# My loop
num_simulations = 1000

# Create a numeric variable to store the means
t_values = numeric(num_simulations)

# Run the loop
for (i in 1:num_simulations) {
    # Number of participants
    n = 20

    # Generate a random sample
    sample = rnorm(n, mean = 1, sd = 1)

    # Calculate the sample mean and standard deviation
    sample_mean = mean(sample)
    sample_sd = sd(sample)

    # Calculate the sample error of the mean 
    sample_standard_error = sample_sd / sqrt(n)

    # Define comparison value
    comparison_value = 0

    # Compute t-value
    t_values[i] = (sample_mean - comparison_value) / sample_standard_error
}

# Plot the sampling distribution of t-values
hist(t_values, breaks=24, main = "Sampling Distribution of t-values", xlim=c(-10, 10),
     xlab = "Sample t-value", col = "skyblue", border = "white")
```

The result is a sampling distribution which is not centred around mean.

![](../images/rmb-week-8_computer-practical/rmb-week-8_computer-practical_img20.png)

This distribution tells us that with 20 participants, a mean of 1 and a standard deviation of 1 - we are mostly likely to get a -value of 5 but that t-value could be as small as 2 or as large as 9 for extreme data samples.

This is very useful, but the issue that is we have had to assume what the real effect is. We get radically different sampling distributions if we change that effect around.

#### Mean of 0.5 and standard deviation of 1 with 20 participants compared to zero.
![](../images/rmb-week-8_computer-practical/rmb-week-8_computer-practical_img19a.png){width=450px}

#### Mean of -1 and standard deviation of 1 with 20 participants compared to zero.
![](../images/rmb-week-8_computer-practical/rmb-week-8_computer-practical_img19b.png){width=450px}

#### Mean of 1.5 and standard deviation of 1 with 20 participants compared to zero.
![](../images/rmb-week-8_computer-practical/rmb-week-8_computer-practical_img19c.png){width=450px}

There are an infinite number of possible sampling distributions when there is a real effect! It isn't possible to know which distribution we should be comparing with without making some massive assumptions about the exact size, variability and direction of effect that we expect to see. This simply isn't possible in practice.

In contrast, the null case is very closely controlled and easier to work with. The interpretation is trickier but rejecting the null hypothesis is a tractable problem that we can do a good job of solving.