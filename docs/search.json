[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methods Computer Practicals",
    "section": "",
    "text": "Week 1 : Descriptive Statistics, Data Processing and R\n\ntoc: true toc-depth: 2"
  },
  {
    "objectID": "index.html#the-dataset",
    "href": "index.html#the-dataset",
    "title": "Week 1 : Descriptive Statistics, Data Processing and R",
    "section": "",
    "text": "We will be exploring data that was collected as part of the ‘Many Labs 3’ project (Ebersole et al., 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‘Big Five’ personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs. extravagant/careless)\nAgreeableness (friendly/compassionate vs. critical/rational)\nNeuroticism (sensitive/nervous vs. resilient/confident)\nOpenness to experience (inventive/curious vs. consistent/cautious)\nExtraversion (outgoing/energetic vs. solitary/reserved)\n\nThis is a very simple (Gosling et al., 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block, 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5."
  },
  {
    "objectID": "index.html#the-challenge",
    "href": "index.html#the-challenge",
    "title": "Week 1 : Descriptive Statistics, Data Processing and R",
    "section": "",
    "text": "We have data from a sample of students in a big data table. This raw data needs to be summarised and visualised so that we can turn this massive block of numbers into something intuitive.\nBy computing a range of key features from the data we can generate a concise overview of the main characteristics of the data.\n\n\n\n\n\n\n\nCheck Your Understanding 1\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nYou have reproduced the descriptive statistics for the big 5 personality factors. Now you can do the same for some of the other factors in Table 2 of section 3.5 in Ebersole et al. (2016).\nSpecifically - try and compute the mean and standard deviations for:\n\nSelf Esteem (see section 3.5.1 for details)\nPerceived Stress (see section 3.5.4)\nNeed for cognition (see section 3.5.5) Can you find variables (columns) that correspond to these factors?\n\nNote: Open datasets sometimes use shorthand for variable names rather than the names used in the manuscript. This is not ideal but does happen in practice! we should be ready take a little time to explore and understand a new dataset when working with it.\n\n\nIn this case the variables in the dataset to use are:\nIn Manuscript –&gt; In Jamovi Self Esteem –&gt; ‘SelfEsteem’ Perceived Stress –&gt; ‘Stress’ Need for cognition –&gt; ‘NFC’\nTalk to your tutor if you’re getting stuck.\n\n\nYou should be able to reproduce the following table. Verify that the computed Mean and Standard Deviations match those published in table 2 of Ebersole et al (2016)"
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "This week we will explore how to use Jamovi to compute and visualise descriptive statistics from a dataset. This will include some revision on Jamovi from last semester and an introduction on how we can use the R coding language to simplify complex data processing.\n\n\n\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nMeasures of central tendency (Mean and Median)\n\n\n\nMeasures of dispersion and spread (Standard Deviation and Interquartile Range)\n\n\n\nMeasures of skew and normality\n\n\n\nRecognising different types of distribution\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nExploring and cleaning datasets Jamovi\n\n\n\nComputing descriptive statistics in Jamovi\n\n\n\nComputing descriptive plots in Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with openly available research data\n\n\n\n\n\n\nWe will be exploring data that was collected as part of the ‘Many Labs 3’ project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‘Big Five’ personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs. extravagant/careless)\nAgreeableness (friendly/compassionate vs. critical/rational)\nNeuroticism (sensitive/nervous vs. resilient/confident)\nOpenness to experience (inventive/curious vs. consistent/cautious)\nExtraversion (outgoing/energetic vs. solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.\n\n\n\nWe have data from a sample of students in a big data table. This raw data needs to be summarised and visualised so that we can turn this massive block of numbers into something intuitive.\nWe can generate a concise overview of the main characteristics of the data by computing and visualising few key features from the data.\n\n\n\n\n\n\n\n\n\n\nRevision from RM-A\n\n\n\nThis computer practical will have some revision from RM-A, please check your notes from RM-A for help with loading data and handling variables.\nIf you aren’t familiar with Jamovi, or would like a refresher - please take a quick look at the following topics in the Jamovi user guide\n\nFirst steps in Jamovi\nAnalyses\nThe spreadsheet\n\nPlease ask your tutor for help in the computer practical sessions if you need any assistance.\n\n\n\n\n\n\n\n\nKey step\n\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file ML3AllSites_RMB.csv from canvas.\n\n\n\nThis is the original dataset provided by the authors of the paper! It is a massive dataset - take a moment to get familiar with it by answering the following questions.\nSome of the questions will require you to run a descriptive statistics analysis on the age variable. Run what ever you need to explore the data and the questions.\n\n\n\n\n\n\n Data Skills - How many participants are included in the dataset?\n\n\n\n\n\nThere are 2402 participants in total - this is the number of rows in the dataset\n\n\n\n\n\n\n\n\n\n Data Skills - Is the dataset complete? do we have all information from everyone?\n\n\n\n\n\nNo, unfortunately some data is missing from some participants. You can see empty cells in quite a few places across the dataset. This means that some analyses may have an N of less than 2402 for analyses using some of these variables.\n\n\n\n\n\n\n\n\n\n Data Skills - Take a look at the configuration of the Age variable. Is it the correct type?\n\n\n\n\n\nJamovi has to make an educated guess at what type of data is included in each column. Looks like it has correctly set the age variable to be ‘Continuous’.\n\n\n\n\n\n\n\n\n\n Data Skills - How many participants do not have age information?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe N for the age descriptives is 2402 and there are 0 missing participants. This is a large sample and there is age information for everyone.\n\n\n\n\n\n\n\n\n\n\n Data Skills - What is the average and standard deviation of the age of the participants?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe mean age is 19.3 years, typical for a student population. The standard deviation is 3.81.\nThis means that a participant who is 19.3+3.81 = 23.11 years old would be one standard deviation above the average age of the sample.\n\n\n\n\n\n\n\n\n\n\n Data Skills - Do you think all the information in the age variable is accurate?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu. Try checking the box to show the 5 most extreme values in the dataset under the ‘Statistics -&gt; Outliers’ submenu.\nTake a close look at the variable and its descriptive statistics.\nThe youngest age in the dataset is -2 and the oldest age is 149! neither of these can be true and are probably typos (or jokes) made during data collection. Some other values are unusual but not impossible - the second youngest student is 13. We do not know what has happened during data collection to lead to this as this is not our dataset. We should be careful about interpreting the mean whilst these variables are included…\n\n\n\n\nThis process of exploring and understanding your data is a critical first step when working on a new analysis. Real data is rarely in perfect condition - we have to take some time and apply common sense to understand the data and what challenges there might be when analysing it.\nIf we do identify issues, such are large amounts of missing data, incorrect data types or mistakes in data entry, we can normally fix them - but only if they’ve been spotted! If we assume our data is perfect and go straight to the analysis then we could end up including a 149 year old student in our sample…\n\n\n\nIts probably not a good idea to include the data from the participants with impossible ages in our analysis. One simple step we can take is just to remove those rows from our analysis.\n\n\n\n\n\n\nData Skills - removing rows from a dataset\n\n\n\nRemove the two participants with impossible ages, you can use the extreme values table in the ‘Descriptives’ output to help find the row numbers. You can remove a row in Jamovi by right clicking on the row number (far left of the Data view) and clicking ‘Delete Row’.\n\nThis will not change the original data file (unless you export the data back out again..), it will only affect the view of the data loaded into Jamovi. If you make a mistake you can simply load the original file and start again.\nNotice that the bottom of the data viewer tells you how many rows have been deleted in that dataset.\n\n\n\n\n\n\n\n\nKey step\n\n\n\nNow you can recompute the descriptive statistics of your age variable and see what has changed.\n\n\n\nThe mean is pretty much the same - still 19.3 - but the standard deviation is much smaller now. It has reduced from 3.81 to 2.70. That is a close to 25% reduction in the estimate of the variability of this data - big difference. Including outliers like these can have a large impact on our analyses.\n\n\n\n\n\n\n Critical thinking - Are you happy with the data in the age variable now?\n\n\n\n\n\nTough question! take a look at the Minimum value - we still have one participant who is 13 years old. Whilst this is more likely than finding a -2 or 149 year old student, it is still extremely unlikely. It is harder to make a judgement for this participant than the others. The first two participants had impossible ages, whereas a 13 year old student is just very unlikely. Sometimes we have to make a judgement call.\nLet’s delete row 709 to remove this participant as well.\nThe recomputed descriptive statistics now has an N=2399. Removing the 13 year old hasn’t changed our mean or standard deviation estimates very much - indicating that it was less important to remove this data point than the first two.\nThe youngest participant is now 16 which is more realistic (though still very impressive for that 16 year old!).\n\n\n\nThis process can be intricate and involves a lot of clicking in Jamovi - it would be easy to make a mistake and not notice that something has gone wrong. We’ll explore some faster and more transparent ways to do this using R code during this course.\n\n\n\n\nOk, let’s move on to personality factors. As we said at the top, the big five factors are recorded on a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. So someone with an Extraversion score or 6 might be very outgoing where as someone with a score of 1 would be more of an introvert.\n\n\n\n\n\n\nKey step\n\n\n\nCompute the descriptive statistics for the big 5 personality factors.\nTake some time to consider the data quality - use the questions we answered with age as a guide for what to think about.\n\n\n\nNotice that we have a few missing data values in these variables. Jamovi will remove these from any analyses so they shouldn’t affect our descriptive statistics.\n\n\n\n\n\n\n Critical thinking - Are you happy to use the data in the big 5 personality factors?\n\n\n\n\n\nYes, these look reasonable. The data range matches what we would expect from the rating scales (values between 1 and 7) and there is only a small amount of missing data in each of the five variables.\nWe don’t need to do any further data cleaning based on these descriptive statistics.\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding 1\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nYou have reproduced the descriptive statistics for the big 5 personality factors. Now you can do the same for some of the other factors in Table 2 of section 3.5 in Ebersole et al. (2016).\nSpecifically - try and compute the mean and standard deviations for the following factors (Descriptions of the factors are the the cited section of Ebersole et al. (2016)):\n\nSelf Esteem (see section 3.5.1)\nDaily Mood (see section 3.5.3)\nNeed for cognition (see section 3.5.5)\n\nCan you find variables (columns) that correspond to these factors?\nNote: Open datasets sometimes use shorthand for variable names rather than the names used in the manuscript. This is not ideal but does happen in practice! we should be ready take a little time to explore and understand a new dataset when working with it.\n\n\nIn this case the variables in the dataset to use are:\n\n\n\nIn Manuscript\nIn Jamovi\n\n\n\n\nSelf Esteem\n‘SelfEsteem’\n\n\nDaily Mood.\n‘Mood’\n\n\nNeed for cognition\n‘NFC’\n\n\n\nTalk to your tutor if you’re getting stuck.\n\n\nYou should be able to reproduce the following table. Verify that the computed Mean and Standard Deviations match those published in table 2 of Ebersole et al (2016)\n\nThe final mean values are very close to the values in Ebersole et al. (2016) - but not identical. Looks like the original paper kept in all the data that had missing or unusual age values.\n\n\n\n\n\nNext, let’s think about the distributions of these variables. Its great to think about summarising a dataset with its mean value, but this doesn’t tell us about the wider distribution.\n\n\n\n\nNext we’ll have a look at computing histograms from this data. Remember that histograms divide the data range into intervals called bins. The choice of bin width and number of bins can affect the appearance and interpretation of the histogram. Most of the time we can use the automatic bin definitions provided by Jamovi.\nWe can use Histograms to explore a few key factors of the wider data distributions that are hard to see from the descriptive statistics tables.\n\nShape: The shape of the histogram can reveal the distribution pattern of the data, such as normal distribution, skewness, or bimodality.\nOutliers: Histograms can help identify outliers or unusual data points that fall outside the general pattern.\n\nJamovi makes it very simple to compute a histogram. Open the ‘Plots’ sub-menu of the descriptive statistics tab and select the ‘Histogram’ checkbox.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a histogram of the participant ages\n\n\n\nThe descriptives tables give us a numerical summary of the data - but this can be hard to build an intuitive understanding of the dataset. A histogram gives us something immediately interpretable on a more ‘human’ level. Have a think about the following questions.\n\n\n\n\n\n\n Data Skills - What is the average age of the pariticpants in this dataset?\n\n\n\n\n\nWe can see that the most common ages are clustered around the late teenage years and early 20s. We can’t tell the exact mean from the histogram alone - we need to compute the descriptives table for this - but histograms are good to get a quick ‘sense’ of the data.\n\n\n\n\n\n\n\n\n\n Data Skills - Do the ages look normally distributed?\n\n\n\n\n\nThere is a long tail to this distribution with a very small number of participants in their 30s, 40s and 50s. This is a strong positive skew and likely indicates that the data are not normally distributed.\nThis would be hard to tell from the descriptives table alone.\nOverall, this is a good match for what we would expect from a student population at a large university.\n\n\n\nWe can get an alternative perspective by visualising our data using a ‘Box Plot’.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a Box plot of the participant ages\n\n\n\nThe boxplot visualises the median and interquartile range (25th to 75th percentile) of the data, as well as highlighting outliers. This gives us less resolution around the middle of the distribution close to the median - but does give a much clearer view of the long tail in the ages of the student data sample.\n\n\n\n\n\n\n Data Skills - How old is the oldest participant in the dataset??\n\n\n\n\n\nThe most extreme outliers in the dataset are labelled with their row ID in the boxplot. The largest age value is in row 1081 - it looks like this is around 55 in the boxplot.\nWe can check the corresponding row in the data table to get some more precision. This tells us that the oldest participant is exactly 54 years old.\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding 2\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nCompute histograms and/or boxplots for the big-5 personality factors and take some notes their distributions. Think about the following questions for each factor:\n\nIs the distribution symmetric?\nIf not, is there a positive or a negative skew?\nAre there any outliers?\n\nWrite some quick notes on your answers - we’ll use these again in the next section!\n\n\nIt might be easiest to do this one factor at a time to avoid being swamped with figures!\nCreate the figure for a single factor, make some notes and make the next figure with the next factor. You can consider saving the figure into a notebook or document if you want to make a more controlled comparison.\n\n\nHere are my notes about each distribution:\n\nOpenness, this does not look like a normal distribution - there is a negative skew in the data.\nConscientiousness, this is definitely not a normal distribution - there is a strong negative skew.\nExtraversion, this is close to being a normal distribution - perhaps a very slight negative skew.\nAgreeableness, this does not look like a normal distribution - there is a negative skew in the data.\nNeuroticism, this does not look like a normal distribution - there is a positive skew in the data.\n\nNone of the factors have obvious outliers.\n\n\n\n\n\n\n\n\nAs a final step, we can see if our intuitions about the distribution of big-5 personality factors match with more formal statistics measures checking for normal distributions and skewness.\n\n\n\n\n\n\nKey step\n\n\n\nCompute statistics Shapero-Wilk’s test for normality and for ‘Skewness’. You can add both of these tests from the ‘Statistics’ sub-menu of the Descriptives analysis.\n\n\n\n\n\nThe Shapiro-Wilk test evaluates whether a sample comes from a normally distributed population. It returns 2 values:\n\n‘W’ The W statistic measures how well the data conforms to a normal distribution. It ranges from 0 to 1 where values close to 1 indicate the data is normally distributed.\n‘p’ is a p-value indicating the probability that we could see a value of W at least as large as the one we have measured if the data really were normally distributed. Typically, we would say that the data are not normally distributed if p is less than or equal to 0.05.\n\n\n\n\n\n\n\n Data Skills - Do our notes from the histograms match the Shapiro-Wilk statistics?\n\n\n\n\n\nYes, If we look at the W statistics, then we see that Extraversion has the highest value indicating that it has the most ‘normal’ data distribution whilst the other four factors are similar. This matches what we saw with the histograms.\nThe p-values from the formal hypothesis test indicate that none of the 5 factors have normal distributions. We couldn’t tell this from our informal evaulation of the histograms and box plots.\n\n\n\n\n\n\nThe skewness metric returns a number close to zero if the data are symmetrical around zero, and a number close to +1 or -1 if the data are skewed. The sign of the skewness value indicates whether there is a positive or a negative skew.\n\n\n\n\n\n\n Data Skills - Do our notes from the histograms match the skewness statistic?\n\n\n\n\n\nYes, its very close. We can see that ‘Extraversion’ has the least skew with a Skewness metric very close to zero (-0.0714). In contrast, ‘Conscientiousness’ has the most skew. Its value of ‘-0.656’ indicates a very substantial negative skewness in the distribution.\nOpenness and Agreeableness have moderate negative skews, whilst Neuroticism has a moderate positive skew.\n\n\n\n\n\n\n\nIn this session we have gone through the process of exploring a dataset from a published scientific article. We have computed a range of descriptive statistics and used histograms and boxplots to visualise the dataset.\nIt is common for publicly available data to contain little mistakes and issues. We need to go through a process like this with every new dataset and it should be like second nature by the end of this module. Remember to use common sense and practical thinking to identify issues and think about possible solutions.\nRemember this process when collecting your own data. Think carefully about how you can avoid typos and mistakes when collecting a dataset. It can save you and your collaborators a lot of time in future!\n\n\n\n\n\nBlock, Jack. 1995. “A Contrarian View of the Five-Factor Approach to Personality Description.” Psychological Bulletin 117 (2): 187–215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. “Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.” Journal of Experimental Social Psychology 67: 68–82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. “A Very Brief Measure of the Big-Five Personality Domains.” Journal of Research in Personality 37 (6): 504–28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#the-dataset",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#the-dataset",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "We will be exploring data that was collected as part of the ‘Many Labs 3’ project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‘Big Five’ personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs. extravagant/careless)\nAgreeableness (friendly/compassionate vs. critical/rational)\nNeuroticism (sensitive/nervous vs. resilient/confident)\nOpenness to experience (inventive/curious vs. consistent/cautious)\nExtraversion (outgoing/energetic vs. solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#the-challenge",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#the-challenge",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "We have data from a sample of students in a big data table. This raw data needs to be summarised and visualised so that we can turn this massive block of numbers into something intuitive.\nWe can generate a concise overview of the main characteristics of the data by computing and visualising few key features from the data.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#research-methods-b",
    "href": "index.html#research-methods-b",
    "title": "Research Methods Computer Practicals",
    "section": "",
    "text": "Week 1 : Descriptive Statistics, Data Processing and R\n\ntoc: true toc-depth: 2"
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#install-the-rj-add-on-in-jamovi",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#install-the-rj-add-on-in-jamovi",
    "title": "Week 1 : Descriptive Statistics, Data Processing and R",
    "section": "",
    "text": "Next you should install the Rj Add-On using this guide for installing Jamovi modules. Once installed you should have a new button on the Ribbon menu at the top of Jamovi…\n\nClick on this and select ‘Rj Editor’ to open a window in which we can write R code! Click on the small cog icon and make sure that ‘Output’ is set to ‘Show code and output’.\n\nWe can use the left-hand window, named Rj editor, to write commands, using the R language. These are the straightforward text commands that tell the computer a series of steps that we wanted to carry out. These commands can be edited and rerun as many times as we need, they can even be shared with collaborators and included as part of scientific publication.\nThis is a very powerful tool once we have a written set of instructions to work with rather than individually clicking through instructions. Though it takes a bit longer to write the commands, our analysis is much more transparent and reproducible once it is done.\nThe outputs of the analysis defined in the Rj editor appear on the right-hand side of the Jamovi window as usual. To check this out, let’s explore using or as a calculator.\n\n\n\n\n\n\nData Skills - R as a calculator\n\n\n\nTry typing a set of sums or equations into the Rj editor. Include one sum on each line and include as many as you like. When you’re ready, you can tell RJ to run the commands by clicking on the green triangle in the top right corner of the RJ editor. This will automatically compute. All of the sums you to find and print the results out in the right hand side window. see the example below.\nYou can use the following symbols to define your sums.\n\n\n\nSymbol\nOperation\n\n\n\n\n+\nAdd\n\n\n-\nSubtract\n\n\n*\nMultiply\n\n\n/\nDivide"
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#functions-and-variables",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#functions-and-variables",
    "title": "Week 1 : Descriptive Statistics, Data Processing and R",
    "section": "",
    "text": "Notice that the one of the equations in my example is a bit different to the others. The final example uses a function called sqrt as a shortcut to compute the square root of the number nine.\n\n\n\n\n\n\nDefinition of a function\n\n\n\nFunctions are used to perform specific, predefined tasks, such as calculations, data transformations, or visualization. You can think of it as a “black box” that takes in a value, performs some internal calculation, and returns the result.\n\n\nThe sqrt function calculates the square root of a given number x.In R code, this looks something like this:\nsqrt(x)\nwith the ‘x’ replaced by any number. For example, we could compute the square root of 36 using the following command.\nsqrt(36)\nAdd this line to your Rj editor and check the result..\nMany functions, take inputs contained in parentheses written after the name of the function. These inputs are sometimes known as the arguments of a function. In the last example, 36 is an argument for the function sqrt.\nSometimes functions take several arguments, and the arguments themselves can sometimes have specific names. For example.\nround(12.345, digits = 0)\nwill round the value x to a whole number with zero digits after the decimal point. Where as\nround(12.345, digits = 2)\nwould round the same number to preserve 2 digits after the decimal point. Try running these examples in Rj with a range of different numbers to get a sense of how it works.\nOften, we don’t just want to see the result of some maths or a function call, we want to store the result to use again later. For this, we want to use variables. For example, here we store the result of a calculator sum in a variable named ‘y’\ny &lt;- 2 + 2\nIf you run this in Rj, you’ll notice that we no longer get the number 4 printed to the screen. This is as the output is now caught and stored rather than printed out. If we still want to see what is contained in y, we can use another helpful function named print.\ny &lt;- 2 + 2\nprint(y)\nWill now also display the content of y.\n\n\n\n\n\n\nDefinition of a variable\n\n\n\nA variable is an object that provides a labelled space to store information. The &lt;- or = symbols indicate that we’re assigning some information to a variable (the two symbols behave the same way, you could see either).\nCrucially, the information on the right hand side is assigned to a variable with the name from the left hand side.\n\n\nA single analysis might have several lines of code that create, store and reuse a several different variables with several different functions. For example:\n\n\n\n\n\n\n\n\nCheck Your Understanding 2\n\n\n\n\nQuestionHintSolution\n\n\nWhat does this R code do? what is the value of x, y and z?\n x &lt;- 10 + 10\n y &lt;- sqrt(16)\n z &lt;- x / y\n\n\nTry copying out and running the code in Rj.\nYou can add some print functions to see the value of the intermediate variables x and y as well. Try adding print(x) on the line immediately after x is defined.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final answers:\nx is 20 and y is 4, so z = 20 / 4 which is 5.\n\n\n\n\n\n\n\nRj has a special feature that the data loaded into our spreadsheet is always contained in a special variable named data. We can refer to this data variable to view and run analyses on our dataset."
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#hint-1",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#hint-1",
    "title": "Week 1 : Descriptive Statistics, Data Processing and R",
    "section": "",
    "text": "Try copying out and running the code in Rj.\nYou can add some ‘print’ functions to see the value of the intermediate variables x and y as well. Try adding ‘print(x)’ on the line immediately after x is defined.\nTalk to your tutor if you’re getting stuck."
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#solution-1",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#solution-1",
    "title": "Week 1 : Descriptive Statistics, Data Processing and R",
    "section": "",
    "text": "The final answer is z = 5!\nx is 20 and y is 4, so z = 20 / 4 which is 5."
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#compute-descriptive-statistics-using-r",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#compute-descriptive-statistics-using-r",
    "title": "Week 1 : Descriptive Statistics, Data Processing and R",
    "section": "",
    "text": "Ok, back to statistics!\nHow can we use our new R skills to compute statistics?\nhe trick is to understand that all the analyses in Jamovi are actually carried out by functions in R. Each analysis we can select from the top ribbon in Jamovi corresponds to a function that can be called in the code. Similarly, all of the options that we select in the analysis, dialogue boxes, correspond to arguments for that function. Let’s take a look in practice.\nTo help understand this Jamovi provides an “R Syntax Mode”, in this mode, Jamovi produces equivalent R code for each analysis. To change to syntax mode, select the application menu (⋮) at the top right of Jamovi, and check the Syntax mode checkbox there.\n\n\n\n\n\n\nKey step\n\n\n\nWith syntax mode on, compute the descriptive statistics for the big five personality factors again.\n\n\nThe outputs should look like this:\n\nNotice that some R code has appeared at the top of the output table. This is relatively complex compared to our previous functions like print or sqrt. Let’s break it down a bit.\n\n\n\n\n\n\nData Skills - Understanding the descriptives function\n\n\n\nThis is the code that Jamovi’s syntax mode will produce (with a little formatting to make the appearance simpler). Take a look through each part to understand what it does. Click the number in the annotation to highlight the corresponding part of the code.\n1jmv::descriptives(\n2    data = data,\n3    vars = vars(Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism)\n    )\n\n1\n\nThis is the name of the function that will do the computation for us. Everything contained within these parentheses are known as arguments, or inputs, for the function\n\n2\n\nThe first input is named data and is assigned to be the special data variable that contains the data loaded into our spreadsheet.\n\n3\n\nThe second input is a list of the data columns in our spreadsheet that we want to compute descriptives from.\n\n\n\n\nThere is a lot of information here that might be hard to understand at first. Try changing the analysis options using the Jamovi buttons and watch what happens to the R syntax. In particular, try adding and removing different variables to the analysis.\n\n\n\n\n\n\nKey step\n\n\n\nOnce you’re ready, you can right click on this R syntax and click ‘Copy’ to copy it to your clipboard.\n\nand pasting it into an Rj window - now you can click run to reproduce your descriptive statistics table using the underlying R code.\n\n\nGreat - well done on completing your first data analysis using R code! You should have the following output that will match the descriptives you’ve seen in the main Jamovi window.\n\nHead back over to the Jamovi descriptives tab and try updating your analysis. You could change the variables that you are computing the descriptive statistics from, add in some extra metrics (such as the Skewness or Shapiro-Wilk) or even generate a plot like a histogram.\nNote that the R code in the syntax box of the output will stay up to date with your changes. Pay attention to how the function updates when you select different options.\n\n\n\n\n\n\n\nCheck Your Understanding 3\n\n\n\n\nQuestionHintSolution\n\n\nWhat is the R code required to compute descriptive statistics from the Openness and Conscientiousness variables, whilst including the Skewness metric, the Shapiro-Wilk statistic and a histogram?\nCan you run this analysis in Rj?\n\n\nRemember that the R code in the syntax mode will update as you define your analysis in Jamovi. Get the analysis right there and you can simply copy over the code.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final code should look like this\njmv::descriptives(\n    data = data,\n    vars = vars(Openness, Conscientiousness),\n    hist = TRUE,\n    skew = TRUE,\n    sw = TRUE)"
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#descriptive-statistics-in-jamovi",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#descriptive-statistics-in-jamovi",
    "title": "Week 1 : Descriptive Statistics, Data Processing and R",
    "section": "",
    "text": "Note\n\n\n\nThis section will have some revision from RM-A, please check your notes from RM-A for help with loading data and handling variables.\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file ML3AllSites.csv from canvas.\n\nThis is the original dataset provided by the authors of the paper! It is a massive dataset - take a moment to get familiar with it by answering the following questions:\n\n\n\n\n\n\nData Skills - How many participants are included in the dataset?\n\n\n\n\n\nThere are 2998 participants in total - this is the number of rows in the dataset\n\n\n\n\n\n\n\n\n\nData Skills - Is the dataset complete? do we have all information from everyone?\n\n\n\n\n\nNo, unfortunately some data is missing from some participants. You can see empty cells in quite a few places across the dataset. This means that some analyses may have an N of less than 2998.\n\n\n\n\n\n\n\n\n\nData Skills - Take a look at the configuration of the Age variable. Is it the correct type?\n\n\n\n\n\nJamovi has to make an educated guess at what type of data is included in each column. Looks like its made a mistake here and set age to be ‘Categorical’.\nChange the data-type to be ‘Continuous’ instead.\n\n\n\n\n\n\n\n\n\nData Skills - What is the average and standard deviation of the age of the participants?\n\n\n\n\n\nThe mean age is 19.3 years, typical for a student population. The standard deviation is 3.81.\nThis means that a participant who is 19.3+3.81 = 23.11 years old would be one standard deviation above the average age of the sample.\n\n\n\n\n\n\n\n\n\nData Skills - How many participants do not have age information?\n\n\n\n\n\n596 participants do not have information about age in the dataset! These might have not been collected, mixed up due to an error or lost during data processing.\n2402 participants still do have age information. This is still a large sample.\nThe good news is that Jamovi detects this missing data and removes it for us when computing descriptive statistics. The estimate of the mean won’t be affected by this missing data.\n\n\n\n\n\n\n\n\n\nData Skills - Do you think all the information in the age variable is accurate?\n\n\n\n\n\nTake a close look at the variable and its descriptive statistics.\nThe youngest age in the dataset is -2 and the oldest age is 149! both of these cannot be true and are probably typos made during data collection. We should be careful about interpreting the mean whilst these variables are included…\n\n\n\nThis process of exploring and understanding your data is a critical first step when working on a new analysis. Real data is rarely in perfect condition - we have to take some time and apply common sense to understand the data and what challenges there might be when analysing it.\nIf we do identify issues, such are large amounts of missing data, incorrect data types or mistakes in data entry, we can normally fix them - but only if they’ve been spotted! If we assume our data is perfect and go straight to the analysis then we could end up including a 149 year old student in our sample…"
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#data-cleaning",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#data-cleaning",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Its probably not a good idea to include the data from the participants with impossible ages in our analysis. One simple step we can take is just to remove those rows from our analysis.\n\n\n\n\n\n\nData Skills - removing rows from a dataset\n\n\n\nRemove the two participants with impossible ages, you can use the extreme values table in the ‘Descriptives’ output to help find the row numbers. You can remove a row in Jamovi by right clicking on the row number (far left of the Data view) and clicking ‘Delete Row’.\n\nThis will not change the original data file (unless you export the data back out again..), it will only affect the view of the data loaded into Jamovi. If you make a mistake you can simply load the original file and start again.\nNotice that the bottom of the data viewer tells you how many rows have been deleted in that dataset.\n\n\n\n\n\n\n\n\nKey step\n\n\n\nNow you can recompute the descriptive statistics of your age variable and see what has changed.\n\n\n\nThe mean is pretty much the same - still 19.3 - but the standard deviation is much smaller now. It has reduced from 3.81 to 2.70. That is a close to 25% reduction in the estimate of the variability of this data - big difference. Including outliers like these can have a large impact on our analyses.\n\n\n\n\n\n\n Critical thinking - Are you happy with the data in the age variable now?\n\n\n\n\n\nTough question! take a look at the Minimum value - we still have one participant who is 13 years old. Whilst this is more likely than finding a -2 or 149 year old student, it is still extremely unlikely. It is harder to make a judgement for this participant than the others. The first two participants had impossible ages, whereas a 13 year old student is just very unlikely. Sometimes we have to make a judgement call.\nLet’s delete row 709 to remove this participant as well.\nThe recomputed descriptive statistics now has an N=2399. Removing the 13 year old hasn’t changed our mean or standard deviation estimates very much - indicating that it was less important to remove this data point than the first two.\nThe youngest participant is now 16 which is more realistic (though still very impressive for that 16 year old!).\n\n\n\nThis process can be intricate and involves a lot of clicking in Jamovi - it would be easy to make a mistake and not notice that something has gone wrong. We’ll explore some faster and more transparent ways to do this using R code during this course.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#descriptive-statistics-and-data-cleaning-in-jamovi",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#descriptive-statistics-and-data-cleaning-in-jamovi",
    "title": "Week 1 : Descriptive Statistics, Data Processing and R",
    "section": "",
    "text": "Key step\n\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file ML3AllSites_RMB.csv from canvas.\n\n\n\nThis is the original dataset provided by the authors of the paper! It is a massive dataset - take a moment to get familiar with it by answering the following questions.\nSome of the questions will require you to run a descriptive statistics analysis on the age variable. Run what ever you need to explore the data and the questions.\n\n\n\n\n\n\n Data Skills - How many participants are included in the dataset?\n\n\n\n\n\nThere are 2402 participants in total - this is the number of rows in the dataset\n\n\n\n\n\n\n\n\n\n Data Skills - Is the dataset complete? do we have all information from everyone?\n\n\n\n\n\nNo, unfortunately some data is missing from some participants. You can see empty cells in quite a few places across the dataset. This means that some analyses may have an N of less than 2402 for analyses using some of these variables.\n\n\n\n\n\n\n\n\n\n Data Skills - Take a look at the configuration of the Age variable. Is it the correct type?\n\n\n\n\n\nJamovi has to make an educated guess at what type of data is included in each column. Looks like it has correctly set the age variable to be ‘Continuous’.\n\n\n\n\n\n\n\n\n\n Data Skills - How many participants do not have age information?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe N for the age descriptives is 2402 and there are 0 missing participants. This is a large sample and there is age information for everyone.\n\n\n\n\n\n\n\n\n\n\n Data Skills - What is the average and standard deviation of the age of the participants?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe mean age is 19.3 years, typical for a student population. The standard deviation is 3.81.\nThis means that a participant who is 19.3+3.81 = 23.11 years old would be one standard deviation above the average age of the sample.\n\n\n\n\n\n\n\n\n\n\n Data Skills - Do you think all the information in the age variable is accurate?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu. Try checking the box to show the 5 most extreme values in the dataset under the ‘Statistics -&gt; Outliers’ submenu.\nTake a close look at the variable and its descriptive statistics.\nThe youngest age in the dataset is -2 and the oldest age is 149! neither of these can be true and are probably typos (or jokes) made during data collection. Some other values are unusual but not impossible - the second youngest student is 13. We do not know what has happened during data collection to lead to this as this is not our dataset. We should be careful about interpreting the mean whilst these variables are included…\n\n\n\n\nThis process of exploring and understanding your data is a critical first step when working on a new analysis. Real data is rarely in perfect condition - we have to take some time and apply common sense to understand the data and what challenges there might be when analysing it.\nIf we do identify issues, such are large amounts of missing data, incorrect data types or mistakes in data entry, we can normally fix them - but only if they’ve been spotted! If we assume our data is perfect and go straight to the analysis then we could end up including a 149 year old student in our sample…"
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#big-5-personality-factors",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#big-5-personality-factors",
    "title": "Week 1 : Descriptive Statistics, Data Processing and R",
    "section": "",
    "text": "Ok, let’s move on to personality factors. As we said at the top, the big five factors are recorded on a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. So someone with an Extraversion score or 6 might be very outgoing where as someone with a score of 1 would be more of an introvert.\n\n\n\n\n\n\nKey step\n\n\n\nCompute the descriptive statistics for the big 5 personality factors.\nTake some time to consider the data quality - use the questions we answered with age as a guide for what to think about.\n\n\nYou should get a descriptive statistics table that looks like this.\n\nNotice that we have a few missing data values in these variables. Jamovi will remove these from any analyses so they shouldn’t affect our descriptive statistics.\n\n\n\n\n\n\n Critical thinking - Are you happy to use the data in the big 5 personality factors?\n\n\n\n\n\nYes, these look reasonable. The data range matches what we would expect from the rating scales (values between 1 and 7) and there is only a small amount of missing data in each of the five variables.\nWe don’t need to do any further data cleaning based on these descriptive statistics.\n\n\n\ntesting123\n\n\n\n\n\n\n\nCheck Your Understanding 1\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nYou have reproduced the descriptive statistics for the big 5 personality factors. Now you can do the same for some of the other factors in Table 2 of section 3.5 in Ebersole et al. (2016).\nSpecifically - try and compute the mean and standard deviations for:\n\nSelf Esteem (see section 3.5.1 for details)\nPerceived Stress (see section 3.5.4)\nNeed for cognition (see section 3.5.5) Can you find variables (columns) that correspond to these factors?\n\nNote: Open datasets sometimes use shorthand for variable names rather than the names used in the manuscript. This is not ideal but does happen in practice! we should be ready take a little time to explore and understand a new dataset when working with it.\n\n\nIn this case the variables in the dataset to use are:\n\n\n\nIn Manuscript\nIn Jamovi\n\n\n\n\nSelf Esteem\n‘SelfEsteem’\n\n\nPerceived Stress\n‘Stress’\n\n\nNeed for cognition\n‘NFC’\n\n\n\nTalk to your tutor if you’re getting stuck.\n\n\nYou should be able to reproduce the following table. Verify that the computed Mean and Standard Deviations match those published in table 2 of Ebersole et al (2016)\n\n\n\n\n\n\nNext, let’s think about the distributions of these variables. Its great to think about summarising a dataset with its mean value, but this doesn’t tell us about the wider distribution."
  },
  {
    "objectID": "tutorials/rmb_week-2_stats-in-Rj.html",
    "href": "tutorials/rmb_week-2_stats-in-Rj.html",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "This week we will explore how to use Jamovi to compute and visualise descriptive statistics from a dataset. This will include some revision on Jamovi from last semester and an introduction on how we can use the R coding language to simplify complex data processing.\n\nLearning objectives for this session\n\n\n\nQuantitative Methods\n\n\n\n\n\nMeasures of central tendency\n\n\n\nMeasures of dispersion and spread\n\n\n\nMeasures of skew and normality\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Rj editor to use R code within Jamovi\n\n\n\nLearn the basics about functions and variables in R\n\n\n\nComputing descriptive statistics using R\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nValidating and working with openly available research data\n\n\n\n\n\n\n\n\n\n\n\nDataset\n\n\n\nNote that this is the same dataset as in week 1\n\n\nWe will be exploring data that was collected as part of the ‘Many Labs 3’ project (Ebersole et al., 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‘Big Five’ personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs. extravagant/careless)\nAgreeableness (friendly/compassionate vs. critical/rational)\nNeuroticism (sensitive/nervous vs. resilient/confident)\nOpenness to experience (inventive/curious vs. consistent/cautious)\nExtraversion (outgoing/energetic vs. solitary/reserved)\n\nThis is a very simple (Gosling et al., 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block, 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.\n\n\n\nLast week, we looked at how we can spend some time understanding a new dataset and compute some descriptive statistics from it.\nNow, we’re going to learn how to use Jamovi and R to make this process more repeatable and more transparent. This will make it much easier for us to come back to the analysis and understand what steps were taken at a later date.\n\n\n\nFirst, you should install the Rj Add-On using this guide for installing Jamovi modules. Once installed you should have a new button on the Ribbon menu at the top of Jamovi…\n\nClick on this and select ‘Rj Editor’ to open a window in which we can write R code! Click on the small cog icon and make sure that ‘Output’ is set to ‘Show code and output’.\n\nWe can use the left-hand window, named Rj editor, to write commands, using the R language. These are the straightforward text commands that tell the computer a series of steps that we wanted to carry out. These commands can be edited and rerun as many times as we need, they can even be shared with collaborators and included as part of scientific publication.\nThis is a very powerful tool once we have a written set of instructions to work with rather than individually clicking through instructions. Though it takes a bit longer to write the commands, our analysis is much more transparent and reproducible once it is done.\nThe outputs of the analysis defined in the Rj editor appear on the right-hand side of the Jamovi window as usual. To check this out, let’s explore using or as a calculator.\n\n\n\n\n\n\nData Skills - R as a calculator\n\n\n\nTry typing a set of sums or equations into the Rj editor. Include one sum on each line and include as many as you like. When you’re ready, you can tell RJ to run the commands by clicking on the green triangle in the top right corner of the RJ editor. This will automatically compute. All of the sums you to find and print the results out in the right hand side window. see the example below.\nYou can use the following symbols to define your sums.\n\n\n\nSymbol\nOperation\n\n\n\n\n+\nAdd\n\n\n-\nSubtract\n\n\n*\nMultiply\n\n\n/\nDivide\n\n\n\n\n\n\n\n\n\n\nNotice that the one of the equations in my example is a bit different to the others. The final example uses a function called sqrt as a shortcut to compute the square root of the number nine.\n\n\n\n\n\n\nDefinition of a function\n\n\n\nFunctions are used to perform specific, predefined tasks, such as calculations, data transformations, or visualization. You can think of it as a “black box” that takes in a value, performs some internal calculation, and returns the result.\n\n\nThe sqrt function calculates the square root of a given number. In R code, this looks something like this:\nsqrt(x)\nwith the ‘x’ replaced by any number. For example, we could compute the square root of 36 using the following command.\nsqrt(36)\nAdd this line to your Rj editor and check the result..\nMany functions, take inputs contained in parentheses written after the name of the function. These inputs are sometimes known as the arguments of a function. In the last example, 36 is an argument for the function sqrt.\nSometimes functions take several arguments, and the arguments themselves can have specific names. For example.\nround(12.345, digits = 0)\nwill round the value x to a whole number with zero digits after the decimal point. Where as\nround(12.345, digits = 2)\nwould round the same number to preserve 2 digits after the decimal point. Try running these examples in Rj with a range of different numbers to get a sense of how it works.\nOften, we don’t just want to see the result of some maths or a function call, we want to store the result to use again later. For this, we want to use variables. For example, here we store the result of a calculator sum in a variable named ‘y’\ny &lt;- 2 + 2\nIf you run this in Rj, you’ll notice that we no longer get the number 4 printed to the screen. This is as the output is now caught and stored rather than printed out. If we still want to see what is contained in y, we can use another helpful function named print.\ny &lt;- 2 + 2\nprint(y)\nWill now also display the content of y.\n\n\n\n\n\n\nDefinition of a variable\n\n\n\nA variable is an object that provides a labelled space to store information. The &lt;- or = symbols indicate that we’re assigning some information to a variable (the two symbols behave the same way, you could see either).\nCrucially, the information on the right hand side is assigned to a variable with the name from the left hand side.\n\n\nA single analysis might have several lines of code that create, store and reuse a several different variables with several different functions. For example:\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat does this R code do? what is the value of x, y and z?\n x &lt;- 10 + 10\n y &lt;- sqrt(16)\n z &lt;- x / y\n\n\nTry copying out and running the code in Rj.\nYou can add some print functions to see the value of the intermediate variables x and y as well. Try adding print(x) on the line immediately after x is defined.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final answers:\nx is 20 and y is 4, so z = 20 / 4 which is 5.\n\n\n\n\n\n\n\nRj has a special feature that the data loaded into our spreadsheet is always contained in a special variable named data. We can refer to this data variable to view and run analyses on our dataset.\n\n\n\nOk, back to statistics! How can we use our new R skills to compute statistics?\nThe trick is to understand that all the analyses in Jamovi are actually carried out by functions in R. Each analysis we can select from the top ribbon in Jamovi corresponds to a function that can be called in the code. Similarly, all of the options that we select in the analysis, dialogue boxes, correspond to arguments for that function. Let’s take a look in practice.\nTo help understand this Jamovi provides an “R Syntax Mode”, in this mode, Jamovi produces equivalent R code for each analysis. To change to syntax mode, select the application menu (⋮) at the top right of Jamovi, and check the Syntax mode checkbox there.\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to load the data file into Jamovi.\nWith syntax mode on, compute the descriptive statistics for the big five personality factors again.\n\n\nThe outputs should look like this:\n\nNotice that some R code has appeared at the top of the output table. This is relatively complex compared to our previous functions like print or sqrt. Let’s break it down a bit.\n\n\n\n\n\n\nData Skills - Understanding the descriptives function\n\n\n\nThis is the code that Jamovi’s syntax mode will produce (with a little formatting to make the appearance simpler). Take a look through each part to understand what it does. Click the number in the annotation to highlight the corresponding part of the code.\n1jmv::descriptives(\n2    data = data,\n3    vars = vars(Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism)\n    )\n\n1\n\nThis is the name of the function that will do the computation for us. Everything contained within these parentheses are known as arguments, or inputs, for the function\n\n2\n\nThe first input is named data and is assigned to be the special data variable that contains the data loaded into our spreadsheet.\n\n3\n\nThe second input is a list of the data columns in our spreadsheet that we want to compute descriptives from.\n\n\n\n\nThere is a lot of information here that might be hard to understand at first. Try changing the analysis options using the Jamovi buttons and watch what happens to the R syntax. In particular, try adding and removing different variables to the analysis.\n\n\n\n\n\n\nKey step\n\n\n\nOnce you’re ready, you can right click on this R syntax and click ‘Copy’ to copy it to your clipboard.\n\nand pasting it into an Rj window - now you can click run to reproduce your descriptive statistics table using the underlying R code.\n\n\nGreat - well done on completing your first data analysis using R code! You should have the following output that will match the descriptives you’ve seen in the main Jamovi window.\n\nHead back over to the Jamovi descriptives tab and try updating your analysis. You could change the variables that you are computing the descriptive statistics from, add in some extra metrics (such as the Skewness or Shapiro-Wilk) or even generate a plot like a histogram.\nNote that the R code in the syntax box of the output will stay up to date with your changes. Pay attention to how the function updates when you select different options.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat is the R code required to compute descriptive statistics from the Openness and Conscientiousness variables, whilst including the Skewness metric, the Shapiro-Wilk statistic and a histogram?\nCan you run this analysis in Rj?\n\n\nRemember that the R code in the syntax mode will update as you define your analysis in Jamovi. Get the analysis right there and you can simply copy over the code.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final code should look like this\njmv::descriptives(\n    data = data,\n    vars = vars(Openness, Conscientiousness),\n    hist = TRUE,\n    skew = TRUE,\n    sw = TRUE)\n\n\n\n\n\n\n\n\n\nRemember that last week we very carefully removed several participants who had entered unrealisic or impossible ages into the datasets. This may have been an accident or mistake but either way we should not use this data.\nR and Rj make it very easy to clean data without the boring, manual work we did last week. R can do the hard work and we can avoid scrolling through very large datasets by hand. Sounds good!\nThe first thing we can do is investigate what range of responses that people actually gave when asked about their age. To do this we can use a new function called unique(). This takes a column of a dataset as an input and returns a list of all the unique responses that it contains.\n\n\n\n\n\n\nKey step\n\n\n\nLoad up an Rj Editor window (or continue in your old one) and run unique() on the age data.\nWe can access a single column of a dataset using the doller sign $. In this case data$age accesses the age column of the data sheet.\n\n\nThe result should look like this:\n\nWhat do you think?\n\n\n\n\n\n\nData Skills - Evaluating the age column\n\n\n\n\n\nOk, most of these responses are fine but there are a lot of strange ones.\n\nFirstly, we can find the extreme values that we saw last week (-2 and 149) - these are clearly mistakes (or some kind of weird prank).\nNext, we can see a lot of people adding text (eg ‘almost 19’). This is not helpful for the computer though it can be socially useful. The computer will read this in words and not numbers. These responses with additional text will count as missing data for any numerical analysis we do.\n&lt;NA&gt; means that the data value is simply missing - perhaps these participants skipped the question.\n\nWe caught some of these issues last time but likely not all. It would be a huge effort to comb through the data line-by-line and identify all these issues."
  },
  {
    "objectID": "tutorials/rmb_week-2_stats-in-Rj.html#the-challenge",
    "href": "tutorials/rmb_week-2_stats-in-Rj.html#the-challenge",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Last week, we looked at how we can spend some time understanding a new dataset and compute some descriptive statistics from it.\nNow, we’re going to learn how to use Jamovi and R to make this process more repeatable and more transparent. This will make it much easier for us to come back to the analysis and understand what steps were taken at a later date."
  },
  {
    "objectID": "tutorials/rmb_week-2_stats-in-Rj.html#the-dataset",
    "href": "tutorials/rmb_week-2_stats-in-Rj.html#the-dataset",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Dataset\n\n\n\nNote that this is the same dataset as in week 1\n\n\nWe will be exploring data that was collected as part of the ‘Many Labs 3’ project (Ebersole et al., 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‘Big Five’ personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs. extravagant/careless)\nAgreeableness (friendly/compassionate vs. critical/rational)\nNeuroticism (sensitive/nervous vs. resilient/confident)\nOpenness to experience (inventive/curious vs. consistent/cautious)\nExtraversion (outgoing/energetic vs. solitary/reserved)\n\nThis is a very simple (Gosling et al., 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block, 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5."
  },
  {
    "objectID": "tutorials/rmb_week-2_stats-in-Rj.html#install-the-rj-add-on-in-jamovi",
    "href": "tutorials/rmb_week-2_stats-in-Rj.html#install-the-rj-add-on-in-jamovi",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "First, you should install the Rj Add-On using this guide for installing Jamovi modules. Once installed you should have a new button on the Ribbon menu at the top of Jamovi…\n\nClick on this and select ‘Rj Editor’ to open a window in which we can write R code! Click on the small cog icon and make sure that ‘Output’ is set to ‘Show code and output’.\n\nWe can use the left-hand window, named Rj editor, to write commands, using the R language. These are the straightforward text commands that tell the computer a series of steps that we wanted to carry out. These commands can be edited and rerun as many times as we need, they can even be shared with collaborators and included as part of scientific publication.\nThis is a very powerful tool once we have a written set of instructions to work with rather than individually clicking through instructions. Though it takes a bit longer to write the commands, our analysis is much more transparent and reproducible once it is done.\nThe outputs of the analysis defined in the Rj editor appear on the right-hand side of the Jamovi window as usual. To check this out, let’s explore using or as a calculator.\n\n\n\n\n\n\nData Skills - R as a calculator\n\n\n\nTry typing a set of sums or equations into the Rj editor. Include one sum on each line and include as many as you like. When you’re ready, you can tell RJ to run the commands by clicking on the green triangle in the top right corner of the RJ editor. This will automatically compute. All of the sums you to find and print the results out in the right hand side window. see the example below.\nYou can use the following symbols to define your sums.\n\n\n\nSymbol\nOperation\n\n\n\n\n+\nAdd\n\n\n-\nSubtract\n\n\n*\nMultiply\n\n\n/\nDivide"
  },
  {
    "objectID": "tutorials/rmb_week-2_stats-in-Rj.html#functions-and-variables",
    "href": "tutorials/rmb_week-2_stats-in-Rj.html#functions-and-variables",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Notice that the one of the equations in my example is a bit different to the others. The final example uses a function called sqrt as a shortcut to compute the square root of the number nine.\n\n\n\n\n\n\nDefinition of a function\n\n\n\nFunctions are used to perform specific, predefined tasks, such as calculations, data transformations, or visualization. You can think of it as a “black box” that takes in a value, performs some internal calculation, and returns the result.\n\n\nThe sqrt function calculates the square root of a given number. In R code, this looks something like this:\nsqrt(x)\nwith the ‘x’ replaced by any number. For example, we could compute the square root of 36 using the following command.\nsqrt(36)\nAdd this line to your Rj editor and check the result..\nMany functions, take inputs contained in parentheses written after the name of the function. These inputs are sometimes known as the arguments of a function. In the last example, 36 is an argument for the function sqrt.\nSometimes functions take several arguments, and the arguments themselves can have specific names. For example.\nround(12.345, digits = 0)\nwill round the value x to a whole number with zero digits after the decimal point. Where as\nround(12.345, digits = 2)\nwould round the same number to preserve 2 digits after the decimal point. Try running these examples in Rj with a range of different numbers to get a sense of how it works.\nOften, we don’t just want to see the result of some maths or a function call, we want to store the result to use again later. For this, we want to use variables. For example, here we store the result of a calculator sum in a variable named ‘y’\ny &lt;- 2 + 2\nIf you run this in Rj, you’ll notice that we no longer get the number 4 printed to the screen. This is as the output is now caught and stored rather than printed out. If we still want to see what is contained in y, we can use another helpful function named print.\ny &lt;- 2 + 2\nprint(y)\nWill now also display the content of y.\n\n\n\n\n\n\nDefinition of a variable\n\n\n\nA variable is an object that provides a labelled space to store information. The &lt;- or = symbols indicate that we’re assigning some information to a variable (the two symbols behave the same way, you could see either).\nCrucially, the information on the right hand side is assigned to a variable with the name from the left hand side.\n\n\nA single analysis might have several lines of code that create, store and reuse a several different variables with several different functions. For example:\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat does this R code do? what is the value of x, y and z?\n x &lt;- 10 + 10\n y &lt;- sqrt(16)\n z &lt;- x / y\n\n\nTry copying out and running the code in Rj.\nYou can add some print functions to see the value of the intermediate variables x and y as well. Try adding print(x) on the line immediately after x is defined.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final answers:\nx is 20 and y is 4, so z = 20 / 4 which is 5.\n\n\n\n\n\n\n\nRj has a special feature that the data loaded into our spreadsheet is always contained in a special variable named data. We can refer to this data variable to view and run analyses on our dataset."
  },
  {
    "objectID": "tutorials/rmb_week-2_stats-in-Rj.html#compute-descriptive-statistics-using-r",
    "href": "tutorials/rmb_week-2_stats-in-Rj.html#compute-descriptive-statistics-using-r",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Ok, back to statistics! How can we use our new R skills to compute statistics?\nThe trick is to understand that all the analyses in Jamovi are actually carried out by functions in R. Each analysis we can select from the top ribbon in Jamovi corresponds to a function that can be called in the code. Similarly, all of the options that we select in the analysis, dialogue boxes, correspond to arguments for that function. Let’s take a look in practice.\nTo help understand this Jamovi provides an “R Syntax Mode”, in this mode, Jamovi produces equivalent R code for each analysis. To change to syntax mode, select the application menu (⋮) at the top right of Jamovi, and check the Syntax mode checkbox there.\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to load the data file into Jamovi.\nWith syntax mode on, compute the descriptive statistics for the big five personality factors again.\n\n\nThe outputs should look like this:\n\nNotice that some R code has appeared at the top of the output table. This is relatively complex compared to our previous functions like print or sqrt. Let’s break it down a bit.\n\n\n\n\n\n\nData Skills - Understanding the descriptives function\n\n\n\nThis is the code that Jamovi’s syntax mode will produce (with a little formatting to make the appearance simpler). Take a look through each part to understand what it does. Click the number in the annotation to highlight the corresponding part of the code.\n1jmv::descriptives(\n2    data = data,\n3    vars = vars(Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism)\n    )\n\n1\n\nThis is the name of the function that will do the computation for us. Everything contained within these parentheses are known as arguments, or inputs, for the function\n\n2\n\nThe first input is named data and is assigned to be the special data variable that contains the data loaded into our spreadsheet.\n\n3\n\nThe second input is a list of the data columns in our spreadsheet that we want to compute descriptives from.\n\n\n\n\nThere is a lot of information here that might be hard to understand at first. Try changing the analysis options using the Jamovi buttons and watch what happens to the R syntax. In particular, try adding and removing different variables to the analysis.\n\n\n\n\n\n\nKey step\n\n\n\nOnce you’re ready, you can right click on this R syntax and click ‘Copy’ to copy it to your clipboard.\n\nand pasting it into an Rj window - now you can click run to reproduce your descriptive statistics table using the underlying R code.\n\n\nGreat - well done on completing your first data analysis using R code! You should have the following output that will match the descriptives you’ve seen in the main Jamovi window.\n\nHead back over to the Jamovi descriptives tab and try updating your analysis. You could change the variables that you are computing the descriptive statistics from, add in some extra metrics (such as the Skewness or Shapiro-Wilk) or even generate a plot like a histogram.\nNote that the R code in the syntax box of the output will stay up to date with your changes. Pay attention to how the function updates when you select different options.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat is the R code required to compute descriptive statistics from the Openness and Conscientiousness variables, whilst including the Skewness metric, the Shapiro-Wilk statistic and a histogram?\nCan you run this analysis in Rj?\n\n\nRemember that the R code in the syntax mode will update as you define your analysis in Jamovi. Get the analysis right there and you can simply copy over the code.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final code should look like this\njmv::descriptives(\n    data = data,\n    vars = vars(Openness, Conscientiousness),\n    hist = TRUE,\n    skew = TRUE,\n    sw = TRUE)"
  },
  {
    "objectID": "tutorials/rmb_week-2_stats-in-Rj.html#cleaning-data-using-the-filter-function",
    "href": "tutorials/rmb_week-2_stats-in-Rj.html#cleaning-data-using-the-filter-function",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Remember that last week we very carefully removed several participants who had entered unrealisic or impossible ages into the datasets. This may have been an accident or mistake but either way we should not use this data.\nR and Rj make it very easy to clean data without the boring, manual work we did last week. R can do the hard work and we can avoid scrolling through very large datasets by hand. Sounds good!\nThe first thing we can do is investigate what range of responses that people actually gave when asked about their age. To do this we can use a new function called unique(). This takes a column of a dataset as an input and returns a list of all the unique responses that it contains.\n\n\n\n\n\n\nKey step\n\n\n\nLoad up an Rj Editor window (or continue in your old one) and run unique() on the age data.\nWe can access a single column of a dataset using the doller sign $. In this case data$age accesses the age column of the data sheet.\n\n\nThe result should look like this:\n\nWhat do you think?\n\n\n\n\n\n\nData Skills - Evaluating the age column\n\n\n\n\n\nOk, most of these responses are fine but there are a lot of strange ones.\n\nFirstly, we can find the extreme values that we saw last week (-2 and 149) - these are clearly mistakes (or some kind of weird prank).\nNext, we can see a lot of people adding text (eg ‘almost 19’). This is not helpful for the computer though it can be socially useful. The computer will read this in words and not numbers. These responses with additional text will count as missing data for any numerical analysis we do.\n&lt;NA&gt; means that the data value is simply missing - perhaps these participants skipped the question.\n\nWe caught some of these issues last time but likely not all. It would be a huge effort to comb through the data line-by-line and identify all these issues."
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#visualising-and-understanding-distributions",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#visualising-and-understanding-distributions",
    "title": "Week 1 : Descriptive Statistics, Data Processing and R",
    "section": "",
    "text": "Question\n\n\n\nTweet from the callout tweet with icon"
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#references",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#references",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Block, Jack. 1995. “A Contrarian View of the Five-Factor Approach to Personality Description.” Psychological Bulletin 117 (2): 187–215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. “Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.” Journal of Experimental Social Psychology 67: 68–82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. “A Very Brief Measure of the Big-Five Personality Domains.” Journal of Research in Personality 37 (6): 504–28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#exploring-a-new-dataset-in-jamovi",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#exploring-a-new-dataset-in-jamovi",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Revision from RM-A\n\n\n\nThis computer practical will have some revision from RM-A, please check your notes from RM-A for help with loading data and handling variables.\nIf you aren’t familiar with Jamovi, or would like a refresher - please take a quick look at the following topics in the Jamovi user guide\n\nFirst steps in Jamovi\nAnalyses\nThe spreadsheet\n\nPlease ask your tutor for help in the computer practical sessions if you need any assistance.\n\n\n\n\n\n\n\n\nKey step\n\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file ML3AllSites_RMB.csv from canvas.\n\n\n\nThis is the original dataset provided by the authors of the paper! It is a massive dataset - take a moment to get familiar with it by answering the following questions.\nSome of the questions will require you to run a descriptive statistics analysis on the age variable. Run what ever you need to explore the data and the questions.\n\n\n\n\n\n\n Data Skills - How many participants are included in the dataset?\n\n\n\n\n\nThere are 2402 participants in total - this is the number of rows in the dataset\n\n\n\n\n\n\n\n\n\n Data Skills - Is the dataset complete? do we have all information from everyone?\n\n\n\n\n\nNo, unfortunately some data is missing from some participants. You can see empty cells in quite a few places across the dataset. This means that some analyses may have an N of less than 2402 for analyses using some of these variables.\n\n\n\n\n\n\n\n\n\n Data Skills - Take a look at the configuration of the Age variable. Is it the correct type?\n\n\n\n\n\nJamovi has to make an educated guess at what type of data is included in each column. Looks like it has correctly set the age variable to be ‘Continuous’.\n\n\n\n\n\n\n\n\n\n Data Skills - How many participants do not have age information?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe N for the age descriptives is 2402 and there are 0 missing participants. This is a large sample and there is age information for everyone.\n\n\n\n\n\n\n\n\n\n\n Data Skills - What is the average and standard deviation of the age of the participants?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe mean age is 19.3 years, typical for a student population. The standard deviation is 3.81.\nThis means that a participant who is 19.3+3.81 = 23.11 years old would be one standard deviation above the average age of the sample.\n\n\n\n\n\n\n\n\n\n\n Data Skills - Do you think all the information in the age variable is accurate?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu. Try checking the box to show the 5 most extreme values in the dataset under the ‘Statistics -&gt; Outliers’ submenu.\nTake a close look at the variable and its descriptive statistics.\nThe youngest age in the dataset is -2 and the oldest age is 149! neither of these can be true and are probably typos (or jokes) made during data collection. Some other values are unusual but not impossible - the second youngest student is 13. We do not know what has happened during data collection to lead to this as this is not our dataset. We should be careful about interpreting the mean whilst these variables are included…\n\n\n\n\nThis process of exploring and understanding your data is a critical first step when working on a new analysis. Real data is rarely in perfect condition - we have to take some time and apply common sense to understand the data and what challenges there might be when analysing it.\nIf we do identify issues, such are large amounts of missing data, incorrect data types or mistakes in data entry, we can normally fix them - but only if they’ve been spotted! If we assume our data is perfect and go straight to the analysis then we could end up including a 149 year old student in our sample…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#visualising-distributions",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#visualising-distributions",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Next we’ll have a look at computing histograms from this data. Remember that histograms divide the data range into intervals called bins. The choice of bin width and number of bins can affect the appearance and interpretation of the histogram. Most of the time we can use the automatic bin definitions provided by Jamovi.\nWe can use Histograms to explore a few key factors of the wider data distributions that are hard to see from the descriptive statistics tables.\n\nShape: The shape of the histogram can reveal the distribution pattern of the data, such as normal distribution, skewness, or bimodality.\nOutliers: Histograms can help identify outliers or unusual data points that fall outside the general pattern.\n\nJamovi makes it very simple to compute a histogram. Open the ‘Plots’ sub-menu of the descriptive statistics tab and select the ‘Histogram’ checkbox.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a histogram of the participant ages\n\n\n\nThe descriptives tables give us a numerical summary of the data - but this can be hard to build an intuitive understanding of the dataset. A histogram gives us something immediately interpretable on a more ‘human’ level. Have a think about the following questions.\n\n\n\n\n\n\n Data Skills - What is the average age of the pariticpants in this dataset?\n\n\n\n\n\nWe can see that the most common ages are clustered around the late teenage years and early 20s. We can’t tell the exact mean from the histogram alone - we need to compute the descriptives table for this - but histograms are good to get a quick ‘sense’ of the data.\n\n\n\n\n\n\n\n\n\n Data Skills - Do the ages look normally distributed?\n\n\n\n\n\nThere is a long tail to this distribution with a very small number of participants in their 30s, 40s and 50s. This is a strong positive skew and likely indicates that the data are not normally distributed.\nThis would be hard to tell from the descriptives table alone.\nOverall, this is a good match for what we would expect from a student population at a large university.\n\n\n\nWe can get an alternative perspective by visualising our data using a ‘Box Plot’.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a Box plot of the participant ages\n\n\n\nThe boxplot visualises the median and interquartile range (25th to 75th percentile) of the data, as well as highlighting outliers. This gives us less resolution around the middle of the distribution close to the median - but does give a much clearer view of the long tail in the ages of the student data sample.\n\n\n\n\n\n\n Data Skills - How old is the oldest participant in the dataset??\n\n\n\n\n\nThe most extreme outliers in the dataset are labelled with their row ID in the boxplot. The largest age value is in row 1081 - it looks like this is around 55 in the boxplot.\nWe can check the corresponding row in the data table to get some more precision. This tells us that the oldest participant is exactly 54 years old.\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding 2\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nCompute histograms and/or boxplots for the big-5 personality factors and take some notes their distributions. Think about the following questions for each factor:\n\nIs the distribution symmetric?\nIf not, is there a positive or a negative skew?\nAre there any outliers?\n\nWrite some quick notes on your answers - we’ll use these again in the next section!\n\n\nIt might be easiest to do this one factor at a time to avoid being swamped with figures!\nCreate the figure for a single factor, make some notes and make the next figure with the next factor. You can consider saving the figure into a notebook or document if you want to make a more controlled comparison.\n\n\nHere are my notes about each distribution:\n\nOpenness, this does not look like a normal distribution - there is a negative skew in the data.\nConscientiousness, this is definitely not a normal distribution - there is a strong negative skew.\nExtraversion, this is close to being a normal distribution - perhaps a very slight negative skew.\nAgreeableness, this does not look like a normal distribution - there is a negative skew in the data.\nNeuroticism, this does not look like a normal distribution - there is a positive skew in the data.\n\nNone of the factors have obvious outliers.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#descriptives-from-big-5-personality-factors",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#descriptives-from-big-5-personality-factors",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Ok, let’s move on to personality factors. As we said at the top, the big five factors are recorded on a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. So someone with an Extraversion score or 6 might be very outgoing where as someone with a score of 1 would be more of an introvert.\n\n\n\n\n\n\nKey step\n\n\n\nCompute the descriptive statistics for the big 5 personality factors.\nTake some time to consider the data quality - use the questions we answered with age as a guide for what to think about.\n\n\n\nNotice that we have a few missing data values in these variables. Jamovi will remove these from any analyses so they shouldn’t affect our descriptive statistics.\n\n\n\n\n\n\n Critical thinking - Are you happy to use the data in the big 5 personality factors?\n\n\n\n\n\nYes, these look reasonable. The data range matches what we would expect from the rating scales (values between 1 and 7) and there is only a small amount of missing data in each of the five variables.\nWe don’t need to do any further data cleaning based on these descriptive statistics.\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding 1\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nYou have reproduced the descriptive statistics for the big 5 personality factors. Now you can do the same for some of the other factors in Table 2 of section 3.5 in Ebersole et al. (2016).\nSpecifically - try and compute the mean and standard deviations for the following factors (Descriptions of the factors are the the cited section of Ebersole et al. (2016)):\n\nSelf Esteem (see section 3.5.1)\nDaily Mood (see section 3.5.3)\nNeed for cognition (see section 3.5.5)\n\nCan you find variables (columns) that correspond to these factors?\nNote: Open datasets sometimes use shorthand for variable names rather than the names used in the manuscript. This is not ideal but does happen in practice! we should be ready take a little time to explore and understand a new dataset when working with it.\n\n\nIn this case the variables in the dataset to use are:\n\n\n\nIn Manuscript\nIn Jamovi\n\n\n\n\nSelf Esteem\n‘SelfEsteem’\n\n\nDaily Mood.\n‘Mood’\n\n\nNeed for cognition\n‘NFC’\n\n\n\nTalk to your tutor if you’re getting stuck.\n\n\nYou should be able to reproduce the following table. Verify that the computed Mean and Standard Deviations match those published in table 2 of Ebersole et al (2016)\n\nThe final mean values are very close to the values in Ebersole et al. (2016) - but not identical. Looks like the original paper kept in all the data that had missing or unusual age values.\n\n\n\n\n\nNext, let’s think about the distributions of these variables. Its great to think about summarising a dataset with its mean value, but this doesn’t tell us about the wider distribution.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#summary",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#summary",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "In this session we have gone through the process of exploring a dataset from a published scientific article. We have computed a range of descriptive statistics and used histograms and boxplots to visualise the dataset.\nIt is common for publicly available data to contain little mistakes and issues. We need to go through a process like this with every new dataset and it should be like second nature by the end of this module. Remember to use common sense and practical thinking to identify issues and think about possible solutions.\nRemember this process when collecting your own data. Think carefully about how you can avoid typos and mistakes when collecting a dataset. It can save you and your collaborators a lot of time in future!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_descriptives-and-rj.html#computing-statistics-for-normality-and-skewness.",
    "href": "tutorials/rmb_week-1_descriptives-and-rj.html#computing-statistics-for-normality-and-skewness.",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "As a final step, we can see if our intuitions about the distribution of big-5 personality factors match with more formal statistics measures checking for normal distributions and skewness.\n\n\n\n\n\n\nKey step\n\n\n\nCompute statistics Shapero-Wilk’s test for normality and for ‘Skewness’. You can add both of these tests from the ‘Statistics’ sub-menu of the Descriptives analysis.\n\n\n\n\n\nThe Shapiro-Wilk test evaluates whether a sample comes from a normally distributed population. It returns 2 values:\n\n‘W’ The W statistic measures how well the data conforms to a normal distribution. It ranges from 0 to 1 where values close to 1 indicate the data is normally distributed.\n‘p’ is a p-value indicating the probability that we could see a value of W at least as large as the one we have measured if the data really were normally distributed. Typically, we would say that the data are not normally distributed if p is less than or equal to 0.05.\n\n\n\n\n\n\n\n Data Skills - Do our notes from the histograms match the Shapiro-Wilk statistics?\n\n\n\n\n\nYes, If we look at the W statistics, then we see that Extraversion has the highest value indicating that it has the most ‘normal’ data distribution whilst the other four factors are similar. This matches what we saw with the histograms.\nThe p-values from the formal hypothesis test indicate that none of the 5 factors have normal distributions. We couldn’t tell this from our informal evaulation of the histograms and box plots.\n\n\n\n\n\n\nThe skewness metric returns a number close to zero if the data are symmetrical around zero, and a number close to +1 or -1 if the data are skewed. The sign of the skewness value indicates whether there is a positive or a negative skew.\n\n\n\n\n\n\n Data Skills - Do our notes from the histograms match the skewness statistic?\n\n\n\n\n\nYes, its very close. We can see that ‘Extraversion’ has the least skew with a Skewness metric very close to zero (-0.0714). In contrast, ‘Conscientiousness’ has the most skew. Its value of ‘-0.656’ indicates a very substantial negative skewness in the distribution.\nOpenness and Agreeableness have moderate negative skews, whilst Neuroticism has a moderate positive skew.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "copilot.html",
    "href": "copilot.html",
    "title": "Using Copilot at University of Birmingham",
    "section": "",
    "text": "Microsoft Copilot is now available to use for University of Birmingham members. See the University documentation pages to get access\nYou are welcome to use Copilot to support your learning of statistics and coding throughout the Research Methods B computer practical sessions. In particular, Copilot can be a useful way to get a second perspective on different technical concepts, to help generate data analysis code or to understand where data analysis code might have gone wrong.\nThroughout the course - we will suggest places where you might benefit from interacting with Copilot to support your learning. Look out for dialogue boxes like this one.\n\n\n\n\n\n\nCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about variables in R. We suggest you try the following prompts to get started.\n\n“Write a simple description of variables in R for a beginner”\n“How can I choose a name for my variable in R”\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\n\n\n\n\n\n\nEthical use of Copilot\n\n\n\nTools like Copilot can be a valuable learning aid, however it is critical that you familiarise yourself with the University’s student and PGR guidance on using GenAI tools ethically for work. This guidance provides you with the risks and limitations of using Generative AI tools in your academic study\nIt is not permitted to use Generative AI tools in assessment unless explicitly stated on the assessment page. It is your responsibility to use these tools in an appropriate way.\n\n\n\n\nHere are some questions with Copilot’s answers to help you decide how and when to use Copilot. Note that Copilot may respond differently if you ask identical prompts.\n\n\n\nStarting to learn R with the help of Copilot can be a smooth and enjoyable process. Here are some steps and tips to get you started:\n\nBegin by asking Copilot to explain fundamental concepts in R. This will give you a solid foundation to build on. - Example Questions: - “What are the basic data types in R?” - “How do I create a vector in R?”\n\nRequest detailed guides for common tasks. This helps you understand the process and logic behind the code. - Example Questions: - “Can you show me how to read a CSV file into R?” - “How do I create a simple plot using ggplot2?”\n\nReinforce your learning by practicing with beginner exercises. This will help you apply what you’ve learned. - Example Questions: - “Can you give me a beginner exercise for data manipulation in R?” - “What are some basic exercises to practice using loops in R?”\n\nLearn about important R packages that can simplify your work. Copilot can introduce you to these packages and show you how to use them. - Example Questions: - “What is the dplyr package and how do I use it?” - “Can you explain how to use the tidyverse package for data analysis?”\n\nWhen you encounter errors, ask Copilot for help in understanding and fixing them. This will improve your debugging skills. - Example Questions: - “I’m getting an error with my R code, can you help me debug it?” - “What does this error message mean in R?”\n\nAsk about best practices for writing clean and efficient R code. This will help you develop good &gt; coding habits. - Example Questions: - “What are some best practices for writing R code?” - “How can I make my R scripts more efficient?”\n\nSeeing examples can be very helpful. Ask for code snippets to understand how to implement different &gt; functions and methods. - Example Questions: - “Can you show me an example of how to use the print function in R?” - “How do I perform a linear regression in R?”\n\nDon’t hesitate to ask about anything you’re curious about, even if it seems basic. Copilot is here to help you learn at your own pace. - Example Questions: - “What are some good resources for learning R?” - “How do I create a histogram in R?”\n\n\n“How do I extract specific rows from a data frame in R?”\n“What are some common data visualization techniques in R?”\n\nBy leveraging Copilot’s capabilities, you can get personalized assistance and resources tailored to your learning needs. Remember, learning a new language takes time and practice, so be patient with yourself and keep experimenting. Happy coding! If you have any specific questions or need further guidance, feel free to ask!\n\n\n\n\n\nCode generated by Copilot can be a great starting point, but it’s important to review and test it thoroughly. Here are a few tips to ensure the code meets your needs:\n\nReview for Accuracy: Check the logic and syntax to make sure it aligns with your requirements.\nTest Thoroughly: Run tests to ensure the code works as expected in different scenarios.\nSecurity Considerations: Be mindful of potential security vulnerabilities and address them as needed.\nCustomization: Adapt and refine the code to better fit your specific use case.\n\nUsing these practices can help you make the most of Copilot’s assistance while ensuring the quality and reliability of your code. If you have any specific coding questions or need help with a particular piece of code, feel free to ask! 😊",
    "crumbs": [
      "Home",
      "Support",
      "Using Copilot at UoB"
    ]
  },
  {
    "objectID": "copilot.html#copilots-advice-about-itself",
    "href": "copilot.html#copilots-advice-about-itself",
    "title": "Using Copilot at University of Birmingham",
    "section": "",
    "text": "Here are some questions with Copilot’s answers to help you decide how and when to use Copilot. Note that Copilot may respond differently if you ask identical prompts.\n\n\n\nStarting to learn R with the help of Copilot can be a smooth and enjoyable process. Here are some steps and tips to get you started:\n\nBegin by asking Copilot to explain fundamental concepts in R. This will give you a solid foundation to build on. - Example Questions: - “What are the basic data types in R?” - “How do I create a vector in R?”\n\nRequest detailed guides for common tasks. This helps you understand the process and logic behind the code. - Example Questions: - “Can you show me how to read a CSV file into R?” - “How do I create a simple plot using ggplot2?”\n\nReinforce your learning by practicing with beginner exercises. This will help you apply what you’ve learned. - Example Questions: - “Can you give me a beginner exercise for data manipulation in R?” - “What are some basic exercises to practice using loops in R?”\n\nLearn about important R packages that can simplify your work. Copilot can introduce you to these packages and show you how to use them. - Example Questions: - “What is the dplyr package and how do I use it?” - “Can you explain how to use the tidyverse package for data analysis?”\n\nWhen you encounter errors, ask Copilot for help in understanding and fixing them. This will improve your debugging skills. - Example Questions: - “I’m getting an error with my R code, can you help me debug it?” - “What does this error message mean in R?”\n\nAsk about best practices for writing clean and efficient R code. This will help you develop good &gt; coding habits. - Example Questions: - “What are some best practices for writing R code?” - “How can I make my R scripts more efficient?”\n\nSeeing examples can be very helpful. Ask for code snippets to understand how to implement different &gt; functions and methods. - Example Questions: - “Can you show me an example of how to use the print function in R?” - “How do I perform a linear regression in R?”\n\nDon’t hesitate to ask about anything you’re curious about, even if it seems basic. Copilot is here to help you learn at your own pace. - Example Questions: - “What are some good resources for learning R?” - “How do I create a histogram in R?”\n\n\n“How do I extract specific rows from a data frame in R?”\n“What are some common data visualization techniques in R?”\n\nBy leveraging Copilot’s capabilities, you can get personalized assistance and resources tailored to your learning needs. Remember, learning a new language takes time and practice, so be patient with yourself and keep experimenting. Happy coding! If you have any specific questions or need further guidance, feel free to ask!\n\n\n\n\n\nCode generated by Copilot can be a great starting point, but it’s important to review and test it thoroughly. Here are a few tips to ensure the code meets your needs:\n\nReview for Accuracy: Check the logic and syntax to make sure it aligns with your requirements.\nTest Thoroughly: Run tests to ensure the code works as expected in different scenarios.\nSecurity Considerations: Be mindful of potential security vulnerabilities and address them as needed.\nCustomization: Adapt and refine the code to better fit your specific use case.\n\nUsing these practices can help you make the most of Copilot’s assistance while ensuring the quality and reliability of your code. If you have any specific coding questions or need help with a particular piece of code, feel free to ask! 😊",
    "crumbs": [
      "Home",
      "Support",
      "Using Copilot at UoB"
    ]
  },
  {
    "objectID": "copilot.html#copilot-giving-advice-about-how-to-use-copilot",
    "href": "copilot.html#copilot-giving-advice-about-how-to-use-copilot",
    "title": "Using Copilot at University of Birmingham",
    "section": "",
    "text": "Here are some questions with Copilot’s answers to help you decide how and when to use Copilot. Note that Copilot may respond differently if you ask identical prompts.\n\n\n\nStarting to learn R with the help of Copilot can be a smooth and enjoyable process. Here are some steps and tips to get you started:\n\nBegin by asking Copilot to explain fundamental concepts in R. This will give you a solid foundation to build on. - Example Questions: - “What are the basic data types in R?” - “How do I create a vector in R?”\n\nRequest detailed guides for common tasks. This helps you understand the process and logic behind the code. - Example Questions: - “Can you show me how to read a CSV file into R?” - “How do I create a simple plot using ggplot2?”\n\nReinforce your learning by practicing with beginner exercises. This will help you apply what you’ve learned. - Example Questions: - “Can you give me a beginner exercise for data manipulation in R?” - “What are some basic exercises to practice using loops in R?”\n\nLearn about important R packages that can simplify your work. Copilot can introduce you to these packages and show you how to use them. - Example Questions: - “What is the dplyr package and how do I use it?” - “Can you explain how to use the tidyverse package for data analysis?”\n\nWhen you encounter errors, ask Copilot for help in understanding and fixing them. This will improve your debugging skills. - Example Questions: - “I’m getting an error with my R code, can you help me debug it?” - “What does this error message mean in R?”\n\nAsk about best practices for writing clean and efficient R code. This will help you develop good &gt; coding habits. - Example Questions: - “What are some best practices for writing R code?” - “How can I make my R scripts more efficient?”\n\nSeeing examples can be very helpful. Ask for code snippets to understand how to implement different &gt; functions and methods. - Example Questions: - “Can you show me an example of how to use the print function in R?” - “How do I perform a linear regression in R?”\n\nDon’t hesitate to ask about anything you’re curious about, even if it seems basic. Copilot is here to help you learn at your own pace. - Example Questions: - “What are some good resources for learning R?” - “How do I create a histogram in R?”\n\n\n“How do I extract specific rows from a data frame in R?”\n“What are some common data visualization techniques in R?”\n\nBy leveraging Copilot’s capabilities, you can get personalized assistance and resources tailored to your learning needs. Remember, learning a new language takes time and practice, so be patient with yourself and keep experimenting. Happy coding! If you have any specific questions or need further guidance, feel free to ask!\n\n\n\n\n\nCode generated by Copilot can be a great starting point, but it’s important to review and test it thoroughly. Here are a few tips to ensure the code meets your needs:\n\nReview for Accuracy: Check the logic and syntax to make sure it aligns with your requirements.\nTest Thoroughly: Run tests to ensure the code works as expected in different scenarios.\nSecurity Considerations: Be mindful of potential security vulnerabilities and address them as needed.\nCustomization: Adapt and refine the code to better fit your specific use case.\n\nUsing these practices can help you make the most of Copilot’s assistance while ensuring the quality and reliability of your code. If you have any specific coding questions or need help with a particular piece of code, feel free to ask! 😊",
    "crumbs": [
      "Home",
      "Support",
      "Using Copilot at UoB"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "This week we will explore how to use Jamovi to compute and visualise descriptive statistics from a dataset. This will include some revision on Jamovi from last semester and an introduction on how we can use the R coding language to simplify complex data processing.\n\n\n\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nMeasures of central tendency (Mean and Median)\n\n\n\nMeasures of dispersion and spread (Standard Deviation and Interquartile Range)\n\n\n\nMeasures of skew and normality\n\n\n\nRecognising different types of distribution\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nExploring and cleaning datasets Jamovi\n\n\n\nComputing descriptive statistics in Jamovi\n\n\n\nComputing descriptive plots in Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with openly available research data\n\n\n\n\n\n\nWe will be exploring data that was collected as part of the ‘Many Labs 3’ project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‘Big Five’ personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs. extravagant/careless)\nAgreeableness (friendly/compassionate vs. critical/rational)\nNeuroticism (sensitive/nervous vs. resilient/confident)\nOpenness to experience (inventive/curious vs. consistent/cautious)\nExtraversion (outgoing/energetic vs. solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.\n\n\n\nWe have data from a sample of students in a big data table. This raw data needs to be summarised and visualised so that we can turn this massive block of numbers into something intuitive.\nWe can generate a concise overview of the main characteristics of the data by computing and visualising few key features from the data.\n\n\n\n\n\n\nOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.\n\n\n\n\n\n\n\n\n\n\n\n\nRevision from RM-A\n\n\n\nThis computer practical will have some revision from RM-A, please check your notes from RM-A for help with loading data and handling variables.\nIf you aren’t familiar with Jamovi, or would like a refresher - please take a quick look at the following topics in the Jamovi user guide\n\nFirst steps in Jamovi\nAnalyses\nThe spreadsheet\n\nPlease ask your tutor for help in the computer practical sessions if you need any assistance.\n\n\n\n\n\n\n\n\nKey step\n\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file ML3AllSites_RMB.csv from canvas.\nYou can import csv files directly into Jamovi. There is more information in the Jamovi Textbook\nIt’s easy to open csv files in Jamovi. From the top left menu (the button with three parallel lines) choose ‘Open’ and browse to where you have stored the csv file on your computer. If you’re on a Mac, it’ll look like the usual Finder window that you use to choose a file; on Windows it looks like an Explorer window. Find the one you want, then click on the ‘Open’ button.\n\n\n\nThis is the original dataset provided by the authors of the paper! It is a massive dataset - take a moment to get familiar with it by answering the following questions.\nSome of the questions will require you to run a descriptive statistics analysis on the age variable. Run what ever you need to explore the data and the questions.\n\n\n\n\n\n\n Data Skills - How many participants are included in the dataset?\n\n\n\n\n\nThere are 2402 participants in total - this is the number of rows in the dataset\n\n\n\n\n\n\n\n\n\n Data Skills - Is the dataset complete? do we have all information from everyone?\n\n\n\n\n\nNo, unfortunately some data is missing from some participants. You can see empty cells in quite a few places across the dataset. This means that some analyses may have an N of less than 2402 for analyses using some of these variables.\n\n\n\n\n\n\n\n\n\n Data Skills - Take a look at the configuration of the Age variable. Is it the correct type?\n\n\n\n\n\nJamovi has to make an educated guess at what type of data is included in each column. Looks like it has correctly set the age variable to be ‘Continuous’.\n\n\n\n\n\n\n\n\n\n Data Skills - How many participants do not have age information?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe N for the age descriptives is 2402 and there are 0 missing participants. This is a large sample and there is age information for everyone.\n\n\n\n\n\n\n\n\n\n\n Data Skills - What is the average and standard deviation of the age of the participants?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe mean age is 19.3 years, typical for a student population. The standard deviation is 3.81.\nThis means that a participant who is 19.3+3.81 = 23.11 years old would be one standard deviation above the average age of the sample.\n\n\n\n\n\n\n\n\n\n\n Data Skills - Do you think all the information in the age variable is accurate?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu. Try checking the box to show the 5 most extreme values in the dataset under the ‘Statistics -&gt; Outliers’ submenu.\nTake a close look at the variable and its descriptive statistics.\nThe youngest age in the dataset is -2 and the oldest age is 149! neither of these can be true and are probably typos (or jokes) made during data collection. Some other values are unusual but not impossible - the second youngest student is 13. We do not know what has happened during data collection to lead to this as this is not our dataset. We should be careful about interpreting the mean whilst these variables are included…\n\n\n\n\nThis process of exploring and understanding your data is a critical first step when working on a new analysis. Real data is rarely in perfect condition - we have to take some time and apply common sense to understand the data and what challenges there might be when analysing it.\nIf we do identify issues, such are large amounts of missing data, incorrect data types or mistakes in data entry, we can normally fix them - but only if they’ve been spotted! If we assume our data is perfect and go straight to the analysis then we could end up including a 149 year old student in our sample…\n\n\n\nIts probably not a good idea to include the data from the participants with impossible ages in our analysis. One simple step we can take is just to remove those rows from our analysis.\n\n\n\n\n\n\nData Skills - removing rows from a dataset\n\n\n\nRemove the two participants with impossible ages, you can use the extreme values table in the ‘Descriptives’ output to help find the row numbers. You can remove a row in Jamovi by right clicking on the row number (far left of the Data view) and clicking ‘Delete Row’.\n\nThis will not change the original data file (unless you export the data back out again..), it will only affect the view of the data loaded into Jamovi. If you make a mistake you can simply load the original file and start again.\nNotice that the bottom of the data viewer tells you how many rows have been deleted in that dataset.\n\n\n\n\n\n\n\n\nKey step\n\n\n\nNow you can recompute the descriptive statistics of your age variable and see what has changed.\n\n\n\nThe mean is pretty much the same - still 19.3 - but the standard deviation is much smaller now. It has reduced from 3.81 to 2.70. That is a close to 25% reduction in the estimate of the variability of this data - big difference. Including outliers like these can have a large impact on our analyses.\n\n\n\n\n\n\n Critical thinking - Are you happy with the data in the age variable now?\n\n\n\n\n\nTough question! take a look at the Minimum value - we still have one participant who is 13 years old. Whilst this is more likely than finding a -2 or 149 year old student, it is still extremely unlikely. It is harder to make a judgement for this participant than the others. The first two participants had impossible ages, whereas a 13 year old student is just very unlikely. Sometimes we have to make a judgement call.\nLet’s delete row 709 to remove this participant as well.\nThe recomputed descriptive statistics now has an N=2399. Removing the 13 year old hasn’t changed our mean or standard deviation estimates very much - indicating that it was less important to remove this data point than the first two.\nThe youngest participant is now 16 which is more realistic (though still very impressive for that 16 year old!).\n\n\n\nThis process can be intricate and involves a lot of clicking in Jamovi - it would be easy to make a mistake and not notice that something has gone wrong. We’ll explore some faster and more transparent ways to do this using R code during this course.\n\n\n\n\nOk, let’s move on to personality factors. As we said at the top, the big five factors are recorded on a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. So someone with an Extraversion score or 6 might be very outgoing where as someone with a score of 1 would be more of an introvert.\n\n\n\n\n\n\nKey step\n\n\n\nCompute the descriptive statistics for the big 5 personality factors.\nTake some time to consider the data quality - use the questions we answered with age as a guide for what to think about.\n\n\n\nNotice that we have a few missing data values in these variables. Jamovi will remove these from any analyses so they shouldn’t affect our descriptive statistics.\n\n\n\n\n\n\n Critical thinking - Are you happy to use the data in the big 5 personality factors?\n\n\n\n\n\nYes, these look reasonable. The data range matches what we would expect from the rating scales (values between 1 and 7) and there is only a small amount of missing data in each of the five variables.\nWe don’t need to do any further data cleaning based on these descriptive statistics.\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding 1\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nYou have reproduced the descriptive statistics for the big 5 personality factors. Now you can do the same for some of the other factors in Table 2 of section 3.5 in Ebersole et al. (2016).\nSpecifically - try and compute the mean and standard deviations for the following factors (Descriptions of the factors are the the cited section of Ebersole et al. (2016)):\n\nSelf Esteem (see section 3.5.1)\nDaily Mood (see section 3.5.3)\nNeed for cognition (see section 3.5.5)\n\nCan you find variables (columns) that correspond to these factors?\nNote: Open datasets sometimes use shorthand for variable names rather than the names used in the manuscript. This is not ideal but does happen in practice! we should be ready take a little time to explore and understand a new dataset when working with it.\n\n\nIn this case the variables in the dataset to use are:\n\n\n\nIn Manuscript\nIn Jamovi\n\n\n\n\nSelf Esteem\n‘SelfEsteem’\n\n\nDaily Mood.\n‘Mood’\n\n\nNeed for cognition\n‘NFC’\n\n\n\nTalk to your tutor if you’re getting stuck.\n\n\nYou should be able to reproduce the following table. Verify that the computed Mean and Standard Deviations match those published in table 2 of Ebersole et al (2016)\n\nThe final mean values are very close to the values in Ebersole et al. (2016) - but not identical. Looks like the original paper kept in all the data that had missing or unusual age values.\n\n\n\n\n\nNext, let’s think about the distributions of these variables. Its great to think about summarising a dataset with its mean value, but this doesn’t tell us about the wider distribution.\n\n\n\n\nNext we’ll have a look at computing histograms from this data. Remember that histograms divide the data range into intervals called bins. The choice of bin width and number of bins can affect the appearance and interpretation of the histogram. Most of the time we can use the automatic bin definitions provided by Jamovi.\nWe can use Histograms to explore a few key factors of the wider data distributions that are hard to see from the descriptive statistics tables.\n\nShape: The shape of the histogram can reveal the distribution pattern of the data, such as normal distribution, skewness, or bimodality.\nOutliers: Histograms can help identify outliers or unusual data points that fall outside the general pattern.\n\nJamovi makes it very simple to compute a histogram. Open the ‘Plots’ sub-menu of the descriptive statistics tab and select the ‘Histogram’ checkbox.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a histogram of the participant ages\n\n\n\nThe descriptives tables give us a numerical summary of the data - but this can be hard to build an intuitive understanding of the dataset. A histogram gives us something immediately interpretable on a more ‘human’ level. Have a think about the following questions.\n\n\n\n\n\n\n Data Skills - What is the average age of the pariticpants in this dataset?\n\n\n\n\n\nWe can see that the most common ages are clustered around the late teenage years and early 20s. We can’t tell the exact mean from the histogram alone - we need to compute the descriptives table for this - but histograms are good to get a quick ‘sense’ of the data.\n\n\n\nIf you haven’t heard about the ‘normal distribution’ in your previous studies, this is a good opportunity to ask Copilot to give you a simple explanation.\n\n\n\n\n\n\n Data Skills - Do the ages look normally distributed?\n\n\n\n\n\nThere is a long tail to this distribution with a very small number of participants in their 30s, 40s and 50s. This is a strong positive skew and likely indicates that the data are not normally distributed.\nThis would be hard to tell from the descriptives table alone.\nOverall, this is a good match for what we would expect from a student population at a large university.\n\n\n\nWe can get an alternative perspective by visualising our data using a ‘Box Plot’.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a Box plot of the participant ages\n\n\n\nThe boxplot visualises the median and interquartile range (25th to 75th percentile) of the data, as well as highlighting outliers. This gives us less resolution around the middle of the distribution close to the median - but does give a much clearer view of the long tail in the ages of the student data sample.\n\n\n\n\n\n\n Data Skills - How old is the oldest participant in the dataset??\n\n\n\n\n\nThe most extreme outliers in the dataset are labelled with their row ID in the boxplot. The largest age value is in row 1081 - it looks like this is around 55 in the boxplot.\nWe can check the corresponding row in the data table to get some more precision. This tells us that the oldest participant is exactly 54 years old.\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding 2\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nCompute histograms and/or boxplots for the big-5 personality factors and take some notes their distributions. Think about the following questions for each factor:\n\nIs the distribution symmetric?\nIf not, is there a positive or a negative skew?\nAre there any outliers?\n\nWrite some quick notes on your answers - we’ll use these again in the next section\n\n\nIt might be easiest to do this one factor at a time to avoid being swamped with figures!\nCreate the figure for a single factor, make some notes and make the next figure with the next factor. You can consider saving the figure into a notebook or document if you want to make a more controlled comparison.\nIf you aren’t sure about any of the concepts - this is a good opportunity to ask your tutors in the session - or use Copilot to give you a simple explanation.\n\n\nHere are my notes about each distribution:\n\nOpenness, this does not look like a normal distribution - there is a negative skew in the data.\nConscientiousness, this is definitely not a normal distribution - there is a strong negative skew.\nExtraversion, this is close to being a normal distribution - perhaps a very slight negative skew.\nAgreeableness, this does not look like a normal distribution - there is a negative skew in the data.\nNeuroticism, this does not look like a normal distribution - there is a positive skew in the data.\n\nNone of the factors have obvious outliers.\n\n\n\n\n\n\n\n\nAs a final step, we can see if our intuitions about the distribution of big-5 personality factors match with more formal statistics measures checking for normal distributions and skewness.\n\n\n\n\n\n\nKey step\n\n\n\nCompute statistics Shapero-Wilk’s test for normality and for ‘Skewness’. You can add both of these tests from the ‘Statistics’ sub-menu of the Descriptives analysis.\n\n\n\n\n\nThe Shapiro-Wilk test evaluates whether a sample comes from a normally distributed population. It returns 2 values:\n\n‘W’ The W statistic measures how well the data conforms to a normal distribution. It ranges from 0 to 1 where values close to 1 indicate the data is normally distributed.\n‘p’ is a p-value indicating the probability that we could see a value of W at least as large as the one we have measured if the data really were normally distributed. Typically, we would say that the data are not normally distributed if p is less than or equal to 0.05.\n\n\n\n\n\n\n\n Data Skills - Do our notes from the histograms match the Shapiro-Wilk statistics?\n\n\n\n\n\nYes, If we look at the W statistics, then we see that Extraversion has the highest value indicating that it has the most ‘normal’ data distribution whilst the other four factors are similar. This matches what we saw with the histograms.\nThe p-values from the formal hypothesis test indicate that none of the 5 factors have normal distributions. We couldn’t tell this from our informal evaulation of the histograms and box plots.\n\n\n\n\n\n\nThe skewness metric returns a number close to zero if the data are symmetrical around zero, and a number close to +1 or -1 if the data are skewed. The sign of the skewness value indicates whether there is a positive or a negative skew.\n\n\n\n\n\n\n Data Skills - Do our notes from the histograms match the skewness statistic?\n\n\n\n\n\nYes, its very close. We can see that ‘Extraversion’ has the least skew with a Skewness metric very close to zero (-0.0714). In contrast, ‘Conscientiousness’ has the most skew. Its value of ‘-0.656’ indicates a very substantial negative skewness in the distribution.\nOpenness and Agreeableness have moderate negative skews, whilst Neuroticism has a moderate positive skew.\n\n\n\n\n\n\n\nIn this session we have gone through the process of exploring a dataset from a published scientific article. We have computed a range of descriptive statistics and used histograms and boxplots to visualise the dataset.\nIt is common for publicly available data to contain little mistakes and issues. We need to go through a process like this with every new dataset and it should be like second nature by the end of this module. Remember to use common sense and practical thinking to identify issues and think about possible solutions.\nRemember this process when collecting your own data. Think carefully about how you can avoid typos and mistakes when collecting a dataset. It can save you and your collaborators a lot of time in future!\n\n\n\n\n\nBlock, Jack. 1995. “A Contrarian View of the Five-Factor Approach to Personality Description.” Psychological Bulletin 117 (2): 187–215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. “Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.” Journal of Experimental Social Psychology 67: 68–82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. “A Very Brief Measure of the Big-Five Personality Domains.” Journal of Research in Personality 37 (6): 504–28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#the-dataset",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#the-dataset",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "We will be exploring data that was collected as part of the ‘Many Labs 3’ project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‘Big Five’ personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs. extravagant/careless)\nAgreeableness (friendly/compassionate vs. critical/rational)\nNeuroticism (sensitive/nervous vs. resilient/confident)\nOpenness to experience (inventive/curious vs. consistent/cautious)\nExtraversion (outgoing/energetic vs. solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#the-challenge",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#the-challenge",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "We have data from a sample of students in a big data table. This raw data needs to be summarised and visualised so that we can turn this massive block of numbers into something intuitive.\nWe can generate a concise overview of the main characteristics of the data by computing and visualising few key features from the data.\n\n\n\n\n\n\nOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#exploring-a-new-dataset-in-jamovi",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#exploring-a-new-dataset-in-jamovi",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Revision from RM-A\n\n\n\nThis computer practical will have some revision from RM-A, please check your notes from RM-A for help with loading data and handling variables.\nIf you aren’t familiar with Jamovi, or would like a refresher - please take a quick look at the following topics in the Jamovi user guide\n\nFirst steps in Jamovi\nAnalyses\nThe spreadsheet\n\nPlease ask your tutor for help in the computer practical sessions if you need any assistance.\n\n\n\n\n\n\n\n\nKey step\n\n\n\nLet’s start by loading the dataset into Jamovi. You can download the file ML3AllSites_RMB.csv from canvas.\nYou can import csv files directly into Jamovi. There is more information in the Jamovi Textbook\nIt’s easy to open csv files in Jamovi. From the top left menu (the button with three parallel lines) choose ‘Open’ and browse to where you have stored the csv file on your computer. If you’re on a Mac, it’ll look like the usual Finder window that you use to choose a file; on Windows it looks like an Explorer window. Find the one you want, then click on the ‘Open’ button.\n\n\n\nThis is the original dataset provided by the authors of the paper! It is a massive dataset - take a moment to get familiar with it by answering the following questions.\nSome of the questions will require you to run a descriptive statistics analysis on the age variable. Run what ever you need to explore the data and the questions.\n\n\n\n\n\n\n Data Skills - How many participants are included in the dataset?\n\n\n\n\n\nThere are 2402 participants in total - this is the number of rows in the dataset\n\n\n\n\n\n\n\n\n\n Data Skills - Is the dataset complete? do we have all information from everyone?\n\n\n\n\n\nNo, unfortunately some data is missing from some participants. You can see empty cells in quite a few places across the dataset. This means that some analyses may have an N of less than 2402 for analyses using some of these variables.\n\n\n\n\n\n\n\n\n\n Data Skills - Take a look at the configuration of the Age variable. Is it the correct type?\n\n\n\n\n\nJamovi has to make an educated guess at what type of data is included in each column. Looks like it has correctly set the age variable to be ‘Continuous’.\n\n\n\n\n\n\n\n\n\n Data Skills - How many participants do not have age information?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe N for the age descriptives is 2402 and there are 0 missing participants. This is a large sample and there is age information for everyone.\n\n\n\n\n\n\n\n\n\n\n Data Skills - What is the average and standard deviation of the age of the participants?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu.\nThe mean age is 19.3 years, typical for a student population. The standard deviation is 3.81.\nThis means that a participant who is 19.3+3.81 = 23.11 years old would be one standard deviation above the average age of the sample.\n\n\n\n\n\n\n\n\n\n\n Data Skills - Do you think all the information in the age variable is accurate?\n\n\n\n\n\nYou can compute this information from the ‘Exploration -&gt; Descriptives’ menu. Try checking the box to show the 5 most extreme values in the dataset under the ‘Statistics -&gt; Outliers’ submenu.\nTake a close look at the variable and its descriptive statistics.\nThe youngest age in the dataset is -2 and the oldest age is 149! neither of these can be true and are probably typos (or jokes) made during data collection. Some other values are unusual but not impossible - the second youngest student is 13. We do not know what has happened during data collection to lead to this as this is not our dataset. We should be careful about interpreting the mean whilst these variables are included…\n\n\n\n\nThis process of exploring and understanding your data is a critical first step when working on a new analysis. Real data is rarely in perfect condition - we have to take some time and apply common sense to understand the data and what challenges there might be when analysing it.\nIf we do identify issues, such are large amounts of missing data, incorrect data types or mistakes in data entry, we can normally fix them - but only if they’ve been spotted! If we assume our data is perfect and go straight to the analysis then we could end up including a 149 year old student in our sample…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#data-cleaning",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#data-cleaning",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Its probably not a good idea to include the data from the participants with impossible ages in our analysis. One simple step we can take is just to remove those rows from our analysis.\n\n\n\n\n\n\nData Skills - removing rows from a dataset\n\n\n\nRemove the two participants with impossible ages, you can use the extreme values table in the ‘Descriptives’ output to help find the row numbers. You can remove a row in Jamovi by right clicking on the row number (far left of the Data view) and clicking ‘Delete Row’.\n\nThis will not change the original data file (unless you export the data back out again..), it will only affect the view of the data loaded into Jamovi. If you make a mistake you can simply load the original file and start again.\nNotice that the bottom of the data viewer tells you how many rows have been deleted in that dataset.\n\n\n\n\n\n\n\n\nKey step\n\n\n\nNow you can recompute the descriptive statistics of your age variable and see what has changed.\n\n\n\nThe mean is pretty much the same - still 19.3 - but the standard deviation is much smaller now. It has reduced from 3.81 to 2.70. That is a close to 25% reduction in the estimate of the variability of this data - big difference. Including outliers like these can have a large impact on our analyses.\n\n\n\n\n\n\n Critical thinking - Are you happy with the data in the age variable now?\n\n\n\n\n\nTough question! take a look at the Minimum value - we still have one participant who is 13 years old. Whilst this is more likely than finding a -2 or 149 year old student, it is still extremely unlikely. It is harder to make a judgement for this participant than the others. The first two participants had impossible ages, whereas a 13 year old student is just very unlikely. Sometimes we have to make a judgement call.\nLet’s delete row 709 to remove this participant as well.\nThe recomputed descriptive statistics now has an N=2399. Removing the 13 year old hasn’t changed our mean or standard deviation estimates very much - indicating that it was less important to remove this data point than the first two.\nThe youngest participant is now 16 which is more realistic (though still very impressive for that 16 year old!).\n\n\n\nThis process can be intricate and involves a lot of clicking in Jamovi - it would be easy to make a mistake and not notice that something has gone wrong. We’ll explore some faster and more transparent ways to do this using R code during this course.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#descriptives-from-big-5-personality-factors",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#descriptives-from-big-5-personality-factors",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Ok, let’s move on to personality factors. As we said at the top, the big five factors are recorded on a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. So someone with an Extraversion score or 6 might be very outgoing where as someone with a score of 1 would be more of an introvert.\n\n\n\n\n\n\nKey step\n\n\n\nCompute the descriptive statistics for the big 5 personality factors.\nTake some time to consider the data quality - use the questions we answered with age as a guide for what to think about.\n\n\n\nNotice that we have a few missing data values in these variables. Jamovi will remove these from any analyses so they shouldn’t affect our descriptive statistics.\n\n\n\n\n\n\n Critical thinking - Are you happy to use the data in the big 5 personality factors?\n\n\n\n\n\nYes, these look reasonable. The data range matches what we would expect from the rating scales (values between 1 and 7) and there is only a small amount of missing data in each of the five variables.\nWe don’t need to do any further data cleaning based on these descriptive statistics.\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding 1\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nYou have reproduced the descriptive statistics for the big 5 personality factors. Now you can do the same for some of the other factors in Table 2 of section 3.5 in Ebersole et al. (2016).\nSpecifically - try and compute the mean and standard deviations for the following factors (Descriptions of the factors are the the cited section of Ebersole et al. (2016)):\n\nSelf Esteem (see section 3.5.1)\nDaily Mood (see section 3.5.3)\nNeed for cognition (see section 3.5.5)\n\nCan you find variables (columns) that correspond to these factors?\nNote: Open datasets sometimes use shorthand for variable names rather than the names used in the manuscript. This is not ideal but does happen in practice! we should be ready take a little time to explore and understand a new dataset when working with it.\n\n\nIn this case the variables in the dataset to use are:\n\n\n\nIn Manuscript\nIn Jamovi\n\n\n\n\nSelf Esteem\n‘SelfEsteem’\n\n\nDaily Mood.\n‘Mood’\n\n\nNeed for cognition\n‘NFC’\n\n\n\nTalk to your tutor if you’re getting stuck.\n\n\nYou should be able to reproduce the following table. Verify that the computed Mean and Standard Deviations match those published in table 2 of Ebersole et al (2016)\n\nThe final mean values are very close to the values in Ebersole et al. (2016) - but not identical. Looks like the original paper kept in all the data that had missing or unusual age values.\n\n\n\n\n\nNext, let’s think about the distributions of these variables. Its great to think about summarising a dataset with its mean value, but this doesn’t tell us about the wider distribution.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#visualising-distributions",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#visualising-distributions",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Next we’ll have a look at computing histograms from this data. Remember that histograms divide the data range into intervals called bins. The choice of bin width and number of bins can affect the appearance and interpretation of the histogram. Most of the time we can use the automatic bin definitions provided by Jamovi.\nWe can use Histograms to explore a few key factors of the wider data distributions that are hard to see from the descriptive statistics tables.\n\nShape: The shape of the histogram can reveal the distribution pattern of the data, such as normal distribution, skewness, or bimodality.\nOutliers: Histograms can help identify outliers or unusual data points that fall outside the general pattern.\n\nJamovi makes it very simple to compute a histogram. Open the ‘Plots’ sub-menu of the descriptive statistics tab and select the ‘Histogram’ checkbox.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a histogram of the participant ages\n\n\n\nThe descriptives tables give us a numerical summary of the data - but this can be hard to build an intuitive understanding of the dataset. A histogram gives us something immediately interpretable on a more ‘human’ level. Have a think about the following questions.\n\n\n\n\n\n\n Data Skills - What is the average age of the pariticpants in this dataset?\n\n\n\n\n\nWe can see that the most common ages are clustered around the late teenage years and early 20s. We can’t tell the exact mean from the histogram alone - we need to compute the descriptives table for this - but histograms are good to get a quick ‘sense’ of the data.\n\n\n\nIf you haven’t heard about the ‘normal distribution’ in your previous studies, this is a good opportunity to ask Copilot to give you a simple explanation.\n\n\n\n\n\n\n Data Skills - Do the ages look normally distributed?\n\n\n\n\n\nThere is a long tail to this distribution with a very small number of participants in their 30s, 40s and 50s. This is a strong positive skew and likely indicates that the data are not normally distributed.\nThis would be hard to tell from the descriptives table alone.\nOverall, this is a good match for what we would expect from a student population at a large university.\n\n\n\nWe can get an alternative perspective by visualising our data using a ‘Box Plot’.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a Box plot of the participant ages\n\n\n\nThe boxplot visualises the median and interquartile range (25th to 75th percentile) of the data, as well as highlighting outliers. This gives us less resolution around the middle of the distribution close to the median - but does give a much clearer view of the long tail in the ages of the student data sample.\n\n\n\n\n\n\n Data Skills - How old is the oldest participant in the dataset??\n\n\n\n\n\nThe most extreme outliers in the dataset are labelled with their row ID in the boxplot. The largest age value is in row 1081 - it looks like this is around 55 in the boxplot.\nWe can check the corresponding row in the data table to get some more precision. This tells us that the oldest participant is exactly 54 years old.\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding 2\n\n\n\nTry your best with this question and don’t forget to speak with your tutor if you get stuck!\n\nQuestionHintSolution\n\n\nCompute histograms and/or boxplots for the big-5 personality factors and take some notes their distributions. Think about the following questions for each factor:\n\nIs the distribution symmetric?\nIf not, is there a positive or a negative skew?\nAre there any outliers?\n\nWrite some quick notes on your answers - we’ll use these again in the next section\n\n\nIt might be easiest to do this one factor at a time to avoid being swamped with figures!\nCreate the figure for a single factor, make some notes and make the next figure with the next factor. You can consider saving the figure into a notebook or document if you want to make a more controlled comparison.\nIf you aren’t sure about any of the concepts - this is a good opportunity to ask your tutors in the session - or use Copilot to give you a simple explanation.\n\n\nHere are my notes about each distribution:\n\nOpenness, this does not look like a normal distribution - there is a negative skew in the data.\nConscientiousness, this is definitely not a normal distribution - there is a strong negative skew.\nExtraversion, this is close to being a normal distribution - perhaps a very slight negative skew.\nAgreeableness, this does not look like a normal distribution - there is a negative skew in the data.\nNeuroticism, this does not look like a normal distribution - there is a positive skew in the data.\n\nNone of the factors have obvious outliers.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#computing-statistics-for-normality-and-skewness.",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#computing-statistics-for-normality-and-skewness.",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "As a final step, we can see if our intuitions about the distribution of big-5 personality factors match with more formal statistics measures checking for normal distributions and skewness.\n\n\n\n\n\n\nKey step\n\n\n\nCompute statistics Shapero-Wilk’s test for normality and for ‘Skewness’. You can add both of these tests from the ‘Statistics’ sub-menu of the Descriptives analysis.\n\n\n\n\n\nThe Shapiro-Wilk test evaluates whether a sample comes from a normally distributed population. It returns 2 values:\n\n‘W’ The W statistic measures how well the data conforms to a normal distribution. It ranges from 0 to 1 where values close to 1 indicate the data is normally distributed.\n‘p’ is a p-value indicating the probability that we could see a value of W at least as large as the one we have measured if the data really were normally distributed. Typically, we would say that the data are not normally distributed if p is less than or equal to 0.05.\n\n\n\n\n\n\n\n Data Skills - Do our notes from the histograms match the Shapiro-Wilk statistics?\n\n\n\n\n\nYes, If we look at the W statistics, then we see that Extraversion has the highest value indicating that it has the most ‘normal’ data distribution whilst the other four factors are similar. This matches what we saw with the histograms.\nThe p-values from the formal hypothesis test indicate that none of the 5 factors have normal distributions. We couldn’t tell this from our informal evaulation of the histograms and box plots.\n\n\n\n\n\n\nThe skewness metric returns a number close to zero if the data are symmetrical around zero, and a number close to +1 or -1 if the data are skewed. The sign of the skewness value indicates whether there is a positive or a negative skew.\n\n\n\n\n\n\n Data Skills - Do our notes from the histograms match the skewness statistic?\n\n\n\n\n\nYes, its very close. We can see that ‘Extraversion’ has the least skew with a Skewness metric very close to zero (-0.0714). In contrast, ‘Conscientiousness’ has the most skew. Its value of ‘-0.656’ indicates a very substantial negative skewness in the distribution.\nOpenness and Agreeableness have moderate negative skews, whilst Neuroticism has a moderate positive skew.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#summary",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#summary",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "In this session we have gone through the process of exploring a dataset from a published scientific article. We have computed a range of descriptive statistics and used histograms and boxplots to visualise the dataset.\nIt is common for publicly available data to contain little mistakes and issues. We need to go through a process like this with every new dataset and it should be like second nature by the end of this module. Remember to use common sense and practical thinking to identify issues and think about possible solutions.\nRemember this process when collecting your own data. Think carefully about how you can avoid typos and mistakes when collecting a dataset. It can save you and your collaborators a lot of time in future!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-1_new-data-and-descriptives.html#references",
    "href": "tutorials/rmb_week-1_new-data-and-descriptives.html#references",
    "title": "Week 1 : Exploring new data and computing descriptives",
    "section": "",
    "text": "Block, Jack. 1995. “A Contrarian View of the Five-Factor Approach to Personality Description.” Psychological Bulletin 117 (2): 187–215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. “Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.” Journal of Experimental Social Psychology 67: 68–82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. “A Very Brief Measure of the Big-Five Personality Domains.” Journal of Research in Personality 37 (6): 504–28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 1 - New data and descriptives"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "This week we will explore how to use Jamovi to compute and visualise descriptive statistics from a dataset. This will include some revision on Jamovi from last semester and an introduction on how we can use the R coding language to simplify complex data processing.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nMeasures of central tendency\n\n\n\nMeasures of dispersion and spread\n\n\n\nMeasures of skew and normality\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Rj editor to use R code within Jamovi\n\n\n\nLearn the basics about functions and variables in R\n\n\n\nComputing descriptive statistics using R\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nValidating and working with openly available research data\n\n\n\nCreate a reproducible data analysis script\n\n\n\n\n\n\n\n\n\n\n\nDataset\n\n\n\nNote that this is the same dataset as in week 1\n\n\nWe will be exploring data that was collected as part of the ‘Many Labs 3’ project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‘Big Five’ personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs. extravagant/careless)\nAgreeableness (friendly/compassionate vs. critical/rational)\nNeuroticism (sensitive/nervous vs. resilient/confident)\nOpenness to experience (inventive/curious vs. consistent/cautious)\nExtraversion (outgoing/energetic vs. solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.\n\n\n\nLast week, we looked at how we can spend some time understanding a new dataset and compute some descriptive statistics from it.\nNow, we’re going to learn how to use Jamovi and R to make this process more repeatable and more transparent. This will make it much easier for us to come back to the analysis and understand what steps were taken at a later date.\n\n\n\n\n\n\nOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.\n\n\n\n\n\nWe’ll be working with the Rj Add-On for Jamovi this week.\n\n\n\n\n\n\nKey step\n\n\n\nBefore continuing, install the Rj add-on for jamovi following the guide for installing Jamovi modules. Once installed you should have a new button on the Ribbon menu at the top of Jamovi…\n\n\n\nClick on this and select ‘Rj Editor’ to open a window in which we can write R code! Click on the small cog icon and make sure that ‘Output’ is set to ‘Show code and output’.\n\nWe can use the left-hand window, named Rj editor, to write commands, using the R language. These are the straightforward text commands that tell the computer a series of steps that we wanted to carry out. These commands can be edited and rerun as many times as we need, they can even be shared with collaborators and included as part of scientific publication.\nThis is a very powerful tool once we have a written set of instructions to work with rather than individually clicking through instructions. Though it takes a bit longer to write the commands, our analysis is much more transparent and reproducible once it is done.\nThe outputs of the analysis defined in the Rj editor appear on the right-hand side of the Jamovi window as usual. To check this out, let’s explore using or as a calculator.\n\n\n\n\n\n\nData Skills - R as a calculator\n\n\n\nTry typing a set of sums or equations into the Rj editor. Include one sum on each line and include as many as you like. When you’re ready, you can tell RJ to run the commands by clicking on the green triangle in the top right corner of the RJ editor. This will automatically compute. All of the sums you to find and print the results out in the right hand side window. see the example below.\nYou can use the following symbols to define your sums.\n\n\n\nSymbol\nOperation\n\n\n\n\n+\nAdd\n\n\n-\nSubtract\n\n\n*\nMultiply\n\n\n/\nDivide\n\n\n\n\n\n\n\n\n\n\nOften, we don’t just want to see the result of a calculation, we want to store the result to use again later. For this, we want to use variables. For example, here we store the result of a calculator sum in a variable named ‘y’\ny &lt;- 2 + 2\nIf you run this in Rj, you’ll notice that we no longer get the number 4 printed to the screen. This is as the output is now caught and stored rather than printed out. If we still want to see what is contained in y, we can use another helpful function named print.\ny &lt;- 2 + 2\nprint(y)\nWill now also display the content of y.\n\n\n\n\n\n\nDefinition of a variable\n\n\n\nA variable is an object that provides a labelled space to store information. The &lt;- or = symbols indicate that we’re assigning some information to a variable (the two symbols behave the same way, you could see either).\nCrucially, the information on the right hand side is assigned to a variable with the name from the left hand side.\n\n\n\n\n\n\n\n\nCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about variables in R. We suggest you try the following prompts to get started.\n\n“Write a simple description of variables in R for a beginner”\n“How can I choose a name for my variable in R”\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\nNotice that the one of the equations in my example is a bit different to the others. The final example uses a function called sqrt as a shortcut to compute the square root of the number nine.\nThe sqrt function calculates the square root of a given number. In R code, this looks something like this:\nsqrt(x)\nwith the ‘x’ replaced by any number. For example, we could compute the square root of 36 using the following command.\nsqrt(36)\nAdd this line to your Rj editor and check the result..\nMany functions, take inputs contained in parentheses written after the name of the function. These inputs are sometimes known as the arguments of a function. In the last example, 36 is an argument for the function sqrt.\nSometimes functions take several arguments, and the arguments themselves can have specific names. For example.\nround(12.345, digits = 0)\nwill round the value x to a whole number with zero digits after the decimal point. Where as\nround(12.345, digits = 2)\nwould round the same number to preserve 2 digits after the decimal point. Try running these examples in Rj with a range of different numbers to get a sense of how it works.\n\n\n\n\n\n\nDefinition of a function\n\n\n\nFunctions are used to perform specific, predefined tasks, such as calculations, data transformations, or visualization. You can think of it as a “black box” that takes in a value, performs some internal calculation, and returns the result.\n\n\n\n\n\n\n\n\nCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about functions in R. We suggest you try the following prompts to get started.\n\n“Write a simple description of functions in R for a beginner”\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\nA single analysis might have several lines of code that create, store and reuse a several different variables with several different functions. For example:\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat does this R code do? what is the value of x, y and z?\n x &lt;- 10 + 10\n y &lt;- sqrt(16)\n z &lt;- x / y\n\n\nTry copying out and running the code in Rj.\nYou can add some print functions to see the value of the intermediate variables x and y as well. Try adding print(x) on the line immediately after x is defined.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final answers:\nx is 20 and y is 4, so z = 20 / 4 which is 5.\n\n\n\n\n\n\n\nRj has a special feature that the data loaded into our spreadsheet is always contained in a special variable named data. We can refer to this data variable to view and run analyses on our dataset.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to load the data file into Jamovi and compute the descriptive statistics for the big five personality factors again.\n\n\n\nThe data we have loaded into our Jamovi session can be accessed in Rj with the data variable. Jamovi automatically makes the data available for us in the R environment.\nWe can view and interact with the data here as well. For example, to view the first few rows of the dataset you can run the head function and pass in data as an argument.\nhead(data)\nand to view the last few rows, you can do the same with the tail function.\ntail(data)\nOften, we’ll only want to work with a subset of the columns in a data frame at any given time. We can access individual columns using the dollar operator. For example, this code will access the age column within the loaded dataset.\ndata$age\nLet’s break this command down into its pieces:\n\n\n\n\n\n\n\nR code\nMeaning\n\n\n\n\ndata\nthis is the name of the whole data array we have loaded in\n\n\n$\nthis is a separator that R uses to help us access subsections of datasets\n\n\nage\nThis is the name of a single column we want to access\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you access the column of Mood scores from the same dataset?\n\n\nYou can use the head function to see the list of all available columns in the dataset. The example above accesses the ‘age’ column, you can tweak this to extract the mood data instead.\n\n\nThe final code should look like this\ndata$Mood\n\n\n\n\n\nWe can also apply functions to these variables. R includes a wide range of standard mathmatical functions that we can use for common computations. This code computes the mean of the age data in our dataset.\nmean(data$age)\nTry running this in Rj. You should get the answer 19.3 - this matches the value we computed using the descriptives analysis last week.\nSimilarly, the following function computes the standard deviation\nsd(data$age)\nIf you run this, you should get the answer 3.808.\n\n\n\nOk, back to statistics! How can we use our new R skills to compute statistics?\nThe trick is to understand that all the analyses in Jamovi are actually carried out by functions in R. Each analysis we can select from the top ribbon in Jamovi corresponds to a function that can be called in the code. Similarly, all of the options that we select in the analysis, dialogue boxes, correspond to arguments for that function. Let’s take a look in practice.\nJamovi provides a collection of R functions that correspond to each analysis dialogue box in Jamovi. This package of functions is named jmv. Thejmv function that computes descriptive statistics is called:\njmv::descriptives\nHere we first name the jmv package specify the descriptives function within it (this link is made with the two colons).\nWe can compute the descriptive statistics for age by passing the data into the function and specifying that we want to use the age variable. Try running the following:\njmv::descriptives(data, vars='age')\nand you should generate the following result table.\n\n\n\n\n\n\n\nNote!\n\n\n\nYou might notice that we still have the unusual ages in this analysis, we’ll discuss how to remove them in the next section\n\n\nJamovi provide a lot of documentation on the descriptives function. This lists every single option that can be passed into jmv::descriptives. There is an option here for every single checkbox and selector in the main Jamovi dialogue.\nThe following code additionally computes a histogram by adding the hist argument.\njmv::descriptives(data, vars='age', hist=TRUE)\n\nWe can end up specifying a lot of these options. Sometimes it can be a good idea to split the function across multiple lines. This is fine as long as the parentheses are in the right place and there are commas in between each option.\nThis code computes descriptive statistics including the skewness, interquartile range and mode.\njmv::descriptives(data, \n  vars='age', \n  skew=TRUE,\n  iqr=TRUE,\n  mode=TRUE,\n  hist=TRUE\n  )\n\nOf course, we could run these analyses by hand. The main advantage here is that we can automate the selection of critical options and reduce the possiblity of mistakes when communicating analyses within teams. As long as we have this line saved, we can rerun it on new datasets without having to recheck every box by hand.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse the examples above and the JMV documentation on the descriptives function to write a function definition that does the following.\nCompute the core descriptive statistics for the Openness variable, make sure to include the range and a boxplot.\n\n\nTake a look at the ‘range’ and ‘box’ options in the documentation page.\nBe careful with the spelling of each argument!\n\n\nThe final code should be:\njmv::descriptives(data, \n  vars='Openness', \n  range=TRUE,\n  box=TRUE\n  )\n\n\n\n\n\n\n\n\n\n\nRemember that last week we very carefully removed several participants who had entered unrealisic or impossible ages into the datasets. This may have been an accident or mistake but either way we should not use this data. Our examples so far have just used all the data.\nR and Rj make it very easy to clean data without the boring, manual work we did last week. R can do the hard work and we can avoid scrolling through very large datasets by hand. Sounds good!\nThe first thing we can do is investigate what range of responses that people actually gave when asked about their age. To do this we can use a new function called unique(). This takes a column of a dataset as an input and returns a list of all the unique responses that it contains.\n\n\n\n\n\n\nKey step\n\n\n\nLoad up an Rj Editor window (or continue in your old one) and run unique() on the age data.\nThe result should look like this:\n\n\n\nMost of these responses are fine but, as we saw last week, there are a lot of strange ones. Most importantly, we can find the extreme values that are very likely errors during data collection (-2 and 149)\nWe need to remove these from our analysis to get the most accurate results.\nTo do this we’re going to use an R library named dplyr (short for ‘Data Pliers’, see their documentation here). This is an extremely powerful library that is very widely used across the world.\nTo use dplyr, we first need to load the library into Rj by adding the following line to our code.\nlibrary(dplyr)\nThis tells Rj that we want to use the functions from this library.\nThe key function we need today is filter() - this extracts rows from a dataset based on logical conditions that we specify in the code. We are going to specify two arguments, firstly we will pass in the data and secondly we will specify the condition.\nIn this example we extract only the rows in which participants have an age above 20.\nlibrary(dplyr)\n\nfilter(data, age&gt;20)\n\nNotice that the dataset printed in the right hand window now only has values from the participants who are older than 20.\nWe can even specify multiple options - here we extract rows with participants older than 20 but still 23 or younger.\nlibrary(dplyr)\n\nfilter(data, age&gt;20, age&lt;=23)\nNext, to recreate our data tidying form last week, we need to extract the rows of participants who are older than -2 and younger than 149.\nThese values might be too specific and might not apply to every dataset. We can apply some common sense and suggest some broad, but much more sensible boundaries for a student population. We also save the result into a variable we can reuse later.\ntidy_data &lt;- filter(data, age &gt; 15, age &lt; 60)\nAnd finally, we can combine this with our call to jmv::descriptives to both filter our data to only the rows we want to use and compute the descriptive statistics on the result.\ntidy_data &lt;- filter(data, age &gt; 15, age &lt; 60)\n\njmv::descriptives(tidy_data, vars='age')\n\n\n\n\n\n\n\nKey step\n\n\n\nPractice changing the age conditions to a few different options to see the difference it makes to the descriptives analysis. What happens when you set a condition that results in zero valid participants?\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse the information in this tutorial to compute descriptive statistics for the Openness variable from Introverted participants. In other words - what is the mean Openness score for participants who are more Introverted than Extraverted.\nCompute the core descriptive statistics for the Openness variable, make sure to include the median and a histogram\n\n\nTo extract the data rows for Introverted participants we want to extract rows where the Extraversion score is less than 3.5\n\n\nThe final code should be:\nlibrary(dplyr)\n\ntidy_data &lt;- filter(data, Extraversion &lt; 3.5)\n\njmv::descriptives(tidy_data, vars='Openness')\n\n\n\n\n\n\n\n\n\n\nIt probably seems like a small difference so far. After all, we same results for our analysis using the check, boxes in Jamovi or the code in R. The Jamovi analysis is certainly quicker to run for the first time and it’s more convenient when exploring your data.\nHowever the R code has several crucial advantages.\n\nThe R code is transparent about what is actually being run to generate the results and by sharing this code, we enable others to repeat our analysis. This makes it really straightforward to run and re-run identical analyses in situations where we need to be really sure that everybody has done the same thing. For example, if we want to be certain that two data sets have been analysed in an identical way then that our code would be the best way to do this.\nR code is the best way to share what we’ve done with the rest of the world. It is common practice for modern academic public it to include the code that was used to generate the results in the paper. This way everybody who wants to check can make sure they fully understand what the authors of the paper have done.\n\nFinally, over the course of the semester we’ll see that R allows us to do much more than what is possible in Jamovi. We can define blocks of R code that can tidy-up and arrange our data, run multiple analyses and generate tables & figures all in one go. Much easier that clicking through all the menus each time we want to rerun an analysis!\n\n\n\n\n\nBlock, Jack. 1995. “A Contrarian View of the Five-Factor Approach to Personality Description.” Psychological Bulletin 117 (2): 187–215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. “Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.” Journal of Experimental Social Psychology 67: 68–82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. “A Very Brief Measure of the Big-Five Personality Domains.” Journal of Research in Personality 37 (6): 504–28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#the-dataset",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#the-dataset",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Dataset\n\n\n\nNote that this is the same dataset as in week 1\n\n\nWe will be exploring data that was collected as part of the ‘Many Labs 3’ project (Ebersole et al. 2016). This collected data from university students in the USA across a wide range of cognitive, behavioural and demographic measures. We will be exploring a subset of this dataset concerning individual differences in personality measures. The experimenters collected data to compute the ‘Big Five’ personality factors from each participant. Wikipedia describes the factors as:\n\nConscientiousness (efficient/organized vs. extravagant/careless)\nAgreeableness (friendly/compassionate vs. critical/rational)\nNeuroticism (sensitive/nervous vs. resilient/confident)\nOpenness to experience (inventive/curious vs. consistent/cautious)\nExtraversion (outgoing/energetic vs. solitary/reserved)\n\nThis is a very simple (Gosling, Rentfrow, and Swann 2003) and influential model that is used to summarise personality differences. Though it is popular, some researchers have criticised big five factors for conceptual and methodological reasons (Block 1995). Read around and form your own view!\nThe data contains the average of two questions about each factor in which participants responded to questions using a scale from 1 to 7. A value of 1 indicates a low score and a value of 7 is a high score. For example, someone with an extraversion score of 2 is more introverted that someone with an extraversion score of 5.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#the-challenge",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#the-challenge",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Last week, we looked at how we can spend some time understanding a new dataset and compute some descriptive statistics from it.\nNow, we’re going to learn how to use Jamovi and R to make this process more repeatable and more transparent. This will make it much easier for us to come back to the analysis and understand what steps were taken at a later date.\n\n\n\n\n\n\nOpening Jamovi\n\n\n\nWe’ll be working with Jamovi throughout this practical.\nWorking on campus? You can open Jamovi through AppsAnywhere, either open the desktop application through the start menu on a campus computer or Birmingham’s AppsAnywhere website.\nTake a look at these frequently asked questions or ask your tutor if you get stuck.\nWorking on your own computer? You can install Jamovi (for free!) folling these instructions. Install the ‘Desktop’ application - not the ‘Cloud’ version.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#install-the-rj-add-on-in-jamovi",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#install-the-rj-add-on-in-jamovi",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "We’ll be working with the Rj Add-On for Jamovi this week.\n\n\n\n\n\n\nKey step\n\n\n\nBefore continuing, install the Rj add-on for jamovi following the guide for installing Jamovi modules. Once installed you should have a new button on the Ribbon menu at the top of Jamovi…\n\n\n\nClick on this and select ‘Rj Editor’ to open a window in which we can write R code! Click on the small cog icon and make sure that ‘Output’ is set to ‘Show code and output’.\n\nWe can use the left-hand window, named Rj editor, to write commands, using the R language. These are the straightforward text commands that tell the computer a series of steps that we wanted to carry out. These commands can be edited and rerun as many times as we need, they can even be shared with collaborators and included as part of scientific publication.\nThis is a very powerful tool once we have a written set of instructions to work with rather than individually clicking through instructions. Though it takes a bit longer to write the commands, our analysis is much more transparent and reproducible once it is done.\nThe outputs of the analysis defined in the Rj editor appear on the right-hand side of the Jamovi window as usual. To check this out, let’s explore using or as a calculator.\n\n\n\n\n\n\nData Skills - R as a calculator\n\n\n\nTry typing a set of sums or equations into the Rj editor. Include one sum on each line and include as many as you like. When you’re ready, you can tell RJ to run the commands by clicking on the green triangle in the top right corner of the RJ editor. This will automatically compute. All of the sums you to find and print the results out in the right hand side window. see the example below.\nYou can use the following symbols to define your sums.\n\n\n\nSymbol\nOperation\n\n\n\n\n+\nAdd\n\n\n-\nSubtract\n\n\n*\nMultiply\n\n\n/\nDivide",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#functions-and-variables",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#functions-and-variables",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Often, we don’t just want to see the result of a calculation, we want to store the result to use again later. For this, we want to use variables. For example, here we store the result of a calculator sum in a variable named ‘y’\ny &lt;- 2 + 2\nIf you run this in Rj, you’ll notice that we no longer get the number 4 printed to the screen. This is as the output is now caught and stored rather than printed out. If we still want to see what is contained in y, we can use another helpful function named print.\ny &lt;- 2 + 2\nprint(y)\nWill now also display the content of y.\n\n\n\n\n\n\nDefinition of a variable\n\n\n\nA variable is an object that provides a labelled space to store information. The &lt;- or = symbols indicate that we’re assigning some information to a variable (the two symbols behave the same way, you could see either).\nCrucially, the information on the right hand side is assigned to a variable with the name from the left hand side.\n\n\nNotice that the one of the equations in my example is a bit different to the others. The final example uses a function called sqrt as a shortcut to compute the square root of the number nine.\n\n\n\n\n\n\nDefinition of a function\n\n\n\nFunctions are used to perform specific, predefined tasks, such as calculations, data transformations, or visualization. You can think of it as a “black box” that takes in a value, performs some internal calculation, and returns the result.\n\n\nThe sqrt function calculates the square root of a given number. In R code, this looks something like this:\nsqrt(x)\nwith the ‘x’ replaced by any number. For example, we could compute the square root of 36 using the following command.\nsqrt(36)\nAdd this line to your Rj editor and check the result..\nMany functions, take inputs contained in parentheses written after the name of the function. These inputs are sometimes known as the arguments of a function. In the last example, 36 is an argument for the function sqrt.\nSometimes functions take several arguments, and the arguments themselves can have specific names. For example.\nround(12.345, digits = 0)\nwill round the value x to a whole number with zero digits after the decimal point. Where as\nround(12.345, digits = 2)\nwould round the same number to preserve 2 digits after the decimal point. Try running these examples in Rj with a range of different numbers to get a sense of how it works.\nA single analysis might have several lines of code that create, store and reuse a several different variables with several different functions. For example:\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat does this R code do? what is the value of x, y and z?\n x &lt;- 10 + 10\n y &lt;- sqrt(16)\n z &lt;- x / y\n\n\nTry copying out and running the code in Rj.\nYou can add some print functions to see the value of the intermediate variables x and y as well. Try adding print(x) on the line immediately after x is defined.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final answers:\nx is 20 and y is 4, so z = 20 / 4 which is 5.\n\n\n\n\n\n\n\nRj has a special feature that the data loaded into our spreadsheet is always contained in a special variable named data. We can refer to this data variable to view and run analyses on our dataset.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#compute-descriptive-statistics-using-r",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#compute-descriptive-statistics-using-r",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Ok, back to statistics! How can we use our new R skills to compute statistics?\nThe trick is to understand that all the analyses in Jamovi are actually carried out by functions in R. Each analysis we can select from the top ribbon in Jamovi corresponds to a function that can be called in the code. Similarly, all of the options that we select in the analysis, dialogue boxes, correspond to arguments for that function. Let’s take a look in practice.\nTo help understand this Jamovi provides an “R Syntax Mode”, in this mode, Jamovi produces equivalent R code for each analysis. To change to syntax mode, select the application menu (⋮) at the top right of Jamovi, and check the Syntax mode checkbox there.\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to load the data file into Jamovi.\nWith syntax mode on, compute the descriptive statistics for the big five personality factors again.\n\n\nThe outputs should look like this:\n\nNotice that some R code has appeared at the top of the output table. This is relatively complex compared to our previous functions like print or sqrt. Let’s break it down a bit.\n\n\n\n\n\n\nData Skills - Understanding the descriptives function\n\n\n\nThis is the code that Jamovi’s syntax mode will produce (with a little formatting to make the appearance simpler). Take a look through each part to understand what it does. Click the number in the annotation to highlight the corresponding part of the code.\n1jmv::descriptives(\n2    data = data,\n3    vars = vars(Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism)\n    )\n\n1\n\nThis is the name of the function that will do the computation for us. Everything contained within these parentheses are known as arguments, or inputs, for the function\n\n2\n\nThe first input is named data and is assigned to be the special data variable that contains the data loaded into our spreadsheet.\n\n3\n\nThe second input is a list of the data columns in our spreadsheet that we want to compute descriptives from.\n\n\n\n\nThere is a lot of information here that might be hard to understand at first. Try changing the analysis options using the Jamovi buttons and watch what happens to the R syntax. In particular, try adding and removing different variables to the analysis.\n\n\n\n\n\n\nKey step\n\n\n\nOnce you’re ready, you can right click on this R syntax and click ‘Copy’ to copy it to your clipboard.\n\nand pasting it into an Rj window - now you can click run to reproduce your descriptive statistics table using the underlying R code.\n\n\nGreat - well done on completing your first data analysis using R code! You should have the following output that will match the descriptives you’ve seen in the main Jamovi window.\n\nHead back over to the Jamovi descriptives tab and try updating your analysis. You could change the variables that you are computing the descriptive statistics from, add in some extra metrics (such as the Skewness or Shapiro-Wilk) or even generate a plot like a histogram.\nNote that the R code in the syntax box of the output will stay up to date with your changes. Pay attention to how the function updates when you select different options.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat is the R code required to compute descriptive statistics from the Openness and Conscientiousness variables, whilst including the Skewness metric, the Shapiro-Wilk statistic and a histogram?\nCan you run this analysis in Rj?\n\n\nRemember that the R code in the syntax mode will update as you define your analysis in Jamovi. Get the analysis right there and you can simply copy over the code.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final code should look like this\njmv::descriptives(\n    data = data,\n    vars = vars(Openness, Conscientiousness),\n    hist = TRUE,\n    skew = TRUE,\n    sw = TRUE)",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#cleaning-data-using-the-filter-function",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#cleaning-data-using-the-filter-function",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Remember that last week we very carefully removed several participants who had entered unrealisic or impossible ages into the datasets. This may have been an accident or mistake but either way we should not use this data.\nR and Rj make it very easy to clean data without the boring, manual work we did last week. R can do the hard work and we can avoid scrolling through very large datasets by hand. Sounds good!\nThe first thing we can do is investigate what range of responses that people actually gave when asked about their age. To do this we can use a new function called unique(). This takes a column of a dataset as an input and returns a list of all the unique responses that it contains.\n\n\n\n\n\n\nKey step\n\n\n\nLoad up an Rj Editor window (or continue in your old one) and run unique() on the age data.\nWe can access a single column of a dataset using the doller sign $. In this case data$age accesses the age column of the data sheet.\n\n\nThe result should look like this:\n\nWhat do you think?\n\n\n\n\n\n\nData Skills - Evaluating the age column\n\n\n\n\n\nOk, most of these responses are fine but there are a lot of strange ones.\n\nFirstly, we can find the extreme values that we saw last week (-2 and 149) - these are clearly mistakes (or some kind of weird prank).\nNext, we can see a lot of people adding text (eg ‘almost 19’). This is not helpful for the computer though it can be socially useful. The computer will read this in words and not numbers. These responses with additional text will count as missing data for any numerical analysis we do.\n&lt;NA&gt; means that the data value is simply missing - perhaps these participants skipped the question.\n\nWe caught some of these issues last time but likely not all. It would be a huge effort to comb through the data line-by-line and identify all these issues.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#references",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#references",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Block, Jack. 1995. “A Contrarian View of the Five-Factor Approach to Personality Description.” Psychological Bulletin 117 (2): 187–215. https://doi.org/10.1037/0033-2909.117.2.187.\n\n\nEbersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. “Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.” Journal of Experimental Social Psychology 67: 68–82. https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012.\n\n\nGosling, Samuel D, Peter J Rentfrow, and William B Swann. 2003. “A Very Brief Measure of the Big-Five Personality Domains.” Journal of Research in Personality 37 (6): 504–28. https://doi.org/10.1016/s0092-6566(03)00046-1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#variables-and-functions",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#variables-and-functions",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Often, we don’t just want to see the result of a calculation, we want to store the result to use again later. For this, we want to use variables. For example, here we store the result of a calculator sum in a variable named ‘y’\ny &lt;- 2 + 2\nIf you run this in Rj, you’ll notice that we no longer get the number 4 printed to the screen. This is as the output is now caught and stored rather than printed out. If we still want to see what is contained in y, we can use another helpful function named print.\ny &lt;- 2 + 2\nprint(y)\nWill now also display the content of y.\n\n\n\n\n\n\nDefinition of a variable\n\n\n\nA variable is an object that provides a labelled space to store information. The &lt;- or = symbols indicate that we’re assigning some information to a variable (the two symbols behave the same way, you could see either).\nCrucially, the information on the right hand side is assigned to a variable with the name from the left hand side.\n\n\n\n\n\n\n\n\nCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about variables in R. We suggest you try the following prompts to get started.\n\n“Write a simple description of variables in R for a beginner”\n“How can I choose a name for my variable in R”\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\nNotice that the one of the equations in my example is a bit different to the others. The final example uses a function called sqrt as a shortcut to compute the square root of the number nine.\nThe sqrt function calculates the square root of a given number. In R code, this looks something like this:\nsqrt(x)\nwith the ‘x’ replaced by any number. For example, we could compute the square root of 36 using the following command.\nsqrt(36)\nAdd this line to your Rj editor and check the result..\nMany functions, take inputs contained in parentheses written after the name of the function. These inputs are sometimes known as the arguments of a function. In the last example, 36 is an argument for the function sqrt.\nSometimes functions take several arguments, and the arguments themselves can have specific names. For example.\nround(12.345, digits = 0)\nwill round the value x to a whole number with zero digits after the decimal point. Where as\nround(12.345, digits = 2)\nwould round the same number to preserve 2 digits after the decimal point. Try running these examples in Rj with a range of different numbers to get a sense of how it works.\n\n\n\n\n\n\nDefinition of a function\n\n\n\nFunctions are used to perform specific, predefined tasks, such as calculations, data transformations, or visualization. You can think of it as a “black box” that takes in a value, performs some internal calculation, and returns the result.\n\n\n\n\n\n\n\n\nCheck Your Understanding with Copilot (Optional)\n\n\n\nTry asking Copilot for more information about functions in R. We suggest you try the following prompts to get started.\n\n“Write a simple description of functions in R for a beginner”\n\nMake sure to ask follow up questions if you would like more information. Remember to talk with your session tutors if you get stuck.\n\n\nA single analysis might have several lines of code that create, store and reuse a several different variables with several different functions. For example:\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat does this R code do? what is the value of x, y and z?\n x &lt;- 10 + 10\n y &lt;- sqrt(16)\n z &lt;- x / y\n\n\nTry copying out and running the code in Rj.\nYou can add some print functions to see the value of the intermediate variables x and y as well. Try adding print(x) on the line immediately after x is defined.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final answers:\nx is 20 and y is 4, so z = 20 / 4 which is 5.\n\n\n\n\n\n\n\nRj has a special feature that the data loaded into our spreadsheet is always contained in a special variable named data. We can refer to this data variable to view and run analyses on our dataset.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#compute-descriptive-statistics-with-r-in-rj",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#compute-descriptive-statistics-with-r-in-rj",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Ok, back to statistics! How can we use our new R skills to compute statistics?\nThe trick is to understand that all the analyses in Jamovi are actually carried out by functions in R. Each analysis we can select from the top ribbon in Jamovi corresponds to a function that can be called in the code. Similarly, all of the options that we select in the analysis, dialogue boxes, correspond to arguments for that function. Let’s take a look in practice.\nTo help understand this Jamovi provides an “R Syntax Mode”, in this mode, Jamovi produces equivalent R code for each analysis. To change to syntax mode, select the application menu (⋮) at the top right of Jamovi, and check the Syntax mode checkbox there. You can find more information on Jamovi’s documentation on Syntax Mode.\nNotice that some R code has appeared at the top of the output table. This is relatively complex compared to our previous functions like print or sqrt. Let’s break it down a bit.\n\n\n\n\n\n\nData Skills - Understanding the descriptives function\n\n\n\nThis is the code that Jamovi’s syntax mode will produce (with a little formatting to make the appearance simpler). Take a look through each part to understand what it does. Click the number in the annotation to highlight the corresponding part of the code.\n1jmv::descriptives(\n2    data = data,\n3    vars = vars(Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism)\n    )\n\n1\n\nThis is the name of the function that will do the computation for us. Everything contained within these parentheses are known as arguments, or inputs, for the function\n\n2\n\nThe first input is named data and is assigned to be the special data variable that contains the data loaded into our spreadsheet.\n\n3\n\nThe second input is a list of the data columns in our spreadsheet that we want to compute descriptives from.\n\n\n\n\nThere is a lot of information here that might be hard to understand at first. Try changing the analysis options using the Jamovi buttons and watch what happens to the R syntax. In particular, try adding and removing different variables to the analysis.\n\n\n\n\n\n\nKey step\n\n\n\nOnce you’re ready, you can right click on this R syntax and click ‘Copy’ to copy it to your clipboard.\n\nand pasting it into an Rj window - now you can click run to reproduce your descriptive statistics table using the underlying R code.\n\n\nGreat - well done on completing your first data analysis using R code! You should have the following output that will match the descriptives you’ve seen in the main Jamovi window.\n\nHead back over to the Jamovi descriptives tab and try updating your analysis. You could change the variables that you are computing the descriptive statistics from, add in some extra metrics (such as the Skewness or Shapiro-Wilk) or even generate a plot like a histogram.\nNote that the R code in the syntax box of the output will stay up to date with your changes. Pay attention to how the function updates when you select different options.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat is the R code required to compute descriptive statistics from the Openness and Conscientiousness variables, whilst including the Skewness metric, the Shapiro-Wilk statistic and a histogram?\nCan you run this analysis in Rj?\n\n\nRemember that the R code in the syntax mode will update as you define your analysis in Jamovi. Get the analysis right there and you can simply copy over the code.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final code should look like this\njmv::descriptives(\n    data = data,\n    vars = vars(Openness, Conscientiousness),\n    hist = TRUE,\n    skew = TRUE,\n    sw = TRUE)",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#selecting-rows-of-data-using-the-filter-function",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#selecting-rows-of-data-using-the-filter-function",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Remember that last week we very carefully removed several participants who had entered unrealisic or impossible ages into the datasets. This may have been an accident or mistake but either way we should not use this data. Our examples so far have just used all the data.\nR and Rj make it very easy to clean data without the boring, manual work we did last week. R can do the hard work and we can avoid scrolling through very large datasets by hand. Sounds good!\nThe first thing we can do is investigate what range of responses that people actually gave when asked about their age. To do this we can use a new function called unique(). This takes a column of a dataset as an input and returns a list of all the unique responses that it contains.\n\n\n\n\n\n\nKey step\n\n\n\nLoad up an Rj Editor window (or continue in your old one) and run unique() on the age data.\nThe result should look like this:\n\n\n\nMost of these responses are fine but, as we saw last week, there are a lot of strange ones. Most importantly, we can find the extreme values that are very likely errors during data collection (-2 and 149)\nWe need to remove these from our analysis to get the most accurate results.\nTo do this we’re going to use an R library named dplyr (short for ‘Data Pliers’, see their documentation here). This is an extremely powerful library that is very widely used across the world.\nTo use dplyr, we first need to load the library into Rj by adding the following line to our code.\nlibrary(dplyr)\nThis tells Rj that we want to use the functions from this library.\nThe key function we need today is filter() - this extracts rows from a dataset based on logical conditions that we specify in the code. We are going to specify two arguments, firstly we will pass in the data and secondly we will specify the condition.\nIn this example we extract only the rows in which participants have an age above 20.\nlibrary(dplyr)\n\nfilter(data, age&gt;20)\n\nNotice that the dataset printed in the right hand window now only has values from the participants who are older than 20.\nWe can even specify multiple options - here we extract rows with participants older than 20 but still 23 or younger.\nlibrary(dplyr)\n\nfilter(data, age&gt;20, age&lt;=23)\nNext, to recreate our data tidying form last week, we need to extract the rows of participants who are older than -2 and younger than 149.\nThese values might be too specific and might not apply to every dataset. We can apply some common sense and suggest some broad, but much more sensible boundaries for a student population. We also save the result into a variable we can reuse later.\ntidy_data &lt;- filter(data, age &gt; 15, age &lt; 60)\nAnd finally, we can combine this with our call to jmv::descriptives to both filter our data to only the rows we want to use and compute the descriptive statistics on the result.\ntidy_data &lt;- filter(data, age &gt; 15, age &lt; 60)\n\njmv::descriptives(tidy_data, vars='age')\n\n\n\n\n\n\n\nKey step\n\n\n\nPractice changing the age conditions to a few different options to see the difference it makes to the descriptives analysis. What happens when you set a condition that results in zero valid participants?\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse the information in this tutorial to compute descriptive statistics for the Openness variable from Introverted participants. In other words - what is the mean Openness score for participants who are more Introverted than Extraverted.\nCompute the core descriptive statistics for the Openness variable, make sure to include the median and a histogram\n\n\nTo extract the data rows for Introverted participants we want to extract rows where the Extraversion score is less than 3.5\n\n\nThe final code should be:\nlibrary(dplyr)\n\ntidy_data &lt;- filter(data, Extraversion &lt; 3.5)\n\njmv::descriptives(tidy_data, vars='Openness')",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#accessing-the-data-table-in-rj",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#accessing-the-data-table-in-rj",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Key step\n\n\n\nTake a moment to load the data file into Jamovi and compute the descriptive statistics for the big five personality factors again.\n\n\n\nThe data we have loaded into our Jamovi session can be accessed in Rj with the data variable. Jamovi automatically makes the data available for us in the R environment.\nWe can view and interact with the data here as well. For example, to view the first few rows of the dataset you can run the head function and pass in data as an argument.\nhead(data)\nand to view the last few rows, you can do the same with the tail function.\ntail(data)\nOften, we’ll only want to work with a subset of the columns in a data frame at any given time. We can access individual columns using the dollar operator. For example, this code will access the age column within the loaded dataset.\ndata$age\nLet’s break this command down into its pieces:\n\n\n\n\n\n\n\nR code\nMeaning\n\n\n\n\ndata\nthis is the name of the whole data array we have loaded in\n\n\n$\nthis is a separator that R uses to help us access subsections of datasets\n\n\nage\nThis is the name of a single column we want to access\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you access the column of Mood scores from the same dataset?\n\n\nYou can use the head function to see the list of all available columns in the dataset. The example above accesses the ‘age’ column, you can tweak this to extract the mood data instead.\n\n\nThe final code should look like this\ndata$Mood\n\n\n\n\n\nWe can also apply functions to these variables. R includes a wide range of standard mathmatical functions that we can use for common computations. This code computes the mean of the age data in our dataset.\nmean(data$age)\nTry running this in Rj. You should get the answer 19.3 - this matches the value we computed using the descriptives analysis last week.\nSimilarly, the following function computes the standard deviation\nsd(data$age)\nIf you run this, you should get the answer 3.808.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#running-desriptive-statistics-using-rj",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#running-desriptive-statistics-using-rj",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "Ok, back to statistics! How can we use our new R skills to compute statistics?\nThe trick is to understand that all the analyses in Jamovi are actually carried out by functions in R. Each analysis we can select from the top ribbon in Jamovi corresponds to a function that can be called in the code. Similarly, all of the options that we select in the analysis, dialogue boxes, correspond to arguments for that function. Let’s take a look in practice.\nJamovi provides a collection of R functions that correspond to each analysis dialogue box in Jamovi. This package of functions is named jmv. Thejmv function that computes descriptive statistics is called:\njmv::descriptives\nHere we first name the jmv package specify the descriptives function within it (this link is made with the two colons).\nWe can compute the descriptive statistics for age by passing the data into the function and specifying that we want to use the age variable. Try running the following:\njmv::descriptives(data, vars='age')\nand you should generate the following result table.\n\n\n\n\n\n\n\nNote!\n\n\n\nYou might notice that we still have the unusual ages in this analysis, we’ll discuss how to remove them in the next section\n\n\nJamovi provide a lot of documentation on the descriptives function. This lists every single option that can be passed into jmv::descriptives. There is an option here for every single checkbox and selector in the main Jamovi dialogue.\nThe following code additionally computes a histogram by adding the hist argument.\njmv::descriptives(data, vars='age', hist=TRUE)\n\nWe can end up specifying a lot of these options. Sometimes it can be a good idea to split the function across multiple lines. This is fine as long as the parentheses are in the right place and there are commas in between each option.\nThis code computes descriptive statistics including the skewness, interquartile range and mode.\njmv::descriptives(data, \n  vars='age', \n  skew=TRUE,\n  iqr=TRUE,\n  mode=TRUE,\n  hist=TRUE\n  )\n\nOf course, we could run these analyses by hand. The main advantage here is that we can automate the selection of critical options and reduce the possiblity of mistakes when communicating analyses within teams. As long as we have this line saved, we can rerun it on new datasets without having to recheck every box by hand.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse the examples above and the JMV documentation on the descriptives function to write a function definition that does the following.\nCompute the core descriptive statistics for the Openness variable, make sure to include the range and a boxplot.\n\n\nTake a look at the ‘range’ and ‘box’ options in the documentation page.\nBe careful with the spelling of each argument!\n\n\nThe final code should be:\njmv::descriptives(data, \n  vars='Openness', \n  range=TRUE,\n  box=TRUE\n  )",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_reproducible-t-tests.html",
    "href": "tutorials/rmb_week-4_reproducible-t-tests.html",
    "title": "Research Methods",
    "section": "",
    "text": "Ok, back to statistics! How can we use our new R skills to compute statistics?\nThe trick is to understand that all the analyses in Jamovi are actually carried out by functions in R. Each analysis we can select from the top ribbon in Jamovi corresponds to a function that can be called in the code. Similarly, all of the options that we select in the analysis, dialogue boxes, correspond to arguments for that function. Let’s take a look in practice.\nTo help understand this Jamovi provides an “R Syntax Mode”, in this mode, Jamovi produces equivalent R code for each analysis. To change to syntax mode, select the application menu (⋮) at the top right of Jamovi, and check the Syntax mode checkbox there. You can find more information on Jamovi’s documentation on Syntax Mode.\nNotice that some R code has appeared at the top of the output table. This is relatively complex compared to our previous functions like print or sqrt. Let’s break it down a bit.\n\n\n\n\n\n\nData Skills - Understanding the descriptives function\n\n\n\nThis is the code that Jamovi’s syntax mode will produce (with a little formatting to make the appearance simpler). Take a look through each part to understand what it does. Click the number in the annotation to highlight the corresponding part of the code.\n1jmv::descriptives(\n2    data = data,\n3    vars = vars(Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism)\n    )\n\n1\n\nThis is the name of the function that will do the computation for us. Everything contained within these parentheses are known as arguments, or inputs, for the function\n\n2\n\nThe first input is named data and is assigned to be the special data variable that contains the data loaded into our spreadsheet.\n\n3\n\nThe second input is a list of the data columns in our spreadsheet that we want to compute descriptives from.\n\n\n\n\nThere is a lot of information here that might be hard to understand at first. Try changing the analysis options using the Jamovi buttons and watch what happens to the R syntax. In particular, try adding and removing different variables to the analysis.\n\n\n\n\n\n\nKey step\n\n\n\nOnce you’re ready, you can right click on this R syntax and click ‘Copy’ to copy it to your clipboard.\n\nand pasting it into an Rj window - now you can click run to reproduce your descriptive statistics table using the underlying R code.\n\n\nGreat - well done on completing your first data analysis using R code! You should have the following output that will match the descriptives you’ve seen in the main Jamovi window.\n\nHead back over to the Jamovi descriptives tab and try updating your analysis. You could change the variables that you are computing the descriptive statistics from, add in some extra metrics (such as the Skewness or Shapiro-Wilk) or even generate a plot like a histogram.\nNote that the R code in the syntax box of the output will stay up to date with your changes. Pay attention to how the function updates when you select different options.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat is the R code required to compute descriptive statistics from the Openness and Conscientiousness variables, whilst including the Skewness metric, the Shapiro-Wilk statistic and a histogram?\nCan you run this analysis in Rj?\n\n\nRemember that the R code in the syntax mode will update as you define your analysis in Jamovi. Get the analysis right there and you can simply copy over the code.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final code should look like this\njmv::descriptives(\n    data = data,\n    vars = vars(Openness, Conscientiousness),\n    hist = TRUE,\n    skew = TRUE,\n    sw = TRUE)"
  },
  {
    "objectID": "tutorials/rmb_week-4_reproducible-t-tests.html#compute-descriptive-statistics-with-r-in-rj",
    "href": "tutorials/rmb_week-4_reproducible-t-tests.html#compute-descriptive-statistics-with-r-in-rj",
    "title": "Research Methods",
    "section": "",
    "text": "Ok, back to statistics! How can we use our new R skills to compute statistics?\nThe trick is to understand that all the analyses in Jamovi are actually carried out by functions in R. Each analysis we can select from the top ribbon in Jamovi corresponds to a function that can be called in the code. Similarly, all of the options that we select in the analysis, dialogue boxes, correspond to arguments for that function. Let’s take a look in practice.\nTo help understand this Jamovi provides an “R Syntax Mode”, in this mode, Jamovi produces equivalent R code for each analysis. To change to syntax mode, select the application menu (⋮) at the top right of Jamovi, and check the Syntax mode checkbox there. You can find more information on Jamovi’s documentation on Syntax Mode.\nNotice that some R code has appeared at the top of the output table. This is relatively complex compared to our previous functions like print or sqrt. Let’s break it down a bit.\n\n\n\n\n\n\nData Skills - Understanding the descriptives function\n\n\n\nThis is the code that Jamovi’s syntax mode will produce (with a little formatting to make the appearance simpler). Take a look through each part to understand what it does. Click the number in the annotation to highlight the corresponding part of the code.\n1jmv::descriptives(\n2    data = data,\n3    vars = vars(Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism)\n    )\n\n1\n\nThis is the name of the function that will do the computation for us. Everything contained within these parentheses are known as arguments, or inputs, for the function\n\n2\n\nThe first input is named data and is assigned to be the special data variable that contains the data loaded into our spreadsheet.\n\n3\n\nThe second input is a list of the data columns in our spreadsheet that we want to compute descriptives from.\n\n\n\n\nThere is a lot of information here that might be hard to understand at first. Try changing the analysis options using the Jamovi buttons and watch what happens to the R syntax. In particular, try adding and removing different variables to the analysis.\n\n\n\n\n\n\nKey step\n\n\n\nOnce you’re ready, you can right click on this R syntax and click ‘Copy’ to copy it to your clipboard.\n\nand pasting it into an Rj window - now you can click run to reproduce your descriptive statistics table using the underlying R code.\n\n\nGreat - well done on completing your first data analysis using R code! You should have the following output that will match the descriptives you’ve seen in the main Jamovi window.\n\nHead back over to the Jamovi descriptives tab and try updating your analysis. You could change the variables that you are computing the descriptive statistics from, add in some extra metrics (such as the Skewness or Shapiro-Wilk) or even generate a plot like a histogram.\nNote that the R code in the syntax box of the output will stay up to date with your changes. Pay attention to how the function updates when you select different options.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat is the R code required to compute descriptive statistics from the Openness and Conscientiousness variables, whilst including the Skewness metric, the Shapiro-Wilk statistic and a histogram?\nCan you run this analysis in Rj?\n\n\nRemember that the R code in the syntax mode will update as you define your analysis in Jamovi. Get the analysis right there and you can simply copy over the code.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final code should look like this\njmv::descriptives(\n    data = data,\n    vars = vars(Openness, Conscientiousness),\n    hist = TRUE,\n    skew = TRUE,\n    sw = TRUE)"
  },
  {
    "objectID": "tutorials/rmb_week-2_intro-to-Rj.html#summary",
    "href": "tutorials/rmb_week-2_intro-to-Rj.html#summary",
    "title": "Week 2 : Computing reproducible statistics using Rj",
    "section": "",
    "text": "It probably seems like a small difference so far. After all, we same results for our analysis using the check, boxes in Jamovi or the code in R. The Jamovi analysis is certainly quicker to run for the first time and it’s more convenient when exploring your data.\nHowever the R code has several crucial advantages.\n\nThe R code is transparent about what is actually being run to generate the results and by sharing this code, we enable others to repeat our analysis. This makes it really straightforward to run and re-run identical analyses in situations where we need to be really sure that everybody has done the same thing. For example, if we want to be certain that two data sets have been analysed in an identical way then that our code would be the best way to do this.\nR code is the best way to share what we’ve done with the rest of the world. It is common practice for modern academic public it to include the code that was used to generate the results in the paper. This way everybody who wants to check can make sure they fully understand what the authors of the paper have done.\n\nFinally, over the course of the semester we’ll see that R allows us to do much more than what is possible in Jamovi. We can define blocks of R code that can tidy-up and arrange our data, run multiple analyses and generate tables & figures all in one go. Much easier that clicking through all the menus each time we want to rerun an analysis!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 2 - Introducing R and Rj"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "This week we will explore how to use Jamovi to compute and visualise descriptive statistics from a dataset. This will include some revision on Jamovi from last semester and an introduction on how we can use the R coding language to simplify complex data processing.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nMeasures of central tendency\n\n\n\nMeasures of dispersion and spread\n\n\n\nMeasures of skew and normality\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nWorking with the Rj editor to use R code within Jamovi\n\n\n\nLearn the basics about functions and variables in R\n\n\n\nComputing descriptive statistics using R\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nValidating and working with openly available research data\n\n\n\nCreate a reproducible data analysis script\n\n\n\n\n\nMetaphors are often used to help people understand abstract concepts in terms of perceptual experiences. Though this is often useful, it can bias perception and decision making in unexpected ways. For example, North and South are abstract concepts that have a spatial association (north = up, south = down). Participants in the USA have a biased expectation that richer people will live in the north of a city (Meier et al. 2011). However, these expectations are not uniform across all people and cultures and we should not naively generalise this result to all people. A follow up study comparing this effect in participants from the USA and Hong Kong replicated the original effect for American participants but found the opposite effect in people from Hong Kong (Huang, Tse, and Cho 2014).\nThe Many Labs 2 project (Klein et al. 2018) performed an additional replication of this effect across a wide range of countries. Participants were presented with the following map outline of a city:\n\nand asked to indicate where they though a fictional person would live within the city. Participants were shown one of two descriptions of the person.\n\n“Dr. Bennett lives in the city. He is a wealthy businessman who has travelled the world. He inherited a significant amount of money from a Great Aunt, and was educated at the best schools growing up. He enjoys fine dining and going to the theater on weekends.”\n\nOr\n\n“Mr. Bennett lives in the city. He is unemployed. He was born and raised in the city he now calls home. He struggles to pay the rent each month, and dropped out of high school before graduation. He enjoys a good hot dog and a six pack of beers when he can.”\n\nThe first description is a caricature of a person with high Socio-Economic Status (SES) whilst the second is a caricature of someone with a low SES. The hypothesis is that people in the United States and Hong Kong have different demographic knowledge that may shape their metaphoric link between positive association and cardinal direction (north vs. south).\n\n\n\nThis week we will use one-sample t-tests to explore whether participants from the UK would expect people with low or high SES to live north or south of the centre of the city.\n\n\n\n\n\n\nA new hypothesis?\n\n\n\nBefore we start, think about what you would predict about participants from the UK. Is there a reason to expect that people in the UK would associate the north of a city with high SES or vice versa?\nMake a note of your answer and the reasons behind it. Note that there isn’t necessarily a correct answer to this question and we won’t ask you for it later.\nIt is good practice to be clear about our predictions and expectations before starting a data analysis.\n\n\n\n\n\nLet’s start by loading the dataset into Jamovi and checking out the data. You can use the file named RMB-2024_computer-practical_huang-2014_country-UK_cond-All.csv.\nThis dataset has several columns of data. The main column of interest is currently named variable. This represents the y-axis position of the persons expectation about where the fictional character would live. A value of zero represents the middle of the city, positive values represent more northern locations and negative values represent more southern locations.\nThe other key variable is named factor. This indicates which group the participant was allocated to. This can be either ‘high’ or ‘low’ depending on whether the participant saw a description of a person with stereotypes of high or low socioeconomic status.\nThis is open-access data and the column names aren’t very informative. It would be better to rename them to something more considered so that we can avoid mistakes later.\n\n\n\n\n\n\nKey step\n\n\n\nRename the following variables using the editor under the ‘Variables’ tab.\n\n‘variable’ should be renamed to ‘NorthSouth_Position’, and\n‘factor’ should be renamed to ‘SES_Story’.\n\nDon’t include any spaces in the new variable names! this can cause an issue later.\n\n\n\nNext, take a look through the data itself. Remember the steps we took in the last couple of weeks. Have a careful scan through the data values and see if you can spot anything that needs to be fixed.\n\n\n\n\n\n\n Data Skills - how many participants are included in the dataset?\n\n\n\n\n\nWe have 136 rows of data observations the spreadsheet.\n\n\n\n\n\n\n\n\n\n Data Skills - what experimental design was used in this experiment?\n\n\n\n\n\nThis is a between-subjects design, though that might not be obvious from the data alone! Checking through the paper methods we can see that each participant was either in the high or low SES condition.\n\n\n\n\n\n\n\n\n\n Data Skills - do we need to tidy up this dataset?\n\n\n\n\n\nNo, in this case the dataset looks very clean. We appear to have data for everyone without any obvious typos or missing information. Looks good to go.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nCompute some descriptive statistics on the NorthSouth column. Make sure that you include the standard error of the mean!\nYou can split the descriptive statistics using a second variable. This\n\n\n\nNext, let’s think about what these values can tell us.\n\n\n\n\n\n\n Data Skills - do we have an equal number of data points in each condition?\n\n\n\n\n\nNo, we have 75 participants in the High condition and 61 participants in the Low condition.\n\n\n\n\n\n\n\n\n\n Data Skills - which condition has a wider variation in NorthSouth Positions chosen by participants?\n\n\n\n\n\nWe can see this from the standard deviations. The High condition has a standard deviation of 63.2 and the Low condition has a standard deviation of 77.8.\nThe responses in the Low SES story condition were more variable.\n\n\n\n\n\n\n\n\n\n Data Skills - which condition has a more northern mean position?\n\n\n\n\n\nRecall from the introduction that positive values indicate a position in the north of the city. The High condition has a mean value of -39.0 and the Low condition has a mean value of 18.6\nThe responses in the Low SES story condition were more northern.\n\n\n\n\n\n\n\n\n\n Data Skills - can we tell from the descriptives whether this is a significant difference?\n\n\n\n\n\nNo! remember that descriptive statistics only summarise the data as it is. They cannot tell us whether we would expect what we see in that data to generalise beyond the specific data sample.\n\n\n\n\n\n\n\n\n\n Data Skills - are we more confident in our estimate of the mean for the ‘North’ or ‘South’ condition?\n\n\n\n\n\nThe standard error of the mean tells us how well we think our estimated mean from the data sample represents the ‘true’ population mean.\nThe standard error of the mean for the ‘Low’ group is higher (9.96 vs 7.3 for ‘High’) - indicating that we are less confident that this is a good representation of our population mean. Note that it could still be a good estimate, we’re just relatively less confident than we are for the ‘High’ condition.\n\n\n\n\n\n\nNext, we’re going to ask whether the participants in the either the High or Low SES conditions had a preference towards locating our fictional character in the North or South of the city. This is our one-sample t-test. The NorthSouth_Position is our data variable and the centre point of the city is the comparison value.\nFirst, we need to split out the data so that we can run the t-test using either the Low or High SES conditions, not using both mixed together.\nWe can do this in Jamovi using a ‘Filter’. Jamovi provides some great documentation on data filters on their website. Take a few minutes and read through the info on ‘Row Filters’ on this page.\nThe most important thing about filters is that they are reversible. We can apply a filter to run an analysis on a subset of the data without actually changing the dataset itself. This means we could make several different filters and turn them on and off to repeat the same analyses on different sections of the dataset. Convenient!\n\n\n\n\n\n\nKey step\n\n\n\nFollowing the guidance on the Jamovi documentation on data filters, create a filter to select ONLY the participants who heard the low SES story.\nThe result should exclude the first 75 rows from your dataset, you can confirm this by looking at the ticks and crosses in the ‘Filter’ column, and by inspecting the values in the very bottom of the window.\n\n\n\nNote that your analyses in results windows will automatically update when you apply a filter to the data. With the filter applied, you will only see the descriptive statistics for the ‘Low’ SES_Story groups.\nIf you click the ‘active’ toggle in the ‘Filter’ window you can turn the filter off and your analysis will return to its initial state.\n\n\n\n\n\n\nKey step\n\n\n\nWith your first Filter turned OFF, create a filter to select ONLY the participants who heard the High SES story.\nThe result should now exclude the last 61 rows from your dataset.\n You can turn your filters on and off to see how this changes the data available for the analysis, though turning both filters on will exclude all the data in this case…\n\n\n\n\n\nNow we’re ready to test our hypothesis! remember that the equation for a one-sample t-test is the following:\n\\[\nt = \\frac{\\text{The sample mean} - \\text{Comparison Value}}{\\text{The standard error of the mean}}\n\\]\nWe’re looking at the difference between our observed mean and a comparison value as a ratio to the precision to which we have estimated our observed mean.\nOur descriptive statistics output contains the mean and standard error of the mean for both groups. Our comparison value is the centre of the city, indicated by a score of 0 (zero).\nWe all the values required for a one-sample t-test already in the descriptive statistics! For the ‘Low’ SES_Story condition, we have:\n\nGroup Mean = 18.6\nComparison Value = 0\nStandard error of the mean = 9.96\n\nSo, the one sample t-test value is:\n\\[\nt = \\frac{18.6 - 0}{9.96} = 1.867\n\\]\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute the t-value for the High SES Story group using a calculator or Rj?\n\n\nFirst, get the three values that you’ll need for the computation. These are the comparison value and the mean and standard error of the mean for the High SES group.\n\n\nThe individual values are\n\nGroup Mean = -39.0\nComparison Value = 0\nStandard error of the mean = 7.30\n\nso the t-value is\n\\[\nt = \\frac{-39.0 - 0}{7.30} = -5.342\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nBefore going any further - make sure that your data filter is set to include only the data from the ‘Low’ SES_Story condition.\n\n\nOf course, we will normally compute the t-test using Jamovi as rather than relying on hand calculations. We can do this by opening the ‘One Sample T-Test’ option under the ‘Analysis -&gt; T-Tests’ menu on the top ribbon.\nOpen the One Sample T-Test option now, and drag the NorthSouth_Position variable over to the ‘dependent variables’ box. The result should appear on the right hand side. Before we inspect the result, we should check and add a number of important options to our analysis.\n\n\nHypothesis - test value it is critical that we select the correct hypothesis or our test will be meaningless. Our ‘test value’, also known as a ‘comparison value’, is set to zero by default. This is correct for our analysis but may need setting to something else for other analyses. Take care to check it makes sense each time.\nHypothesis - test tail we can also specify whether we’re going to perform a one- or two-tailed test. Here we want to specify a two-tailed test in which the mean does not equal the test value.\nAssumption Checks we should add a test to check that our data are normally distributed. Selecting the ‘Normality test’ box will compute a Shapio-Wilks statistic for you\nAdditional Statistics there are a number of important options here. For now, add the ‘Descriptive plots’ option to your analysis. This will show us a visualisation of the estimated mean and its confidence intervals.\n\nLet’s take a look at the outputs, first the t-test table.\n\nThis contains the information we need to report our t-test: the t-value, the degrees of freedom and the p-value.\n\nt(60) = 1.87, p = 0.066\n\nThis t-value should match the number you computed by hand earlier on. Note that the footnote of the table reminds us of the hypothesis that we specified. Make sure that this matches what you expect!\nNext, the normality check:\n\nThis reports the Shapiro-Wilk W statistic and associated p-value. In this case the p-value is not significant which indicates that the data are normally distributed and we’re happy to go ahead with the parametric t-test.\nNext, the descriptives plot:\n\nThis is a really useful visualisation of our test. We can see the mean of the data (black dot) and the test value (dotted line). We also have the 95% confidence intervals of the estimated mean. We can see that the confidence intervals overlap with the test value - this tells us that it is likely our test will not be significant - according to our confidence intervals, the true population mean might include our test value zero which would indicate no real difference.\n\n\n\n\n\n\nKey step\n\n\n\nChange your data filter to compute the one-sample t-test for the ‘High’ SES_Story condition. The test should update once your filter is on without you having to respecify the test.\n\n\n\n\n\n\n\n\n Data Skills - Is the mean position of participants in the ‘High’ condition significantly different to zero?\n\n\n\n\n\nYes, we do have a significant difference - we could report our test as follows\n\nt(74) = -5.35, p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n Data Skills - Is it valid to use parametric statistics on this data?\n\n\n\n\n\nThe Shapiro Wilk statistic has a p-value of 0.038 which is less than p = 0.05. This suggets that our data are not normally distributed. We should inspect the data in more detail and consider using a non-parametric alternative.\nThe Wilcoxon Rank test is the non-parametric alternative for One Sample t-test. You can add this to the analysis to see if it gives the same result as the parametric test.\n\n\n\n\n\n\n\n\n\n Data Skills - Do the confidence intervals of the estimated mean cross zero?\n\n\n\n\n\nNo - in the ‘High’ condition the confidence intervals of the mean are a long way from zero.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nGo back to your notes from the start of the session. Do the results match your predictions about whether a UK data sample would expect a high or low socio-economic status people to live in the north or south of the city?\n\n\n\n\n\n\nFinally - we should make a reproducible version of our analysis using Rj.\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and add the following line to load the dplyr library.\nlibrary(dplyr)\n\n\nFirstly, we’ll need to replicate our data filter to select only the ‘High’ or ‘Low’ conditions for our analysis. We covered this in detail last week - if you need a refresher, take a look at the information on selecting rows of data using the filter function from week 2\nLet’s build a filter to select only the ‘High’ condition.\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\nNext - turn on Jamovi’s syntax mode to display the R code associated with our analyses. This should create some R code at the top of each results block.\n\nWe can right click on this code to copy it before pasting it into the Rj window. Our code should then look like this:\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\n\njmv::ttestOneS(\n    data = data,\n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\n\n\n\n\n\n\nWarning\n\n\n\nSome people have encountered an issue with running this code block - if your R session stalls whilst running the code above, please try again whilst removing plots=TRUE.\nApologies - we’re working on a fix for this.\n\n\nThis command runs the function ttestOneS which computes the One Sample t-test. Jamovi has automatically detected that we want to add the normality test and the descriptive plots and included these options in the function call (see norm=TRUE and plots=TRUE).\nThere is a keyword argument in R for every option in Jamovi - take a look at the jmv library documentation for ttestOneS to see a full list.\nBefore running the code we need to make one adjustment. The function runs on the full dataset data by default but we want to use our new filtered data high_ses instead. Update the function to take in the filtered data as an input.\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\n\njmv::ttestOneS(\n    data = high_ses,\n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\nNow running the code should output result tables that match our previous analysis. Importantly the form of data preprocessing (the filtering) and the test itself are all clearly specified in the code. Making it easier for others to understand what analysis we have run.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you add a code block to compute the one-sample t-test for only the ‘Low’ SES_Story condition\n\n\nRemember that you’ll need to write a new filter line to create a copy of the dataset with only the ‘Low’ values for SES_Story. You can save this into a variable and pass it in as the data argument to ttestOneS\n\n\nYour final code should look like this:\nlow_ses &lt;- filter(data, SES_Story == 'Low')\n\njmv::ttestOneS(\n    data = low_ses, \n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\n\n\n\n\n\nYou can keep adding R code to your script to run multiple analyses at once, this way we can run the t-test for both conditions with out having to go back and forth in Jamovi toggling the data filters on and off.\nIt takes a little more setup but the R code has some big advantages once you get used to it!\n\n\n\nWe’ve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#the-dataset",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#the-dataset",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Metaphors are often used to help people understand abstract concepts in terms of perceptual experiences. Though this is often useful, it can bias perception and decision making in unexpected ways. For example, North and South are abstract concepts that have a spatial association (north = up, south = down). Participants in the USA have a biased expectation that richer people will live in the north of a city (Meier et al. 2011). However, these expectations are not uniform across all people and cultures and we should not naively generalise this result to all people. A follow up study comparing this effect in participants from the USA and Hong Kong replicated the original effect for American participants but found the opposite effect in people from Hong Kong (Huang, Tse, and Cho 2014).\nThe Many Labs 2 project (Klein et al. 2018) performed an additional replication of this effect across a wide range of countries. Participants were presented with the following map outline of a city:\n\nand asked to indicate where they though a fictional person would live within the city. Participants were shown one of two descriptions of the person.\n\n“Dr. Bennett lives in the city. He is a wealthy businessman who has travelled the world. He inherited a significant amount of money from a Great Aunt, and was educated at the best schools growing up. He enjoys fine dining and going to the theater on weekends.”\n\nOr\n\n“Mr. Bennett lives in the city. He is unemployed. He was born and raised in the city he now calls home. He struggles to pay the rent each month, and dropped out of high school before graduation. He enjoys a good hot dog and a six pack of beers when he can.”\n\nThe first description is a caricature of a person with high Socio-Economic Status (SES) whilst the second is a caricature of someone with a low SES. The hypothesis is that people in the United States and Hong Kong have different demographic knowledge that may shape their metaphoric link between positive association and cardinal direction (north vs. south).",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#the-challenge",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#the-challenge",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "This week we will use one-sample t-tests to explore whether participants from the UK would expect people with low or high SES to live north or south of the centre of the city.\n\n\n\n\n\n\nA new hypothesis?\n\n\n\nBefore we start, think about what you would predict about participants from the UK. Is there a reason to expect that people in the UK would associate the north of a city with high SES or vice versa?\nMake a note of your answer and the reasons behind it. Note that there isn’t necessarily a correct answer to this question and we won’t ask you for it later.\nIt is good practice to be clear about our predictions and expectations before starting a data analysis.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#getting-started-with-descriptive-statistics",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#getting-started-with-descriptive-statistics",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Let’s start by loading the dataset into Jamovi and checking out the data. You can use the file named RMB-2024_computer-practical_huang-2014_country-UK_cond-All.csv.\nThis dataset has several columns of data. The main column of interest is currently named variable. This represents the y-axis position of the persons expectation about where the fictional character would live. A value of zero represents the middle of the city, positive values represent more northern locations and negative values represent more southern locations.\nThe other key variable is named factor. This indicates which group the participant was allocated to. This can be either ‘high’ or ‘low’ depending on whether the participant saw a description of a person with stereotypes of high or low socioeconomic status.\nThis is open-access data and the column names aren’t very informative. It would be better to rename them to something more considered so that we can avoid mistakes later.\n\n\n\n\n\n\nKey step\n\n\n\nRename the following variables using the editor under the ‘Variables’ tab.\n\n‘variable’ should be renamed to ‘NorthSouth_Position’, and\n‘factor’ should be renamed to ‘SES_Story’.\n\nDon’t include any spaces in the new variable names! this can cause an issue later.\n\n\n\nNext, take a look through the data itself. Remember the steps we took in the last couple of weeks. Have a careful scan through the data values and see if you can spot anything that needs to be fixed.\n\n\n\n\n\n\n Data Skills - how many participants are included in the dataset?\n\n\n\n\n\nWe have 136 rows of data observations the spreadsheet.\n\n\n\n\n\n\n\n\n\n Data Skills - what experimental design was used in this experiment?\n\n\n\n\n\nThis is a between-subjects design, though that might not be obvious from the data alone! Checking through the paper methods we can see that each participant was either in the high or low SES condition.\n\n\n\n\n\n\n\n\n\n Data Skills - do we need to tidy up this dataset?\n\n\n\n\n\nNo, in this case the dataset looks very clean. We appear to have data for everyone without any obvious typos or missing information. Looks good to go.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nCompute some descriptive statistics on the NorthSouth column. Make sure that you include the standard error of the mean!\nYou can split the descriptive statistics using a second variable. This\n\n\n\nNext, let’s think about what these values can tell us.\n\n\n\n\n\n\n Data Skills - do we have an equal number of data points in each condition?\n\n\n\n\n\nNo, we have 75 participants in the High condition and 61 participants in the Low condition.\n\n\n\n\n\n\n\n\n\n Data Skills - which condition has a wider variation in NorthSouth Positions chosen by participants?\n\n\n\n\n\nWe can see this from the standard deviations. The High condition has a standard deviation of 63.2 and the Low condition has a standard deviation of 77.8.\nThe responses in the Low SES story condition were more variable.\n\n\n\n\n\n\n\n\n\n Data Skills - which condition has a more northern mean position?\n\n\n\n\n\nRecall from the introduction that positive values indicate a position in the north of the city. The High condition has a mean value of -39.0 and the Low condition has a mean value of 18.6\nThe responses in the Low SES story condition were more northern.\n\n\n\n\n\n\n\n\n\n Data Skills - can we tell from the descriptives whether this is a significant difference?\n\n\n\n\n\nNo! remember that descriptive statistics only summarise the data as it is. They cannot tell us whether we would expect what we see in that data to generalise beyond the specific data sample.\n\n\n\n\n\n\n\n\n\n Data Skills - are we more confident in our estimate of the mean for the ‘North’ or ‘South’ condition?\n\n\n\n\n\nThe standard error of the mean tells us how well we think our estimated mean from the data sample represents the ‘true’ population mean.\nThe standard error of the mean for the ‘Low’ group is higher (9.96 vs 7.3 for ‘High’) - indicating that we are less confident that this is a good representation of our population mean. Note that it could still be a good estimate, we’re just relatively less confident than we are for the ‘High’ condition.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#reproducible-one-sample-t-tests",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#reproducible-one-sample-t-tests",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Finally - we should make a reproducible version of our analysis using Rj.\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and add the following line to load the dplyr library.\nlibrary(dplyr)\n\n\nFirstly, we’ll need to replicate our data filter to select only the ‘High’ or ‘Low’ conditions for our analysis. We covered this in detail last week - if you need a refresher, take a look at the information on selecting rows of data using the filter function from week 2\nLet’s build a filter to select only the ‘High’ condition.\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\nNext - turn on Jamovi’s syntax mode to display the R code associated with our analyses. This should create some R code at the top of each results block.\n\nWe can right click on this code to copy it before pasting it into the Rj window. Our code should then look like this:\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\n\njmv::ttestOneS(\n    data = data,\n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\n\n\n\n\n\n\nWarning\n\n\n\nSome people have encountered an issue with running this code block - if your R session stalls whilst running the code above, please try again whilst removing plots=TRUE.\nApologies - we’re working on a fix for this.\n\n\nThis command runs the function ttestOneS which computes the One Sample t-test. Jamovi has automatically detected that we want to add the normality test and the descriptive plots and included these options in the function call (see norm=TRUE and plots=TRUE).\nThere is a keyword argument in R for every option in Jamovi - take a look at the jmv library documentation for ttestOneS to see a full list.\nBefore running the code we need to make one adjustment. The function runs on the full dataset data by default but we want to use our new filtered data high_ses instead. Update the function to take in the filtered data as an input.\nlibrary(dplyr)\n\nhigh_ses &lt;- filter(data, SES_Story == 'High')\n\njmv::ttestOneS(\n    data = high_ses,\n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\nNow running the code should output result tables that match our previous analysis. Importantly the form of data preprocessing (the filtering) and the test itself are all clearly specified in the code. Making it easier for others to understand what analysis we have run.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you add a code block to compute the one-sample t-test for only the ‘Low’ SES_Story condition\n\n\nRemember that you’ll need to write a new filter line to create a copy of the dataset with only the ‘Low’ values for SES_Story. You can save this into a variable and pass it in as the data argument to ttestOneS\n\n\nYour final code should look like this:\nlow_ses &lt;- filter(data, SES_Story == 'Low')\n\njmv::ttestOneS(\n    data = low_ses, \n    vars = NorthSouth_Position,\n    norm = TRUE,\n    plots = TRUE)\n\n\n\n\n\nYou can keep adding R code to your script to run multiple analyses at once, this way we can run the t-test for both conditions with out having to go back and forth in Jamovi toggling the data filters on and off.\nIt takes a little more setup but the R code has some big advantages once you get used to it!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#data-filters-in-jamovi",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#data-filters-in-jamovi",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Next, we’re going to ask whether the participants in the either the High or Low SES conditions had a preference towards locating our fictional character in the North or South of the city. This is our one-sample t-test. The NorthSouth_Position is our data variable and the centre point of the city is the comparison value.\nFirst, we need to split out the data so that we can run the t-test using either the Low or High SES conditions, not using both mixed together.\nWe can do this in Jamovi using a ‘Filter’. Jamovi provides some great documentation on data filters on their website. Take a few minutes and read through the info on ‘Row Filters’ on this page.\nThe most important thing about filters is that they are reversible. We can apply a filter to run an analysis on a subset of the data without actually changing the dataset itself. This means we could make several different filters and turn them on and off to repeat the same analyses on different sections of the dataset. Convenient!\n\n\n\n\n\n\nKey step\n\n\n\nFollowing the guidance on the Jamovi documentation on data filters, create a filter to select ONLY the participants who heard the low SES story.\nThe result should exclude the first 75 rows from your dataset, you can confirm this by looking at the ticks and crosses in the ‘Filter’ column, and by inspecting the values in the very bottom of the window.\n\n\n\nNote that your analyses in results windows will automatically update when you apply a filter to the data. With the filter applied, you will only see the descriptive statistics for the ‘Low’ SES_Story groups.\nIf you click the ‘active’ toggle in the ‘Filter’ window you can turn the filter off and your analysis will return to its initial state.\n\n\n\n\n\n\nKey step\n\n\n\nWith your first Filter turned OFF, create a filter to select ONLY the participants who heard the High SES story.\nThe result should now exclude the last 61 rows from your dataset.\n You can turn your filters on and off to see how this changes the data available for the analysis, though turning both filters on will exclude all the data in this case…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#one-sample-t-tests",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#one-sample-t-tests",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Now we’re ready to test our hypothesis! remember that the equation for a one-sample t-test is the following:\n\\[\nt = \\frac{\\text{The sample mean} - \\text{Comparison Value}}{\\text{The standard error of the mean}}\n\\]\nWe’re looking at the difference between our observed mean and a comparison value as a ratio to the precision to which we have estimated our observed mean.\nOur descriptive statistics output contains the mean and standard error of the mean for both groups. Our comparison value is the centre of the city, indicated by a score of 0 (zero).\nWe all the values required for a one-sample t-test already in the descriptive statistics! For the ‘Low’ SES_Story condition, we have:\n\nGroup Mean = 18.6\nComparison Value = 0\nStandard error of the mean = 9.96\n\nSo, the one sample t-test value is:\n\\[\nt = \\frac{18.6 - 0}{9.96} = 1.867\n\\]\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute the t-value for the High SES Story group using a calculator or Rj?\n\n\nFirst, get the three values that you’ll need for the computation. These are the comparison value and the mean and standard error of the mean for the High SES group.\n\n\nThe individual values are\n\nGroup Mean = -39.0\nComparison Value = 0\nStandard error of the mean = 7.30\n\nso the t-value is\n\\[\nt = \\frac{-39.0 - 0}{7.30} = -5.342\n\\]",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/frequently-asked-questions.html",
    "href": "tutorials/frequently-asked-questions.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "First check the start menu, if you can find both Jamovi and Rstudio then just open both from there and get started. Any programme that is not in the start menu can be opened using AppsAnywhere. This is available in the start menu of every campus computer – general information about AppsAnywhere can be found on this IT services page\n\n\n\nThis should be available as an icon on the desktop, or from within the start menu. If not, they you can try the web interface - https://apps.bham.ac.uk/ . The website may ask you to confirm that AppsAnywhere is installed on your device – you can click ‘I have it already’ if using a campus computer.\n\n\n\nOnce you have access to AppsAnywhere, you may have to log-in using your UoB credentials. The homepage of AppsAnywhere contains a search bar and will look something like this:\n\nYou can search for Jamovi and Rstudio in the search bar and click ‘Launch’ to run them. If this is the first time you’re running the app it may take a few minutes to get started. This is as AppsAnywhere is downloading and installing the programme on your computer.\n\n\n\nAppyAnywhere needs to be able to link to your desktop computer to run properly, the validation process ensures that this is working properly. Normally all you need to do is click to start validation and answer ‘Open AppsAnywhere Launcher’ when prompted by the browser.",
    "crumbs": [
      "Home",
      "Support",
      "FAQs"
    ]
  },
  {
    "objectID": "tutorials/frequently-asked-questions.html#i-cant-find-jamovirstudio-on-my-campus-computer.",
    "href": "tutorials/frequently-asked-questions.html#i-cant-find-jamovirstudio-on-my-campus-computer.",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "First check the start menu, if you can find both Jamovi and Rstudio then just open both from there and get started. Any programme that is not in the start menu can be opened using AppsAnywhere. This is available in the start menu of every campus computer – general information about AppsAnywhere can be found on this IT services page",
    "crumbs": [
      "Home",
      "Support",
      "FAQs"
    ]
  },
  {
    "objectID": "tutorials/frequently-asked-questions.html#i-cant-find-appsanywhere-on-my-campus-computer.",
    "href": "tutorials/frequently-asked-questions.html#i-cant-find-appsanywhere-on-my-campus-computer.",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "This should be available as an icon on the desktop, or from within the start menu. If not, they you can try the web interface - https://apps.bham.ac.uk/ . The website may ask you to confirm that AppsAnywhere is installed on your device – you can click ‘I have it already’ if using a campus computer.",
    "crumbs": [
      "Home",
      "Support",
      "FAQs"
    ]
  },
  {
    "objectID": "tutorials/frequently-asked-questions.html#how-do-i-use-appsanywhere",
    "href": "tutorials/frequently-asked-questions.html#how-do-i-use-appsanywhere",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Once you have access to AppsAnywhere, you may have to log-in using your UoB credentials. The homepage of AppsAnywhere contains a search bar and will look something like this:\n\nYou can search for Jamovi and Rstudio in the search bar and click ‘Launch’ to run them. If this is the first time you’re running the app it may take a few minutes to get started. This is as AppsAnywhere is downloading and installing the programme on your computer.",
    "crumbs": [
      "Home",
      "Support",
      "FAQs"
    ]
  },
  {
    "objectID": "tutorials/frequently-asked-questions.html#appsanywhere-keeps-asking-me-about-validation-and-i-dont-know-what-to-do.",
    "href": "tutorials/frequently-asked-questions.html#appsanywhere-keeps-asking-me-about-validation-and-i-dont-know-what-to-do.",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "AppyAnywhere needs to be able to link to your desktop computer to run properly, the validation process ensures that this is working properly. Normally all you need to do is click to start validation and answer ‘Open AppsAnywhere Launcher’ when prompted by the browser.",
    "crumbs": [
      "Home",
      "Support",
      "FAQs"
    ]
  },
  {
    "objectID": "tutorials/frequently-asked-questions.html#appsanywhere-keeps-asking-me-about-validation.",
    "href": "tutorials/frequently-asked-questions.html#appsanywhere-keeps-asking-me-about-validation.",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "AppyAnywhere needs to be able to link to your desktop computer to run properly, the validation process ensures that this is working properly. Normally all you need to do is click to start validation and answer ‘Open AppsAnywhere Launcher’ when prompted by the browser.",
    "crumbs": [
      "Home",
      "Support",
      "FAQs"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#one-sample-t-tests-in-jamovi",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#one-sample-t-tests-in-jamovi",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "Key step\n\n\n\nBefore going any further - make sure that your data filter is set to include only the data from the ‘Low’ SES_Story condition.\n\n\nOf course, we will normally compute the t-test using Jamovi as rather than relying on hand calculations. We can do this by opening the ‘One Sample T-Test’ option under the ‘Analysis -&gt; T-Tests’ menu on the top ribbon.\nOpen the One Sample T-Test option now, and drag the NorthSouth_Position variable over to the ‘dependent variables’ box. The result should appear on the right hand side. Before we inspect the result, we should check and add a number of important options to our analysis.\n\n\nHypothesis - test value it is critical that we select the correct hypothesis or our test will be meaningless. Our ‘test value’, also known as a ‘comparison value’, is set to zero by default. This is correct for our analysis but may need setting to something else for other analyses. Take care to check it makes sense each time.\nHypothesis - test tail we can also specify whether we’re going to perform a one- or two-tailed test. Here we want to specify a two-tailed test in which the mean does not equal the test value.\nAssumption Checks we should add a test to check that our data are normally distributed. Selecting the ‘Normality test’ box will compute a Shapio-Wilks statistic for you\nAdditional Statistics there are a number of important options here. For now, add the ‘Descriptive plots’ option to your analysis. This will show us a visualisation of the estimated mean and its confidence intervals.\n\nLet’s take a look at the outputs, first the t-test table.\n\nThis contains the information we need to report our t-test: the t-value, the degrees of freedom and the p-value.\n\nt(60) = 1.87, p = 0.066\n\nThis t-value should match the number you computed by hand earlier on. Note that the footnote of the table reminds us of the hypothesis that we specified. Make sure that this matches what you expect!\nNext, the normality check:\n\nThis reports the Shapiro-Wilk W statistic and associated p-value. In this case the p-value is not significant which indicates that the data are normally distributed and we’re happy to go ahead with the parametric t-test.\nNext, the descriptives plot:\n\nThis is a really useful visualisation of our test. We can see the mean of the data (black dot) and the test value (dotted line). We also have the 95% confidence intervals of the estimated mean. We can see that the confidence intervals overlap with the test value - this tells us that it is likely our test will not be significant - according to our confidence intervals, the true population mean might include our test value zero which would indicate no real difference.\n\n\n\n\n\n\nKey step\n\n\n\nChange your data filter to compute the one-sample t-test for the ‘High’ SES_Story condition. The test should update once your filter is on without you having to respecify the test.\n\n\n\n\n\n\n\n\n Data Skills - Is the mean position of participants in the ‘High’ condition significantly different to zero?\n\n\n\n\n\nYes, we do have a significant difference - we could report our test as follows\n\nt(74) = -5.35, p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n Data Skills - Is it valid to use parametric statistics on this data?\n\n\n\n\n\nThe Shapiro Wilk statistic has a p-value of 0.038 which is less than p = 0.05. This suggets that our data are not normally distributed. We should inspect the data in more detail and consider using a non-parametric alternative.\nThe Wilcoxon Rank test is the non-parametric alternative for One Sample t-test. You can add this to the analysis to see if it gives the same result as the parametric test.\n\n\n\n\n\n\n\n\n\n Data Skills - Do the confidence intervals of the estimated mean cross zero?\n\n\n\n\n\nNo - in the ‘High’ condition the confidence intervals of the mean are a long way from zero.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nGo back to your notes from the start of the session. Do the results match your predictions about whether a UK data sample would expect a high or low socio-economic status people to live in the north or south of the city?",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-3_one-sample-t-tests.html#summary",
    "href": "tutorials/rmb_week-3_one-sample-t-tests.html#summary",
    "title": "Week 3 : One-Sample t-tests",
    "section": "",
    "text": "We’ve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 3 - One-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "This week we will explore how to use Jamovi to test hypotheses about a dataset using t-tests. We will touch on some revision from previous weeks so please do jump back to past computer practicals for revision if required\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nIndependent samples t-tests\n\n\n\nPaired samples t-tests\n\n\n\nAssumptions of parametric tests\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nComputing a new variable from existing data\n\n\n\nComputing checks for normality and homogeneity of variance\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nExploring and understanding new datasets\n\n\n\n\n\nWe’ll be working with the dataset we collected after the break during the lecture in Week 3. This is a partial replication of a study published in the journal Psychological Science (Miller et al. 2023). The original paper found the following results summarised in the abstract.\n\nRecent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 (N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces—a phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 (N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI.\n\nWe ran a quiz that students in the lecture could join on their phones/laptops. There were two parts to the quiz:\nThe faces section included 12 faces that were either photographs or real people or AI generated images of people. The stimuli were taked from the materials released by (Miller et al. 2023) on the Open Science Framework. Responses were either ‘Real person’ or ‘AI generated’.\n\nThe confidence section asked participants to rate their confidence in their ability to do the following three things.\n\nDistinguish real photos of people from AI generated images (Experimental condition)\nDistinguish photos of happy people from photos of sad people (Emotional control condition)\nDistinguish photos of people you used to know in primary school from strangers (Memory control condition)\n\nScores were recorded on a scale from 1 (Completely confidence) to 10 (Not at all confident). The confidence section was repeated before and after the faces section to see if participants confidence changed as a result of doing the AI faces task\n\n\n\nThis week we will use both one sample and two sample t-tests to explore the following hypotheses.\n\nPeople are able to distinguish AI generated faces from real photos of humans.\nConfident people are better at distinguishing AI faces from real faces.\nPeople’s confidence in distinguishing AI generated faces will reduce after performing the task, but their confidence about emotion perception and memory will not change.\n\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to think about these hypotheses. Which statistical test is most appropriate for each? Do they call for a one-tailed or a two-tailed test?\n\n\n\n\n\nIt is critical to take some time to understand the data we work with before running critical hypothesis tests. Here we’ll take a look through the dataset to understand what information is present and if we’re happy to proceed with the analysis. This is similar to what we did in week 1 - you can refer back to the week 1 materials for additional guidance if you need it.\n\n\n\n\n\n\nKey step\n\n\n\nBefore going any further, the data file rmb-week-3_lecture-quiz-data_ai-faces.csv into a new Jamovi session.\n\n\nTake a read through the data columns. We have 26 in total with the following information.\n\n\n\n\n\n\n\nColumn Names\nDescription\n\n\n\n\nFirst Name\nParticipant ID - always ‘Anonymous’\n\n\nDataUse\nParticipant response to data re-use question\n\n\nAIConfidenceBefore\nConfidence in distinguishing AI faces from real BEFORE the task : 1 (Completely confident) to 10 (Not at all confident)\n\n\nEmoConfidenceBefore\nConfidence in distinguishing happy from sad faces BEFORE the task (Emotional control) : 1 (Completely confidence) to 10 (Not at all confident)\n\n\nMemoryConfidenceBefore\nConfidence in recognising a face from a long time ago BEFORE the task (Memory control) : 1 (Completely confidence) to 10 (Not at all confident)\n\n\nFace1_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace2_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace3_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace4_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace5_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace6_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace7_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace8_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace9_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace10_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace11_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace12_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nQuiz1\nResponse for revision quiz question\n\n\nQuiz2\nResponse for revision quiz question\n\n\nQuiz3\nResponse for revision quiz question\n\n\nAIConfidenceAfter\nConfidence in distinguishing AI faces from real AFTER the task\n\n\nEmoConfidenceAfter\nConfidence in distinguishing happy from sad faces AFTER the task (Emotional control)\n\n\nMemoryConfidenceAfter\nConfidence in recognising a face from a long time ago AFTER the task (Memory control)\n\n\n\nWork through the following questions, try to get an answer yourself before clicking to see the result. Data exploration is a critical skill that you’ll need whenever looking a new data throughout your degree.\n\n\n\n\n\n\n Data Skills - how many participants took part in the quiz?\n\n\n\n\n\nWe have 124 rows of data observations the spreadsheet.\n\n\n\n\n\n\n\n\n\n Data Skills - did everyone consent to have their data included in this practical?\n\n\n\n\n\nYes, the responses in the DataUse column are always positive - “Yes, I’m happy for my data to be included”. We removed the data with negative responses before sharing the data here.\n\n\n\n\n\n\n\n\n\n Data Skills - proportion of responsese would we expect to be accurate if participants responded randomly in the face questions.?\n\n\n\n\n\nWe would expect 50% accuracy if participants answered randomly. There were only two response options ‘Real person’ or ‘AI generated’.\n\n\n\nYou can use Descriptive Statistics to answer the following questions.\n\n\n\n\n\n\n Data Skills - which face did participants identify most accurately?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nFace 11 was most accurately identified as AI generated with 91.9% accuracy!\nFace 4 (AI) and Face 1 (Real) were close behind.\n\n\n\n\n\n\n\n\n\n Data Skills - which face did participants identify least accurately?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nFace 9 was least accurately indified as a real human with 31.8% accuracy. Face 5 (AI was second least accurate)\n\n\n\n\n\n\n\n\n\n Data Skills - did we get complete data from all participants in the face task?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nNo, quite a few participants dropped in and out during the task. We’re missing between 6 (Face 2 and 3) and 21 (Face 12) participants on each question.\n\n\n\n\n\n\n\n\n\n Data Skills - were participants most confident in their AI discrimination, emotion recognition or memory before the face task?\n\n\n\n\n\nCompute desriptive statistics for all AIConfidenceBefore, EmoConfidenceBefore and MemoryConfidenceBefore.\nParticipants were most confident in their ability to distinguish happy from sad faces in the Emotional face control condition with a score of 2.51. Participants were least confident in their ability to distinguish AI faces from real faces with a score of 4.54.\n\n\n\n\n\n\n\n\n\n Data Skills - are the confidence scores before face task normally distributed?\n\n\n\n\n\nCompute desriptive statistics for all AIConfidenceBefore, EmoConfidenceBefore and MemoryConfidenceBefore. Add the ‘Shapiro-Wilk’ statistic to the table.\nIt looks like none of these variabiles are normally distributied… The W statistics for AIConfidenceBefore is much higher than the other two, but the p-values indicate that all three show a departure from a normal distribution.\nAdd a Histogram to your descriptive plots - we can see that AI confidence looks close to normally distributed but there is a very large skew in both Emotional confidence and Memory confidence - some participants had very low confidence scores in these conditions!\n\n\n\n\n\n\nThe descriptive statistics gave us a good overview of the dataset and we can start working towards testing our hypotheses.\nOne critical piece of information is missing though! we have accuracy for each individual face but not an overall score for each participant. We’ll need to compute this new variable ourselves from the average accuracy of all twelve faces.\n\n\n\n\n\n\nData Skills - computing a variable from other columns\n\n\n\nWe can define our own variables in Jamovi using the ‘Compute’ function in the ‘Variables’ or the ‘Data’ tabs. Open a new Transformed variable.\nThis will open a menu with an option to give the new variable an name and description. Name the variable ‘PropFacesCorrect’ to indicate that it contatins the proportion of faces that the participant responded correctly on. You can add a description if you like though this is optional.\nThe variable is defined within the formula box below the name definitions. We want to compute the average accuracy across all 12 faces so we can add the formula to compute that into the box.\n\nThe formula should add all the columns together and divide the result by 12 (the total number of faces). Make sure that all the additions are grouped by parentheses! otherwise Jamovi will only divide the final value by 12 and add it to the others. This is an example of BODMAS - Brackets, Of, Division/Multiplication, Addition/Subtraction that you might have covered in maths in school. talk to your tutor to make sure that this step makes sense.\nThe formula should look something like this, I’ve removed some faces to simplify the visualisation. You should include them all.\n(Face1_Real + Face2_Real + ... + Face11_AI + Face12_AI) / 12\nOnce this is complete, you should be able to find your new column of values.\n\n\nNow, let’s take a look at our new variable. Compute some descritive statistics!\n\n\n\n\n\n\n Data Skills - how many participants gave a response to all 12 faces?\n\n\n\n\n\nWe have an N of 86 with 38 participants not responding to one or more of the faces.\n\n\n\n\n\n\n\n\n\n Data Skills - are the average accuracies normally distributed?\n\n\n\n\n\nTake a look at the Shaprio-Wilk statistic and the histogram of PropFacesCorrect, this looks like a normally distributed data variable.\n\n\n\n\n\n\n\n\n\n Data Skills - what proportion correct did the most and least accurate participants get?\n\n\n\n\n\nTake a look at the maxiumum and minumim of hte descriptive statistics, and perhaps add the table of ‘Most extreme’ data values in the Outiers section.\nTwo individuals managed to get all 12 faces correct! Two individuals were at 25% accurate corresponding to 3 out of 12 faces correct.\n\n\n\n\n\n\nOk, we’re ready to test the first hypothesis. Use the information you know about the dataset and try to find an answer!\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nTest the following hypothesis:\n\nPeople are able to distinguish AI generated faces from real photos of humans\n\nWhat sort of hypothesis is this and what is the most approprate statistical test?\nCompute the statistics, do the data support the experimental or the null hypothesis?\n\n\nWe could write a statistical version of this hypothesis as something like this\n\nPeople are able to distinguish AI generated faces from real photos of faces at an accuracy greater than chance level.\n\nWhat sort of test do you need to run?\n\n\nWe need to run a one sample t-test that compares the PropCorrectFaces variable to a chance level of 0.5 (corresponding to 50%). The results could be reported as follows\n\nA one sample t-test comparing the group average proportion of correctly identified faces (M = 0.634, SD=0.164) to chance level (proportion correct = 0.5) showed a significant effect, t(85) = 7.52, p&lt;0.001. Participants were on average more accurate than chance at distinguishing AI generated face from photos of real faces.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 1?\n\n\n\n\n\nYes, the statistical test allows us to reject the null hypothesis that participant performance was no different to chance level on this task and accept the experimental hypothesis that participants are able to distinguish AI generated faces from real photos of humans.\n\n\n\n\n\n\nNow the second hypothesis. We don’t have everything we need to test this hypothesis yet. We’ll need some way to split our participants into two groups - one with high confidence in AI face detection and one with low confidence. Time to compute another variable.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCompute a new variable named ConfidentBefore that separates the groups based on the median AI face detection confidence before the task.\n\n\nYou’ll need the median value for AIConfidenceBefore - you can compute this from descriptive statistics.\nThe computed variable will need some logical condition (using operators like ‘&gt;’, ‘&lt;’ or ‘==’) that separates participants with confidence above and below the median.\n\n\nThe median value for AIConfidenceBefore is 4, so our computed variable definition will look like this.\n\nThe values in ConfidentBefore will now be ‘True’ for people with high confidence and ‘False’ for people with low confidence. It doesn’t matter if you’ve done this the other way around - the tests will still work but the results will be flipped in the other direction (multiplied by -1)\n\n\n\n\n\nWith our new variable, we have what we need to run an independent samples t-test. This is very straightforward following the analyses we’ve run previously in the module.\nOpen the ‘Independent Sample t-test’ menu under ‘t-tests’. To run the analysis, drag PropFacesCorrect across as our dependent variable and our new ConfidentBefore variable as the grouping variable. The results should appear on the right automatically.\nOnce you have computed the core test - add the following options:\n\nDescriptives\nHomogeneity Test\nNormality Test\n\n\nLet’s think through the results\n\n\n\n\n\n\n Data Skills - are the data normally distributed?\n\n\n\n\n\nThe Shairo-Wilk statistic is not significant, indicating that the data are likely to be normally distributed\n\n\n\n\n\n\n\n\n\n Data Skills - do we have homogeneity of variance?\n\n\n\n\n\nLevene’s statistic is not significant, indicating that the variance of the two groups is comparable. We can confirm this by looking at the standard deviations in the descriptives table. 0.171 and 0.158 are fairly similar so the test report makes intuitive sense.\n\n\n\n\n\n\n\n\n\n Data Skills - which test should we report? Student’s, Welch’s or Mann-Whitney U?\n\n\n\n\n\nBoth assumptions of the standard Student’s t-test we looked at above are met by the data - we can proceed with Student’s test.\n\n\n\n\n\n\n\n\n\n Data Skills - were confident partiticpants or unconfident participants more accurate?\n\n\n\n\n\nConfident participants were more accurate! participants in this study are good at judging their ability.\nWe can see this by looking at the means in the descriptive statistics and the descriptives plot if you have run it.\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 2?\n\n\n\n\n\nYes, overall it seems like we can reject the null for hypothesis 2 and accept the experimental hypothesis that more confident participants are better at distinguishing AI faces from real human faces.\n\n\n\n\n\n\nNext we want to explore whether performing the face decision task changes peoples confidences in their abililty to detect AI generated faces. Remember that all participants categorised 12 faces with immediate correct/incorrect feedback and made confidence ratings at the start and end of the task.\nOpen the ‘Paired Sample t-test’ menu under ‘t-tests’. To run the analysis, drag AIConfidenceBefore and AIConfidenceAfter across as our pair of dependent variables. This is the format for running paired samples t-tests, the rest of the options should be familiar from our previous analyses.\nOnce you have computed the core test, do the same for the EmoConfidence and MemoryConfidence, and add the following options:\n\nDescriptives\nNormality Test\n\nThe results should appear on the right automatically.\n\nLet’s think through the results…\n\n\n\n\n\n\n Data Skills - does confidence in distinguishing AI faces from real faces change after completing the face task?\n\n\n\n\n\nYes, the results indicate that there is a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to distinguish AI faces from real faces before (M=4.48, SD=2.20) and after (M=5.66, SD=2.57) the face perception task. t(76) = -3.769, p&lt;0.001. Confidence was significantly reduced after completing the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - does confidence in distinguishing sad faces from happy faces change after completing the face task?\n\n\n\n\n\nNo, the results indicate that there is not a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to distinguish sad faces from happy faces before (M=2.57, SD=1.49) and after (M=2.74, SD=1.37) the face perception task. t(64) = -1.1017, p=0.313. Confidence was unchanged before and after the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - does confidence in recognising a face you haven’t seen for a long time change after completing the face task?\n\n\n\n\n\nNo, the results indicate that there is not a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to recognise faces of people they haven’t seen for a long time before (M=3.67, SD=1.89) and after (M=3.70, SD=1.86) the face perception task. t(66) = -0.234, p=0.816. Confidence was unchanged before and after the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - What test should we report for these comparisons? Are the parametric assumptions met?\n\n\n\n\n\nCheck the results of hte normality tests. The Shapiro-Wilk statistic is significant for all three tests! We should consider the non-parametric alternative test - add the ‘Wilcoxon Rank’ test to your analysis.\nWe should report the Wilcoxon Rank test along with the medians for non-parametric data. For example:\n\nA Wilcoxon Rank test showed a significant difference in participants confidence in their ability to distinguish AI faces from real faces before (Median=4) and after (Median=5) the face perception task. W=680, p&lt;0.001. Confidence was significantly reduced after completing the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 3?\n\n\n\n\n\nYes, overall it seems like we can reject the null for hypothesis 3 and accept the experimental hypothesis. Participants were less confident in their ability to detect AI generated faces after completing the task but their confidence in detecting emotions and remembering faces from a long time ago remains unchanged.\n\n\n\n\n\n\nLet’s use our new skills from this practical to answer one last question.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse a t-test to test this hypothesis:\n\nPeople are more accurate at identifying photos of real people compared to AI generated photos\n\nYou’ll several of the skills from this session to answer the question… think through what sort of variables you’ll need and what sort of test you’ll need.\n\n\nWe need 2 new variables to answer this question. We have already computed PropCorrectFaces in an earlier section, but we now need to make separate versions of this for AI faces and real faces…\nThe variable transforms will look something like this:\n \nWhat sort of t-test will you need?\n\n\nWe need to compute a paired samples t-test to answer the question as each participant contributes to both the AI face and real face conditions. Compute the test along with some descriptive statistics, we can report the test as follows\n\nA paired samples t-test showed a significant difference in the correct identification of AI faces (M=0.702, SD=0.198) compared to real faces (M=0.566, SD=0.229). t(85) = -4.60, p&lt;0.001. AI faces were identified more accurately than real faces.\n\n\n\n\n\n\n\n\n\n\nWe’ve computed a range of tests to statistically assess our hypotheses today! One experiment can often yield enough data to run a wide range of analyses. It is always a good idea to start with your hypotheses and predictions to break the analysis down into manageable chunks.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#compute-descriptive-statistics-with-r-in-rj",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#compute-descriptive-statistics-with-r-in-rj",
    "title": "Research Methods",
    "section": "",
    "text": "Ok, back to statistics! How can we use our new R skills to compute statistics?\nThe trick is to understand that all the analyses in Jamovi are actually carried out by functions in R. Each analysis we can select from the top ribbon in Jamovi corresponds to a function that can be called in the code. Similarly, all of the options that we select in the analysis, dialogue boxes, correspond to arguments for that function. Let’s take a look in practice.\nTo help understand this Jamovi provides an “R Syntax Mode”, in this mode, Jamovi produces equivalent R code for each analysis. To change to syntax mode, select the application menu (⋮) at the top right of Jamovi, and check the Syntax mode checkbox there. You can find more information on Jamovi’s documentation on Syntax Mode.\nNotice that some R code has appeared at the top of the output table. This is relatively complex compared to our previous functions like print or sqrt. Let’s break it down a bit.\n\n\n\n\n\n\nData Skills - Understanding the descriptives function\n\n\n\nThis is the code that Jamovi’s syntax mode will produce (with a little formatting to make the appearance simpler). Take a look through each part to understand what it does. Click the number in the annotation to highlight the corresponding part of the code.\n1jmv::descriptives(\n2    data = data,\n3    vars = vars(Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism)\n    )\n\n1\n\nThis is the name of the function that will do the computation for us. Everything contained within these parentheses are known as arguments, or inputs, for the function\n\n2\n\nThe first input is named data and is assigned to be the special data variable that contains the data loaded into our spreadsheet.\n\n3\n\nThe second input is a list of the data columns in our spreadsheet that we want to compute descriptives from.\n\n\n\n\nThere is a lot of information here that might be hard to understand at first. Try changing the analysis options using the Jamovi buttons and watch what happens to the R syntax. In particular, try adding and removing different variables to the analysis.\n\n\n\n\n\n\nKey step\n\n\n\nOnce you’re ready, you can right click on this R syntax and click ‘Copy’ to copy it to your clipboard.\n\nand pasting it into an Rj window - now you can click run to reproduce your descriptive statistics table using the underlying R code.\n\n\nGreat - well done on completing your first data analysis using R code! You should have the following output that will match the descriptives you’ve seen in the main Jamovi window.\n\nHead back over to the Jamovi descriptives tab and try updating your analysis. You could change the variables that you are computing the descriptive statistics from, add in some extra metrics (such as the Skewness or Shapiro-Wilk) or even generate a plot like a histogram.\nNote that the R code in the syntax box of the output will stay up to date with your changes. Pay attention to how the function updates when you select different options.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat is the R code required to compute descriptive statistics from the Openness and Conscientiousness variables, whilst including the Skewness metric, the Shapiro-Wilk statistic and a histogram?\nCan you run this analysis in Rj?\n\n\nRemember that the R code in the syntax mode will update as you define your analysis in Jamovi. Get the analysis right there and you can simply copy over the code.\nTalk to your tutor if you’re getting stuck.\n\n\nThe final code should look like this\njmv::descriptives(\n    data = data,\n    vars = vars(Openness, Conscientiousness),\n    hist = TRUE,\n    skew = TRUE,\n    sw = TRUE)"
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#the-dataset",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#the-dataset",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "We’ll be working with the dataset we collected after the break during the lecture in Week 3. This is a partial replication of a study published in the journal Psychological Science (Miller et al. 2023). The original paper found the following results summarised in the abstract.\n\nRecent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 (N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces—a phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 (N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI.\n\nWe ran a quiz that students in the lecture could join on their phones/laptops. There were two parts to the quiz:\nThe faces section included 12 faces that were either photographs or real people or AI generated images of people. The stimuli were taked from the materials released by (Miller et al. 2023) on the Open Science Framework. Responses were either ‘Real person’ or ‘AI generated’.\n\nThe confidence section asked participants to rate their confidence in their ability to do the following three things.\n\nDistinguish real photos of people from AI generated images (Experimental condition)\nDistinguish photos of happy people from photos of sad people (Emotional control condition)\nDistinguish photos of people you used to know in primary school from strangers (Memory control condition)\n\nScores were recorded on a scale from 1 (Completely confidence) to 10 (Not at all confident). The confidence section was repeated before and after the faces section to see if participants confidence changed as a result of doing the AI faces task",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#the-challenge",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#the-challenge",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "This week we will use both one sample and two sample t-tests to explore the following hypotheses.\n\nPeople are able to distinguish AI generated faces from real photos of humans.\nConfident people are better at distinguishing AI faces from real faces.\nPeople’s confidence in distinguishing AI generated faces will reduce after performing the task, but their confidence about emotion perception and memory will not change.\n\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to think about these hypotheses. Which statistical test is most appropriate for each? Do they call for a one-tailed or a two-tailed test?",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#exploring-the-data",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#exploring-the-data",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "It is critical to take some time to understand the data we work with before running critical hypothesis tests. Here we’ll take a look through the dataset to understand what information is present and if we’re happy to proceed with the analysis. This is similar to what we did in week 1 - you can refer back to the week 1 materials for additional guidance if you need it.\n\n\n\n\n\n\nKey step\n\n\n\nBefore going any further, the data file rmb-week-3_lecture-quiz-data_ai-faces.csv into a new Jamovi session.\n\n\nTake a read through the data columns. We have 26 in total with the following information.\n\n\n\n\n\n\n\nColumn Names\nDescription\n\n\n\n\nFirst Name\nParticipant ID - always ‘Anonymous’\n\n\nDataUse\nParticipant response to data re-use question\n\n\nAIConfidenceBefore\nConfidence in distinguishing AI faces from real BEFORE the task : 1 (Completely confident) to 10 (Not at all confident)\n\n\nEmoConfidenceBefore\nConfidence in distinguishing happy from sad faces BEFORE the task (Emotional control) : 1 (Completely confidence) to 10 (Not at all confident)\n\n\nMemoryConfidenceBefore\nConfidence in recognising a face from a long time ago BEFORE the task (Memory control) : 1 (Completely confidence) to 10 (Not at all confident)\n\n\nFace1_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace2_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace3_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace4_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace5_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace6_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace7_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace8_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace9_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace10_Real\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace11_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nFace12_AI\nResult for face (1 is correct response, 0 is incorrect)\n\n\nQuiz1\nResponse for revision quiz question\n\n\nQuiz2\nResponse for revision quiz question\n\n\nQuiz3\nResponse for revision quiz question\n\n\nAIConfidenceAfter\nConfidence in distinguishing AI faces from real AFTER the task\n\n\nEmoConfidenceAfter\nConfidence in distinguishing happy from sad faces AFTER the task (Emotional control)\n\n\nMemoryConfidenceAfter\nConfidence in recognising a face from a long time ago AFTER the task (Memory control)\n\n\n\nWork through the following questions, try to get an answer yourself before clicking to see the result. Data exploration is a critical skill that you’ll need whenever looking a new data throughout your degree.\n\n\n\n\n\n\n Data Skills - how many participants took part in the quiz?\n\n\n\n\n\nWe have 124 rows of data observations the spreadsheet.\n\n\n\n\n\n\n\n\n\n Data Skills - did everyone consent to have their data included in this practical?\n\n\n\n\n\nYes, the responses in the DataUse column are always positive - “Yes, I’m happy for my data to be included”. We removed the data with negative responses before sharing the data here.\n\n\n\n\n\n\n\n\n\n Data Skills - proportion of responsese would we expect to be accurate if participants responded randomly in the face questions.?\n\n\n\n\n\nWe would expect 50% accuracy if participants answered randomly. There were only two response options ‘Real person’ or ‘AI generated’.\n\n\n\nYou can use Descriptive Statistics to answer the following questions.\n\n\n\n\n\n\n Data Skills - which face did participants identify most accurately?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nFace 11 was most accurately identified as AI generated with 91.9% accuracy!\nFace 4 (AI) and Face 1 (Real) were close behind.\n\n\n\n\n\n\n\n\n\n Data Skills - which face did participants identify least accurately?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nFace 9 was least accurately indified as a real human with 31.8% accuracy. Face 5 (AI was second least accurate)\n\n\n\n\n\n\n\n\n\n Data Skills - did we get complete data from all participants in the face task?\n\n\n\n\n\nCompute desriptive statistics for all 12 faces and look at the ‘Mean’.\nNo, quite a few participants dropped in and out during the task. We’re missing between 6 (Face 2 and 3) and 21 (Face 12) participants on each question.\n\n\n\n\n\n\n\n\n\n Data Skills - were participants most confident in their AI discrimination, emotion recognition or memory before the face task?\n\n\n\n\n\nCompute desriptive statistics for all AIConfidenceBefore, EmoConfidenceBefore and MemoryConfidenceBefore.\nParticipants were most confident in their ability to distinguish happy from sad faces in the Emotional face control condition with a score of 2.51. Participants were least confident in their ability to distinguish AI faces from real faces with a score of 4.54.\n\n\n\n\n\n\n\n\n\n Data Skills - are the confidence scores before face task normally distributed?\n\n\n\n\n\nCompute desriptive statistics for all AIConfidenceBefore, EmoConfidenceBefore and MemoryConfidenceBefore. Add the ‘Shapiro-Wilk’ statistic to the table.\nIt looks like none of these variabiles are normally distributied… The W statistics for AIConfidenceBefore is much higher than the other two, but the p-values indicate that all three show a departure from a normal distribution.\nAdd a Histogram to your descriptive plots - we can see that AI confidence looks close to normally distributed but there is a very large skew in both Emotional confidence and Memory confidence - some participants had very low confidence scores in these conditions!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#computing-overall-accuracy",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#computing-overall-accuracy",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "The descriptive statistics gave us a good overview of the dataset and we can start working towards testing our hypotheses.\nOne critical piece of information is missing though! we have accuracy for each individual face but not an overall score for each participant. We’ll need to compute this new variable ourselves from the average accuracy of all twelve faces.\n\n\n\n\n\n\nData Skills - computing a variable from other columns\n\n\n\nWe can define our own variables in Jamovi using the ‘Compute’ function in the ‘Variables’ or the ‘Data’ tabs. Open a new Transformed variable.\nThis will open a menu with an option to give the new variable an name and description. Name the variable ‘PropFacesCorrect’ to indicate that it contatins the proportion of faces that the participant responded correctly on. You can add a description if you like though this is optional.\nThe variable is defined within the formula box below the name definitions. We want to compute the average accuracy across all 12 faces so we can add the formula to compute that into the box.\n\nThe formula should add all the columns together and divide the result by 12 (the total number of faces). Make sure that all the additions are grouped by parentheses! otherwise Jamovi will only divide the final value by 12 and add it to the others. This is an example of BODMAS - Brackets, Of, Division/Multiplication, Addition/Subtraction that you might have covered in maths in school. talk to your tutor to make sure that this step makes sense.\nThe formula should look something like this, I’ve removed some faces to simplify the visualisation. You should include them all.\n(Face1_Real + Face2_Real + ... + Face11_AI + Face12_AI) / 12\nOnce this is complete, you should be able to find your new column of values.\n\n\nNow, let’s take a look at our new variable. Compute some descritive statistics!\n\n\n\n\n\n\n Data Skills - how many participants gave a response to all 12 faces?\n\n\n\n\n\nWe have an N of 86 with 38 participants not responding to one or more of the faces.\n\n\n\n\n\n\n\n\n\n Data Skills - are the average accuracies normally distributed?\n\n\n\n\n\nTake a look at the Shaprio-Wilk statistic and the histogram of PropFacesCorrect, this looks like a normally distributed data variable.\n\n\n\n\n\n\n\n\n\n Data Skills - what proportion correct did the most and least accurate participants get?\n\n\n\n\n\nTake a look at the maxiumum and minumim of hte descriptive statistics, and perhaps add the table of ‘Most extreme’ data values in the Outiers section.\nTwo individuals managed to get all 12 faces correct! Two individuals were at 25% accurate corresponding to 3 out of 12 faces correct.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-1---people-are-able-to-distinguish-ai-generated-faces-from-real-photos-of-humans",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-1---people-are-able-to-distinguish-ai-generated-faces-from-real-photos-of-humans",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "Ok, we’re ready to test the first hypothesis. Use the information you know about the dataset and try to find an answer!\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nTest the following hypothesis:\n\nPeople are able to distinguish AI generated faces from real photos of humans\n\nWhat sort of hypothesis is this and what is the most approprate statistical test?\nCompute the statistics, do the data support the experimental or the null hypothesis?\n\n\nWe could write a statistical version of this hypothesis as something like this\n\nPeople are able to distinguish AI generated faces from real photos of faces at an accuracy greater than chance level.\n\nWhat sort of test do you need to run?\n\n\nWe need to run a one sample t-test that compares the PropCorrectFaces variable to a chance level of 0.5 (corresponding to 50%). The results could be reported as follows\n\nA one sample t-test comparing the group average proportion of correctly identified faces (M = 0.634, SD=0.164) to chance level (proportion correct = 0.5) showed a significant effect, t(85) = 7.52, p&lt;0.001. Participants were on average more accurate than chance at distinguishing AI generated face from photos of real faces.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 1?\n\n\n\n\n\nYes, the statistical test allows us to reject the null hypothesis that participant performance was no different to chance level on this task and accept the experimental hypothesis that participants are able to distinguish AI generated faces from real photos of humans.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-2---confident-people-are-better-at-distinguishing-ai-faces-from-real-faces.",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-2---confident-people-are-better-at-distinguishing-ai-faces-from-real-faces.",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "Now the second hypothesis. We don’t have everything we need to test this hypothesis yet. We’ll need some way to split our participants into two groups - one with high confidence in AI face detection and one with low confidence. Time to compute another variable.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCompute a new variable named ConfidentBefore that separates the groups based on the median AI face detection confidence before the task.\n\n\nYou’ll need the median value for AIConfidenceBefore - you can compute this from descriptive statistics.\nThe computed variable will need some logical condition (using operators like ‘&gt;’, ‘&lt;’ or ‘==’) that separates participants with confidence above and below the median.\n\n\nThe median value for AIConfidenceBefore is 4, so our computed variable definition will look like this.\n\nThe values in ConfidentBefore will now be ‘True’ for people with high confidence and ‘False’ for people with low confidence. It doesn’t matter if you’ve done this the other way around - the tests will still work but the results will be flipped in the other direction (multiplied by -1)\n\n\n\n\n\nWith our new variable, we have what we need to run an independent samples t-test. This is very straightforward following the analyses we’ve run previously in the module.\nOpen the ‘Independent Sample t-test’ menu under ‘t-tests’. To run the analysis, drag PropFacesCorrect across as our dependent variable and our new ConfidentBefore variable as the grouping variable. The results should appear on the right automatically.\nOnce you have computed the core test - add the following options:\n\nDescriptives\nHomogeneity Test\nNormality Test\n\n\nLet’s think through the results\n\n\n\n\n\n\n Data Skills - are the data normally distributed?\n\n\n\n\n\nThe Shairo-Wilk statistic is not significant, indicating that the data are likely to be normally distributed\n\n\n\n\n\n\n\n\n\n Data Skills - do we have homogeneity of variance?\n\n\n\n\n\nLevene’s statistic is not significant, indicating that the variance of the two groups is comparable. We can confirm this by looking at the standard deviations in the descriptives table. 0.171 and 0.158 are fairly similar so the test report makes intuitive sense.\n\n\n\n\n\n\n\n\n\n Data Skills - which test should we report? Student’s, Welch’s or Mann-Whitney U?\n\n\n\n\n\nBoth assumptions of the standard Student’s t-test we looked at above are met by the data - we can proceed with Student’s test.\n\n\n\n\n\n\n\n\n\n Data Skills - were confident partiticpants or unconfident participants more accurate?\n\n\n\n\n\nConfident participants were more accurate! participants in this study are good at judging their ability.\nWe can see this by looking at the means in the descriptive statistics and the descriptives plot if you have run it.\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 2?\n\n\n\n\n\nYes, overall it seems like we can reject the null for hypothesis 2 and accept the experimental hypothesis that more confident participants are better at distinguishing AI faces from real human faces.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-3---peoples-confidence-in-their-ability-to-distinguish-ai-generated-faces-will-reduce-after-performing-the-task",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#hypothesis-3---peoples-confidence-in-their-ability-to-distinguish-ai-generated-faces-will-reduce-after-performing-the-task",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "Next we want to explore whether performing the face decision task changes peoples confidences in their abililty to detect AI generated faces. Remember that all participants categorised 12 faces with immediate correct/incorrect feedback and made confidence ratings at the start and end of the task.\nOpen the ‘Paired Sample t-test’ menu under ‘t-tests’. To run the analysis, drag AIConfidenceBefore and AIConfidenceAfter across as our pair of dependent variables. This is the format for running paired samples t-tests, the rest of the options should be familiar from our previous analyses.\nOnce you have computed the core test, do the same for the EmoConfidence and MemoryConfidence, and add the following options:\n\nDescriptives\nNormality Test\n\nThe results should appear on the right automatically.\n\nLet’s think through the results…\n\n\n\n\n\n\n Data Skills - does confidence in distinguishing AI faces from real faces change after completing the face task?\n\n\n\n\n\nYes, the results indicate that there is a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to distinguish AI faces from real faces before (M=4.48, SD=2.20) and after (M=5.66, SD=2.57) the face perception task. t(76) = -3.769, p&lt;0.001. Confidence was significantly reduced after completing the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - does confidence in distinguishing sad faces from happy faces change after completing the face task?\n\n\n\n\n\nNo, the results indicate that there is not a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to distinguish sad faces from happy faces before (M=2.57, SD=1.49) and after (M=2.74, SD=1.37) the face perception task. t(64) = -1.1017, p=0.313. Confidence was unchanged before and after the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - does confidence in recognising a face you haven’t seen for a long time change after completing the face task?\n\n\n\n\n\nNo, the results indicate that there is not a significant difference. We could report the results like this:\n\nA paired samples t-test showed a significant difference in participants confidence in their ability to recognise faces of people they haven’t seen for a long time before (M=3.67, SD=1.89) and after (M=3.70, SD=1.86) the face perception task. t(66) = -0.234, p=0.816. Confidence was unchanged before and after the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - What test should we report for these comparisons? Are the parametric assumptions met?\n\n\n\n\n\nCheck the results of hte normality tests. The Shapiro-Wilk statistic is significant for all three tests! We should consider the non-parametric alternative test - add the ‘Wilcoxon Rank’ test to your analysis.\nWe should report the Wilcoxon Rank test along with the medians for non-parametric data. For example:\n\nA Wilcoxon Rank test showed a significant difference in participants confidence in their ability to distinguish AI faces from real faces before (Median=4) and after (Median=5) the face perception task. W=680, p&lt;0.001. Confidence was significantly reduced after completing the task.\n\n\n\n\n\n\n\n\n\n\n Data Skills - do the results support hypothesis 3?\n\n\n\n\n\nYes, overall it seems like we can reject the null for hypothesis 3 and accept the experimental hypothesis. Participants were less confident in their ability to detect AI generated faces after completing the task but their confidence in detecting emotions and remembering faces from a long time ago remains unchanged.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#bonus-hypothesis-people-are-more-accurate-at-identifying-photos-of-real-people-compared-to-ai-generated-photos.",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#bonus-hypothesis-people-are-more-accurate-at-identifying-photos-of-real-people-compared-to-ai-generated-photos.",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "Let’s use our new skills from this practical to answer one last question.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nUse a t-test to test this hypothesis:\n\nPeople are more accurate at identifying photos of real people compared to AI generated photos\n\nYou’ll several of the skills from this session to answer the question… think through what sort of variables you’ll need and what sort of test you’ll need.\n\n\nWe need 2 new variables to answer this question. We have already computed PropCorrectFaces in an earlier section, but we now need to make separate versions of this for AI faces and real faces…\nThe variable transforms will look something like this:\n \nWhat sort of t-test will you need?\n\n\nWe need to compute a paired samples t-test to answer the question as each participant contributes to both the AI face and real face conditions. Compute the test along with some descriptive statistics, we can report the test as follows\n\nA paired samples t-test showed a significant difference in the correct identification of AI faces (M=0.702, SD=0.198) compared to real faces (M=0.566, SD=0.229). t(85) = -4.60, p&lt;0.001. AI faces were identified more accurately than real faces.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-4_two-sample-t-tests.html#summary",
    "href": "tutorials/rmb_week-4_two-sample-t-tests.html#summary",
    "title": "Week 4 : Two-Sample t-tests",
    "section": "",
    "text": "We’ve computed a range of tests to statistically assess our hypotheses today! One experiment can often yield enough data to run a wide range of analyses. It is always a good idea to start with your hypotheses and predictions to break the analysis down into manageable chunks.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 4 - Two-Sample T-tests"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "This week we will explore how to use the jmv package in R to test hypotheses about a dataset using t-tests and to compute effect sizes corresponding to those tests. We will use those effect sizes to help interpret the sensitivity and power of our experiments.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nCohen’s d effect size\n\n\n\nPower analysis\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nUse mutate() in R to compute a new variable\n\n\n\nCompute t-tests using the jmv R package\n\n\n\nRun a power analysis in Rj\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nUnderstand and validate code written by someone else\n\n\n\nUse power analysis to recommend a sample size for a replication\n\n\n\n\n\n\n\n\n\n\n\nDataset\n\n\n\nPlease note, this is the same dataset as we used in week 4 - but make sure that you load in the rmb-week-3_lecture-quiz-data_ai-faces-fixed.omv file from this week and NOT the week 4 file. It contains a small fix that we need for this week.\n\n\nWe’ll be working with the dataset we collected after the break during the lecture in Week 3. This is a partial replication of a study published in the journal Psychological Science (Miller et al. 2023). The original paper found the following results summarised in the abstract.\n\nRecent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 (N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces—a phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 (N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI.\n\nWe ran a quiz that students in the lecture could join on their phones/laptops. There were two parts to the quiz:\nThe faces section included 12 faces that were either photographs or real people or AI generated images of people. The stimuli were taked from the materials released by (Miller et al. 2023) on the Open Science Framework. Responses were either ‘Real person’ or ‘AI generated’.\n\nThe confidence section asked participants to rate their confidence in their ability to do the following three things.\n\nDistinguish real photos of people from AI generated images (Experimental condition)\nDistinguish photos of happy people from photos of sad people (Emotional control condition)\nDistinguish photos of people you used to know in primary school from strangers (Memory control condition)\n\nScores were recorded on a scale from 1 (Completely confidence) to 10 (Not at all confident). The confidence section was repeated before and after the faces section to see if participants confidence changed as a result of doing the AI faces task\n\n\n\nThis week we will explore some factors that influence t-stats and p-values whilst introducing the concept of loops in R. We will finish by using effect sizes to compute a power analysis.\n\n\n\nLast week, we used Jamovi to create several new variables that allowed us to run our analyses. We can do the same in a more transparent and reproducible way using R code.\nWe need to create both our proportion of correct faces across the 12 stimuli in one variable and a grouping variable which indicates whether each participant was confident in their ability to tell the difference between AI faces and real faces.\nWe can do both with the mutate() from the dplyr library, this provides functionality that lets us create, modify, and delete columns within a dataset. Take a look at the official documentation for mutate for more information.\nWhen computing a new varaible with mutate() we three pieces of information.\n\nThe dataset to work on\nThe name of the new variable to be created\nThe definition of how to compute the new variable for existing variables.\n\n\n\nLet’s start with our grouping variable. Our three pieces of information are\n\nThe dataset to work on - is data, which refers to the original datasheet loaded into Jamovi\nThe name of the new variable - is ConfidentBefore, the same as we used last week\nThe definition - is AIConfidenceBefore&lt;4, again the same as we used last week\n\nWe can combine these into a single line to create the variable. Note that we save the result into a new variable named my_data, we’ll use this from now on to avoid confusion with the original datasheet.\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\nWe can check that this has done what we expected by running head to see the first few rows of the dataset.\nhead(my_data)\nThe result should look like this:\n\nWe can see on in the results on the right hand side that the additional colume ConfidentBefore now appears with TRUE and FALSE values for each participant.\n\n\n\nLet’s do the same for total proportion of correctly identified faces. We can use the same principle as we used for the grouping variable and use mutate() along with the variable definition and name to create our new column.\nThis definition is pretty long as we have 12 different faces to work with! You can copy the whole line using the copy icon in the right hand side of the code box. Take a moment to look through and understand each part: which part is the Dataset, the Name and the Definition?\nmy_data &lt;- mutate(my_data, PropFacesCorrect = (Face1_Real + Face2_Real + Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face7_Real + Face8_Real + Face9_Real + Face10_Real + Face11_AI + Face12_AI) / 12)\nand validate the overall result using head() to make sure that my_data now has the column we expected.\n\nThe final column of the dataset is now PropFacesCorrect and contains the proportion of face trials that participant got right.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute variables containing the proportion of correct responses for photos of real people and AI generated faces separately?\nStore the results in PropRealCorrectFaces for real faces and PropAICorrectFaces for AI generated faces.\n\n\nWe can use the code we wrote to compute the proportion of correct responses for all faces as a starting point.\nThink about how you could modify this line to compute the result for either real or AI faces on their own? What would you need to change in the code?\n\n\nThe following code will compute the variables\nmy_data &lt;- mutate(my_data, PropRealFacesCorrect = (Face1_Real + Face2_Real + Face7_Real + Face8_Real + Face9_Real + Face10_Real) / 6)\nmy_data &lt;- mutate(my_data, PropAIFacesCorrect = (Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face11_AI + Face12_AI) / 6)\nWe’ve separated out the columns to sum all the real or AI faces together and changed the division to divide the results by 6 rather than 12.\nChecking the results with head() should produce the following outputs with four additional columns!\n\n\n\n\n\n\n\n\n\n\n\nWe have the ingredients for our hypothesis test. Let’s use R to explore the following hypothesis (Hypothesis 2 from week 4).\n\nConfident people are better at distinguishing AI faces from real faces\n\nWe can compute this test using the following code that calls ttestIS() - this is the function that computes independent samples t-tests. Read more about it on the ttestIS documentation page.\nYou can click the numbers by the definitions at the bottom to highlight the corresponding part of the code.\n1jmv::ttestIS(\n2    formula = PropFacesCorrect ~ ConfidentBefore,\n3    data = my_data,\n4    effectSize = TRUE)\n\n1\n\njmv::ttestIS is the name of the R function that Jamovi uses to compute independent samples t-tests\n\n2\n\nThis tells ttestIS the formula that defines the test we want to run.\n\n3\n\nThis tells ttestIS which dataset we want to analyse\n\n4\n\nThis adds an additional effect size computation to the results\n\n\nMost of this will be familiar from previous weeks, but let’s think about the formula in a little more detail.\nIn R, the tilde (~) is used in formula definitions to specify the relationship between variables, particularly in statistical modeling and data analysis. Here, the tilde separates the dependent variable (response) from the independent variables (predictors). In our example, PropFacesCorrect is the dependent variable and ConfidentBefore is our independent variable (grouping variable) - so this formula\nPropFacesCorrect ~ ConfidentBefore\nis essentially telling ttestIS() to “Compute the ttest on PropFacesCorrect using ConfidentBefore as our groups”.\nWe can now run all our code to get the results of the ttest - the code clearly tells the reader how all the relevant variables were computed and what hypothesis test has been run all in one screen.\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nRead through the ttestIS documentation page. How could you change your code to add the following assumption checks\n\nTest for homogeneity of variance\nTest for normal distribution\n\n\n\nThe documentation page contains a list of all the possible information that we can pass as an input to our ttestIS() function call. Each item in the list corresponds to the options available in the Jamovi dialogue box.\nHave a look at the norm and eqv definitions. What would you need to add to the function call to run these additional checks?\n\n\nThe following code will compute all the checks in addition to the t-test.\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE,\n    norm = TRUE,\n    eqv = TRUE)                             \nThe results should look like this:\n\nDo the assumptions for Student’s t-test hold for this data?\n\n\n\n\n\n\nYour script might be getting quite long at this point! It is always a good idea to keep code neat and tidy where possible so that other people are able to read it, and so that we can read it if we come back to the analysis in the future.\nThere are many ways to keep things organised in a script. Here are two good hints, code comments can organise your script without changing the output and print() statements can help organise your code and the outputs.\n\n\nAny line of R code that starts with a hashtag is called a ‘comment’. The writing on this line is there for our information and will not be run by the computer. Adding code comments is a useful way to annotate your code indicating what each line or each section is doing. For example:\n# This is code comment\n\n# Compute which participants were more confident than the median\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\n\nThis will make it easier to understand what the coding is doing in future.\n\n\n\nWe can use the print() function to help organise our code as well. The text within the call to print() will not be executed by the computer but will simply be printed into the output console. This can be useful to break the output of your code into sections and to inlcude additional information about the analysis next to the outputs themselves.\nFor example:\nprint('Hypothesis 1')\nprint('Confident people are better at distinguishing AI faces from real faces')\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)\nThis code will print out information about the hypothesis next to the outputs.\nA full cleaned and commented version of our code might look like this.\n# Compute which participants were more confident than the median\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\n\n# Compute the proportion of all faces each participant got right\nmy_data &lt;- mutate(my_data, PropFacesCorrect = (Face1_Real + Face2_Real + Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face7_Real + Face8_Real + Face9_Real + Face10_Real + Face11_AI + Face12_AI) / 12)\n\n# Compute the proportion of REAL faces each participant got right\nmy_data &lt;- mutate(my_data, PropRealFacesCorrect = (Face1_Real + Face2_Real + Face7_Real + Face8_Real + Face9_Real + Face10_Real) / 6)\n\n# Compute the proportion of AI faces each participant got right\nmy_data &lt;- mutate(my_data, PropAIFacesCorrect = (Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face11_AI + Face12_AI) / 6)\n\n# Hypothesis test\nprint('Hypothesis 1')\nprint('Confident people are better at distinguishing AI faces from real faces')\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)\nand running this code produces the following outputs including the test from our print statements\n\nNice and clear what is happening at each stage! There is no perfect or ‘correct’ way to tidy up your code (though some people can get opinionated about this…). Choose a mix of comments and print statements that makes sense to you.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nExtend your code to test a second hypothesis.\n\nConfident participants are more accurate than unconfident participants when identifying photos of real faces.\n\nInclude some code comments and print statements\n\n\nThis will require a second indenpendent samples t-test asking whether the mean proportion correct value for only real faces is different between our confident and not-confident groups.\nUse the previous t-test as a starting point, can you copy this and modify it to do what we need?\n\n\nThe following code will test the new hypothesis with some comments and print statements\n# Hypothesis test\nprint('Hypothesis 2')\nprint('Confident participants are more accurate than unconfident participants when identifying photos of real faces.')\njmv::ttestIS(                                       \n    formula = PropRealFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)                       \nThe output of the whole script should now look like this.\n\nNote how the print statements help to separate the results into interpretable chunks.\n\n\n\n\n\n\nNow we’re going to explore how the size of the data sample changes the t-values that w\nIntroduce slice_sample Introduce simple loops\n\n\n\n\nImagine that of your friends has been running some analyses on this dataset and would like you to check over their work. They send you the following R code that they have put together.\n# Compute some new variables\nmy_data &lt;- mutate(data, ChangeAIConfidence = AIConfidenceBefore - AIConfidenceAfter)\nmy_data &lt;- mutate(my_data, ChangeEmoConfidence = EmoConfidenceBefore - EmoConfidenceAfter)\nmy_data &lt;- mutate(my_data, ChangeMemoryConfidence = MemoryConfidenceBefore - MemoryConfidenceAfter)\n\n# Run the tests\njmv::ttestOneS(\n    data = my_data,\n    vars = vars(ChangeAIConfidence, ChangeEmoConfidence, ChangeMemoryConfidence),\n    testValue = 0.0,\n    wilcoxon = TRUE,\n    desc = TRUE,\n    norm = TRUE)\nLet’s think about this code a bit, your friend didn’t really explain the plan…\n\n\n\n\n\n\n Data Skills - what variables are created?\n\n\n\n\n\nThe code creates three new variables, ChangeAIConfidence, ChangeEmoConfidence and ChangeMemoryConfidence - each is computed by subtracting the confidence values after the face task from the confidence values before the task.\nThese variables represent the change in confidence from completing the task.\n\n\n\n\n\n\n\n\n\n Data Skills - what statistical test is performed?\n\n\n\n\n\nThe code runs a one sample t-test comparing the data mean to 0.0 using the jamovi function jmv::ttestOneS(). The code runs the test on all three variables. We can see that the input vars is passed a list of all three variable names.\n\n\n\n\n\n\n\n\n\n Data Skills - what additional computations are performed?\n\n\n\n\n\nIn addition to the t-tests, the code runs a Wilcoxon Rank test, a Shapiro-Wilk test for normality and a descriptive statistics table.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and run this code before going any further! Take a moment to read through the code and decide what it is doing.\n\n\n\n\n\n\nClick here if you want to check that you have the right outputs.\n\n\n\n\n\nThe results should look like this:\n\n\n\n\n\n\nThey have written the following about the results.\n\nA one sample t-test found that the change in participant confidence in distinguishing AI faces from real faces was significantly reduced after performing the task. The difference in confidence before and after the task (M=-1.1, SD=2.7) was significantly below zero, t(76) = 3.538, p = 0.0007. In contrast there was no change after the task confidence in emotion recognition (M=-.017, SD=1.341), t(76) = -1.012, p = 0.313 and face memory (M=-0.02985, SD=1.044), t(76) = -0.234, p = 0.816.\n\nLet’s think about whether this is accurate.\n\n\n\n\n\n\n Data Skills - are the reported statistics correct? take your time to read carefully and compare to the outputs\n\n\n\n\n\nNo! there are a number of mistakes throughout this writing. How many of the following mistakes did you spot?\n\nA one sample t-test found that the change in participant confidence in distinguishing AI faces from real faces was significantly reduced after performing the task. The difference in confidence before and after the task (M=-1.1, SD=2.7) was significantly below zero, t(76) = 3.538, p = 0.0007. In contrast there was no change after the task confidence in emotion recognition (M=-.017, SD=1.341), t(76) = -1.012, p = 0.313 and face memory (M=-0.02985, SD=1.044), t(76) = -0.234, p = 0.816.\n\n\n3.538 -&gt; the sign of the t-values on this test is wrong. Take a look a the output table, this should be -3.538.\n76 -&gt; the degrees of freedom for the second and third tests are reported incorrectly (perhaps copied from the first test?). These should be 64 and 66 respectively\n\n\n\n\n\n\n\n\n\n\n Data Skills - was the correct test reported?\n\n\n\n\n\nAgain, no! there are mistakes here as well. The friend has done the right thing by computing the Shapiro-Wilk statistics but appears to have ignored them in the writing.\nShapiro-Wilk is significant for all three variables - we should strongly consider reporting the Wilcoxon Rank Sum test rather than Student’s t.\nThey could have run this analysis as a paired t-test, but the one sample t-test of the differences is also a valid approach.\n\n\n\nComputing statistics and accurately reporting the results takes a lot of precision! take your time when writing code and results sections - make sure to check and double check your work as you go…\n\n\n\nLet’s think more about effect sizes - computing effect sizes in Jamovi and R is really easy. Simply click the check box to add ‘Effect size’ under the ‘Addtional Statistics’ section of the Jamovi window - or add effectSize = TRUE to the function call in R (we’ve already been doing this above!).\n\n\n\n\n\n\nKey step\n\n\n\nUpdate your friend’s code to include computation of effect sizes - the results should appear in the t-test results table on the right.\n\n\n\nThere are effect sizes computed for every t-test and its non-parametric alternatives.\n\nCohen’s d is the parametric effect size corresponding to Student’s t-test\nRank Biserial Correlation is the non-parametric effect size corresponding to Wilcoxon’s test.\n\nThough they have methodological differences, these effect sizes can be interpreted in the same way as a measure of the magnitude of an effect.\nRemember that this is different to the t-value which is a measure of evidence against the null hypothesis. The important difference is that we can have strong evidence against the null either as the difference is large or if we have measured a small difference very precisely. The effect size only cares about the size of the difference between conditions.\nNote that Jamovi/R provide a ‘signed’ effect size indicating the direction of the effect in the same way that a t-statistic does. For the following work we can ignore this sign and focus on the magnitudes only. In other words, we’ll consider an effect size of +0.5 or -0.5 to indicate the same magitude of effect.\nThe creator of many of our classic effect size measures provided a guideline for what might be considered a ‘small’ or a ‘large’ effect.\n\n\n\nEffect Size\nCohen’s d\nInterpretation\n\n\n\n\nSmall\n0.2\nSmall effect\n\n\nMedium\n0.5\nMedium effect\n\n\nLarge\n0.8\nLarge effect\n\n\n\nThese can be useful guidelines - through they have been criticised strongly for both the arbitrary nature of the thresholds and for over-simplifying interpretation of effect sizes. We can still use them as an informal indicator to help us quickly interpret our results.\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeAIConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.403 is just below the thresold to be a ‘medium’ effect (ignoring the sign of the effect size). The effect size for the Rank Biserial correlation is similar.\n\n\n\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeEmoConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.126 is just below the thresold to be a ‘small’ effect (ignoring the sign of the effect size). The effect size for the Rank Biserial correlation is larger but still closer to small than medium.\n\n\n\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeMemoryConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.0286 is tiny! the Rank Biserial correlation is even smaller…\n\n\n\n\n\n\nOne core application for effect sizes is that we can use the results to inform our sample size planning for future studies. we can do this with four linked pieces of information\n\nEffect Size this is our effect size, usually Cohen’s d for analyses in RM-B\nSignificance Thresold - this is the ‘alpha’ level at which we would consider a result significant, usually 5% or 0.05.\nPower - this is probability of correctly detecting an effect if one exists in the data, we usually want 80% power.\nNumber of datapoints (N) - this is our sample size.\n\nThese four pieces of information are linked such that if we have any three of these values we can always compute the fourth.\nThis lets us do some very useful calculations to see how well our dataset is enabling us to answer our hypothesis and to plan for future studies. For examples\n\nIf we know the effect size, significance threshold and number of participants in a study, we can compute the power to tell us the probability that we can correctly detect an effect. This is known as ‘sensitivity’.\n\nor\n\nIf we want to compute how many participants we’ll need in a future study and we already know the effect size from previous literature, we can specify the power and significance threshold we want and compute the number of participants that we’ll need.\n\nVery useful! We can compute these different values using a Jamovi module known as ‘jpower’\n\n\n\n\n\n\nKey step - Install the jpower addon in Jamovi\n\n\n\nClick through the ‘Modules’ button on the top right of the Jamovi ribbon. Click to the list of ‘Available’ modules and install ‘jpower’\n\nClick through to open the jpower menu and select a ‘one-sample t-test’ from the options. You should see the following dialogue box\n\n\n\nLet’s take a moment to understand the options.\nFirst - notice that we have the four pieces of information listed near the top of the options.\n\n\n\njpower menu\nInterpretation\n\n\n\n\nMinimally interesting effect size (δ)\nEffect size\n\n\nMinimum desired power\nPower\n\n\nN\nNumber of participants\n\n\nα (type 1 error rate)\nSignificance threshold\n\n\n\nBy default, one of these options is grayed out and not modifible. This is as jpower works to compute one value at a time whilst asking you to specify the other three. By default, we’re computing N as specified by the ‘Calculate’ drop down menu at the top of the dialogue box. Try changing this to calculate one of the other values and notice that the greyed out box changes each time.\n\n\n\n\n\n\nKey step - Install the jpower addon in Jamovi\n\n\n\nLet’s run through an example. Say that we want to answer the following question:\n\nWhat is the probability the ChangeAIConfidence has been correctly detected in our previous analysis.\n\nWe should run through the following steps.\n\nSelect that we want to Calcuate ‘Power’ in the top drop down menu.\nSet the ‘Minimally interesting effect size (δ)’ to our observed effect size from before. This is 0.429 (ignoring the sign)\nSet the ‘N’ to 77 - this is the number of participants in the test (degrees of freedom are 76)\nSet ‘α (type 1 error rate)’ to 0.05 - this is usually the default\n\nThe results should appear automatically.\n\n\n\njpower gives us some really useful explanatory text along with the results. Read this carefully to understand what is going on. Firstly,\n\nThe purpose of a power analysis is to evaluate the sensitivity of a design and test. You have chosen to calculate the sensitivity of the chosen design for detecting the specified effect size.\n\nThis clarifies the type of analyses we have run - we want to compute the sensitivity of our analysis given particular data and effect size. Next the results are summarised\n\nA design with a sample size of 77 can detect effect sizes of δ ≥ 0.429 with a probability of at least 0.96, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α= 0.05.\n\nThis is the critical information. This tells us that the power is 0.96 for our analysis, indicating that we have a 96% chance of correctly detecting an effect size at least as large as the one from our test. Great news!\nFinally, the ‘Power by Effect size’ table tells us the power we would have for different effect sizes. For example, in this case if we have an effect size of 0.3 - we would have power between 50% and 80%. If the effect size were 0.2 we would have less than 50% power.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat is the sensitivity (power) of the effect size estimated by Cohen’s d for the ChangeEmoConfidence variable?\n\n\nTake a look at our previous results. Cohen’d d for ChangeEmoConfidence was -0.126.\n\n\nOur estimated power for the ChangeEmoConfidence is only 19.4%.\n\nA design with a sample size of 77 can detect effect sizes of δ ≥ 0.126 with a probability of at least0.194, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α = 0.05.\n\n\nThis is the practical consequence of effect sizes! The smaller effect for ChangeEmoConfidence is harder to detect than the larger value for ChangeAIConfidence so we have a much lower chance of getting the right answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nIt might be the case that our face recognition really does change participants confidence in recognising emotional faces, and our study simply doesn’t have enough participants to detect the change with statistical confidence. Perhaps we should consider running a larger study?\nIf we assume that our value for Cohen’s d is accurate and we assume that there really is an effect to be detected (these are pretty brave assumptions!), how many participants would a future study need to have 80% chance of correctly detecting the effect?\n\n\nWe’ll need to change our analysis to calculate ‘N’ rather than ‘Power’ using the drop down menu at the top. Can you specify the power?\n\n\nWe would need at least 497 participants to reject the null hypothesis if the estimated effect size for the change in emotional face recognition before and after our task is accurate! that is a lot….\n\nWe would need a sample size of 497 to reliably (with probability greater than 0.8) detect an effect size of δ ≥ 0.126, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α = 0.05.\n\n\nThis information suggests that this looking for differences in emotional face confidence is probably not worth pursuing futher. Our experiment wasn’t perfect but this effect is so small that it is likely to be of little to no practical significance.\n\n\n\n\n\n\n\n\nWe have explored some new methods for creating new variables and running t-tests with effect sizes using R code. The R code we wrote is a really clear way to specify how variables were manipulated and which tests were run. Quite complex analyses with several stages can be clearly expressed this way.\nFinally, we used effect sizes to explore the power and sensitivity of our experiment and explore the possiblity of a follow up study.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#the-dataset",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#the-dataset",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Dataset\n\n\n\nPlease note, this is the same dataset as we used in week 4 - but make sure that you load in the rmb-week-3_lecture-quiz-data_ai-faces-fixed.omv file from this week and NOT the week 4 file. It contains a small fix that we need for this week.\n\n\nWe’ll be working with the dataset we collected after the break during the lecture in Week 3. This is a partial replication of a study published in the journal Psychological Science (Miller et al. 2023). The original paper found the following results summarised in the abstract.\n\nRecent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 (N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces—a phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 (N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI.\n\nWe ran a quiz that students in the lecture could join on their phones/laptops. There were two parts to the quiz:\nThe faces section included 12 faces that were either photographs or real people or AI generated images of people. The stimuli were taked from the materials released by (Miller et al. 2023) on the Open Science Framework. Responses were either ‘Real person’ or ‘AI generated’.\n\nThe confidence section asked participants to rate their confidence in their ability to do the following three things.\n\nDistinguish real photos of people from AI generated images (Experimental condition)\nDistinguish photos of happy people from photos of sad people (Emotional control condition)\nDistinguish photos of people you used to know in primary school from strangers (Memory control condition)\n\nScores were recorded on a scale from 1 (Completely confidence) to 10 (Not at all confident). The confidence section was repeated before and after the faces section to see if participants confidence changed as a result of doing the AI faces task",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#the-challenge",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#the-challenge",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "This week we will explore some factors that influence t-stats and p-values whilst introducing the concept of loops in R. We will finish by using effect sizes to compute a power analysis.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#introducing-sampling-and-looping-in-r",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#introducing-sampling-and-looping-in-r",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Introduce slice_sample Introduce simple loops",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-t-tests-on-smaller-data-samples",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-t-tests-on-smaller-data-samples",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Add independent samples t-test Jamovi code to loop - run 5 subsamples for 10, 25, 50 and 100 participants and see difference in t-values.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#use-an-effect-size-for-a-power-analysis",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#use-an-effect-size-for-a-power-analysis",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "One core application for effect sizes is that we can use the results to inform our sample size planning for future studies. we can do this with four linked pieces of information\n\nEffect Size this is our effect size, usually Cohen’s d for analyses in RM-B\nSignificance Thresold - this is the ‘alpha’ level at which we would consider a result significant, usually 5% or 0.05.\nPower - this is probability of correctly detecting an effect if one exists in the data, we usually want 80% power.\nNumber of datapoints (N) - this is our sample size.\n\nThese four pieces of information are linked such that if we have any three of these values we can always compute the fourth.\nThis lets us do some very useful calculations to see how well our dataset is enabling us to answer our hypothesis and to plan for future studies. For examples\n\nIf we know the effect size, significance threshold and number of participants in a study, we can compute the power to tell us the probability that we can correctly detect an effect. This is known as ‘sensitivity’.\n\nor\n\nIf we want to compute how many participants we’ll need in a future study and we already know the effect size from previous literature, we can specify the power and significance threshold we want and compute the number of participants that we’ll need.\n\nVery useful! We can compute these different values using a Jamovi module known as ‘jpower’\n\n\n\n\n\n\nKey step - Install the jpower addon in Jamovi\n\n\n\nClick through the ‘Modules’ button on the top right of the Jamovi ribbon. Click to the list of ‘Available’ modules and install ‘jpower’\n\nClick through to open the jpower menu and select a ‘one-sample t-test’ from the options. You should see the following dialogue box\n\n\n\nLet’s take a moment to understand the options.\nFirst - notice that we have the four pieces of information listed near the top of the options.\n\n\n\njpower menu\nInterpretation\n\n\n\n\nMinimally interesting effect size (δ)\nEffect size\n\n\nMinimum desired power\nPower\n\n\nN\nNumber of participants\n\n\nα (type 1 error rate)\nSignificance threshold\n\n\n\nBy default, one of these options is grayed out and not modifible. This is as jpower works to compute one value at a time whilst asking you to specify the other three. By default, we’re computing N as specified by the ‘Calculate’ drop down menu at the top of the dialogue box. Try changing this to calculate one of the other values and notice that the greyed out box changes each time.\n\n\n\n\n\n\nKey step - Install the jpower addon in Jamovi\n\n\n\nLet’s run through an example. Say that we want to answer the following question:\n\nWhat is the probability the ChangeAIConfidence has been correctly detected in our previous analysis.\n\nWe should run through the following steps.\n\nSelect that we want to Calcuate ‘Power’ in the top drop down menu.\nSet the ‘Minimally interesting effect size (δ)’ to our observed effect size from before. This is 0.429 (ignoring the sign)\nSet the ‘N’ to 77 - this is the number of participants in the test (degrees of freedom are 76)\nSet ‘α (type 1 error rate)’ to 0.05 - this is usually the default\n\nThe results should appear automatically.\n\n\n\njpower gives us some really useful explanatory text along with the results. Read this carefully to understand what is going on. Firstly,\n\nThe purpose of a power analysis is to evaluate the sensitivity of a design and test. You have chosen to calculate the sensitivity of the chosen design for detecting the specified effect size.\n\nThis clarifies the type of analyses we have run - we want to compute the sensitivity of our analysis given particular data and effect size. Next the results are summarised\n\nA design with a sample size of 77 can detect effect sizes of δ ≥ 0.429 with a probability of at least 0.96, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α= 0.05.\n\nThis is the critical information. This tells us that the power is 0.96 for our analysis, indicating that we have a 96% chance of correctly detecting an effect size at least as large as the one from our test. Great news!\nFinally, the ‘Power by Effect size’ table tells us the power we would have for different effect sizes. For example, in this case if we have an effect size of 0.3 - we would have power between 50% and 80%. If the effect size were 0.2 we would have less than 50% power.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat is the sensitivity (power) of the effect size estimated by Cohen’s d for the ChangeEmoConfidence variable?\n\n\nTake a look at our previous results. Cohen’d d for ChangeEmoConfidence was -0.126.\n\n\nOur estimated power for the ChangeEmoConfidence is only 19.4%.\n\nA design with a sample size of 77 can detect effect sizes of δ ≥ 0.126 with a probability of at least0.194, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α = 0.05.\n\n\nThis is the practical consequence of effect sizes! The smaller effect for ChangeEmoConfidence is harder to detect than the larger value for ChangeAIConfidence so we have a much lower chance of getting the right answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow many participants would we need to have 80% chance of correctly detecting the estimated effect for ChangeEmoConfidence?\n\n\nWe’ll need to change our analysis to calculate ‘N’ rather than ‘Power’ using the drop down menu at the top.\n\n\nOur estimated power for the ChangeEmoConfidence is only 19.4%.\n\nA design with a sample size of 77 can detect effect sizes of δ ≥ 0.126 with a probability of at least0.194, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α = 0.05.\n\n\nThis is the practical consequence of effect sizes! The smaller effect for ChangeEmoConfidence is harder to detect than the larger value for ChangeAIConfidence so we have a much lower chance of getting the right answer.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#summary",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#summary",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "We have explored some new methods for creating new variables and running t-tests with effect sizes using R code. The R code we wrote is a really clear way to specify how variables were manipulated and which tests were run. Quite complex analyses with several stages can be clearly expressed this way.\nFinally, we used effect sizes to explore the power and sensitivity of our experiment and explore the possiblity of a follow up study.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-a-new-variable-with-mutate",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-a-new-variable-with-mutate",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "data &lt;- mutate(data, ConfidentBefore2 = AIConfidenceBefore&lt;4)",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#extract-random-subsamples-of-a-dataframe",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#extract-random-subsamples-of-a-dataframe",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Introduce slice_sample Introduce simple loops",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-new-variables-with-mutate",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-new-variables-with-mutate",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Last week, we used Jamovi to create several new variables that allowed us to run our analyses. We can do the same in a more transparent and reproducible way using R code.\nWe need to create both our proportion of correct faces across the 12 stimuli in one variable and a grouping variable which indicates whether each participant was confident in their ability to tell the difference between AI faces and real faces.\nWe can do both with the mutate() from the dplyr library, this provides functionality that lets us create, modify, and delete columns within a dataset. Take a look at the official documentation for mutate for more information.\nWhen computing a new varaible with mutate() we three pieces of information.\n\nThe dataset to work on\nThe name of the new variable to be created\nThe definition of how to compute the new variable for existing variables.\n\n\n\nLet’s start with our grouping variable. Our three pieces of information are\n\nThe dataset to work on - is data, which refers to the original datasheet loaded into Jamovi\nThe name of the new variable - is ConfidentBefore, the same as we used last week\nThe definition - is AIConfidenceBefore&lt;4, again the same as we used last week\n\nWe can combine these into a single line to create the variable. Note that we save the result into a new variable named my_data, we’ll use this from now on to avoid confusion with the original datasheet.\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\nWe can check that this has done what we expected by running head to see the first few rows of the dataset.\nhead(my_data)\nThe result should look like this:\n\nWe can see on in the results on the right hand side that the additional colume ConfidentBefore now appears with TRUE and FALSE values for each participant.\n\n\n\nLet’s do the same for total proportion of correctly identified faces. We can use the same principle as we used for the grouping variable and use mutate() along with the variable definition and name to create our new column.\nThis definition is pretty long as we have 12 different faces to work with! You can copy the whole line using the copy icon in the right hand side of the code box. Take a moment to look through and understand each part: which part is the Dataset, the Name and the Definition?\nmy_data &lt;- mutate(my_data, PropFacesCorrect = (Face1_Real + Face2_Real + Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face7_Real + Face8_Real + Face9_Real + Face10_Real + Face11_AI + Face12_AI) / 12)\nand validate the overall result using head() to make sure that my_data now has the column we expected.\n\nThe final column of the dataset is now PropFacesCorrect and contains the proportion of face trials that participant got right.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute variables containing the proportion of correct responses for photos of real people and AI generated faces separately?\nStore the results in PropRealCorrectFaces for real faces and PropAICorrectFaces for AI generated faces.\n\n\nWe can use the code we wrote to compute the proportion of correct responses for all faces as a starting point.\nThink about how you could modify this line to compute the result for either real or AI faces on their own? What would you need to change in the code?\n\n\nThe following code will compute the variables\nmy_data &lt;- mutate(my_data, PropRealFacesCorrect = (Face1_Real + Face2_Real + Face7_Real + Face8_Real + Face9_Real + Face10_Real) / 6)\nmy_data &lt;- mutate(my_data, PropAIFacesCorrect = (Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face11_AI + Face12_AI) / 6)\nWe’ve separated out the columns to sum all the real or AI faces together and changed the division to divide the results by 6 rather than 12.\nChecking the results with head() should produce the following outputs with four additional columns!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#summary-1",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#summary-1",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "data &lt;- mutate(data, ConfidentBefore2 = AIConfidenceBefore&lt;4)\n\ngroup_data &lt;- group_by(data, ConfidentBefore)\n\nfor (i in 1:5) {\n\nsmall_data &lt;- slice_sample(group_data, n=5)\n\nresult &lt;- jmv::ttestIS(\n    formula = PropFacesCorrect ~ ConfidentBefore,\n    data = small_data,\n    effectSize = TRUE)\n\nprint(as.data.frame(result$ttest)$stat)\n    \n}",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-a-t-test-using-r",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-a-t-test-using-r",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "We have the ingredients for our hypothesis test. Let’s use R to explore the following hypothesis (Hypothesis 2 from week 4).\n\nConfident people are better at distinguishing AI faces from real faces\n\nWe can compute this test using the following code that calls ttestIS() - this is the function that computes independent samples t-tests. Read more about it on the ttestIS documentation page.\nYou can click the numbers by the definitions at the bottom to highlight the corresponding part of the code.\n1jmv::ttestIS(\n2    formula = PropFacesCorrect ~ ConfidentBefore,\n3    data = my_data,\n4    effectSize = TRUE)\n\n1\n\njmv::ttestIS is the name of the R function that Jamovi uses to compute independent samples t-tests\n\n2\n\nThis tells ttestIS the formula that defines the test we want to run.\n\n3\n\nThis tells ttestIS which dataset we want to analyse\n\n4\n\nThis adds an additional effect size computation to the results\n\n\nMost of this will be familiar from previous weeks, but let’s think about the formula in a little more detail.\nIn R, the tilde (~) is used in formula definitions to specify the relationship between variables, particularly in statistical modeling and data analysis. Here, the tilde separates the dependent variable (response) from the independent variables (predictors). In our example, PropFacesCorrect is the dependent variable and ConfidentBefore is our independent variable (grouping variable) - so this formula\nPropFacesCorrect ~ ConfidentBefore\nis essentially telling ttestIS() to “Compute the ttest on PropFacesCorrect using ConfidentBefore as our groups”.\nWe can now run all our code to get the results of the ttest - the code clearly tells the reader how all the relevant variables were computed and what hypothesis test has been run all in one screen.\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nRead through the ttestIS documentation page. How could you change your code to add the following assumption checks\n\nTest for homogeneity of variance\nTest for normal distribution\n\n\n\nThe documentation page contains a list of all the possible information that we can pass as an input to our ttestIS() function call. Each item in the list corresponds to the options available in the Jamovi dialogue box.\nHave a look at the norm and eqv definitions. What would you need to add to the function call to run these additional checks?\n\n\nThe following code will compute all the checks in addition to the t-test.\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE,\n    norm = TRUE,\n    eqv = TRUE)                             \nThe results should look like this:\n\nDo the assumptions for Student’s t-test hold for this data?\n\n\n\n\n\n\nYour script might be getting quite long at this point! It is always a good idea to keep code neat and tidy where possible so that other people are able to read it, and so that we can read it if we come back to the analysis in the future.\nThere are many ways to keep things organised in a script. Here are two good hints, code comments can organise your script without changing the output and print() statements can help organise your code and the outputs.\n\n\nAny line of R code that starts with a hashtag is called a ‘comment’. The writing on this line is there for our information and will not be run by the computer. Adding code comments is a useful way to annotate your code indicating what each line or each section is doing. For example:\n# This is code comment\n\n# Compute which participants were more confident than the median\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\n\nThis will make it easier to understand what the coding is doing in future.\n\n\n\nWe can use the print() function to help organise our code as well. The text within the call to print() will not be executed by the computer but will simply be printed into the output console. This can be useful to break the output of your code into sections and to inlcude additional information about the analysis next to the outputs themselves.\nFor example:\nprint('Hypothesis 1')\nprint('Confident people are better at distinguishing AI faces from real faces')\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)\nThis code will print out information about the hypothesis next to the outputs.\nA full cleaned and commented version of our code might look like this.\n# Compute which participants were more confident than the median\nmy_data &lt;- mutate(data, ConfidentBefore = AIConfidenceBefore&lt;4)\n\n# Compute the proportion of all faces each participant got right\nmy_data &lt;- mutate(my_data, PropFacesCorrect = (Face1_Real + Face2_Real + Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face7_Real + Face8_Real + Face9_Real + Face10_Real + Face11_AI + Face12_AI) / 12)\n\n# Compute the proportion of REAL faces each participant got right\nmy_data &lt;- mutate(my_data, PropRealFacesCorrect = (Face1_Real + Face2_Real + Face7_Real + Face8_Real + Face9_Real + Face10_Real) / 6)\n\n# Compute the proportion of AI faces each participant got right\nmy_data &lt;- mutate(my_data, PropAIFacesCorrect = (Face3_AI + Face4_AI + Face5_AI + Face6_AI + Face11_AI + Face12_AI) / 6)\n\n# Hypothesis test\nprint('Hypothesis 1')\nprint('Confident people are better at distinguishing AI faces from real faces')\njmv::ttestIS(                                       \n    formula = PropFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)\nand running this code produces the following outputs including the test from our print statements\n\nNice and clear what is happening at each stage! There is no perfect or ‘correct’ way to tidy up your code (though some people can get opinionated about this…). Choose a mix of comments and print statements that makes sense to you.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nExtend your code to test a second hypothesis.\n\nConfident participants are more accurate than unconfident participants when identifying photos of real faces.\n\nInclude some code comments and print statements\n\n\nThis will require a second indenpendent samples t-test asking whether the mean proportion correct value for only real faces is different between our confident and not-confident groups.\nUse the previous t-test as a starting point, can you copy this and modify it to do what we need?\n\n\nThe following code will test the new hypothesis with some comments and print statements\n# Hypothesis test\nprint('Hypothesis 2')\nprint('Confident participants are more accurate than unconfident participants when identifying photos of real faces.')\njmv::ttestIS(                                       \n    formula = PropRealFacesCorrect ~ ConfidentBefore,  \n    data = my_data,                                \n    effectSize = TRUE)                       \nThe output of the whole script should now look like this.\n\nNote how the print statements help to separate the results into interpretable chunks.\n\n\n\n\n\n\nNow we’re going to explore how the size of the data sample changes the t-values that w\nIntroduce slice_sample Introduce simple loops",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#analysis-from-a-friend",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#analysis-from-a-friend",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Imagine that of your friends has been running some analyses on this dataset and would like you to check over their work. They send you the following R code that they have put together.\n# Compute some new variables\nmy_data &lt;- mutate(data, ChangeAIConfidence = AIConfidenceBefore - AIConfidenceAfter)\nmy_data &lt;- mutate(my_data, ChangeEmoConfidence = EmoConfidenceBefore - EmoConfidenceAfter)\nmy_data &lt;- mutate(my_data, ChangeMemoryConfidence = MemoryConfidenceBefore - MemoryConfidenceAfter)\n\n# Run the tests\njmv::ttestOneS(\n    data = my_data,\n    vars = vars(ChangeAIConfidence, ChangeEmoConfidence, ChangeMemoryConfidence),\n    testValue = 0.0,\n    wilcoxon = TRUE,\n    desc = TRUE,\n    norm = TRUE)\nLet’s think about this code a bit, your friend didn’t really explain the plan…\n\n\n\n\n\n\n Data Skills - what variables are created?\n\n\n\n\n\nThe code creates three new variables, ChangeAIConfidence, ChangeEmoConfidence and ChangeMemoryConfidence - each is computed by subtracting the confidence values after the face task from the confidence values before the task.\nThese variables represent the change in confidence from completing the task.\n\n\n\n\n\n\n\n\n\n Data Skills - what statistical test is performed?\n\n\n\n\n\nThe code runs a one sample t-test comparing the data mean to 0.0 using the jamovi function jmv::ttestOneS(). The code runs the test on all three variables. We can see that the input vars is passed a list of all three variable names.\n\n\n\n\n\n\n\n\n\n Data Skills - what additional computations are performed?\n\n\n\n\n\nIn addition to the t-tests, the code runs a Wilcoxon Rank test, a Shapiro-Wilk test for normality and a descriptive statistics table.\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and run this code before going any further! Take a moment to read through the code and decide what it is doing.\n\n\n\n\n\n\nClick here if you want to check that you have the right outputs.\n\n\n\n\n\nThe results should look like this:\n\n\n\n\n\n\nThey have written the following about the results.\n\nA one sample t-test found that the change in participant confidence in distinguishing AI faces from real faces was significantly reduced after performing the task. The difference in confidence before and after the task (M=-1.1, SD=2.7) was significantly below zero, t(76) = 3.538, p = 0.0007. In contrast there was no change after the task confidence in emotion recognition (M=-.017, SD=1.341), t(76) = -1.012, p = 0.313 and face memory (M=-0.02985, SD=1.044), t(76) = -0.234, p = 0.816.\n\nLet’s think about whether this is accurate.\n\n\n\n\n\n\n Data Skills - are the reported statistics correct? take your time to read carefully and compare to the outputs\n\n\n\n\n\nNo! there are a number of mistakes throughout this writing. How many of the following mistakes did you spot?\n\nA one sample t-test found that the change in participant confidence in distinguishing AI faces from real faces was significantly reduced after performing the task. The difference in confidence before and after the task (M=-1.1, SD=2.7) was significantly below zero, t(76) = 3.538, p = 0.0007. In contrast there was no change after the task confidence in emotion recognition (M=-.017, SD=1.341), t(76) = -1.012, p = 0.313 and face memory (M=-0.02985, SD=1.044), t(76) = -0.234, p = 0.816.\n\n\n3.538 -&gt; the sign of the t-values on this test is wrong. Take a look a the output table, this should be -3.538.\n76 -&gt; the degrees of freedom for the second and third tests are reported incorrectly (perhaps copied from the first test?). These should be 64 and 66 respectively\n\n\n\n\n\n\n\n\n\n\n Data Skills - was the correct test reported?\n\n\n\n\n\nAgain, no! there are mistakes here as well. The friend has done the right thing by computing the Shapiro-Wilk statistics but appears to have ignored them in the writing.\nShapiro-Wilk is significant for all three variables - we should strongly consider reporting the Wilcoxon Rank Sum test rather than Student’s t.\nThey could have run this analysis as a paired t-test, but the one sample t-test of the differences is also a valid approach.\n\n\n\nComputing statistics and accurately reporting the results takes a lot of precision! take your time when writing code and results sections - make sure to check and double check your work as you go…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-effect-sizes-for-t-tests",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#compute-effect-sizes-for-t-tests",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "Let’s think more about effect sizes - computing effect sizes in Jamovi and R is really easy. Simply click the check box to add ‘Effect size’ under the ‘Addtional Statistics’ section of the Jamovi window - or add effectSize = TRUE to the function call in R (we’ve already been doing this above!).\n\n\n\n\n\n\nKey step\n\n\n\nUpdate your friend’s code to include computation of effect sizes - the results should appear in the t-test results table on the right.\n\n\n\nThere are effect sizes computed for every t-test and its non-parametric alternatives.\n\nCohen’s d is the parametric effect size corresponding to Student’s t-test\nRank Biserial Correlation is the non-parametric effect size corresponding to Wilcoxon’s test.\n\nThough they have methodological differences, these effect sizes can be interpreted in the same way as a measure of the magnitude of an effect.\nRemember that this is different to the t-value which is a measure of evidence against the null hypothesis. The important difference is that we can have strong evidence against the null either as the difference is large or if we have measured a small difference very precisely. The effect size only cares about the size of the difference between conditions.\nNote that Jamovi/R provide a ‘signed’ effect size indicating the direction of the effect in the same way that a t-statistic does. For the following work we can ignore this sign and focus on the magnitudes only. In other words, we’ll consider an effect size of +0.5 or -0.5 to indicate the same magitude of effect.\nThe creator of many of our classic effect size measures provided a guideline for what might be considered a ‘small’ or a ‘large’ effect.\n\n\n\nEffect Size\nCohen’s d\nInterpretation\n\n\n\n\nSmall\n0.2\nSmall effect\n\n\nMedium\n0.5\nMedium effect\n\n\nLarge\n0.8\nLarge effect\n\n\n\nThese can be useful guidelines - through they have been criticised strongly for both the arbitrary nature of the thresholds and for over-simplifying interpretation of effect sizes. We can still use them as an informal indicator to help us quickly interpret our results.\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeAIConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.403 is just below the thresold to be a ‘medium’ effect (ignoring the sign of the effect size). The effect size for the Rank Biserial correlation is similar.\n\n\n\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeEmoConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.126 is just below the thresold to be a ‘small’ effect (ignoring the sign of the effect size). The effect size for the Rank Biserial correlation is larger but still closer to small than medium.\n\n\n\n\n\n\n\n\n\n Data Skills - Is the effect size for ChangeMemoryConfidence small, medium or large?\n\n\n\n\n\nThe effect size -0.0286 is tiny! the Rank Biserial correlation is even smaller…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-5_pvalue-effect-size.html#use-an-effect-size-for-a-sensitivity-power-analysis",
    "href": "tutorials/rmb_week-5_pvalue-effect-size.html#use-an-effect-size-for-a-sensitivity-power-analysis",
    "title": "Week 5 : p-values and effect sizes",
    "section": "",
    "text": "One core application for effect sizes is that we can use the results to inform our sample size planning for future studies. we can do this with four linked pieces of information\n\nEffect Size this is our effect size, usually Cohen’s d for analyses in RM-B\nSignificance Thresold - this is the ‘alpha’ level at which we would consider a result significant, usually 5% or 0.05.\nPower - this is probability of correctly detecting an effect if one exists in the data, we usually want 80% power.\nNumber of datapoints (N) - this is our sample size.\n\nThese four pieces of information are linked such that if we have any three of these values we can always compute the fourth.\nThis lets us do some very useful calculations to see how well our dataset is enabling us to answer our hypothesis and to plan for future studies. For examples\n\nIf we know the effect size, significance threshold and number of participants in a study, we can compute the power to tell us the probability that we can correctly detect an effect. This is known as ‘sensitivity’.\n\nor\n\nIf we want to compute how many participants we’ll need in a future study and we already know the effect size from previous literature, we can specify the power and significance threshold we want and compute the number of participants that we’ll need.\n\nVery useful! We can compute these different values using a Jamovi module known as ‘jpower’\n\n\n\n\n\n\nKey step - Install the jpower addon in Jamovi\n\n\n\nClick through the ‘Modules’ button on the top right of the Jamovi ribbon. Click to the list of ‘Available’ modules and install ‘jpower’\n\nClick through to open the jpower menu and select a ‘one-sample t-test’ from the options. You should see the following dialogue box\n\n\n\nLet’s take a moment to understand the options.\nFirst - notice that we have the four pieces of information listed near the top of the options.\n\n\n\njpower menu\nInterpretation\n\n\n\n\nMinimally interesting effect size (δ)\nEffect size\n\n\nMinimum desired power\nPower\n\n\nN\nNumber of participants\n\n\nα (type 1 error rate)\nSignificance threshold\n\n\n\nBy default, one of these options is grayed out and not modifible. This is as jpower works to compute one value at a time whilst asking you to specify the other three. By default, we’re computing N as specified by the ‘Calculate’ drop down menu at the top of the dialogue box. Try changing this to calculate one of the other values and notice that the greyed out box changes each time.\n\n\n\n\n\n\nKey step - Install the jpower addon in Jamovi\n\n\n\nLet’s run through an example. Say that we want to answer the following question:\n\nWhat is the probability the ChangeAIConfidence has been correctly detected in our previous analysis.\n\nWe should run through the following steps.\n\nSelect that we want to Calcuate ‘Power’ in the top drop down menu.\nSet the ‘Minimally interesting effect size (δ)’ to our observed effect size from before. This is 0.429 (ignoring the sign)\nSet the ‘N’ to 77 - this is the number of participants in the test (degrees of freedom are 76)\nSet ‘α (type 1 error rate)’ to 0.05 - this is usually the default\n\nThe results should appear automatically.\n\n\n\njpower gives us some really useful explanatory text along with the results. Read this carefully to understand what is going on. Firstly,\n\nThe purpose of a power analysis is to evaluate the sensitivity of a design and test. You have chosen to calculate the sensitivity of the chosen design for detecting the specified effect size.\n\nThis clarifies the type of analyses we have run - we want to compute the sensitivity of our analysis given particular data and effect size. Next the results are summarised\n\nA design with a sample size of 77 can detect effect sizes of δ ≥ 0.429 with a probability of at least 0.96, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α= 0.05.\n\nThis is the critical information. This tells us that the power is 0.96 for our analysis, indicating that we have a 96% chance of correctly detecting an effect size at least as large as the one from our test. Great news!\nFinally, the ‘Power by Effect size’ table tells us the power we would have for different effect sizes. For example, in this case if we have an effect size of 0.3 - we would have power between 50% and 80%. If the effect size were 0.2 we would have less than 50% power.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nWhat is the sensitivity (power) of the effect size estimated by Cohen’s d for the ChangeEmoConfidence variable?\n\n\nTake a look at our previous results. Cohen’d d for ChangeEmoConfidence was -0.126.\n\n\nOur estimated power for the ChangeEmoConfidence is only 19.4%.\n\nA design with a sample size of 77 can detect effect sizes of δ ≥ 0.126 with a probability of at least0.194, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α = 0.05.\n\n\nThis is the practical consequence of effect sizes! The smaller effect for ChangeEmoConfidence is harder to detect than the larger value for ChangeAIConfidence so we have a much lower chance of getting the right answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nIt might be the case that our face recognition really does change participants confidence in recognising emotional faces, and our study simply doesn’t have enough participants to detect the change with statistical confidence. Perhaps we should consider running a larger study?\nIf we assume that our value for Cohen’s d is accurate and we assume that there really is an effect to be detected (these are pretty brave assumptions!), how many participants would a future study need to have 80% chance of correctly detecting the effect?\n\n\nWe’ll need to change our analysis to calculate ‘N’ rather than ‘Power’ using the drop down menu at the top. Can you specify the power?\n\n\nWe would need at least 497 participants to reject the null hypothesis if the estimated effect size for the change in emotional face recognition before and after our task is accurate! that is a lot….\n\nWe would need a sample size of 497 to reliably (with probability greater than 0.8) detect an effect size of δ ≥ 0.126, assuming a two-sided criterion for detection that allows for a maximum Type I error rate of α = 0.05.\n\n\nThis information suggests that this looking for differences in emotional face confidence is probably not worth pursuing futher. Our experiment wasn’t perfect but this effect is so small that it is likely to be of little to no practical significance.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 5 - p-values and Effect Sizes"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-7_non-parametrics.html",
    "href": "tutorials/rmb-week-7_non-parametrics.html",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "This week we will explore how to use the jmv package in R to test hypotheses about a dataset using t-tests and to compute effect sizes corresponding to those tests. We will use those effect sizes to help interpret the sensitivity and power of our experiments.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nAssumptions of parametric tests\n\n\n\nData transformation with the log() function\n\n\n\nHistograms\n\n\n\nQQ plots\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nData visualisations using ggplot\n\n\n\nRun non-parametric alternative tests in Jamovi and R\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nReplicate part of a published analysis\n\n\n\n\n\n\n\n\n\nContent Info\n\n\n\nThis tutorial refers to a video in which participants watched unpleasant events to simulate a traumatic experience in the lab. The content of this video is not disclosed or dicussed in any more detail than this during this tutorial - those who are interested can find more information in methods section the published article. This tutorial does explore the emotional impact that the video has on participants.\n\n\n\n\nFlashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we’re going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind’s eye and cause distress. We investigated whether reconsolidation—the process during which memories become malleable when recalled—can be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group.\n\n\n\nThis week - we will explore two of the four conditions to test the following hypothesis:\n\nPlaying tetris will after a trumatic event will reduce number of subsequent intrusive memories\n\nAlong the way, we’ll take a detailed tour of our options when working with non-normally distributed data. We’ll begin to look at some more advanced options for creating data visualisations using a R package named ggplot - or not.\n\n\n\nLet’s start by getting familiar with the dataset. Load the file into Jamovi and take a look at the ‘Variables’ tab.\n\nThe creators of this dataset have helpfully included annotations for many of the variables they’ve measured. This is extremely useful for us and definitely worth a few moments for you to do the same when working with your own data.\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to read through the variable descriptions. You’ll need to refer back to this list throughout the session to find relevant variables.\n\n\nNext - take a look at the data sheet and the Condition variable. These relate to the different conditions in the experiment - all participants watched the film containing traumatic events and the four conditions describe what participants did after watching the film on day 1 of the experiment.\n\n\n\n\n\n\n\nCondition\nDescription\n\n\n\n\nNo-Task Control\nParticipants sit quietly\n\n\nReactivation+Tetris\nParticipants rewatch the film and then play Tetris\n\n\nTetris Only\nParticipants play Tetris\n\n\nReactivation Only\nParticipants rewatch the film\n\n\n\nParticipants were asked to keep an diary of any intrusive thoughts that they had prior to the experiment (day zero) and during the week after the experiment (days one to seven).\n\n\n\n\n\n\n Data Skills - which variable tracks the diary of intrusive memories?\n\n\n\n\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary contains the number of intrusive memories the participant recorded in the week after watching the film.\nDay_Zero_Image_Based_Intrusions_in_Intrusion_Diary is very similar - it contains the number of intrusive memories the participant recorded in the time prior to watching the film. This is a good measure to include so we can rule out differences in the baseline number of intrusive memories between our groups.\n\n\n\n\n\n\n\n\n\n Data Skills - what information do we have about participant’s performance when playing tetris?\n\n\n\n\n\nTetris_Total_Score contains the cumulative score of all tetris games the participant played - participants were allowed to restart their game if they finished before the time was up.\nWe also have Self_Rated_Tetris_Performance which measures how participants rated the difficulty of the tetris task and Tetris_Demand_Rating which measures how participants thought that playing tetris would affect their intrusive memories.\n\n\n\n\n\n\nLet’s start by taking a look at how participants in the conditions that played Tetris scored during their game time. Compute descriptive statistics on the Tetris_Total_Score variable and take a moment to get a sense of what it is telling you.\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the descriptive statistics of Tetris_Total_Score to answer the following questions in the spaces provided.\nThe average Tetris score was .\nThe most successful Tetris player scored  points.\nThe Tetris scores are positively skewednot skewednegatively skewed with a skewness of .\nThe Shapiro-Wilk statistic has a value of W =  and the p-value indicates that the scores are significantly non-normally distributednormally distributed\nWe should use parametricnon-parametric statistical approaches when analysing Tetris_Total_Score.\n\n\n\n\n\n\n\n\nLet’s use this data to test a hypothesis.\n\nPerformance whilst playing Tetris will be different at different times of day.\n\nWe can test this with an independent samples t-test as we’ve done in previous weeks. Set up the t-test and include the following:\n\nWelchs’ t-test\nMann-Whitney U\nEffect Size\nHomogeneity test\nNormality test\n\n\nLet’s unpack the results.\n\n\n\n\n\n\n Data Skills - does the Normality assumption check confirm what we saw in the descriptives?\n\n\n\n\n\nYes, we get the same picture but with a little less detail. Shapiro-Wilk’s test is significant indicating a non-normal distribution, but we still need the descriptive stats to understand HOW the distribution is different from a normal distribution.\nImportantly, this confirms that we should report the non-parametric alternative test - the Mann-Whitney U.\nYou could add the descriptives to the t-test page - but notice that this only includes a handful of metrics that are specifically related to t-tests (mean, standard deviation etc). We need the full list on the ‘Descriptives’ tab to investigate distributions in detail.\n\n\n\n\n\n\n\n\n\n Data Skills - what does Levene’s test indicate and does this matter for us?\n\n\n\n\n\nLevene’s test explores homogeneity of variance, the result is not significant indicating that we are safe to assume homogeneity of variance between our groups.\nHowever, the data are not normally distributed - non-parametric tests don’t have an assumption of homogeneity of variance at all so, for this analysis, Levene’s test isn’t really relevant.\n\n\n\n\n\n\n\n\n\n Data Skills - what does the Mann-Whitney U test indicate about our hypothesis?\n\n\n\n\n\nThe Mann-Whitney U test is not significant - it seems like tetris scores are similar for participants who played in the morning and those who played in the afternoon.\n\n\n\n\n\n\nOn to our main hypothesis. (James et al. 2015) were most interested in the combination of the reactivation task and the tetris intervention and their effect on intrusive memories about the film. Let’s focus on only the two conditions we can compare with a t-test.\n\n\n\n\n\n\nKey step\n\n\n\nSet up a Jamovi data filter to focus our analysis on only the two conditions “No-Task Control” and “Reactivation+Tetris” conditions. Take a look at week 3 section 4 to revise data filters if you need to.\nYou can use this code to select both conditions together:\nCondition == \"No-Task Control\" or Condition == \"Reactivation+Tetris\"\nFinally, compute an independent samples t-test to test the following:\n\nPerforming a Reactivation and Tetris task changes the number of intrusive memories in the week after watching a traumatic film.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThere are  participants in each group.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p &lt; \nLevene’s test for homogeneity of variance indicated that we cancan not assume homogeneity of variance between the No-Task and Reactivation+Tetris groups, F(1, 32) = , p = .\nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in the week after the film between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .\n\n\n\n\n\n\n\n\n\n\n\nOur hypothesis test gives statistical support to the idea that playing tetris reduces intrusive memories - but could there be other explanations? This experiment is likely not perfect and it is possible that some bad luck in the data collection could have impacted the results. In this section we’ll validate a few additional factors in the experimenta..\n\n\n(James et al. 2015) collected data variable to act as a control condition and rule out one possible confound in the experiment. Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary indicates the number of intrusive memories each participant had in the time before the experiment.\nThere isn’t a huge number of participants in each condition, it is possible that one group of participants was having more or less intrusive memories to begin with - irrespective of the experimental procedure.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a t-test to test the following:\n\nThere are no differences in the number of instrusive memories between the two experimental group BEFORE the experiment started.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThe participant in the ‘No-Task Control’ group had mean of  intrusive memories prior to the experiment starting.\nThe participant in the ‘Reactivation+Tetris’ group had mean of  intrusive memories prior to the experiment starting.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p = \nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in day before between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .\n\n\n\n\n\n\n\n\n\n\n\nAnother important factor to check was that the film actually changed how the participants felt. If the film didn’t do anything to impact the participants emotions then it is possible that we don’t actually understand the source of the difference we saw in the main hypothesis.\nThe film is the main experimental manipulation that so it is critical to check that it worked as we expected.\n\n\n\n\n\n\nKey step\n\n\n\nCompute paired samples t-test’s to check the following:\n\nWatching the traumatic film will change participants emotional state.\n\nCheck ALL of participants reported sadness, hopelessness, fear, horror and anxiousness.\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nTrue or False? The data variables for these factors are Normally Distributed according to Shaprio-Wilk statistics.\n\nSadness TRUEFALSE\nHopeless TRUEFALSE\nFear TRUEFALSE\nHorror TRUEFALSE\nAxious TRUEFALSE\n\nHow did the following factors change or not after watching the film?\n\nSadness increasedstayed the samedecreased after watching the film\nHopelessless increasedstayed the samedecreased after watching the film\nFear increasedstayed the samedecreased after watching the film\nHorror increasedstayed the samedecreased after watching the film\nAxiousness increasedstayed the samedecreased after watching the film\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - did the film manipulation work?\n\n\n\n\n\nYes, all factors apart from Hopelessless were increased by watching the film, indicating that the film had a strongly negative impact on participants in the experiment.\n\n\n\n\n\n\n\n\n\n\nWe’ve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code."
  },
  {
    "objectID": "tutorials/rmb-week-7_non-parametrics.html#the-dataset",
    "href": "tutorials/rmb-week-7_non-parametrics.html#the-dataset",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Flashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we’re going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind’s eye and cause distress. We investigated whether reconsolidation—the process during which memories become malleable when recalled—can be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group."
  },
  {
    "objectID": "tutorials/rmb-week-7_non-parametrics.html#the-challenge",
    "href": "tutorials/rmb-week-7_non-parametrics.html#the-challenge",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "This week - we will explore two of the four conditions to test the following hypothesis:\n\nPlaying tetris will after a trumatic event will reduce number of subsequent intrusive memories\n\nAlong the way, we’ll take a detailed tour of our options when working with non-normally distributed data. We’ll begin to look at some more advanced options for creating data visualisations using a R package named ggplot - or not."
  },
  {
    "objectID": "tutorials/rmb-week-7_non-parametrics.html#exploring-the-data-using-skills-so-far",
    "href": "tutorials/rmb-week-7_non-parametrics.html#exploring-the-data-using-skills-so-far",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Let’s start by getting familiar with the dataset. Load the file into Jamovi and take a look at the ‘Variables’ tab.\n\nThe creators of this dataset have helpfully included annotations for many of the variables they’ve measured. This is extremely useful for us and definitely worth a few moments for you to do the same when working with your own data.\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to read through the variable descriptions. You’ll need to refer back to this list throughout the session to find relevant variables.\n\n\nNext - take a look at the data sheet and the Condition variable. These relate to the different conditions in the experiment - all participants watched the film containing traumatic events and the four conditions describe what participants did after watching the film on day 1 of the experiment.\n\n\n\n\n\n\n\nCondition\nDescription\n\n\n\n\nNo-Task Control\nParticipants sit quietly\n\n\nReactivation+Tetris\nParticipants rewatch the film and then play Tetris\n\n\nTetris Only\nParticipants play Tetris\n\n\nReactivation Only\nParticipants rewatch the film\n\n\n\nParticipants were asked to keep an diary of any intrusive thoughts that they had prior to the experiment (day zero) and during the week after the experiment (days one to seven).\n(James et al. 2015) were most interested in the combination of the reactivation task and the tetris intervention and their effect on intrusive memories about the film. Let’s focus on only the two conditions we can compare with a t-test.\n\n\n\n\n\n\nKey step\n\n\n\nSet up a Jamovi data filter to focus our analysis on only the two conditions “No-Task Control” and “Reactivation+Tetris” conditions. Take a look at week 3 section 4 to revise data filters if you need to.\nYou can use this code to select both conditions together:\nCondition == \"No-Task Control\" or Condition == \"Reactivation+Tetris\"\n\n\nExplore descriptives - what do you find? Strong postive skew and some kutosis in both, QQ plot confirms. Day one more prominent.\nCompute independent samples t-test with effect size and normality checks.\nHow would you report the Mann-Whitney U test.\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\nAnswer the following questions in the spaces provided.\n2 + 2 is \n2 + 2 is"
  },
  {
    "objectID": "webexercises.html",
    "href": "webexercises.html",
    "title": "Webexercises",
    "section": "",
    "text": "This is a Web Exercise template created by the psychology teaching team at the University of Glasgow, based on ideas from Software Carpentry. This template shows how instructors can easily create interactive web documents that students can use in self-guided learning.\nThe {webexercises} package provides a number of functions that you use in inline R code or through code chunk options to create HTML widgets (text boxes, pull down menus, buttons that reveal hidden content). Examples are given below. Render this file to HTML to see how it works.\nNOTE: To use the widgets in the compiled HTML file, you need to have a JavaScript-enabled browser."
  },
  {
    "objectID": "webexercises.html#example-questions",
    "href": "webexercises.html#example-questions",
    "title": "Webexercises",
    "section": "Example Questions",
    "text": "Example Questions\n\nFill-In-The-Blanks (fitb())\nCreate fill-in-the-blank questions using fitb(), providing the answer as the first argument.\n\n2 + 2 is \n\nYou can also create these questions dynamically, using variables from your R session.\n\nThe square root of 16 is: \n\nThe blanks are case-sensitive; if you don’t care about case, use the argument ignore_case = TRUE.\n\nWhat is the letter after D? \n\nIf you want to ignore differences in whitespace use, use the argument ignore_ws = TRUE (which is the default) and include spaces in your answer anywhere they could be acceptable.\n\nHow do you load the tidyverse package? \n\nYou can set more than one possible correct answer by setting the answers as a vector.\n\nType a vowel: \n\nYou can use regular expressions to test answers against more complex rules.\n\nType any 3 letters: \n\n\n\nMultiple Choice (mcq())\n\n“Never gonna give you up, never gonna: let you goturn you downrun awaylet you down”\n“I bless the rainsguess it rainssense the rain down in Africa” -Toto\n\n\n\nTrue or False (torf())\n\nTrue or False? You can permute values in a vector using sample(). TRUEFALSE\n\n\n\nLonger MCQs (longmcq())\nWhen your answers are very long, sometimes a drop-down select box gets formatted oddly. You can use longmcq() to deal with this. Since the answers are long, It’s probably best to set up the options inside an R chunk with echo=FALSE.\nWhat is a p-value?\n\n the probability that the null hypothesis is true the probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is true the probability of making an error in your conclusion\n\nWhat is true about a 95% confidence interval of the mean?\n\n there is a 95% probability that the true mean lies within this range if you repeated the process many times, 95% of intervals calculated in this way contain the true mean 95% of the data fall within this range"
  },
  {
    "objectID": "webexercises.html#checked-sections",
    "href": "webexercises.html#checked-sections",
    "title": "Webexercises",
    "section": "Checked sections",
    "text": "Checked sections\nCreate sections with the class webex-check to add a button that hides feedback until it is pressed. Add the class webex-box to draw a box around the section (or use your own styles).\n\nI am going to learn a lot: TRUEFALSE\nWhat is a p-value?\n\n the probability that the null hypothesis is true the probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is true the probability of making an error in your conclusion"
  },
  {
    "objectID": "webexercises.html#hidden-solutions-and-hints",
    "href": "webexercises.html#hidden-solutions-and-hints",
    "title": "Webexercises",
    "section": "Hidden solutions and hints",
    "text": "Hidden solutions and hints\nYou can fence off a solution area that will be hidden behind a button using hide() before the solution and unhide() after, each as inline R code. Pass the text you want to appear on the button to the hide() function.\nIf the solution is a code chunk, instead of using hide() and unhide(), simply set the webex.hide chunk option to TRUE, or set it to the string you wish to display on the button.\nRecreate the scatterplot below, using the built-in cars dataset.\n\n\n\n\n\n\n\n\n\n\n\nI need a hint\n\nSee the documentation for plot() (?plot)\n\n\n\n\n\nClick here to see the solution\n\nplot(cars$speed, cars$dist)"
  },
  {
    "objectID": "tutorials/rmb-week-7_non-parametrics.html#exploring-and-filtering-the-data",
    "href": "tutorials/rmb-week-7_non-parametrics.html#exploring-and-filtering-the-data",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Let’s start by getting familiar with the dataset. Load the file into Jamovi and take a look at the ‘Variables’ tab.\n\nThe creators of this dataset have helpfully included annotations for many of the variables they’ve measured. This is extremely useful for us and definitely worth a few moments for you to do the same when working with your own data.\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to read through the variable descriptions. You’ll need to refer back to this list throughout the session to find relevant variables.\n\n\nNext - take a look at the data sheet and the Condition variable. These relate to the different conditions in the experiment - all participants watched the film containing traumatic events and the four conditions describe what participants did after watching the film on day 1 of the experiment.\n\n\n\n\n\n\n\nCondition\nDescription\n\n\n\n\nNo-Task Control\nParticipants sit quietly\n\n\nReactivation+Tetris\nParticipants rewatch the film and then play Tetris\n\n\nTetris Only\nParticipants play Tetris\n\n\nReactivation Only\nParticipants rewatch the film\n\n\n\nParticipants were asked to keep an diary of any intrusive thoughts that they had prior to the experiment (day zero) and during the week after the experiment (days one to seven)."
  },
  {
    "objectID": "tutorials/rmb-week-7_non-parametrics.html#is-participant-performance-on-tetric-normally-distributed",
    "href": "tutorials/rmb-week-7_non-parametrics.html#is-participant-performance-on-tetric-normally-distributed",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Let’s start by taking a look at how participants in the Tetris condition scored during their game time. Compute descriptive statistics on the Tetris_Total_Score variable and take a moment to get a sense of what it is telling you.\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the descriptive statistics of Tetris_Total_Score to answer the following questions in the spaces provided.\n\n\n\n\n\n\n\n\nExplore descriptives - what do you find? Strong postive skew and some kutosis in both, QQ plot confirms. Day one more prominent.\nCompute independent samples t-test with effect size and normality checks.\nHow would you report the Mann-Whitney U test.\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\nAnswer the following questions in the spaces provided.\n2 + 2 is \n2 + 2 is"
  },
  {
    "objectID": "tutorials/rmb-week-7_non-parametrics.html#is-participant-performance-on-tetris-normally-distributed",
    "href": "tutorials/rmb-week-7_non-parametrics.html#is-participant-performance-on-tetris-normally-distributed",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Let’s start by taking a look at how participants in the conditions that played Tetris scored during their game time. Compute descriptive statistics on the Tetris_Total_Score variable and take a moment to get a sense of what it is telling you.\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the descriptive statistics of Tetris_Total_Score to answer the following questions in the spaces provided.\nThe average Tetris score was .\nThe most successful Tetris player scored  points.\nThe Tetris scores are positively skewednot skewednegatively skewed with a skewness of .\nThe Shapiro-Wilk statistic has a value of W =  and the p-value indicates that the scores are significantly non-normally distributednormally distributed\nWe should use parametricnon-parametric statistical approaches when analysing Tetris_Total_Score.\n\n\n\n\n\n\n\n\nLet’s use this data to test a hypothesis.\n\nPerformance whilst playing Tetris will be different at different times of day.\n\nWe can test this with an independent samples t-test as we’ve done in previous weeks. Set up the t-test and include the following:\n\nWelchs’ t-test\nMann-Whitney U\nEffect Size\nHomogeneity test\nNormality test\n\n\nLet’s unpack the results.\n\n\n\n\n\n\n Data Skills - does the Normality assumption check confirm what we saw in the descriptives?\n\n\n\n\n\nYes, we get the same picture but with a little less detail. Shapiro-Wilk’s test is significant indicating a non-normal distribution, but we still need the descriptive stats to understand HOW the distribution is different from a normal distribution.\nImportantly, this confirms that we should report the non-parametric alternative test - the Mann-Whitney U.\nYou could add the descriptives to the t-test page - but notice that this only includes a handful of metrics that are specifically related to t-tests (mean, standard deviation etc). We need the full list on the ‘Descriptives’ tab to investigate distributions in detail.\n\n\n\n\n\n\n\n\n\n Data Skills - what does Levene’s test indicate and does this matter for us?\n\n\n\n\n\nLevene’s test explores homogeneity of variance, the result is not significant indicating that we are safe to assume homogeneity of variance between our groups.\nHowever, the data are not normally distributed - non-parametric tests don’t have an assumption of homogeneity of variance at all so, for this analysis, Levene’s test isn’t really relevant.\n\n\n\n\n\n\n\n\n\n Data Skills - what does the Mann-Whitney U test indicate about our hypothesis?\n\n\n\n\n\nThe Mann-Whitney U test is not significant - it seems like tetris scores are similar for participants who played in the morning and those who played in the afternoon."
  },
  {
    "objectID": "tutorials/rmb-week-7_non-parametrics.html#dataset-description",
    "href": "tutorials/rmb-week-7_non-parametrics.html#dataset-description",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Let’s start by getting familiar with the dataset. Load the file into Jamovi and take a look at the ‘Variables’ tab.\n\nThe creators of this dataset have helpfully included annotations for many of the variables they’ve measured. This is extremely useful for us and definitely worth a few moments for you to do the same when working with your own data.\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to read through the variable descriptions. You’ll need to refer back to this list throughout the session to find relevant variables.\n\n\nNext - take a look at the data sheet and the Condition variable. These relate to the different conditions in the experiment - all participants watched the film containing traumatic events and the four conditions describe what participants did after watching the film on day 1 of the experiment.\n\n\n\n\n\n\n\nCondition\nDescription\n\n\n\n\nNo-Task Control\nParticipants sit quietly\n\n\nReactivation+Tetris\nParticipants rewatch the film and then play Tetris\n\n\nTetris Only\nParticipants play Tetris\n\n\nReactivation Only\nParticipants rewatch the film\n\n\n\nParticipants were asked to keep an diary of any intrusive thoughts that they had prior to the experiment (day zero) and during the week after the experiment (days one to seven).\n\n\n\n\n\n\n Data Skills - which variable tracks the diary of intrusive memories?\n\n\n\n\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary contains the number of intrusive memories the participant recorded in the week after watching the film.\nDay_Zero_Image_Based_Intrusions_in_Intrusion_Diary is very similar - it contains the number of intrusive memories the participant recorded in the time prior to watching the film. This is a good measure to include so we can rule out differences in the baseline number of intrusive memories between our groups.\n\n\n\n\n\n\n\n\n\n Data Skills - what information do we have about participant’s performance when playing tetris?\n\n\n\n\n\nTetris_Total_Score contains the cumulative score of all tetris games the participant played - participants were allowed to restart their game if they finished before the time was up.\nWe also have Self_Rated_Tetris_Performance which measures how participants rated the difficulty of the tetris task and Tetris_Demand_Rating which measures how participants thought that playing tetris would affect their intrusive memories."
  },
  {
    "objectID": "tutorials/rmb-week-7_non-parametrics.html#is-tetris-effective-at-reducing-intrusive-memories",
    "href": "tutorials/rmb-week-7_non-parametrics.html#is-tetris-effective-at-reducing-intrusive-memories",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "On to our main hypothesis. (James et al. 2015) were most interested in the combination of the reactivation task and the tetris intervention and their effect on intrusive memories about the film. Let’s focus on only the two conditions we can compare with a t-test.\n\n\n\n\n\n\nKey step\n\n\n\nSet up a Jamovi data filter to focus our analysis on only the two conditions “No-Task Control” and “Reactivation+Tetris” conditions. Take a look at week 3 section 4 to revise data filters if you need to.\nYou can use this code to select both conditions together:\nCondition == \"No-Task Control\" or Condition == \"Reactivation+Tetris\"\nFinally, compute an independent samples t-test to test the following:\n\nPerforming a Reactivation and Tetris task changes the number of intrusive memories in the week after watching a traumatic film.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThere are  participants in each group.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p &lt; \nLevene’s test for homogeneity of variance indicated that we cancan not assume homogeneity of variance between the No-Task and Reactivation+Tetris groups, F(1, 32) = , p = .\nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in the week after the film between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = ."
  },
  {
    "objectID": "tutorials/rmb-week-7_non-parametrics.html#control-analyses-testing-for-potential-confounds.",
    "href": "tutorials/rmb-week-7_non-parametrics.html#control-analyses-testing-for-potential-confounds.",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Our hypothesis test gives statistical support to the idea that playing tetris reduces intrusive memories - but could there be other explanations? This experiment is likely not perfect and it is possible that some bad luck in the data collection could have impacted the results. In this section we’ll validate a few additional factors in the experimenta..\n\n\n(James et al. 2015) collected data variable to act as a control condition and rule out one possible confound in the experiment. Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary indicates the number of intrusive memories each participant had in the time before the experiment.\nThere isn’t a huge number of participants in each condition, it is possible that one group of participants was having more or less intrusive memories to begin with - irrespective of the experimental procedure.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a t-test to test the following:\n\nThere are no differences in the number of instrusive memories between the two experimental group BEFORE the experiment started.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThe participant in the ‘No-Task Control’ group had mean of  intrusive memories prior to the experiment starting.\nThe participant in the ‘Reactivation+Tetris’ group had mean of  intrusive memories prior to the experiment starting.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p = \nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in day before between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .\n\n\n\n\n\n\n\n\n\n\n\nAnother important factor to check was that the film actually changed how the participants felt. If the film didn’t do anything to impact the participants emotions then it is possible that we don’t actually understand the source of the difference we saw in the main hypothesis.\nThe film is the main experimental manipulation that so it is critical to check that it worked as we expected.\n\n\n\n\n\n\nKey step\n\n\n\nCompute paired samples t-test’s to check the following:\n\nWatching the traumatic film will change participants emotional state.\n\nCheck ALL of participants reported sadness, hopelessness, fear, horror and anxiousness.\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nTrue or False? The data variables for these factors are Normally Distributed according to Shaprio-Wilk statistics.\n\nSadness TRUEFALSE\nHopeless TRUEFALSE\nFear TRUEFALSE\nHorror TRUEFALSE\nAxious TRUEFALSE\n\nHow did the following factors change or not after watching the film?\n\nSadness increasedstayed the samedecreased after watching the film\nHopelessless increasedstayed the samedecreased after watching the film\nFear increasedstayed the samedecreased after watching the film\nHorror increasedstayed the samedecreased after watching the film\nAxiousness increasedstayed the samedecreased after watching the film\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - did the film manipulation work?\n\n\n\n\n\nYes, all factors apart from Hopelessless were increased by watching the film, indicating that the film had a strongly negative impact on participants in the experiment."
  },
  {
    "objectID": "tutorials/rmb-week-7_non-parametrics.html#control-analyses-testing-for-potential-confounds.-1",
    "href": "tutorials/rmb-week-7_non-parametrics.html#control-analyses-testing-for-potential-confounds.-1",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Explore descriptives - what do you find? Strong postive skew and some kutosis in both, QQ plot confirms. Day one more prominent.\nCompute independent samples t-test with effect size and normality checks.\nHow would you report the Mann-Whitney U test."
  },
  {
    "objectID": "tutorials/rmb-week-7_non-parametrics.html#summary",
    "href": "tutorials/rmb-week-7_non-parametrics.html#summary",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "We’ve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code."
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html",
    "href": "tutorials/rmb_week-7_non-parametrics.html",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "This week we will explore how to check the assumptions of hypothesis tests and make sure that we’re using the right tests. We will introduce the idea of transforming data to squeeze the data into the shape of a normal distribution.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nAssumptions of parametric tests\n\n\n\nLog transforms and Box-Cox transforms\n\n\n\nHistograms\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nCompute data transformations using Jamovi\n\n\n\nRun non-parametric alternative tests in Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nReplicate part of a published analysis\n\n\n\n\n\n\n\n\n\nContent Info\n\n\n\nThis tutorial refers to a video in which participants watched unpleasant events to simulate a traumatic experience in the lab. The content of this video is not disclosed or dicussed in any more detail than this during this tutorial - those who are interested can find more information in methods section the published article. This tutorial does explore the emotional impact that the video has on participants.\n\n\n\n\nFlashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we’re going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind’s eye and cause distress. We investigated whether reconsolidation—the process during which memories become malleable when recalled—can be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group.\n\n\n\nThis week - we will explore two of the four conditions to test the following hypothesis:\n\nPlaying tetris will after a trumatic event will reduce number of subsequent intrusive memories\n\nAlong the way, we’ll take a detailed tour of our options when working with non-normally distributed data.\nThe theme this week is PRECISION! we need high precision when interpreting and analysing results, the exercises this week are designed to give you practice in paying close attention to the details of your data…\n\n\n\nLet’s start by getting familiar with the dataset. Load the file into Jamovi and take a look at the ‘Variables’ tab.\n\nThe creators of this dataset have helpfully included annotations for many of the variables they’ve measured. This is extremely useful for us and definitely worth a few moments for you to do the same when working with your own data.\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to read through the variable descriptions. You’ll need to refer back to this list throughout the session to find relevant variables.\n\n\nNext - take a look at the data sheet and the Condition variable. These relate to the different conditions in the experiment - all participants watched the film containing traumatic events and the four conditions describe what participants did after watching the film on day 1 of the experiment.\n\n\n\n\n\n\n\nCondition\nDescription\n\n\n\n\nNo-Task Control\nParticipants sit quietly\n\n\nReactivation+Tetris\nParticipants rewatch the film and then play Tetris\n\n\nTetris Only\nParticipants play Tetris\n\n\nReactivation Only\nParticipants rewatch the film\n\n\n\nParticipants were asked to keep an diary of any intrusive thoughts that they had prior to the experiment (day zero) and during the week after the experiment (days one to seven).\n\n\n\n\n\n\n Data Skills - which variable tracks the diary of intrusive memories?\n\n\n\n\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary contains the number of intrusive memories the participant recorded in the week after watching the film.\nDay_Zero_Image_Based_Intrusions_in_Intrusion_Diary is very similar - it contains the number of intrusive memories the participant recorded in the time prior to watching the film. This is a good measure to include so we can rule out differences in the baseline number of intrusive memories between our groups.\n\n\n\n\n\n\n\n\n\n Data Skills - what information do we have about participant’s performance when playing tetris?\n\n\n\n\n\nTetris_Total_Score contains the cumulative score of all tetris games the participant played - participants were allowed to restart their game if they finished before the time was up.\nWe also have Self_Rated_Tetris_Performance which measures how participants rated the difficulty of the tetris task and Tetris_Demand_Rating which measures how participants thought that playing tetris would affect their intrusive memories.\n\n\n\n\n\n\nLet’s start by taking a look at how participants in the conditions that played Tetris scored during their game time. Compute descriptive statistics on the Tetris_Total_Score variable and take a moment to get a sense of what it is telling you.\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the descriptive statistics of Tetris_Total_Score to answer the following questions in the spaces provided.\nThe average Tetris score was .\nThe most successful Tetris player scored  points.\nThe Tetris scores are positively skewednot skewednegatively skewed with a skewness of .\nThe Shapiro-Wilk statistic has a value of W =  and the p-value indicates that the scores are significantly non-normally distributednormally distributed\nWe should use parametricnon-parametric statistical approaches when analysing Tetris_Total_Score.\n\n\n\n\n\n\n\n\nLet’s use this data to test a hypothesis.\n\nPerformance whilst playing Tetris will be different at different times of day.\n\nWe can test this with an independent samples t-test as we’ve done in previous weeks. Set up the t-test and include the following:\n\nWelchs’ t-test\nMann-Whitney U\nEffect Size\nHomogeneity test\nNormality test\n\n\nLet’s unpack the results.\n\n\n\n\n\n\n Data Skills - does the Normality assumption check confirm what we saw in the descriptives?\n\n\n\n\n\nYes, we get the same picture but with a little less detail. Shapiro-Wilk’s test is significant indicating a non-normal distribution, but we still need the descriptive stats to understand HOW the distribution is different from a normal distribution.\nImportantly, this confirms that we should report the non-parametric alternative test - the Mann-Whitney U.\nYou could add the descriptives to the t-test page - but notice that this only includes a handful of metrics that are specifically related to t-tests (mean, standard deviation etc). We need the full list on the ‘Descriptives’ tab to investigate distributions in detail.\n\n\n\n\n\n\n\n\n\n Data Skills - what does Levene’s test indicate and does this matter for us?\n\n\n\n\n\nLevene’s test explores homogeneity of variance, the result is not significant indicating that we are safe to assume homogeneity of variance between our groups.\nHowever, the data are not normally distributed - non-parametric tests don’t have an assumption of homogeneity of variance at all so, for this analysis, Levene’s test isn’t really relevant.\n\n\n\n\n\n\n\n\n\n Data Skills - what does the Mann-Whitney U test indicate about our hypothesis?\n\n\n\n\n\nThe Mann-Whitney U test is not significant - it seems like tetris scores are similar for participants who played in the morning and those who played in the afternoon.\n\n\n\n\n\n\nOn to our main hypothesis. (James et al. 2015) were most interested in the combination of the reactivation task and the tetris intervention and their effect on intrusive memories about the film. Let’s focus on only the two conditions we can compare with a t-test.\n\n\n\n\n\n\nKey step\n\n\n\nSet up a Jamovi data filter to focus our analysis on only the two conditions “No-Task Control” and “Reactivation+Tetris” conditions. Take a look at week 3 section 4 to revise data filters if you need to.\nYou can use this code to select both conditions together:\nCondition == \"No-Task Control\" or Condition == \"Reactivation+Tetris\"\nFinally, compute an independent samples t-test to test the following:\n\nPerforming a Reactivation and Tetris task changes the number of intrusive memories in the week after watching a traumatic film.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThere are  participants in each group.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p &lt; \nLevene’s test for homogeneity of variance indicated that we cancan not assume homogeneity of variance between the No-Task and Reactivation+Tetris groups, F(1, 32) = , p = .\nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in the week after the film between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .\n\n\n\n\n\n\n\n\n\n\n\nOur hypothesis test gives statistical support to the idea that playing tetris reduces intrusive memories - but could there be other explanations? This experiment is likely not perfect and it is possible that some bad luck in the data collection could have impacted the results. In this section we’ll validate a few additional factors in the experimenta..\n\n\n(James et al. 2015) collected data variable to act as a control condition and rule out one possible confound in the experiment. Days_Zero_Image_Based_Intrusions_in_Intrusion_Diary indicates the number of intrusive memories each participant had in the time before the experiment.\nThere isn’t a huge number of participants in each condition, it is possible that one group of participants was having more or less intrusive memories to begin with - irrespective of the experimental procedure.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a t-test to test the following:\n\nThere are no differences in the number of instrusive memories between the two experimental group BEFORE the experiment started.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThe participant in the ‘No-Task Control’ group had mean of  intrusive memories prior to the experiment starting.\nThe participant in the ‘Reactivation+Tetris’ group had mean of  intrusive memories prior to the experiment starting.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p = \nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in day before between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .\n\n\n\n\n\n\n\n\n\n\n\nAnother important factor to check was that the film actually changed how the participants felt. If the film didn’t do anything to impact the participants emotions then it is possible that we don’t actually understand the source of the difference we saw in the main hypothesis.\nThe film is the main experimental manipulation that so it is critical to check that it worked as we expected.\n\n\n\n\n\n\nKey step\n\n\n\nCompute paired samples t-test’s to check the following:\n\nWatching the traumatic film will change participants emotional state.\n\nCheck ALL of participants reported sadness, hopelessness, fear, horror and anxiousness.\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nTrue or False? The data variables for these factors are Normally Distributed according to Shaprio-Wilk statistics.\n\nSadness TRUEFALSE\nHopeless TRUEFALSE\nFear TRUEFALSE\nHorror TRUEFALSE\nAxious TRUEFALSE\n\nHow did the following factors change or not after watching the film?\n\nSadness increasedstayed the samedecreased after watching the film\nHopelessless increasedstayed the samedecreased after watching the film\nFear increasedstayed the samedecreased after watching the film\nHorror increasedstayed the samedecreased after watching the film\nAxiousness increasedstayed the samedecreased after watching the film\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - did the film manipulation work?\n\n\n\n\n\nYes, all factors apart from Hopelessless showed a statistically significant increase after watching the film. This indicates that the film had a strongly impact on participants negative emotions. It seems like the experimental manipulation worked.\n\n\n\n\n\n\n\nThere are times when we might not want to use a non-parametric test, for example if we specifically want to interpret the mean of the data rather than the median or if we’re running many similar tests and we don’t want to be switching back and forth between parametric and non-parametric tests.\nWe have another option when working with non-normal data. This is to apply a reversible transformation to the data observations in a variable to change the distribution to one that is closer to being normally distributed. When this process works, we are able to run the parametric test on the transformed data rather thans swapping to the non-parametric alternative.\nThis is valid when the transformation preserves the relationships in the dataset, that is it maintains the relative order of the observations. If data point A is larger than B before transformation then it should still be larger after the transformation. It is fine if the transformation changes the numbers themselves - we just have to be careful when making an interpretation.\nWe’ll use the LOG data transformation in this section\n\n\n\n\n\n\n\nTransform\nExplanation\n\n\n\n\nLog Transform\nThe logarithm function, typically base 10 (log), is applied to each data point.\n\n\n\nOne downside of the log transform is that it has a weird value when the datapoint being transformed is zero or negative. log(0) equals negative infinity! this will break a lot of analyses that happen afterwards but fortunately there is a straightforward solution.\nWhen your data contains zeros or negatives you can simply add an offset value to make your data positive and then apply the log transform. Again, this is valid as long as we are preserving the relationships within our data by adding exactly the same offset to every single point in the dataset.\nWe can easily transform variables using Jamovi and have done something very similar when computing variables in previous weeks.\n\n\n\n\n\n\nData Skills - transforming a variable.\n\n\n\nYou can read about transforming variables in detail on the Jamovi documentation page on transforming variables. There are a few different steps but they are all straightforward.\nFirst, head to the ‘Data’ view and right click on the variable you want to transform, in this case we want Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary\n\nThis will open a dialogue box where you can define the new variable. Give your new variable a descriptive name and make sure that the ‘Source variable’ is the one that you expect. Note that you might see a ‘Circular definition’ warning until you rename your variable.\n\nNext we need to specify the transformation that we want to use. Click into the ‘using transform’ drop-down box and select “Create New Transform”.\nHere you can define the transformation that you want to apply. You can define a single transform and apply it to multiple variables. Give the transformation a descriptive name and specify that you want the transfrom to apply the LOG10() function to the $source variable. ($source is a way to tell Jamovi to use ‘whichever variable is passed into the transform’ rather than defining the transform on only 1 variable here.).\nWe also include a small offset value of +1 here. This is as some participants had no intrusive memories at all and we want to avoid issues with computing log(0) in our analysis.\n\nYou can click on the Fx logo to select functions from a list if you prefer, there are lots of pre-defined options.\n\nOnce you’re happy with the transformation, you can close the transformation definition dialogue and specify that we want to use the ‘LogTransform’ that we just defined to transfrom Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary to make our new variable.\n\nThere are a lot of steps here - but each is fairly straightforward. Ask your tutor if you get stuck.\n\n\nNow that we have our new variable we can explore what it has done. Let’s compare the descriptive statistics for the original and the transformed variable.\n\n \n\n\n\n\n\n\n Data Skills - has the log transform changed mean of the data?\n\n\n\n\n\nYes, the log transform has changed the mean by a long way. The original data shows that the average participant had 3.5 intrusive memories but that number has changed to 0.54 in the transformed data.\nThis does not mean that the number has really reduced! remember that the transformation is changing the absolute values of the observations but should be preserving the relative relationships between the data points.\nThis is fine, but we must not report the log-transformed values as the actual means. The participants still had an average of 3.5 intrusive memories whether we transformed the data or not.\n\n\n\n\n\n\n\n\n\n Data Skills - has the log transform changed the skewness and kurtosis of the data?\n\n\n\n\n\nYes, it has had a big impact. Both the skewness and kurtosis values are much closer to zero in the transformed data compared to the original. Both indicates that the transformed data are more normally distributed.\n\n\n\n\n\n\n\n\n\n Data Skills - is there a statistically significant difference from a normal distribution in the transformed data?\n\n\n\n\n\nNo, whereas the Shapiro-Wilk statistic showed a significant departure from a normal distribution for the original data (W = 0.8, p &lt; 0.001) it shows that the transformed data is now indistinguishable from a normal distribution (W = 0.953, p = 0.130)\n\n\n\nFinally - let’s see if we can use our transformed variables to compute a parametric t-test. Repeat the independent samples t-test from earlier but include all three variables in the ‘Dependent Variables’ box. Make sure to include\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary - original data\nDays_One_to_Seven_Transformed - Log transformed data\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nWith the original data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . We can proceed with Student’s t-testWelch’s t-testMann-Whitney U test.\nUsing the log-transformed data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . Levene’s test found that the we cancan not assume that the groups have homogeneity of variance, F(1, 34) = , p = . We can proceed with Student’s t-testWelch’s t-testMann-Whitney U test.\n\n\n\nNote that Welch’s t-test is valid whether or not we can assume homogeneity of variance…\n\n\n\n\n\n\n\n\nWe have explored an interesting dataset in some detail and worked with serveral different analysis approaches. Specifically, we have several options when working on checking the assumptions of our analyses. We have to be careful when computing and reporting hypothesis tests that we’re using the correct tests in a way that is supported by the data. Sometimes we can use a transform to continuous using parametric tests when the assumptions are not supported but this will not always work.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#the-dataset",
    "href": "tutorials/rmb_week-7_non-parametrics.html#the-dataset",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Flashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we’re going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind’s eye and cause distress. We investigated whether reconsolidation—the process during which memories become malleable when recalled—can be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#the-challenge",
    "href": "tutorials/rmb_week-7_non-parametrics.html#the-challenge",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "This week - we will explore two of the four conditions to test the following hypothesis:\n\nPlaying tetris will after a trumatic event will reduce number of subsequent intrusive memories\n\nAlong the way, we’ll take a detailed tour of our options when working with non-normally distributed data.\nThe theme this week is PRECISION! we need high precision when interpreting and analysing results, the exercises this week are designed to give you practice in paying close attention to the details of your data…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#dataset-description",
    "href": "tutorials/rmb_week-7_non-parametrics.html#dataset-description",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Let’s start by getting familiar with the dataset. Load the file into Jamovi and take a look at the ‘Variables’ tab.\n\nThe creators of this dataset have helpfully included annotations for many of the variables they’ve measured. This is extremely useful for us and definitely worth a few moments for you to do the same when working with your own data.\n\n\n\n\n\n\nKey step\n\n\n\nTake a moment to read through the variable descriptions. You’ll need to refer back to this list throughout the session to find relevant variables.\n\n\nNext - take a look at the data sheet and the Condition variable. These relate to the different conditions in the experiment - all participants watched the film containing traumatic events and the four conditions describe what participants did after watching the film on day 1 of the experiment.\n\n\n\n\n\n\n\nCondition\nDescription\n\n\n\n\nNo-Task Control\nParticipants sit quietly\n\n\nReactivation+Tetris\nParticipants rewatch the film and then play Tetris\n\n\nTetris Only\nParticipants play Tetris\n\n\nReactivation Only\nParticipants rewatch the film\n\n\n\nParticipants were asked to keep an diary of any intrusive thoughts that they had prior to the experiment (day zero) and during the week after the experiment (days one to seven).\n\n\n\n\n\n\n Data Skills - which variable tracks the diary of intrusive memories?\n\n\n\n\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary contains the number of intrusive memories the participant recorded in the week after watching the film.\nDay_Zero_Image_Based_Intrusions_in_Intrusion_Diary is very similar - it contains the number of intrusive memories the participant recorded in the time prior to watching the film. This is a good measure to include so we can rule out differences in the baseline number of intrusive memories between our groups.\n\n\n\n\n\n\n\n\n\n Data Skills - what information do we have about participant’s performance when playing tetris?\n\n\n\n\n\nTetris_Total_Score contains the cumulative score of all tetris games the participant played - participants were allowed to restart their game if they finished before the time was up.\nWe also have Self_Rated_Tetris_Performance which measures how participants rated the difficulty of the tetris task and Tetris_Demand_Rating which measures how participants thought that playing tetris would affect their intrusive memories.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#is-participant-performance-on-tetris-normally-distributed",
    "href": "tutorials/rmb_week-7_non-parametrics.html#is-participant-performance-on-tetris-normally-distributed",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Let’s start by taking a look at how participants in the conditions that played Tetris scored during their game time. Compute descriptive statistics on the Tetris_Total_Score variable and take a moment to get a sense of what it is telling you.\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse the descriptive statistics of Tetris_Total_Score to answer the following questions in the spaces provided.\nThe average Tetris score was .\nThe most successful Tetris player scored  points.\nThe Tetris scores are positively skewednot skewednegatively skewed with a skewness of .\nThe Shapiro-Wilk statistic has a value of W =  and the p-value indicates that the scores are significantly non-normally distributednormally distributed\nWe should use parametricnon-parametric statistical approaches when analysing Tetris_Total_Score.\n\n\n\n\n\n\n\n\nLet’s use this data to test a hypothesis.\n\nPerformance whilst playing Tetris will be different at different times of day.\n\nWe can test this with an independent samples t-test as we’ve done in previous weeks. Set up the t-test and include the following:\n\nWelchs’ t-test\nMann-Whitney U\nEffect Size\nHomogeneity test\nNormality test\n\n\nLet’s unpack the results.\n\n\n\n\n\n\n Data Skills - does the Normality assumption check confirm what we saw in the descriptives?\n\n\n\n\n\nYes, we get the same picture but with a little less detail. Shapiro-Wilk’s test is significant indicating a non-normal distribution, but we still need the descriptive stats to understand HOW the distribution is different from a normal distribution.\nImportantly, this confirms that we should report the non-parametric alternative test - the Mann-Whitney U.\nYou could add the descriptives to the t-test page - but notice that this only includes a handful of metrics that are specifically related to t-tests (mean, standard deviation etc). We need the full list on the ‘Descriptives’ tab to investigate distributions in detail.\n\n\n\n\n\n\n\n\n\n Data Skills - what does Levene’s test indicate and does this matter for us?\n\n\n\n\n\nLevene’s test explores homogeneity of variance, the result is not significant indicating that we are safe to assume homogeneity of variance between our groups.\nHowever, the data are not normally distributed - non-parametric tests don’t have an assumption of homogeneity of variance at all so, for this analysis, Levene’s test isn’t really relevant.\n\n\n\n\n\n\n\n\n\n Data Skills - what does the Mann-Whitney U test indicate about our hypothesis?\n\n\n\n\n\nThe Mann-Whitney U test is not significant - it seems like tetris scores are similar for participants who played in the morning and those who played in the afternoon.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#is-tetris-effective-at-reducing-intrusive-memories",
    "href": "tutorials/rmb_week-7_non-parametrics.html#is-tetris-effective-at-reducing-intrusive-memories",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "On to our main hypothesis. (James et al. 2015) were most interested in the combination of the reactivation task and the tetris intervention and their effect on intrusive memories about the film. Let’s focus on only the two conditions we can compare with a t-test.\n\n\n\n\n\n\nKey step\n\n\n\nSet up a Jamovi data filter to focus our analysis on only the two conditions “No-Task Control” and “Reactivation+Tetris” conditions. Take a look at week 3 section 4 to revise data filters if you need to.\nYou can use this code to select both conditions together:\nCondition == \"No-Task Control\" or Condition == \"Reactivation+Tetris\"\nFinally, compute an independent samples t-test to test the following:\n\nPerforming a Reactivation and Tetris task changes the number of intrusive memories in the week after watching a traumatic film.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThere are  participants in each group.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p &lt; \nLevene’s test for homogeneity of variance indicated that we cancan not assume homogeneity of variance between the No-Task and Reactivation+Tetris groups, F(1, 32) = , p = .\nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in the week after the film between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#control-analyses-testing-for-potential-confounds.",
    "href": "tutorials/rmb_week-7_non-parametrics.html#control-analyses-testing-for-potential-confounds.",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "Our hypothesis test gives statistical support to the idea that playing tetris reduces intrusive memories - but could there be other explanations? This experiment is likely not perfect and it is possible that some bad luck in the data collection could have impacted the results. In this section we’ll validate a few additional factors in the experimenta..\n\n\n(James et al. 2015) collected data variable to act as a control condition and rule out one possible confound in the experiment. Days_Zero_Image_Based_Intrusions_in_Intrusion_Diary indicates the number of intrusive memories each participant had in the time before the experiment.\nThere isn’t a huge number of participants in each condition, it is possible that one group of participants was having more or less intrusive memories to begin with - irrespective of the experimental procedure.\n\n\n\n\n\n\nKey step\n\n\n\nCompute a t-test to test the following:\n\nThere are no differences in the number of instrusive memories between the two experimental group BEFORE the experiment started.\n\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nThe participant in the ‘No-Task Control’ group had mean of  intrusive memories prior to the experiment starting.\nThe participant in the ‘Reactivation+Tetris’ group had mean of  intrusive memories prior to the experiment starting.\nThe Shapio-Wilk statistic indicated that the data are significantly non-normally distributednormally distributed, W = , p = \nA Mann-Whitney U test showed a significantno significant difference in intrusive memories in day before between the No Task (Median=) and the Reactivation plus Tetris (Median=) groups. U = , p = , Rank Biserial Correlation = .\n\n\n\n\n\n\n\n\n\n\n\nAnother important factor to check was that the film actually changed how the participants felt. If the film didn’t do anything to impact the participants emotions then it is possible that we don’t actually understand the source of the difference we saw in the main hypothesis.\nThe film is the main experimental manipulation that so it is critical to check that it worked as we expected.\n\n\n\n\n\n\nKey step\n\n\n\nCompute paired samples t-test’s to check the following:\n\nWatching the traumatic film will change participants emotional state.\n\nCheck ALL of participants reported sadness, hopelessness, fear, horror and anxiousness.\nTake a moment to understand the results and compute any additional statistics that you might need to help your interpretation and reporting.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nTrue or False? The data variables for these factors are Normally Distributed according to Shaprio-Wilk statistics.\n\nSadness TRUEFALSE\nHopeless TRUEFALSE\nFear TRUEFALSE\nHorror TRUEFALSE\nAxious TRUEFALSE\n\nHow did the following factors change or not after watching the film?\n\nSadness increasedstayed the samedecreased after watching the film\nHopelessless increasedstayed the samedecreased after watching the film\nFear increasedstayed the samedecreased after watching the film\nHorror increasedstayed the samedecreased after watching the film\nAxiousness increasedstayed the samedecreased after watching the film\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Skills - did the film manipulation work?\n\n\n\n\n\nYes, all factors apart from Hopelessless showed a statistically significant increase after watching the film. This indicates that the film had a strongly impact on participants negative emotions. It seems like the experimental manipulation worked.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#transforming-non-nomrally-distributed-data",
    "href": "tutorials/rmb_week-7_non-parametrics.html#transforming-non-nomrally-distributed-data",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "There are times when we might not want to use a non-parametric test, for example if we specifically want to interpret the mean of the data rather than the median or if we’re running many similar tests and we don’t want to be switching back and forth between parametric and non-parametric tests.\nWe have another option when working with non-normal data. This is to apply a reversible transformation to the data observations in a variable to change the distribution to one that is closer to being normally distributed. When this process works, we are able to run the parametric test on the transformed data rather thans swapping to the non-parametric alternative.\nThis is valid when the transformation preserves the relationships in the dataset, that is it maintains the relative order of the observations. If data point A is larger than B before transformation then it should still be larger after the transformation. It is fine if the transformation changes the numbers themselves - we just have to be careful when making an interpretation.\nWe’ll use the LOG data transformation in this section\n\n\n\n\n\n\n\nTransform\nExplanation\n\n\n\n\nLog Transform\nThe logarithm function, typically base 10 (log), is applied to each data point.\n\n\n\nOne downside of the log transform is that it has a weird value when the datapoint being transformed is zero or negative. log(0) equals negative infinity! this will break a lot of analyses that happen afterwards but fortunately there is a straightforward solution.\nWhen your data contains zeros or negatives you can simply add an offset value to make your data positive and then apply the log transform. Again, this is valid as long as we are preserving the relationships within our data by adding exactly the same offset to every single point in the dataset.\nWe can easily transform variables using Jamovi and have done something very similar when computing variables in previous weeks.\n\n\n\n\n\n\nData Skills - transforming a variable.\n\n\n\nYou can read about transforming variables in detail on the Jamovi documentation page on transforming variables. There are a few different steps but they are all straightforward.\nFirst, head to the ‘Data’ view and right click on the variable you want to transform, in this case we want Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary\n\nThis will open a dialogue box where you can define the new variable. Give your new variable a descriptive name and make sure that the ‘Source variable’ is the one that you expect. Note that you might see a ‘Circular definition’ warning until you rename your variable.\n\nNext we need to specify the transformation that we want to use. Click into the ‘using transform’ drop-down box and select “Create New Transform”.\nHere you can define the transformation that you want to apply. You can define a single transform and apply it to multiple variables. Give the transformation a descriptive name and specify that you want the transfrom to apply the LOG10() function to the $source variable. ($source is a way to tell Jamovi to use ‘whichever variable is passed into the transform’ rather than defining the transform on only 1 variable here.).\nWe also include a small offset value of +1 here. This is as some participants had no intrusive memories at all and we want to avoid issues with computing log(0) in our analysis.\n\nYou can click on the Fx logo to select functions from a list if you prefer, there are lots of pre-defined options.\n\nOnce you’re happy with the transformation, you can close the transformation definition dialogue and specify that we want to use the ‘LogTransform’ that we just defined to transfrom Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary to make our new variable.\n\nThere are a lot of steps here - but each is fairly straightforward. Ask your tutor if you get stuck.\n\n\nNow that we have our new variable we can explore what it has done. Let’s compare the descriptive statistics for the original and the transformed variable.\n\n \n\n\n\n\n\n\n Data Skills - has the log transform changed mean of the data?\n\n\n\n\n\nYes, the log transform has changed the mean by a long way. The original data shows that the average participant had 3.5 intrusive memories but that number has changed to 0.54 in the transformed data.\nThis does not mean that the number has really reduced! remember that the transformation is changing the absolute values of the observations but should be preserving the relative relationships between the data points.\nThis is fine, but we must not report the log-transformed values as the actual means. The participants still had an average of 3.5 intrusive memories whether we transformed the data or not.\n\n\n\n\n\n\n\n\n\n Data Skills - has the log transform changed the skewness and kurtosis of the data?\n\n\n\n\n\nYes, it has had a big impact. Both the skewness and kurtosis values are much closer to zero in the transformed data compared to the original. Both indicates that the transformed data are more normally distributed.\n\n\n\n\n\n\n\n\n\n Data Skills - is there a statistically significant difference from a normal distribution in the transformed data?\n\n\n\n\n\nNo, whereas the Shapiro-Wilk statistic showed a significant departure from a normal distribution for the original data (W = 0.8, p &lt; 0.001) it shows that the transformed data is now indistinguishable from a normal distribution (W = 0.953, p = 0.130)\n\n\n\nFinally - let’s see if we can use our transformed variables to compute a parametric t-test. Repeat the independent samples t-test from earlier but include all three variables in the ‘Dependent Variables’ box. Make sure to include\n\nDays_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary - original data\nDays_One_to_Seven_Transformed - Log transformed data\n\n\n\n\n\n\n\n\nCheck Your Understanding - Data Quiz\n\n\n\n\nQuestionsHint\n\n\nUse your results on the independant samples t-test to answer the following questions in the spaces provided.\nWith the original data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . We can proceed with Student’s t-testWelch’s t-testMann-Whitney U test.\nUsing the log-transformed data, a Shapiro-Wilk statistic found a a significantno significant difference from a normal distribution, W=, p = . Levene’s test found that the we cancan not assume that the groups have homogeneity of variance, F(1, 34) = , p = . We can proceed with Student’s t-testWelch’s t-testMann-Whitney U test.\n\n\n\nNote that Welch’s t-test is valid whether or not we can assume homogeneity of variance…",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb_week-7_non-parametrics.html#summary",
    "href": "tutorials/rmb_week-7_non-parametrics.html#summary",
    "title": "Week 7 : Testing data distributions and Non-parametric alternative tests",
    "section": "",
    "text": "We have explored an interesting dataset in some detail and worked with serveral different analysis approaches. Specifically, we have several options when working on checking the assumptions of our analyses. We have to be careful when computing and reporting hypothesis tests that we’re using the correct tests in a way that is supported by the data. Sometimes we can use a transform to continuous using parametric tests when the assumptions are not supported but this will not always work.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 7 - Non-parametric tests and data transformation"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html",
    "href": "tutorials/rmb-week-9_one-way-anova.html",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "This week we will explore how to run ANOVA analyses to compare means between two or more groups. This is an important analysis as many experimental and scientific contexts cannot be simiplified down to two groups for a t-test, and running many t-tests would increase our chances of false positive findings..\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nOne-Way ANOVA\n\n\n\nLog transforms\n\n\n\nAssumptions of ANOVA\n\n\n\nEstimated Marginal Means\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nRun and explore ANOVA analyses in Jamovi\n\n\n\nCompute data transformations using Jamovi\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWork with a published dataset\n\n\n\n\n\n\n\n\n\nContent Info\n\n\n\nThis is the same dataset that we used in week 7.\n\n\n\n\n\n\n\n\nContent Info\n\n\n\nThis tutorial refers to a video in which participants watched unpleasant events to simulate a traumatic experience in the lab. The content of this video is not disclosed or dicussed in any more detail than this during this tutorial - those who are interested can find more information in methods section the published article. This tutorial does explore the emotional impact that the video has on participants.\n\n\n\n\nFlashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we’re going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind’s eye and cause distress. We investigated whether reconsolidation—the process during which memories become malleable when recalled—can be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group.\n\n\n\nThis week - we will explore two of the four conditions to test the following hypothesis:\n\nPlaying tetris will after a trumatic event will reduce number of subsequent intrusive memories\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun yourself with Control variable - do you see a difference?"
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#the-dataset",
    "href": "tutorials/rmb-week-9_one-way-anova.html#the-dataset",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Flashbacks and intrusive thoughts are a core features of post-traumatic stress disorder (PTSD). These can be highly distressing and disruptive for the person suffering from them and there are few early interventions that are known to help.\nOne unexpected and intriguing proposal in the last few decades suggests that carrying out very engaging visuo-spatial tasks with a neutral emotion in the immediate aftermath of a traumatic event may help reduce later flashbacks (Holmes et al. 2009). This is thought to be due to intense visuo-spatial processing disrupting the formations of the flashback memories, which are themselves flashbacks.\nThis week, we’re going to look at the dataset from one experiment attempting a replication and extension of this effect (James et al. 2015). The abstract of the paper is below\n\nMemory of a traumatic event becomes consolidated within hours. Intrusive memories can then flash back repeatedly into the mind’s eye and cause distress. We investigated whether reconsolidation—the process during which memories become malleable when recalled—can be blocked using a cognitive task and whether such an approach can reduce these unbidden intrusions. We predicted that reconsolidation of a reactivated visual memory of experimental trauma could be disrupted by engaging in a visuospatial task that would compete for visual working memory resources. We showed that intrusive memories were virtually abolished by playing the computer game Tetris following a memory-reactivation task 24 hr after initial exposure to experimental trauma. Furthermore, both memory reactivation and playing Tetris were required to reduce subsequent intrusions (Experiment 2), consistent with reconsolidation-update mechanisms. A simple, noninvasive cognitive-task procedure administered after emotional memory has already consolidated (i.e., &gt; 24 hours after exposure to experimental trauma) may prevent the recurrence of intrusive memories of those emotional events\n\nThis experiment shows participants a 12-minute distressing film to simulate a traumatic event. Participants completed three sessions in the experiment day 0, day 1 and day 7. They also kept a diary recording their thoughts and memories about the film throughout the week. Participants viewed the film on day 0 and completed a range of experimental tasks on days 1 and 7.\nCrucially, participants were allocated to one of four conditions in the time immediately following the video. One group played tetris as a distraction, a second group completed a reactivation task which was throught to help reduce the emotional response to the video, a third group did both tetris and reconsolidation and a fourth group was a no-task control group."
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#the-challenge",
    "href": "tutorials/rmb-week-9_one-way-anova.html#the-challenge",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "This week - we will explore two of the four conditions to test the following hypothesis:\n\nPlaying tetris will after a trumatic event will reduce number of subsequent intrusive memories"
  },
  {
    "objectID": "tutorials/rmb-week-8_tidy-data.html",
    "href": "tutorials/rmb-week-8_tidy-data.html",
    "title": "Week 8 : Tidy data",
    "section": "",
    "text": "Week 8 : Tidy data\nDatasets we want to work with are often difficult to work with. It is common to find interesting and useful datasets that have strange column names, missing data, mistakes in data entry or weird formatting. We can and still should use these datasets but we have to TIDY them up first.\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nTidy data principles\n\n\n\nLog transforms and Box-Cox transforms\n\n\n\nHistograms\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nData formatting using dplyr functions.\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nWorking with open access data"
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions.html",
    "href": "tutorials/rmb-week-8_sampling-distributions.html",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "The sampling distribution is central to how we can move from t-values to p-values - yet it is one of the trickier parts of this course. The lectures and pre-lecture materials frequently deal with computer simulations to show the properties of the tests we’re using, so this week you will write your own simulation to help understand the sampling distribution!\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nSampling Distributions\n\n\n\nt-values\n\n\n\np-values\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nComputing a one sample t-test in R\n\n\n\nSimulating data in R\n\n\n\nWriting loops in R\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nCreating shareable code to demonstrate a statistical concept\n\n\n\n\n\nTLDR: There is no data to download! we’re going to make the data ourselves.\nThe dataset this week is very simple - we’re going to create it ourselves using R. At this point, we know a lot about the types of questions that we can ask with t-tests and we know a lot about the data processing around it. We can check our understanding of different aspects of these statistical tests by simulating data samples that have known properties (eg mean and standard deviation) and checking if the outcome of the t-test is what we would expect.\nThis way we can numerically validate the tests we’re running and take a look under the hood and see what sort of processes Jamovi and R are using to compute the tests for us.\n\n\n\n\n\n\nContent Info\n\n\n\nThere is a lot of code in this session - don’t be afraid to ask for help from your tutors!\n\n\n\n\n\nToday, we’ll first introduce some concepts and functions in R to simulate a data sample and compute our own one sample t-test. With that in hand, we will set up code to run thousands of tests in one go to help us to better understand sampling distributions.\n\n\n\nThe rnorm() function in R is used to generate random numbers from a normal (Gaussian) distribution. A call to rnorm() looks like this:\nrnorm(n, mean = 0, sd = 1)\nWe can see that rnorm() takes three values as inputs\n\n\n\n\n\n\n\ninput\nDescription\n\n\n\n\nn\nThe number of random numbers you want to generate, we always need to specify this.\n\n\nmean\nThe mean (average) of the normal distribution. The default value is 0 if it isn’t specified\n\n\nsd\nThe standard deviation of the normal distribution. The default value is 1 if it isn’t specified\n\n\n\nSo we could tweak these input arguments to simulate any number of data points from any normal distribution that we wish.\n\n\n\n\n\n\nCheck Your Understanding - R code\n\n\n\nWhat code would we need to write to simulate the following?\n20 data points from a distribution with mean of 1 and standard deviation of 2\n\nR code here -&gt; .\n\n33 data points from a distribution with mean of -3 and standard deviation of 1.23\n\nR code here -&gt; .\n\nfive thousand data points from a distribution with mean of 1.789 and standard deviation of 18.2\n\nR code here -&gt; .\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nOpen Jamovi, install the Rj add-on and open a new Rj window before we go any further\n\n\nWe can use rnorm() within the Rj code window as usual. Here I’m simulating 20 data points from a distribution with a mean of 1 and a standard deviation of 2. Try running the same code in your window.\n\nThese 20 simulated numbers could now represent a data sample for a variable in an analysis. The difference is we have absolute confidence about the ‘truth’ of the population statistics underlying this data sample.\nYou may notice that you do get 20 numbers, but that you get a different 20 numbers than I did! This is as rnorm() works hard to generate random numbers that are different each time. You can verify this by running your code multiple times, even though the code stays the same you will get a different data sample each time. Remember that the code and the underlying distribution remain the always the same but the data sample is changing.\nThis is actually a good thing for us. Real experiments and data collection always have an element of random luck and we want our simulations to reflect this as well.\n\n\n\nIn week 2 we discussed how sampling relatively small amount of data from a wider population can impact our estimates of values like the mean.\nTo recap, let’s say we’re interested in computing the average attainment for secondary school children on a particular vocabulary test. Our target population might be ‘all secondary school students in the UK’ but is it practically impossible to get data from every single student currently in secondard school. We have to make do with smaller data samples of a few tens or few hundreds of students (or even more, if we’re lucky).\nOur simulations do exactly this. The parameters we pass into rnorm() define ‘population level’ characteristics in the form of a normal distribution and each time we run the function we draw a ‘sample’ of a fixed number of data points from that population. We can think of this as rerunning our data sampling every time the function is executed. This would be massively complicated, time consuming and expensive in real life, but with a few assumptions a computer can simulate it in a fraction of a second.\nWe can now start to explore how sampling variability - AKA the random differences between data samples from the same population - can impact our estimates of the mean and standard deviation.\nUpdate your code to store the data sample into a variable and compute the mean with mean() and standard deviation with sd(). The result might look like this.\nsample = rnorm(20, mean=1, sd=2)\n\n# Compute the mean\nprint('Mean:')\nprint(mean(sample))\n\n# Compute the standard deviation\nprint('Standard Deviation:')\nprint(sd(sample))\n\nRunning this code will show the mean and standard deviation of each individual data sample, try running this a few times. Notice that the values are sometimes close to our ‘population’ estimates of mean = 1 and standard deviation = 2 but also sometimes quite far away.\nTry tweaking your code to answer these questions\n\n\n\n\n\n\n Data Skills - is the estimate of the mean better or worse with a sample of 200?\n\n\n\n\n\nUpdate your code to use this data simulation\nsample = rnorm(200, mean=1, sd=2)\nRunning this a few times should show you mean values much closer to 1 and standard deviations much closer to 2 than we saw with a sample of 20 values. They still won’t be perfect though.\n\n\n\n\n\n\n\n\n\n Data Skills - is the estimate of the mean better or worse when the population standard deviation is 10?\n\n\n\n\n\nUpdate your code to use this data simulation\nsample = rnorm(200, mean=1, sd=10)\nRunning this a few times should make your estimates of the mean and standard deviation much more variable again. When variability in the population is very wide, large data samples won’t necessarily make up the difference.\n\n\n\nWe can get an informal sense of sampling variability by clicking to rerun this code a few times, but R code allows us to do much better, we can tweak the code to rerun this analysis hundreds or thousands of times to explicitly quantify the variability we see.\nTo do this, we’ll need a code element called a loop which looks something like this\nnum_simulation = 10\n\n# Loop to perform simulations\nfor (i in 1:num_simulations) {\n    print(i)\n}\nThere are two aspects to this code block. The first sets the number of simulated data samples we want to draw.\nnum_simulations = 10\nThis line sets the variable num_simulations to 10. This means you want to perform 10 iterations in the loop.\nThe second part is the loop itself:\nfor (i in 1:num_simulations) {\n    print(i)\n}\nThis is a for loop that will iterate from 1 to num_simulations (which is 10 in this case). Here’s what happens inside the loop:\n\nInitialization: The loop starts with i set to 1.\nCondition: The loop will continue to run as long as i is less than or equal to num_simulations.\nIncrement: After each iteration, i is incremented by 1.\n\nInside the loop, the print(i) statement is executed, which prints the current value of i to the console.\nThe output of this code will be the numbers from 1 to 10, each printed on a new line. The full output might look like this:\n\nPrinting the value of i is instructive but really we want our loop to include some interesting computations. Next, we should move our sampling code within the curly brackets {} of the loop.\nEverything within the those curly brackets will be repeated on every iteration of the loop. So if we includ the code to draw a sample and compute the mean, it will be repeated every time. Here is an example:\n# My loop\nnum_simulations = 10\n\nfor (i in 1:num_simulations) {\n    sample = rnorm(20, mean=1, sd=10)\n    print(mean(sample))\n}\n\nThe output of the loop is now the mean of each data sample, though the exact values you get will be slightly different to mine.\nWe can see that 20 samples drawn from a population with mean = 1, sd = 10 we have extremely variable estimates of the mean. Try reducing the standard deviation back down to 2 to see the result.\n# My loop\nnum_simulations = 10\n\nfor (i in 1:num_simulations) {\n    sample = rnorm(20, mean=1, sd=2)\n    print(mean(sample))\n}\n\nThese are much closer to our expected population value of 1.\n\n\n\nWe’re very close to completing our loop. There is one final issue to sort.\nAt the moment, the estimated sample is lost every time the code in the loop restarts - we need to store the whole set of mean values for every single simulation if we want to properly see what is going on.\nFor this we need to add two elements to the loop. Firstly we need to create a variable to store the means we estimate. We do this before the loop starts.\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nand secondly, within the loop, we need to add the estimated mean to our new mean_values variable.\n# Create a numeric variable to store the means\n    mean_values[i] = mean(sample)\nmean_values can store many numbers altogether in a list or vector. Each individual element can be accessed using the square bracket notation. For example mean_values[5] would access the fifth value in the list. The code above will store each value in to the ith position in the list so that we keep them all.\nThe final code should look like this:\n# My loop\nnum_simulations = 10\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(20, mean=1, sd=2)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\nmean_values now contains all of our simulated means that we can visualise using a histogram with the hist() function.\n\nWhich will produce a simple Histogram of the values for us. Note that, as ever, your histogram will be a little different to mine. Try rerunning the code a few times to get a sense of the variabity.\nWe can make our plot a bit more attractive and informative by specifying some additional inputs. These just change the visualisation of the histogram rather than the analysis.\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Means\", xlim=c(-2, 4),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\nThis has a lot of inputs, lets break down what they all mean\n\n\n\nInput\nDescription\n\n\n\n\nmean_values\nThe data sample to visualise\n\n\nmain\nThe main title\n\n\nxlim\nThe start and end points of the x-axis within a collection\n\n\nxlab\nThe label for the x-axis\n\n\ncol\nThe colour of the bars\n\n\nborder\nThe colour of the borders of the bars\n\n\n\nAdding these to the function call makes a much nicer and more informative image.\n\nYou can tweak the titles and axis labels to be sometime informative to you, and you can change the colours to be anything you like. Try customising the plot - update the labels and colours as you prefer them.\n\n\n\n\n\n\nClick for names of colours in R\n\n\n\n\n\nYou can use the colour names defined here to customise your histogram.\n\n\n\n\n\n\n\nOnce your histogram is customised and ready to go - we can put our code to work and understand the core concepts.\nLet’s review what we have so far.\n\nWe have constructed a loop that draws data samples from a defined population\nWe compute and store the mean of that data sample\nWe use a histogram to visualise the distribution of means estimated from all of the data samples\nThis is the sampling distribution of the mean.\n\n\n\n\n\n\n\nKey step\n\n\n\nMake sure you have an organised script ready to go at this point, try adding comments to understand what each line does. You can delete some of the older parts of the code that aren’t included in the loop if you like. Your code should look something like this.:\n# My loop\nnum_simulations = 10\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(20, mean=1, sd=2)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Means\", xlim=c(-2, 4),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\n\nRecall that the variance of the sampling distribution is one of its key parameters - it tells us how precisely our estimated means map onto the known population mean. (Recall from lecture 3 that the standard error of the mean can be computed from this number).\nSampling distributions with wider variances indicate that we have less precisely estimated the mean, whereas tighter distributions indicate that we have more confidence. Let’s use our code to illustrate this concept.\n\n\n\n\n\n\nKey step\n\n\n\nChange your code to repeat the simulation 1000 times rather than just 10, you should get a much more ‘normal’ distribution that changes less when you rerun the code.\n\n\n\nThis distribution is the sampling distribution of the mean for the simulated data. We can learn the following things:\n\nWe are most likely to estimate a value of the mean that is close to 1\nThe spread of the distribution indicates that values between 0.5 and 1.5 are still fairly common\nWe will very occasionally estimate a mean around 0 or around 2\n\nTry changing your code to answer these questions.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when each data sample has 100 data points rather than just 20?\n\n\nFor this question, you’ll need to update the first input to rnorm() to have 100 data points rather than 20.\n\n\nWith more data in each data sample - we can see that the spread in the sampling distribution has reduced. In other words, each sample has a similar estimate of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nWe will very occasionally estimate a mean around 0.5 or around 1.5, but these are rare\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when we increase the standard deviation to 10?\n\n\nFor this question, you’ll need to update the sd input to rnorm() to be 10 rather than 2.\n\n\nWith more variability in each data sample - we can see that the variance in the sampling distribution has increased. In other words, each sample can have extremely different estimates of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nIt is still common to get estimates of the mean of 0.5 or around 1.5.\nWith this much variability, some data samples have a mean estimate as far away as -1 or 3!\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nBack in week 4 we computed the following descriptives for participants confidence in a range of face processing tasks.\n\nCan you compute the sampling distribution of the mean for the AIConfidenceBefore variable?\n\n\nFor this question, you’ll need to the number of data samples, the mean and the standard deviation using the values in hte descriptive statistics.\nIts also a good idea to update the xlim of your histogram to something around c(2, 7).\n\n\nThe code and result should look something like this:\n\nBased on this, we can see that the most commonly estimated mean value is very close to our expected mean of 4.57 and that almost every data sample has a mean of between 4 and 5.\nThis is a useful way to visualise how accurate we are likely to have been when estimating our sample means.\n\n\n\n\n\n\n\n\nSo far we’ve looked at sampling distributions of the mean but we can do the same thing with t-tests to get a sense of how varaible a t statistic might be under different circumstances. This also gives us some important insights into how t and p values are related.\nTo do this we’ll need to make a couple of additions to our code.\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window before starting this segment. The code changes will be large and you may want to refer back to the previous parts for revision later.\n\n\nThe first thing we’ll need is to compute a t value for a one sample t-test from our data vector from this definition:\n\\[\nt = \\frac{\\text{The sample mean} - \\text{Comparison Value}}{\\text{The standard error of the mean}}\n\\]\nWe need to use this code to compute the three ingredients and the t-test itself. Copy this code into your new Rj window and take a moment to understand each line.\n# Number of participants\nn = 20\n\n# Generate a random sample\nsample = rnorm(n, mean = 0, sd = 1)\n\n# Calculate the sample mean and standard deviation\nsample_mean = mean(sample)\nsample_sd = sd(sample)\n\n# Calculate the sample error of the mean \nsample_standard_error = sample_sd / sqrt(n)\n\n# Define comparison value\ncomparison_value = 0\n\n# Compute t-value\nt_value = (sample_mean - comparison_value) / sample_standard_error\nprint(t_value)\nNote that we’re simulating data with a mean of zero and using a comparison_value of zero as well. This is our Null Model - the t-values we get from this code are ones that might appear if there is no true effect in the data.\n\nAs before, you can run this code a few times to see the variability in the final t-value. Remember this this is a null model - most of the t-values are close to zero, but some are quite large…\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nFor the final part of new code, we need to combine our t-test with the loop from the previous section.\nCopy this code into your Rj window (you can overwrite the previous code if you like), and fill in the blanks indicated by __YOUR_CODE_HERE__ (you should delete __YOUR_CODE_HERE__ and replace it with your answer).\nIf you get it right you will see a histogram of sampling distribution of t values.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = __YOUR_CODE_HERE__\n\n    # Generate a random sample\n    sample = __YOUR_CODE_HERE__\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = __YOUR_CODE_HERE__\n    sample_sd = __YOUR_CODE_HERE__\n\n    # Calculate the sample error of the mean \n    sample_standard_error = __YOUR_CODE_HERE__\n\n    # Define comparison value\n    comparison_value = __YOUR_CODE_HERE__\n\n    # Compute t-value\n    t_value[i] = __YOUR_CODE_HERE__\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, main = \"Sampling Distribution of t-values\", xlim=c(-4, 4),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n\n\nAll the answers are in previous sections, pay particular attention to the code defining the t-test in section 5.\nChat with one of your tutors if you get stuck.\n\n\nWell done if you got this correct. The final code should look like this:\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 0, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, main = \"Sampling Distribution of t-values\", xlim=c(-4, 4),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n    \n\n\n\n\n\n\nAfter this exercise, you can now compute the null model for a one sample t-test yourself. With this code you can run an analysis to explore how likely it is to observe different t-values when we assume the null hypothesis is true.\nThink about the following questions and tweak your code to find the answers.\n\n\n\n\n\n\n Data Skills - how does the sampling distribution change when we have large samples? Use n = 250 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 25:\n    # Number of participants\n    n = 250\n\nRunning this a few times, you should see that the sampling distribution gets narrower compared to what we saw for n = 20. We commonly see values between -2 and 2 but almost never see values of -3 or +3.\n\n\n\n\n\n\n\n\n\n Data Skills - how does the sampling distribution change when we have very small samples? Use n = 4 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 5:\n    # Number of participants\n    n = 4\n\nRunning this a few times, you should see that the sampling distribution gets broader than for smaller samples. We see more values close to 4 and -4 than we did with a dataset of 20 or 250.\n\n\n\nIn all of these analyses we are absolutely assuming that the null hypothesis of no difference is true. With this assumptions, our sampling distribution shows that we get more extreme t-values (around +/- 4) by chance when working with smaller data samples. By contrast for larger data samples these same values occur extremely rarely.\nWhen data are perfectly normally distributed (parametric assumptions are met), Jamovi and R can compute these null distributions for us using a shortcut without simulating lots of data samples but the result is the same.\nTo conduct a hypothesis test from real data, we would compute the t-statistic as normal and can then compare the observed value to this sampling distribution that assumes no difference and has the same sample size. If the t-value we’ve observed from the real data would be extremely unlikely to have occured if the null is true, then we can reject that hypothesis.\n\n\n\nFinally, let’s consider why we test against the null distribution of no effect. This is fairly straightforward from the code we have written so far.\nWe can absolutely compute a sampling distribution for a case where there is a real difference between the mean and the comparison value. For example, this code simulates data with a mean of 1 and compares that to a comparison value of 0. Note that the rnorm() function inputs have changed.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 1, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, breaks=24, main = \"Sampling Distribution of t-values\", xlim=c(-10, 10),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\nThe result is a sampling distribution which is not centred around mean.\n\nThis distribution tells us that with 20 participants, a mean of 1 and a standard deviation of 1 - we are mostly likely to get a -value of 5 but that t-value could be as small as 2 or as large as 9 for extreme data samples.\nThis is very useful, but the issue that is we have had to assume what the real effect is. We get radically different sampling distributions if we change that effect around.\n\n\n\n\n\n\n\n\n\n\n\nThere are an infinite number of possible sampling distributions when there is a real effect! It isn’t possible to know which distribution we should be comparing with without making some massive assumptions about the exact size, variability and direction of effect that we expect to see. This simply isn’t possible in practice.\nIn contrast, the null case is very closely controlled and easier to work with. The interpretation is trickier but rejecting the null hypothesis is a tractable problem that we can do a good job of solving.\n\n\n\n\nWe’ve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code."
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions.html#the-dataset",
    "href": "tutorials/rmb-week-8_sampling-distributions.html#the-dataset",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "TLDR: There is no data to download! we’re going to make the data ourselves.\nThe dataset this week is very simple - we’re going to create it ourselves using R. At this point, we know a lot about the types of questions that we can ask with t-tests and we know a lot about the data processing around it. We can check our understanding of different aspects of these statistical tests by simulating data samples that have known properties (eg mean and standard deviation) and checking if the outcome of the t-test is what we would expect.\nThis way we can numerically validate the tests we’re running and take a look under the hood and see what sort of processes Jamovi and R are using to compute the tests for us.\n\n\n\n\n\n\nContent Info\n\n\n\nThere is a lot of code in this session - don’t be afraid to ask for help from your tutors!"
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions.html#the-challenge",
    "href": "tutorials/rmb-week-8_sampling-distributions.html#the-challenge",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "Today, we’ll first introduce some concepts and functions in R to simulate a data sample and compute our own one sample t-test. With that in hand, we will set up code to run thousands of tests in one go to help us to better understand sampling distributions."
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions.html#simulating-data-samples-in-r",
    "href": "tutorials/rmb-week-8_sampling-distributions.html#simulating-data-samples-in-r",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "The rnorm() function in R is used to generate random numbers from a normal (Gaussian) distribution. A call to rnorm() looks like this:\nrnorm(n, mean = 0, sd = 1)\nWe can see that rnorm() takes three values as inputs\n\n\n\n\n\n\n\ninput\nDescription\n\n\n\n\nn\nThe number of random numbers you want to generate, we always need to specify this.\n\n\nmean\nThe mean (average) of the normal distribution. The default value is 0 if it isn’t specified\n\n\nsd\nThe standard deviation of the normal distribution. The default value is 1 if it isn’t specified\n\n\n\nSo we could tweak these input arguments to simulate any number of data points from any normal distribution that we wish.\n\n\n\n\n\n\nCheck Your Understanding - R code\n\n\n\nWhat code would we need to write to simulate the following?\n20 data points from a distribution with mean of 1 and standard deviation of 2\n\nR code here -&gt; .\n\n33 data points from a distribution with mean of -3 and standard deviation of 1.23\n\nR code here -&gt; .\n\nfive thousand data points from a distribution with mean of 1.789 and standard deviation of 18.2\n\nR code here -&gt; .\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nOpen Jamovi, install the Rj add-on and open a new Rj window before we go any further\n\n\nWe can use rnorm() within the Rj code window as usual. Here I’m simulating 20 data points from a distribution with a mean of 1 and a standard deviation of 2. Try running the same code in your window.\n\nThese 20 simulated numbers could now represent a data sample for a variable in an analysis. The difference is we have absolute confidence about the ‘truth’ of the population statistics underlying this data sample.\nYou may notice that you do get 20 numbers, but that you get a different 20 numbers than I did! This is as rnorm() works hard to generate random numbers that are different each time. You can verify this by running your code multiple times, even though the code stays the same you will get a different data sample each time. Remember that the code and the underlying distribution remain the always the same but the data sample is changing.\nThis is actually a good thing for us. Real experiments and data collection always have an element of random luck and we want our simulations to reflect this as well."
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions.html#simulating-data-samples-in-r-1",
    "href": "tutorials/rmb-week-8_sampling-distributions.html#simulating-data-samples-in-r-1",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "Generate 10 random numbers from a standard normal distribution (mean = 0, sd = 1):\nrandom_numbers &lt;- rnorm(10) print(random_numbers) Generate 10 random numbers from a normal distribution with mean = 5 and sd = 2:\nrandom_numbers &lt;- rnorm(10, mean = 5, sd = 2) print(random_numbers) Generate 1000 random numbers and plot a histogram:\nrandom_numbers &lt;- rnorm(1000, mean = 0, sd = 1) hist(random_numbers, breaks = 30, main = “Histogram of Random Numbers”, xlab = “Value”, col = “lightblue”, border = “white”)\n# Set parameters\nset.seed(123)               # For reproducibility\nn = 30                      # Sample size\nmu = 0                      # Population mean\nsigma = 1                   # Population standard deviation\ncomparison_value = 0        # Pre specified comparison value\nnum_simulations = 1000      # Number of simulations\n\n# Initialize a vector to store t-values\nt_values = numeric(num_simulations)\n\n# Loop to perform simulations\nfor (i in 1:num_simulations) {\n  # Generate a random sample\n  sample &lt;- rnorm(n, mean = mu, sd = sigma)\n  \n  # Calculate the sample mean and standard deviation\n  sample_mean &lt;- mean(sample)\n  sample_sd &lt;- sd(sample)\n  \n  # Calculate the t-value\n  t_value &lt;- (sample_mean - comparison_value) / (sample_sd / sqrt(n))\n  \n  # Store the t-value\n  t_values[i] &lt;- t_value\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, breaks = 30, main = \"Sampling Distribution of t-values\",\n     xlab = \"t-value\", col = \"skyblue\", border = \"white\")"
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions.html#estimating-the-samping-distribution-of-the-mean",
    "href": "tutorials/rmb-week-8_sampling-distributions.html#estimating-the-samping-distribution-of-the-mean",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "In week 2 we discussed how sampling relatively small amount of data from a wider population can impact our estimates of values like the mean.\nTo recap, let’s say we’re interested in computing the average attainment for secondary school children on a particular vocabulary test. Our target population might be ‘all secondary school students in the UK’ but is it practically impossible to get data from every single student currently in secondard school. We have to make do with smaller data samples of a few tens or few hundreds of students (or even more, if we’re lucky).\nOur simulations do exactly this. The parameters we pass into rnorm() define ‘population level’ characteristics in the form of a normal distribution and each time we run the function we draw a ‘sample’ of a fixed number of data points from that population. We can think of this as rerunning our data sampling every time the function is executed. This would be massively complicated, time consuming and expensive in real life, but with a few assumptions a computer can simulate it in a fraction of a second.\nWe can now start to explore how sampling variability - AKA the random differences between data samples from the same population - can impact our estimates of the mean and standard deviation.\nUpdate your code to store the data sample into a variable and compute the mean with mean() and standard deviation with sd(). The result might look like this.\nsample = rnorm(20, mean=1, sd=2)\n\n# Compute the mean\nprint('Mean:')\nprint(mean(sample))\n\n# Compute the standard deviation\nprint('Standard Deviation:')\nprint(sd(sample))\n\nRunning this code will show the mean and standard deviation of each individual data sample, try running this a few times. Notice that the values are sometimes close to our ‘population’ estimates of mean = 1 and standard deviation = 2 but also sometimes quite far away.\nTry tweaking your code to answer these questions\n\n\n\n\n\n\n Data Skills - is the estimate of the mean better or worse with a sample of 200?\n\n\n\n\n\nUpdate your code to use this data simulation\nsample = rnorm(200, mean=1, sd=2)\nRunning this a few times should show you mean values much closer to 1 and standard deviations much closer to 2 than we saw with a sample of 20 values. They still won’t be perfect though.\n\n\n\n\n\n\n\n\n\n Data Skills - is the estimate of the mean better or worse when the population standard deviation is 10?\n\n\n\n\n\nUpdate your code to use this data simulation\nsample = rnorm(200, mean=1, sd=10)\nRunning this a few times should make your estimates of the mean and standard deviation much more variable again. When variability in the population is very wide, large data samples won’t necessarily make up the difference.\n\n\n\nWe can get an informal sense of sampling variability by clicking to rerun this code a few times, but R code allows us to do much better, we can tweak the code to rerun this analysis hundreds or thousands of times to explicitly quantify the variability we see.\nTo do this, we’ll need a code element called a loop which looks something like this\nnum_simulation = 10\n\n# Loop to perform simulations\nfor (i in 1:num_simulations) {\n    print(i)\n}\nThere are two aspects to this code block. The first sets the number of simulated data samples we want to draw.\nnum_simulations = 10\nThis line sets the variable num_simulations to 10. This means you want to perform 10 iterations in the loop.\nThe second part is the loop itself:\nfor (i in 1:num_simulations) {\n    print(i)\n}\nThis is a for loop that will iterate from 1 to num_simulations (which is 10 in this case). Here’s what happens inside the loop:\n\nInitialization: The loop starts with i set to 1.\nCondition: The loop will continue to run as long as i is less than or equal to num_simulations.\nIncrement: After each iteration, i is incremented by 1.\n\nInside the loop, the print(i) statement is executed, which prints the current value of i to the console.\nThe output of this code will be the numbers from 1 to 10, each printed on a new line. The full output might look like this:\n\nPrinting the value of i is instructive but really we want our loop to include some interesting computations. Next, we should move our sampling code within the curly brackets {} of the loop.\nEverything within the those curly brackets will be repeated on every iteration of the loop. So if we includ the code to draw a sample and compute the mean, it will be repeated every time. Here is an example:\n# My loop\nnum_simulations = 10\n\nfor (i in 1:num_simulations) {\n    sample = rnorm(20, mean=1, sd=10)\n    print(mean(sample))\n}\n\nThe output of the loop is now the mean of each data sample, though the exact values you get will be slightly different to mine.\nWe can see that 20 samples drawn from a population with mean = 1, sd = 10 we have extremely variable estimates of the mean. Try reducing the standard deviation back down to 2 to see the result.\n# My loop\nnum_simulations = 10\n\nfor (i in 1:num_simulations) {\n    sample = rnorm(20, mean=1, sd=2)\n    print(mean(sample))\n}\n\nThese are much closer to our expected population value of 1."
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions.html#visualising-the-samping-distribution-of-the-mean",
    "href": "tutorials/rmb-week-8_sampling-distributions.html#visualising-the-samping-distribution-of-the-mean",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "We’re very close to completing our loop. There is one final issue to sort.\nAt the moment, the estimated sample is lost every time the code in the loop restarts - we need to store the whole set of mean values for every single simulation if we want to properly see what is going on.\nFor this we need to add two elements to the loop. Firstly we need to create a variable to store the means we estimate. We do this before the loop starts.\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nand secondly, within the loop, we need to add the estimated mean to our new mean_values variable.\n# Create a numeric variable to store the means\n    mean_values[i] = mean(sample)\nmean_values can store many numbers altogether in a list or vector. Each individual element can be accessed using the square bracket notation. For example mean_values[5] would access the fifth value in the list. The code above will store each value in to the ith position in the list so that we keep them all.\nThe final code should look like this:\n# My loop\nnum_simulations = 10\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(20, mean=1, sd=2)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\nmean_values now contains all of our simulated means that we can visualise using a histogram with the hist() function.\n\nWhich will produce a simple Histogram of the values for us. Note that, as ever, your histogram will be a little different to mine. Try rerunning the code a few times to get a sense of the variabity.\nWe can make our plot a bit more attractive and informative by specifying some additional inputs. These just change the visualisation of the histogram rather than the analysis.\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Means\", xlim=c(-2, 4),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\nThis has a lot of inputs, lets break down what they all mean\n\n\n\nInput\nDescription\n\n\n\n\nmean_values\nThe data sample to visualise\n\n\nmain\nThe main title\n\n\nxlim\nThe start and end points of the x-axis within a collection\n\n\nxlab\nThe label for the x-axis\n\n\ncol\nThe colour of the bars\n\n\nborder\nThe colour of the borders of the bars\n\n\n\nAdding these to the function call makes a much nicer and more informative image.\n\nYou can tweak the titles and axis labels to be sometime informative to you, and you can change the colours to be anything you like. Try customising the plot - update the labels and colours as you prefer them.\n\n\n\n\n\n\nClick for names of colours in R\n\n\n\n\n\nYou can use the colour names defined here to customise your histogram."
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions.html#understanding-the-samping-distribution-of-the-mean",
    "href": "tutorials/rmb-week-8_sampling-distributions.html#understanding-the-samping-distribution-of-the-mean",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "Once your histogram is customised and ready to go - we can put our code to work and understand the core concepts.\nLet’s review what we have so far.\n\nWe have constructed a loop that draws data samples from a defined population\nWe compute and store the mean of that data sample\nWe use a histogram to visualise the distribution of means estimated from all of the data samples\nThis is the sampling distribution of the mean.\n\n\n\n\n\n\n\nKey step\n\n\n\nMake sure you have an organised script ready to go at this point, try adding comments to understand what each line does. You can delete some of the older parts of the code that aren’t included in the loop if you like. Your code should look something like this.:\n# My loop\nnum_simulations = 10\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(20, mean=1, sd=2)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Means\", xlim=c(-2, 4),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\n\nRecall that the variance of the sampling distribution is one of its key parameters - it tells us how precisely our estimated means map onto the known population mean. (Recall from lecture 3 that the standard error of the mean can be computed from this number).\nSampling distributions with wider variances indicate that we have less precisely estimated the mean, whereas tighter distributions indicate that we have more confidence. Let’s use our code to illustrate this concept.\n\n\n\n\n\n\nKey step\n\n\n\nChange your code to repeat the simulation 1000 times rather than just 10, you should get a much more ‘normal’ distribution that changes less when you rerun the code.\n\n\n\nThis distribution is the sampling distribution of the mean for the simulated data. We can learn the following things:\n\nWe are most likely to estimate a value of the mean that is close to 1\nThe spread of the distribution indicates that values between 0.5 and 1.5 are still fairly common\nWe will very occasionally estimate a mean around 0 or around 2\n\nTry changing your code to answer these questions.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when each data sample has 100 data points rather than just 20?\n\n\nFor this question, you’ll need to update the first input to rnorm() to have 100 data points rather than 20.\n\n\nWith more data in each data sample - we can see that the spread in the sampling distribution has reduced. In other words, each sample has a similar estimate of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nWe will very occasionally estimate a mean around 0.5 or around 1.5, but these are rare\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when we increase the standard deviation to 10?\n\n\nFor this question, you’ll need to update the sd input to rnorm() to be 10 rather than 2.\n\n\nWith more variability in each data sample - we can see that the variance in the sampling distribution has increased. In other words, each sample can have extremely different estimates of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nIt is still common to get estimates of the mean of 0.5 or around 1.5.\nWith this much variability, some data samples have a mean estimate as far away as -1 or 3!\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nBack in week 4 we computed the following descriptives for participants confidence in a range of face processing tasks.\n\nCan you compute the sampling distribution of the mean for the AIConfidenceBefore variable?\n\n\nFor this question, you’ll need to the number of data samples, the mean and the standard deviation using the values in hte descriptive statistics.\nIts also a good idea to update the xlim of your histogram to something around c(2, 7).\n\n\nThe code and result should look something like this:\n\nBased on this, we can see that the most commonly estimated mean value is very close to our expected mean of 4.57 and that almost every data sample has a mean of between 4 and 5.\nThis is a useful way to visualise how accurate we are likely to have been when estimating our sample means."
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions.html#sampling-distributions-for-one-sample-t-tests",
    "href": "tutorials/rmb-week-8_sampling-distributions.html#sampling-distributions-for-one-sample-t-tests",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "So far we’ve looked at sampling distributions of the mean but we can do the same thing with t-tests to get a sense of how varaible a t statistic might be under different circumstances. This also gives us some important insights into how t and p values are related.\nTo do this we’ll need to make a couple of additions to our code.\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window before starting this segment. The code changes will be large and you may want to refer back to the previous parts for revision later.\n\n\nThe first thing we’ll need is to compute a t value for a one sample t-test from our data vector from this definition:\n\\[\nt = \\frac{\\text{The sample mean} - \\text{Comparison Value}}{\\text{The standard error of the mean}}\n\\]\nWe need to use this code to compute the three ingredients and the t-test itself. Copy this code into your new Rj window and take a moment to understand each line.\n# Number of participants\nn = 20\n\n# Generate a random sample\nsample = rnorm(n, mean = 0, sd = 1)\n\n# Calculate the sample mean and standard deviation\nsample_mean = mean(sample)\nsample_sd = sd(sample)\n\n# Calculate the sample error of the mean \nsample_standard_error = sample_sd / sqrt(n)\n\n# Define comparison value\ncomparison_value = 0\n\n# Compute t-value\nt_value = (sample_mean - comparison_value) / sample_standard_error\nprint(t_value)\nNote that we’re simulating data with a mean of zero and using a comparison_value of zero as well. This is our Null Model - the t-values we get from this code are ones that might appear if there is no true effect in the data.\n\nAs before, you can run this code a few times to see the variability in the final t-value. Remember this this is a null model - most of the t-values are close to zero, but some are quite large…\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nFor the final part of new code, we need to combine our t-test with the loop from the previous section.\nCopy this code into your Rj window (you can overwrite the previous code if you like), and fill in the blanks indicated by __YOUR_CODE_HERE__ (you should delete __YOUR_CODE_HERE__ and replace it with your answer).\nIf you get it right you will see a histogram of sampling distribution of t values.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = __YOUR_CODE_HERE__\n\n    # Generate a random sample\n    sample = __YOUR_CODE_HERE__\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = __YOUR_CODE_HERE__\n    sample_sd = __YOUR_CODE_HERE__\n\n    # Calculate the sample error of the mean \n    sample_standard_error = __YOUR_CODE_HERE__\n\n    # Define comparison value\n    comparison_value = __YOUR_CODE_HERE__\n\n    # Compute t-value\n    t_value[i] = __YOUR_CODE_HERE__\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, main = \"Sampling Distribution of t-values\", xlim=c(-4, 4),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n\n\nAll the answers are in previous sections, pay particular attention to the code defining the t-test in section 5.\nChat with one of your tutors if you get stuck.\n\n\nWell done if you got this correct. The final code should look like this:\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 0, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, main = \"Sampling Distribution of t-values\", xlim=c(-4, 4),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n    \n\n\n\n\n\n\nAfter this exercise, you can now compute the null model for a one sample t-test yourself. With this code you can run an analysis to explore how likely it is to observe different t-values when we assume the null hypothesis is true.\nThink about the following questions and tweak your code to find the answers.\n\n\n\n\n\n\n Data Skills - how does the sampling distribution change when we have large samples? Use n = 250 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 25:\n    # Number of participants\n    n = 250\n\nRunning this a few times, you should see that the sampling distribution gets narrower compared to what we saw for n = 20. We commonly see values between -2 and 2 but almost never see values of -3 or +3.\n\n\n\n\n\n\n\n\n\n Data Skills - how does the sampling distribution change when we have very small samples? Use n = 4 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 5:\n    # Number of participants\n    n = 4\n\nRunning this a few times, you should see that the sampling distribution gets broader than for smaller samples. We see more values close to 4 and -4 than we did with a dataset of 20 or 250.\n\n\n\nIn all of these analyses we are absolutely assuming that the null hypothesis of no difference is true. With this assumptions, our sampling distribution shows that we get more extreme t-values (around +/- 4) by chance when working with smaller data samples. By contrast for larger data samples these same values occur extremely rarely.\nWhen data are perfectly normally distributed (parametric assumptions are met), Jamovi and R can compute these null distributions for us using a shortcut without simulating lots of data samples but the result is the same.\nTo conduct a hypothesis test from real data, we would compute the t-statistic as normal and can then compare the observed value to this sampling distribution that assumes no difference and has the same sample size. If the t-value we’ve observed from the real data would be extremely unlikely to have occured if the null is true, then we can reject that hypothesis."
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions.html#why-do-we-test-the-null-distribution",
    "href": "tutorials/rmb-week-8_sampling-distributions.html#why-do-we-test-the-null-distribution",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "Finally, let’s consider why we test against the null distribution of no effect. This is fairly straightforward from the code we have written so far.\nWe can absolutely compute a sampling distribution for a case where there is a real difference between the mean and the comparison value. For example, this code simulates data with a mean of 1 and compares that to a comparison value of 0. Note that the rnorm() function inputs have changed.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 1, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, breaks=24, main = \"Sampling Distribution of t-values\", xlim=c(-10, 10),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\nThe result is a sampling distribution which is not centred around mean.\n\nThis distribution tells us that with 20 participants, a mean of 1 and a standard deviation of 1 - we are mostly likely to get a -value of 5 but that t-value could be as small as 2 or as large as 9 for extreme data samples.\nThis is very useful, but the issue that is we have had to assume what the real effect is. We get radically different sampling distributions if we change that effect around.\n\n\n\n\n\n\n\n\n\n\n\nThere are an infinite number of possible sampling distributions when there is a real effect! It isn’t possible to know which distribution we should be comparing with without making some massive assumptions about the exact size, variability and direction of effect that we expect to see. This simply isn’t possible in practice.\nIn contrast, the null case is very closely controlled and easier to work with. The interpretation is trickier but rejecting the null hypothesis is a tractable problem that we can do a good job of solving."
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions.html#summary",
    "href": "tutorials/rmb-week-8_sampling-distributions.html#summary",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "We’ve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code."
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "The sampling distribution is central to how we can move from t-values to p-values - yet it is one of the trickier parts of this course. The lectures and pre-lecture materials frequently deal with computer simulations to show the properties of the tests we’re using, so this week you will write your own simulation to help understand the sampling distribution!\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nSampling Distributions\n\n\n\nt-values\n\n\n\np-values\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nComputing a one sample t-test in R\n\n\n\nSimulating data in R\n\n\n\nWriting loops in R\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nCreating shareable code to demonstrate a statistical concept\n\n\n\n\n\nTLDR: There is no data to download! we’re going to make the data ourselves.\nThe dataset this week is very simple - we’re going to create it ourselves using R. At this point, we know a lot about the types of questions that we can ask with t-tests and we know a lot about the data processing around it. We can check our understanding of different aspects of these statistical tests by simulating data samples that have known properties (eg mean and standard deviation) and checking if the outcome of the t-test is what we would expect.\nThis way we can numerically validate the tests we’re running and take a look under the hood and see what sort of processes Jamovi and R are using to compute the tests for us.\n\n\n\n\n\n\nContent Info\n\n\n\nThere is a lot of code in this session - don’t be afraid to ask for help from your tutors!\n\n\n\n\n\nToday, we’ll first introduce some concepts and functions in R to simulate a data sample and compute our own one sample t-test. With that in hand, we will set up code to run thousands of tests in one go to help us to better understand sampling distributions.\n\n\n\nThe rnorm() function in R is used to generate random numbers from a normal (Gaussian) distribution. A call to rnorm() looks like this:\nrnorm(n, mean = 0, sd = 1)\nWe can see that rnorm() takes three values as inputs\n\n\n\n\n\n\n\ninput\nDescription\n\n\n\n\nn\nThe number of random numbers you want to generate, we always need to specify this.\n\n\nmean\nThe mean (average) of the normal distribution. The default value is 0 if it isn’t specified\n\n\nsd\nThe standard deviation of the normal distribution. The default value is 1 if it isn’t specified\n\n\n\nSo we could tweak these input arguments to simulate any number of data points from any normal distribution that we wish.\n\n\n\n\n\n\nCheck Your Understanding - R code\n\n\n\nWhat code would we need to write to simulate the following?\n20 data points from a distribution with mean of 1 and standard deviation of 2\n\nR code here -&gt; .\n\n33 data points from a distribution with mean of -3 and standard deviation of 1.23\n\nR code here -&gt; .\n\nfive thousand data points from a distribution with mean of 1.789 and standard deviation of 18.2\n\nR code here -&gt; .\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nOpen Jamovi, install the Rj add-on and open a new Rj window before we go any further\n\n\nWe can use rnorm() within the Rj code window as usual. Here I’m simulating 20 data points from a distribution with a mean of 1 and a standard deviation of 2. Try running the same code in your window.\n\nThese 20 simulated numbers could now represent a data sample for a variable in an analysis. The difference is we have absolute confidence about the ‘truth’ of the population statistics underlying this data sample.\nYou may notice that you do get 20 numbers, but that you get a different 20 numbers than I did! This is as rnorm() works hard to generate random numbers that are different each time.\n\n\n\n\n\n\nKey step\n\n\n\nYou can verify the random nature of the simulated data by running your code multiple times.\nEven though the code stays the same you will get a different data sample each time. Remember that the code and the underlying distribution remain the always the same but the data sample is changing.\n\n\nThis is actually a good thing for us. Real experiments and data collection always have an element of random luck and we want our simulations to reflect this as well.\n\n\n\n\n\n\nData skills - revision on sampling and populations\n\n\n\nSampling is a fundamental process in statistics where a subset of individuals from a population is selected to estimate characteristics of the whole population. This is built from the assumption that the subset of individuals in our sample is representative of that population.\nSome samples will be more representative than others. We can do a lot of practical work and planning to get the best possible sample for our research but there is always some random chance in who ends up in a data sample and who does not.\n\nThere are two main reasons why a sample might not be representative.\nBias if the data collection has included some bias (intentional or accidental) towards a subgroup of our population then it is very likely that this will lead to an unrepresentative sample. This is not something we can solve with statistics. Good reseach design, consideration of ethics and thorough planning is the only way to avoid systematic biases.\nLuck even if we have a unbiased data collection approach, two samples that draw from the sample population can still be different due to random chance. This will always be true when we are restricted to looking at a subset of the full population - fortunately, this is something we can quantify with statistics. We’ll explore how in this session.\nThe code we write this week will simulate drawing a subset of individuals from a population. Each iteration of the code could be considered as a separate replication of the same experiment.\n\n\n\n\n\nIn week 2 we discussed how sampling relatively small amount of data from a wider population can impact our estimates of values like the mean.\nTo recap, let’s say we’re interested in computing the average attainment for secondary school children on a particular vocabulary test. Our target population might be ‘all secondary school students in the UK’ but is it practically impossible to get data from every single student currently in secondard school. We have to make do with smaller data samples of a few tens or few hundreds of students (or even more, if we’re lucky).\nOur simulations do exactly this. The parameters we pass into rnorm() define ‘population level’ characteristics in the form of a normal distribution and each time we run the function we draw a ‘sample’ of a fixed number of data points from that population. We can think of this as rerunning our data sampling every time the function is executed. This would be massively complicated, time consuming and expensive in real life, but with a few assumptions a computer can simulate it in a fraction of a second.\nWe can now start to explore how sampling variability - AKA the random differences between data samples from the same population - can impact our estimates of the mean and standard deviation.\nUpdate your code to store the data sample into a variable and compute the mean with mean() and standard deviation with sd(). The result might look like this.\nsample = rnorm(20, mean=1, sd=2)\n\n# Compute the mean\nprint('Mean:')\nprint(mean(sample))\n\n# Compute the standard deviation\nprint('Standard Deviation:')\nprint(sd(sample))\n\nRunning this code will show the mean and standard deviation of each individual data sample, try running this a few times. Notice that the values are sometimes close to our ‘population’ estimates of mean = 1 and standard deviation = 2 but also sometimes quite far away.\nTry tweaking your code to answer these questions\n\n\n\n\n\n\n Data Skills - is the estimate of the mean better or worse with a sample of 200?\n\n\n\n\n\nUpdate your code to use this data simulation\nsample = rnorm(200, mean=1, sd=2)\nRunning this a few times should show you mean values much closer to 1 and standard deviations much closer to 2 than we saw with a sample of 20 values. They still won’t be perfect though.\n\n\n\n\n\n\n\n\n\n Data Skills - is the estimate of the mean better or worse when the population standard deviation is 10?\n\n\n\n\n\nUpdate your code to use this data simulation\nsample = rnorm(200, mean=1, sd=10)\nRunning this a few times should make your estimates of the mean and standard deviation much more variable again. When variability in the population is very wide, large data samples won’t necessarily make up the difference.\n\n\n\nWe can get an informal sense of sampling variability by clicking to rerun this code a few times, but R code allows us to do much better, we can tweak the code to rerun this analysis hundreds or thousands of times to explicitly quantify the variability we see.\nTo do this, we’ll need a code element called a loop which looks something like this\nnum_simulation = 10\n\n# Loop to perform simulations\nfor (i in 1:num_simulations) {\n    print(i)\n}\nThere are two aspects to this code block. The first sets the number of simulated data samples we want to draw.\nnum_simulations = 10\nThis line sets the variable num_simulations to 10. This means you want to perform 10 iterations in the loop.\nThe second part is the loop itself:\nfor (i in 1:num_simulations) {\n    print(i)\n}\nThis is a for loop that will iterate from 1 to num_simulations (which is 10 in this case). Here’s what happens inside the loop:\n\nInitialization: The loop starts with i set to 1.\nCondition: The loop will continue to run as long as i is less than or equal to num_simulations.\nIncrement: After each iteration, i is incremented by 1.\n\nInside the loop, the print(i) statement is executed, which prints the current value of i to the console.\nThe output of this code will be the numbers from 1 to 10, each printed on a new line. The full output might look like this:\n\nPrinting the value of i is instructive but really we want our loop to include some interesting computations. Next, we should move our sampling code within the curly brackets {} of the loop.\nEverything within the those curly brackets will be repeated on every iteration of the loop. So if we includ the code to draw a sample and compute the mean, it will be repeated every time. Here is an example:\n# My loop\nnum_simulations = 10\n\nfor (i in 1:num_simulations) {\n    sample = rnorm(20, mean=1, sd=10)\n    print(mean(sample))\n}\n\nThe output of the loop is now the mean of each data sample, though the exact values you get will be slightly different to mine.\nWe can see that 20 samples drawn from a population with mean = 1, sd = 10 we have extremely variable estimates of the mean. Try reducing the standard deviation back down to 2 to see the result.\n# My loop\nnum_simulations = 10\n\nfor (i in 1:num_simulations) {\n    sample = rnorm(20, mean=1, sd=2)\n    print(mean(sample))\n}\n\nThese are much closer to our expected population value of 1.\n\n\n\nWe’re very close to completing our loop. There is one final issue to sort.\nAt the moment, the estimated sample is lost every time the code in the loop restarts - we need to store the whole set of mean values for every single simulation if we want to properly see what is going on.\nFor this we need to add two elements to the loop. Firstly we need to create a variable to store the means we estimate. We do this before the loop starts.\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nand secondly, within the loop, we need to add the estimated mean to our new mean_values variable.\n# Create a numeric variable to store the means\n    mean_values[i] = mean(sample)\nmean_values can store many numbers altogether in a list or vector. Each individual element can be accessed using the square bracket notation. For example mean_values[5] would access the fifth value in the list. The code above will store each value in to the ith position in the list so that we keep them all.\nThe final code should look like this:\n# My loop\nnum_simulations = 10\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(20, mean=1, sd=2)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\nmean_values now contains all of our simulated means that we can visualise using a histogram with the hist() function.\n\nWhich will produce a simple Histogram of the values for us. Note that, as ever, your histogram will be a little different to mine. Try rerunning the code a few times to get a sense of the variabity.\nWe can make our plot a bit more attractive and informative by specifying some additional inputs. These just change the visualisation of the histogram rather than the analysis.\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Means\", xlim=c(-2, 4),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\nThis has a lot of inputs, lets break down what they all mean\n\n\n\nInput\nDescription\n\n\n\n\nmean_values\nThe data sample to visualise\n\n\nmain\nThe main title\n\n\nxlim\nThe start and end points of the x-axis within a collection\n\n\nxlab\nThe label for the x-axis\n\n\ncol\nThe colour of the bars\n\n\nborder\nThe colour of the borders of the bars\n\n\n\nAdding these to the function call makes a much nicer and more informative image.\n\nYou can tweak the titles and axis labels to be sometime informative to you, and you can change the colours to be anything you like. Try customising the plot - update the labels and colours as you prefer them.\n\n\n\n\n\n\nClick for names of colours in R\n\n\n\n\n\nYou can use the colour names defined here to customise your histogram.\n\n\n\n\n\n\n\nOnce your histogram is customised and ready to go - we can put our code to work and understand the core concepts.\nLet’s review what we have so far.\n\nWe have constructed a loop that draws data samples from a defined population\nWe compute and store the mean of that data sample\nWe use a histogram to visualise the distribution of means estimated from all of the data samples\nThis is the sampling distribution of the mean.\n\n\n\n\n\n\n\nKey step\n\n\n\nMake sure you have an organised script ready to go at this point, try adding comments to understand what each line does. You can delete some of the older parts of the code that aren’t included in the loop if you like. Your code should look something like this.:\n# My loop\nnum_simulations = 10\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(20, mean=1, sd=2)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Means\", xlim=c(-2, 4),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\n\nRecall that the variance of the sampling distribution is one of its key parameters - it tells us how precisely our estimated means map onto the known population mean. (Recall from lecture 3 that the standard error of the mean can be computed from this number).\nSampling distributions with wider variances indicate that we have less precisely estimated the mean, whereas tighter distributions indicate that we have more confidence. Let’s use our code to illustrate this concept.\n\n\n\n\n\n\nKey step\n\n\n\nChange your code to repeat the simulation 1000 times rather than just 10, you should get a much more ‘normal’ distribution that changes less when you rerun the code.\n\n\n\nThis distribution is the sampling distribution of the mean for the simulated data. We can learn the following things:\n\nWe are most likely to estimate a value of the mean that is close to 1\nThe spread of the distribution indicates that values between 0.5 and 1.5 are still fairly common\nWe will very occasionally estimate a mean around 0 or around 2\n\nTry changing your code to answer these questions.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when each data sample has 100 data points rather than just 20?\n\n\nFor this question, you’ll need to update the first input to rnorm() to have 100 data points rather than 20.\n\n\nWith more data in each data sample - we can see that the spread in the sampling distribution has reduced. In other words, each sample has a similar estimate of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nWe will very occasionally estimate a mean around 0.5 or around 1.5, but these are rare\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when we increase the standard deviation to 10?\n\n\nFor this question, you’ll need to update the sd input to rnorm() to be 10 rather than 2.\n\n\nWith more variability in each data sample - we can see that the variance in the sampling distribution has increased. In other words, each sample can have extremely different estimates of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nIt is still common to get estimates of the mean of 0.5 or around 1.5.\nWith this much variability, some data samples have a mean estimate as far away as -1 or 3!\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nBack in week 4 we computed the following descriptives for participants confidence in a range of face processing tasks.\n\nCan you compute the sampling distribution of the mean for the AIConfidenceBefore variable?\n\n\nFor this question, you’ll need to the number of data samples, the mean and the standard deviation using the values in hte descriptive statistics.\nIts also a good idea to update the xlim of your histogram to something around c(2, 7).\n\n\nThe code and result should look something like this:\n\nBased on this, we can see that the most commonly estimated mean value is very close to our expected mean of 4.57 and that almost every data sample has a mean of between 4 and 5.\nThis is a useful way to visualise how accurate our sample mean estimates are likely to be.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and copy your code loop into it before starting this segment.\nWe will make large changes to the code and you may want to refer back to the previous parts for revision later.\n\n\nWe can compute sampling distributions for any summary statistic that we want to. This can be useful for quantifying sampling variability properties other than the mean. One straightforward example is the sampling distribution of the maximum.\nLet’s use the heights of human women as an example. According to Our World in Data, the height of women born between 1980 and 1994 is normally distributed with a mean values of 164.7cm and the standard deviation is 7.07cm.\nWe can use these parameters to define a population that we can use to simulate our data samples. This code will draw a data sample of the height of a single person.\nrnorm(1, mean=164.7, sd=7.07)\nWe can plug this into our code to compute the sampling distributions for height. The full loop will look like this - remember to update the xlim input to the histograms as well!\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(10, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\nThis is the sampling distribution of the mean for samples of 10 data points draw from the population distribution of women’s heights. The sampling distribution shows that the average height of a group of 10 women drawn randomly from the population will nearly always fall between 160cm and 170cm.\n\nWe can adapt this code to ask other questions - for example how tall is the tallest person in a group of 10 women likely to be?\nWe can adapt our code to compute this sampling distribution of maximums as well. Here is an exmaple - look out for sections with NEW CODE indicating a change.\nCritically, we use the max() function to return the maximum value of each data sample, in exactly the same way that we used mean() to compute the average.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nmax_values = numeric(num_simulations)  # &lt;- NEW CODE\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(10, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n    max_values[i] = max(sample)  # &lt;- NEW CODE\n}\n\n# Plot the sampling distribution\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\n# NEW CODE\nhist(max_values, main = \"Sampling Distribution of Maximums\", xlim=c(130, 200),\n     xlab = \"Sample Maximum\", col = \"skyblue\", border = \"white\")\n    \n\nThis shows us that the tallest person in a group of 10 women is likely to be between 170cm and 180cm.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute the sampling distribution of the minimum value to compute the height of the shortest person in groups of 10 women?\nUpdate the code that you have already written.\n\n\nTake a look at the changes highlighted in the previous section, can you repeat those changes to include an equivalent to the maximum?\n\n\nThe following code computes and displays the sampling distribution of the minimum value.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nmax_values = numeric(num_simulations)\nmin_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(4, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n    max_values[i] = max(sample)\n    min_values[i] = min(sample)\n    \n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\nhist(max_values, main = \"Sampling Distribution of Maximums\", xlim=c(130, 200),\n     xlab = \"Sample Maximum\", col = \"skyblue\", border = \"white\")\n    \nhist(min_values, main = \"Sampling Distribution of Minimums\", xlim=c(130, 200),\n     xlab = \"Sample Minimum\", col = \"skyblue\", border = \"white\")\n\n\n\n\n\n\n\n\n\nThroughout this course, we have seen many data variables that are not normally distributed. Examples include dice rolls, reaction times and the intrusive memories from last week. These are interesting datasets but don’t follow some of the nice assumptions that we can use with normally distributed data.\nNow that we have our loops to compute sampling distributions, we can explore how much of a problem non normally distributed data is for our estimates.\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and copy your code loop from section 5 into it before starting this segment.\nWe will make large changes to the code and you may want to refer back to the previous parts for revision later.\n\n\nLet’s simulate some data for an experiment. Let’s simulate data from an experiment where participants indicate their confidence in performing some action on a scale from 1 to 10. The absolute truth of these data observations is that every participant has a equal probability of selecting any confidence from 1 to 10 - this is known as a uniform distribution and it is very different to a normal distribution.\nWe can simulate this data using runif() - note that this is an abbreviation of ‘random uniform’, not an indication to ‘run if’ something happens…\nTo use runif() we specify an n as usual in addition to the smallest and largest values that might occur.\nrunif(n, min = 1, max = 10)\nAs usual, we can slot this into our data loop to simulate our uniform distribution. Let’s start by drawing a single value during each repetition of the experiment. This will visualise our distribution.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = runif(1, min = 1, max = 10)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(0, 11),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\nYou can run this a few times to verify that the histogram shows a nearly uniform distribution where all outcomes are approximately equally likely to occur.\nNow, something odd happens when we increase the size of each data sample drawn from our population parameters. Rather than visualising the response of a single individual - lets take a look at the average response of 10 different individuals.\nUpdate the code to have an n of 10 rather than 1 in the call to runif().\n\nNow, though the raw data are uniformly distributed - the averages of uniform variables look like they are more normally distributed. Strange.\nThis is a property called the Central Limit Theorem. The Central Limit Theorem states that, given a sufficiently large sample size, the sampling distribution of the sample mean will be approximately normally distributed, regardless of the shape of the population distribution.\nThis is important as it tells us that the properties of data can change when we make computations from it. In our example, each individual response is uniformly distributed but the average response is approximately normally distributed.\nSecondly, this is why the normal distribution is so important. The Central Limit Theorem means that is weirdly likely that we will encounter normally distributed data variables - any variable that is an aggregate of other things becomes more and more normally distributed. We often deal with this sort of composite data in Psychology,\nThe learning statistics with jamovi textbook as an excellent description of central limit theorem if you want more information.\n\n\n\nWe’ve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#the-dataset",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#the-dataset",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "TLDR: There is no data to download! we’re going to make the data ourselves.\nThe dataset this week is very simple - we’re going to create it ourselves using R. At this point, we know a lot about the types of questions that we can ask with t-tests and we know a lot about the data processing around it. We can check our understanding of different aspects of these statistical tests by simulating data samples that have known properties (eg mean and standard deviation) and checking if the outcome of the t-test is what we would expect.\nThis way we can numerically validate the tests we’re running and take a look under the hood and see what sort of processes Jamovi and R are using to compute the tests for us.\n\n\n\n\n\n\nContent Info\n\n\n\nThere is a lot of code in this session - don’t be afraid to ask for help from your tutors!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#the-challenge",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#the-challenge",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "Today, we’ll first introduce some concepts and functions in R to simulate a data sample and compute our own one sample t-test. With that in hand, we will set up code to run thousands of tests in one go to help us to better understand sampling distributions.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#simulating-data-samples-in-r",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#simulating-data-samples-in-r",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "The rnorm() function in R is used to generate random numbers from a normal (Gaussian) distribution. A call to rnorm() looks like this:\nrnorm(n, mean = 0, sd = 1)\nWe can see that rnorm() takes three values as inputs\n\n\n\n\n\n\n\ninput\nDescription\n\n\n\n\nn\nThe number of random numbers you want to generate, we always need to specify this.\n\n\nmean\nThe mean (average) of the normal distribution. The default value is 0 if it isn’t specified\n\n\nsd\nThe standard deviation of the normal distribution. The default value is 1 if it isn’t specified\n\n\n\nSo we could tweak these input arguments to simulate any number of data points from any normal distribution that we wish.\n\n\n\n\n\n\nCheck Your Understanding - R code\n\n\n\nWhat code would we need to write to simulate the following?\n20 data points from a distribution with mean of 1 and standard deviation of 2\n\nR code here -&gt; .\n\n33 data points from a distribution with mean of -3 and standard deviation of 1.23\n\nR code here -&gt; .\n\nfive thousand data points from a distribution with mean of 1.789 and standard deviation of 18.2\n\nR code here -&gt; .\n\n\n\n\n\n\n\n\n\nKey step\n\n\n\nOpen Jamovi, install the Rj add-on and open a new Rj window before we go any further\n\n\nWe can use rnorm() within the Rj code window as usual. Here I’m simulating 20 data points from a distribution with a mean of 1 and a standard deviation of 2. Try running the same code in your window.\n\nThese 20 simulated numbers could now represent a data sample for a variable in an analysis. The difference is we have absolute confidence about the ‘truth’ of the population statistics underlying this data sample.\nYou may notice that you do get 20 numbers, but that you get a different 20 numbers than I did! This is as rnorm() works hard to generate random numbers that are different each time.\n\n\n\n\n\n\nKey step\n\n\n\nYou can verify the random nature of the simulated data by running your code multiple times.\nEven though the code stays the same you will get a different data sample each time. Remember that the code and the underlying distribution remain the always the same but the data sample is changing.\n\n\nThis is actually a good thing for us. Real experiments and data collection always have an element of random luck and we want our simulations to reflect this as well.\n\n\n\n\n\n\nData skills - revision on sampling and populations\n\n\n\nSampling is a fundamental process in statistics where a subset of individuals from a population is selected to estimate characteristics of the whole population. This is built from the assumption that the subset of individuals in our sample is representative of that population.\nSome samples will be more representative than others. We can do a lot of practical work and planning to get the best possible sample for our research but there is always some random chance in who ends up in a data sample and who does not.\n\nThere are two main reasons why a sample might not be representative.\nBias if the data collection has included some bias (intentional or accidental) towards a subgroup of our population then it is very likely that this will lead to an unrepresentative sample. This is not something we can solve with statistics. Good reseach design, consideration of ethics and thorough planning is the only way to avoid systematic biases.\nLuck even if we have a unbiased data collection approach, two samples that draw from the sample population can still be different due to random chance. This will always be true when we are restricted to looking at a subset of the full population - fortunately, this is something we can quantify with statistics. We’ll explore how in this session.\nThe code we write this week will simulate drawing a subset of individuals from a population. Each iteration of the code could be considered as a separate replication of the same experiment.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#estimating-the-samping-distribution-of-the-mean",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#estimating-the-samping-distribution-of-the-mean",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "In week 2 we discussed how sampling relatively small amount of data from a wider population can impact our estimates of values like the mean.\nTo recap, let’s say we’re interested in computing the average attainment for secondary school children on a particular vocabulary test. Our target population might be ‘all secondary school students in the UK’ but is it practically impossible to get data from every single student currently in secondard school. We have to make do with smaller data samples of a few tens or few hundreds of students (or even more, if we’re lucky).\nOur simulations do exactly this. The parameters we pass into rnorm() define ‘population level’ characteristics in the form of a normal distribution and each time we run the function we draw a ‘sample’ of a fixed number of data points from that population. We can think of this as rerunning our data sampling every time the function is executed. This would be massively complicated, time consuming and expensive in real life, but with a few assumptions a computer can simulate it in a fraction of a second.\nWe can now start to explore how sampling variability - AKA the random differences between data samples from the same population - can impact our estimates of the mean and standard deviation.\nUpdate your code to store the data sample into a variable and compute the mean with mean() and standard deviation with sd(). The result might look like this.\nsample = rnorm(20, mean=1, sd=2)\n\n# Compute the mean\nprint('Mean:')\nprint(mean(sample))\n\n# Compute the standard deviation\nprint('Standard Deviation:')\nprint(sd(sample))\n\nRunning this code will show the mean and standard deviation of each individual data sample, try running this a few times. Notice that the values are sometimes close to our ‘population’ estimates of mean = 1 and standard deviation = 2 but also sometimes quite far away.\nTry tweaking your code to answer these questions\n\n\n\n\n\n\n Data Skills - is the estimate of the mean better or worse with a sample of 200?\n\n\n\n\n\nUpdate your code to use this data simulation\nsample = rnorm(200, mean=1, sd=2)\nRunning this a few times should show you mean values much closer to 1 and standard deviations much closer to 2 than we saw with a sample of 20 values. They still won’t be perfect though.\n\n\n\n\n\n\n\n\n\n Data Skills - is the estimate of the mean better or worse when the population standard deviation is 10?\n\n\n\n\n\nUpdate your code to use this data simulation\nsample = rnorm(200, mean=1, sd=10)\nRunning this a few times should make your estimates of the mean and standard deviation much more variable again. When variability in the population is very wide, large data samples won’t necessarily make up the difference.\n\n\n\nWe can get an informal sense of sampling variability by clicking to rerun this code a few times, but R code allows us to do much better, we can tweak the code to rerun this analysis hundreds or thousands of times to explicitly quantify the variability we see.\nTo do this, we’ll need a code element called a loop which looks something like this\nnum_simulation = 10\n\n# Loop to perform simulations\nfor (i in 1:num_simulations) {\n    print(i)\n}\nThere are two aspects to this code block. The first sets the number of simulated data samples we want to draw.\nnum_simulations = 10\nThis line sets the variable num_simulations to 10. This means you want to perform 10 iterations in the loop.\nThe second part is the loop itself:\nfor (i in 1:num_simulations) {\n    print(i)\n}\nThis is a for loop that will iterate from 1 to num_simulations (which is 10 in this case). Here’s what happens inside the loop:\n\nInitialization: The loop starts with i set to 1.\nCondition: The loop will continue to run as long as i is less than or equal to num_simulations.\nIncrement: After each iteration, i is incremented by 1.\n\nInside the loop, the print(i) statement is executed, which prints the current value of i to the console.\nThe output of this code will be the numbers from 1 to 10, each printed on a new line. The full output might look like this:\n\nPrinting the value of i is instructive but really we want our loop to include some interesting computations. Next, we should move our sampling code within the curly brackets {} of the loop.\nEverything within the those curly brackets will be repeated on every iteration of the loop. So if we includ the code to draw a sample and compute the mean, it will be repeated every time. Here is an example:\n# My loop\nnum_simulations = 10\n\nfor (i in 1:num_simulations) {\n    sample = rnorm(20, mean=1, sd=10)\n    print(mean(sample))\n}\n\nThe output of the loop is now the mean of each data sample, though the exact values you get will be slightly different to mine.\nWe can see that 20 samples drawn from a population with mean = 1, sd = 10 we have extremely variable estimates of the mean. Try reducing the standard deviation back down to 2 to see the result.\n# My loop\nnum_simulations = 10\n\nfor (i in 1:num_simulations) {\n    sample = rnorm(20, mean=1, sd=2)\n    print(mean(sample))\n}\n\nThese are much closer to our expected population value of 1.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#visualising-the-samping-distribution-of-the-mean",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#visualising-the-samping-distribution-of-the-mean",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "We’re very close to completing our loop. There is one final issue to sort.\nAt the moment, the estimated sample is lost every time the code in the loop restarts - we need to store the whole set of mean values for every single simulation if we want to properly see what is going on.\nFor this we need to add two elements to the loop. Firstly we need to create a variable to store the means we estimate. We do this before the loop starts.\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nand secondly, within the loop, we need to add the estimated mean to our new mean_values variable.\n# Create a numeric variable to store the means\n    mean_values[i] = mean(sample)\nmean_values can store many numbers altogether in a list or vector. Each individual element can be accessed using the square bracket notation. For example mean_values[5] would access the fifth value in the list. The code above will store each value in to the ith position in the list so that we keep them all.\nThe final code should look like this:\n# My loop\nnum_simulations = 10\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(20, mean=1, sd=2)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\nmean_values now contains all of our simulated means that we can visualise using a histogram with the hist() function.\n\nWhich will produce a simple Histogram of the values for us. Note that, as ever, your histogram will be a little different to mine. Try rerunning the code a few times to get a sense of the variabity.\nWe can make our plot a bit more attractive and informative by specifying some additional inputs. These just change the visualisation of the histogram rather than the analysis.\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Means\", xlim=c(-2, 4),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\nThis has a lot of inputs, lets break down what they all mean\n\n\n\nInput\nDescription\n\n\n\n\nmean_values\nThe data sample to visualise\n\n\nmain\nThe main title\n\n\nxlim\nThe start and end points of the x-axis within a collection\n\n\nxlab\nThe label for the x-axis\n\n\ncol\nThe colour of the bars\n\n\nborder\nThe colour of the borders of the bars\n\n\n\nAdding these to the function call makes a much nicer and more informative image.\n\nYou can tweak the titles and axis labels to be sometime informative to you, and you can change the colours to be anything you like. Try customising the plot - update the labels and colours as you prefer them.\n\n\n\n\n\n\nClick for names of colours in R\n\n\n\n\n\nYou can use the colour names defined here to customise your histogram.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#understanding-the-samping-distribution-of-the-mean",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#understanding-the-samping-distribution-of-the-mean",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "Once your histogram is customised and ready to go - we can put our code to work and understand the core concepts.\nLet’s review what we have so far.\n\nWe have constructed a loop that draws data samples from a defined population\nWe compute and store the mean of that data sample\nWe use a histogram to visualise the distribution of means estimated from all of the data samples\nThis is the sampling distribution of the mean.\n\n\n\n\n\n\n\nKey step\n\n\n\nMake sure you have an organised script ready to go at this point, try adding comments to understand what each line does. You can delete some of the older parts of the code that aren’t included in the loop if you like. Your code should look something like this.:\n# My loop\nnum_simulations = 10\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(20, mean=1, sd=2)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Means\", xlim=c(-2, 4),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\n\nRecall that the variance of the sampling distribution is one of its key parameters - it tells us how precisely our estimated means map onto the known population mean. (Recall from lecture 3 that the standard error of the mean can be computed from this number).\nSampling distributions with wider variances indicate that we have less precisely estimated the mean, whereas tighter distributions indicate that we have more confidence. Let’s use our code to illustrate this concept.\n\n\n\n\n\n\nKey step\n\n\n\nChange your code to repeat the simulation 1000 times rather than just 10, you should get a much more ‘normal’ distribution that changes less when you rerun the code.\n\n\n\nThis distribution is the sampling distribution of the mean for the simulated data. We can learn the following things:\n\nWe are most likely to estimate a value of the mean that is close to 1\nThe spread of the distribution indicates that values between 0.5 and 1.5 are still fairly common\nWe will very occasionally estimate a mean around 0 or around 2\n\nTry changing your code to answer these questions.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when each data sample has 100 data points rather than just 20?\n\n\nFor this question, you’ll need to update the first input to rnorm() to have 100 data points rather than 20.\n\n\nWith more data in each data sample - we can see that the spread in the sampling distribution has reduced. In other words, each sample has a similar estimate of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nWe will very occasionally estimate a mean around 0.5 or around 1.5, but these are rare\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when we increase the standard deviation to 10?\n\n\nFor this question, you’ll need to update the sd input to rnorm() to be 10 rather than 2.\n\n\nWith more variability in each data sample - we can see that the variance in the sampling distribution has increased. In other words, each sample can have extremely different estimates of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nIt is still common to get estimates of the mean of 0.5 or around 1.5.\nWith this much variability, some data samples have a mean estimate as far away as -1 or 3!\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nBack in week 4 we computed the following descriptives for participants confidence in a range of face processing tasks.\n\nCan you compute the sampling distribution of the mean for the AIConfidenceBefore variable?\n\n\nFor this question, you’ll need to the number of data samples, the mean and the standard deviation using the values in hte descriptive statistics.\nIts also a good idea to update the xlim of your histogram to something around c(2, 7).\n\n\nThe code and result should look something like this:\n\nBased on this, we can see that the most commonly estimated mean value is very close to our expected mean of 4.57 and that almost every data sample has a mean of between 4 and 5.\nThis is a useful way to visualise how accurate we are likely to have been when estimating our sample means.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#sampling-distributions-for-one-sample-t-tests",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#sampling-distributions-for-one-sample-t-tests",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "So far we’ve looked at sampling distributions of the mean but we can do the same thing with t-tests to get a sense of how varaible a t statistic might be under different circumstances. This also gives us some important insights into how t and p values are related.\nTo do this we’ll need to make a couple of additions to our code.\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window before starting this segment. The code changes will be large and you may want to refer back to the previous parts for revision later.\n\n\nThe first thing we’ll need is to compute a t value for a one sample t-test from our data vector from this definition:\n\\[\nt = \\frac{\\text{The sample mean} - \\text{Comparison Value}}{\\text{The standard error of the mean}}\n\\]\nWe need to use this code to compute the three ingredients and the t-test itself. Copy this code into your new Rj window and take a moment to understand each line.\n# Number of participants\nn = 20\n\n# Generate a random sample\nsample = rnorm(n, mean = 0, sd = 1)\n\n# Calculate the sample mean and standard deviation\nsample_mean = mean(sample)\nsample_sd = sd(sample)\n\n# Calculate the sample error of the mean \nsample_standard_error = sample_sd / sqrt(n)\n\n# Define comparison value\ncomparison_value = 0\n\n# Compute t-value\nt_value = (sample_mean - comparison_value) / sample_standard_error\nprint(t_value)\nNote that we’re simulating data with a mean of zero and using a comparison_value of zero as well. This is our Null Model - the t-values we get from this code are ones that might appear if there is no true effect in the data.\n\nAs before, you can run this code a few times to see the variability in the final t-value. Remember this this is a null model - most of the t-values are close to zero, but some are quite large…\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nFor the final part of new code, we need to combine our t-test with the loop from the previous section.\nCopy this code into your Rj window (you can overwrite the previous code if you like), and fill in the blanks indicated by __YOUR_CODE_HERE__ (you should delete __YOUR_CODE_HERE__ and replace it with your answer).\nIf you get it right you will see a histogram of sampling distribution of t values.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = __YOUR_CODE_HERE__\n\n    # Generate a random sample\n    sample = __YOUR_CODE_HERE__\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = __YOUR_CODE_HERE__\n    sample_sd = __YOUR_CODE_HERE__\n\n    # Calculate the sample error of the mean \n    sample_standard_error = __YOUR_CODE_HERE__\n\n    # Define comparison value\n    comparison_value = __YOUR_CODE_HERE__\n\n    # Compute t-value\n    t_value[i] = __YOUR_CODE_HERE__\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, main = \"Sampling Distribution of t-values\", xlim=c(-4, 4),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n\n\nAll the answers are in previous sections, pay particular attention to the code defining the t-test in section 5.\nChat with one of your tutors if you get stuck.\n\n\nWell done if you got this correct. The final code should look like this:\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 0, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, main = \"Sampling Distribution of t-values\", xlim=c(-4, 4),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n    \n\n\n\n\n\n\nAfter this exercise, you can now compute the null model for a one sample t-test yourself. With this code you can run an analysis to explore how likely it is to observe different t-values when we assume the null hypothesis is true.\nThink about the following questions and tweak your code to find the answers.\n\n\n\n\n\n\n Data Skills - how does the sampling distribution change when we have large samples? Use n = 250 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 25:\n    # Number of participants\n    n = 250\n\nRunning this a few times, you should see that the sampling distribution gets narrower compared to what we saw for n = 20. We commonly see values between -2 and 2 but almost never see values of -3 or +3.\n\n\n\n\n\n\n\n\n\n Data Skills - how does the sampling distribution change when we have very small samples? Use n = 4 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 5:\n    # Number of participants\n    n = 4\n\nRunning this a few times, you should see that the sampling distribution gets broader than for smaller samples. We see more values close to 4 and -4 than we did with a dataset of 20 or 250.\n\n\n\nIn all of these analyses we are absolutely assuming that the null hypothesis of no difference is true. With this assumptions, our sampling distribution shows that we get more extreme t-values (around +/- 4) by chance when working with smaller data samples. By contrast for larger data samples these same values occur extremely rarely.\nWhen data are perfectly normally distributed (parametric assumptions are met), Jamovi and R can compute these null distributions for us using a shortcut without simulating lots of data samples but the result is the same.\nTo conduct a hypothesis test from real data, we would compute the t-statistic as normal and can then compare the observed value to this sampling distribution that assumes no difference and has the same sample size. If the t-value we’ve observed from the real data would be extremely unlikely to have occured if the null is true, then we can reject that hypothesis.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#why-do-we-test-the-null-distribution",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#why-do-we-test-the-null-distribution",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "Finally, let’s consider why we test against the null distribution of no effect. This is fairly straightforward from the code we have written so far.\nWe can absolutely compute a sampling distribution for a case where there is a real difference between the mean and the comparison value. For example, this code simulates data with a mean of 1 and compares that to a comparison value of 0. Note that the rnorm() function inputs have changed.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 1, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, breaks=24, main = \"Sampling Distribution of t-values\", xlim=c(-10, 10),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\nThe result is a sampling distribution which is not centred around mean.\n\nThis distribution tells us that with 20 participants, a mean of 1 and a standard deviation of 1 - we are mostly likely to get a -value of 5 but that t-value could be as small as 2 or as large as 9 for extreme data samples.\nThis is very useful, but the issue that is we have had to assume what the real effect is. We get radically different sampling distributions if we change that effect around.\n\n\n\n\n\n\n\n\n\n\n\nThere are an infinite number of possible sampling distributions when there is a real effect! It isn’t possible to know which distribution we should be comparing with without making some massive assumptions about the exact size, variability and direction of effect that we expect to see. This simply isn’t possible in practice.\nIn contrast, the null case is very closely controlled and easier to work with. The interpretation is trickier but rejecting the null hypothesis is a tractable problem that we can do a good job of solving.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#summary",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#summary",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "We’ve done a lot of work to consolidate our skills on filtering data and computing one sample t-tests! You should now be able to compute these tests in Jamovi and create reproducible R code.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "The sampling distribution is central to how we can move from t-values to p-values - yet it is one of the trickier parts of this course. The lectures and pre-lecture materials frequently deal with computer simulations to show the properties of the tests we’re using, so this week you will write your own simulation to help understand the sampling distribution!\n\nLearning objectives for this session\n\n\n\n\n\n\n\nQuantitative Methods\n\n\n\n\n\nSampling Distributions\n\n\n\nt-values\n\n\n\np-values\n\n\n\n\n\n\n\n\n\n\n\nData Skills\n\n\n\n\n\nComputing a one sample t-test in R\n\n\n\nSimulating data in R\n\n\n\nWriting loops in R\n\n\n\n\n\n\n\n\n\n\n\nOpen Science\n\n\n\n\n\nCreating shareable code to demonstrate a statistical concept\n\n\n\n\n\nTLDR: There is no data to download! we’re going to make the data ourselves.\nThe dataset this week is very simple - we’re going to create it ourselves using R. At this point, we know a lot about the types of questions that we can ask with t-tests and we know a lot about the data processing around it. We can check our understanding of different aspects of these statistical tests by simulating data samples that have known properties (eg mean and standard deviation) and checking if the outcome of the t-test is what we would expect.\nThis way we can numerically validate the tests we’re running and take a look under the hood and see what sort of processes Jamovi and R are using to compute the tests for us.\n\n\n\n\n\n\nContent Info\n\n\n\nThere is a lot of code in this session - don’t be afraid to ask for help from your tutors!\n\n\n\n\n\nToday, we’ll first introduce some concepts and functions in R to simulate a data sample and compute our own one sample t-test. With that in hand, we will set up code to run thousands of tests in one go to help us to better understand sampling distributions.\n\n\n\nSo far we’ve looked at sampling distributions of the mean but we can do the same thing with t-tests to get a sense of how varaible a t statistic might be under different circumstances. This also gives us some important insights into how t and p values are related.\nTo do this we’ll need to make a couple of additions to our code.\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window before starting this segment. The code changes will be large and you may want to refer back to the previous parts for revision later.\n\n\nThe first thing we’ll need is to compute a t value for a one sample t-test from our data vector from this definition:\n\\[\nt = \\frac{\\text{The sample mean} - \\text{Comparison Value}}{\\text{The standard error of the mean}}\n\\]\nWe need to use this code to compute the three ingredients and the t-test itself. Copy this code into your new Rj window and take a moment to understand each line.\n# Number of participants\nn = 20\n\n# Generate a random sample\nsample = rnorm(n, mean = 0, sd = 1)\n\n# Calculate the sample mean and standard deviation\nsample_mean = mean(sample)\nsample_sd = sd(sample)\n\n# Calculate the sample error of the mean \nsample_standard_error = sample_sd / sqrt(n)\n\n# Define comparison value\ncomparison_value = 0\n\n# Compute t-value\nt_value = (sample_mean - comparison_value) / sample_standard_error\nprint(t_value)\nNote that we’re simulating data with a mean of zero and using a comparison_value of zero as well. This is our Null Model - the t-values we get from this code are ones that might appear if there is no true effect in the data.\n\nAs before, you can run this code a few times to see the variability in the final t-value. Remember this this is a null model - most of the t-values are close to zero, but some are quite large…\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nFor the final part of new code, we need to combine our t-test with the loop from the previous section.\nCopy this code into your Rj window (you can overwrite the previous code if you like), and fill in the blanks indicated by __YOUR_CODE_HERE__ (you should delete __YOUR_CODE_HERE__ and replace it with your answer).\nIf you get it right you will see a histogram of sampling distribution of t values.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = __YOUR_CODE_HERE__\n\n    # Generate a random sample\n    sample = __YOUR_CODE_HERE__\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = __YOUR_CODE_HERE__\n    sample_sd = __YOUR_CODE_HERE__\n\n    # Calculate the sample error of the mean \n    sample_standard_error = __YOUR_CODE_HERE__\n\n    # Define comparison value\n    comparison_value = __YOUR_CODE_HERE__\n\n    # Compute t-value\n    t_value[i] = __YOUR_CODE_HERE__\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, main = \"Sampling Distribution of t-values\", xlim=c(-4, 4),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n\n\nAll the answers are in previous sections, pay particular attention to the code defining the t-test in section 5.\nChat with one of your tutors if you get stuck.\n\n\nWell done if you got this correct. The final code should look like this:\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 0, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, main = \"Sampling Distribution of t-values\", xlim=c(-4, 4),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n    \n\n\n\n\n\n\nAfter this exercise, you can now compute the null model for a one sample t-test yourself. With this code you can run an analysis to explore how likely it is to observe different t-values when we assume the null hypothesis is true.\nThink about the following questions and tweak your code to find the answers.\n\n\n\n\n\n\n Data Skills - how does the sampling distribution change when we have large samples? Use n = 250 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 25:\n    # Number of participants\n    n = 250\n\nRunning this a few times, you should see that the sampling distribution gets narrower compared to what we saw for n = 20. We commonly see values between -2 and 2 but almost never see values of -3 or +3.\n\n\n\n\n\n\n\n\n\n Data Skills - how does the sampling distribution change when we have very small samples? Use n = 4 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 5:\n    # Number of participants\n    n = 4\n\nRunning this a few times, you should see that the sampling distribution gets broader than for smaller samples. We see more values close to 4 and -4 than we did with a dataset of 20 or 250.\n\n\n\nIn all of these analyses we are absolutely assuming that the null hypothesis of no difference is true. With this assumptions, our sampling distribution shows that we get more extreme t-values (around +/- 4) by chance when working with smaller data samples. By contrast for larger data samples these same values occur extremely rarely.\nWhen data are perfectly normally distributed (parametric assumptions are met), Jamovi and R can compute these null distributions for us using a shortcut without simulating lots of data samples but the result is the same.\nTo conduct a hypothesis test from real data, we would compute the t-statistic as normal and can then compare the observed value to this sampling distribution that assumes no difference and has the same sample size. If the t-value we’ve observed from the real data would be extremely unlikely to have occured if the null is true, then we can reject that hypothesis.\n\n\n\nFinally, let’s consider why we test against the null distribution of no effect. This is fairly straightforward from the code we have written so far.\nWe can absolutely compute a sampling distribution for a case where there is a real difference between the mean and the comparison value. For example, this code simulates data with a mean of 1 and compares that to a comparison value of 0. Note that the rnorm() function inputs have changed.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 1, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, breaks=24, main = \"Sampling Distribution of t-values\", xlim=c(-10, 10),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\nThe result is a sampling distribution which is not centred around mean.\n\nThis distribution tells us that with 20 participants, a mean of 1 and a standard deviation of 1 - we are mostly likely to get a -value of 5 but that t-value could be as small as 2 or as large as 9 for extreme data samples.\nThis is very useful, but the issue that is we have had to assume what the real effect is. We get radically different sampling distributions if we change that effect around.\n\n\n\n\n\n\n\n\n\n\n\nThere are an infinite number of possible sampling distributions when there is a real effect! It isn’t possible to know which distribution we should be comparing with without making some massive assumptions about the exact size, variability and direction of effect that we expect to see. This simply isn’t possible in practice.\nIn contrast, the null case is very closely controlled and easier to work with. The interpretation is trickier but rejecting the null hypothesis is a tractable problem that we can do a good job of solving.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#the-dataset",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#the-dataset",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "TLDR: There is no data to download! we’re going to make the data ourselves.\nThe dataset this week is very simple - we’re going to create it ourselves using R. At this point, we know a lot about the types of questions that we can ask with t-tests and we know a lot about the data processing around it. We can check our understanding of different aspects of these statistical tests by simulating data samples that have known properties (eg mean and standard deviation) and checking if the outcome of the t-test is what we would expect.\nThis way we can numerically validate the tests we’re running and take a look under the hood and see what sort of processes Jamovi and R are using to compute the tests for us.\n\n\n\n\n\n\nContent Info\n\n\n\nThere is a lot of code in this session - don’t be afraid to ask for help from your tutors!",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#the-challenge",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#the-challenge",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "Today, we’ll first introduce some concepts and functions in R to simulate a data sample and compute our own one sample t-test. With that in hand, we will set up code to run thousands of tests in one go to help us to better understand sampling distributions.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#sampling-distributions-for-one-sample-t-tests",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#sampling-distributions-for-one-sample-t-tests",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "So far we’ve looked at sampling distributions of the mean but we can do the same thing with t-tests to get a sense of how varaible a t statistic might be under different circumstances. This also gives us some important insights into how t and p values are related.\nTo do this we’ll need to make a couple of additions to our code.\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window before starting this segment. The code changes will be large and you may want to refer back to the previous parts for revision later.\n\n\nThe first thing we’ll need is to compute a t value for a one sample t-test from our data vector from this definition:\n\\[\nt = \\frac{\\text{The sample mean} - \\text{Comparison Value}}{\\text{The standard error of the mean}}\n\\]\nWe need to use this code to compute the three ingredients and the t-test itself. Copy this code into your new Rj window and take a moment to understand each line.\n# Number of participants\nn = 20\n\n# Generate a random sample\nsample = rnorm(n, mean = 0, sd = 1)\n\n# Calculate the sample mean and standard deviation\nsample_mean = mean(sample)\nsample_sd = sd(sample)\n\n# Calculate the sample error of the mean \nsample_standard_error = sample_sd / sqrt(n)\n\n# Define comparison value\ncomparison_value = 0\n\n# Compute t-value\nt_value = (sample_mean - comparison_value) / sample_standard_error\nprint(t_value)\nNote that we’re simulating data with a mean of zero and using a comparison_value of zero as well. This is our Null Model - the t-values we get from this code are ones that might appear if there is no true effect in the data.\n\nAs before, you can run this code a few times to see the variability in the final t-value. Remember this this is a null model - most of the t-values are close to zero, but some are quite large…\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nFor the final part of new code, we need to combine our t-test with the loop from the previous section.\nCopy this code into your Rj window (you can overwrite the previous code if you like), and fill in the blanks indicated by __YOUR_CODE_HERE__ (you should delete __YOUR_CODE_HERE__ and replace it with your answer).\nIf you get it right you will see a histogram of sampling distribution of t values.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = __YOUR_CODE_HERE__\n\n    # Generate a random sample\n    sample = __YOUR_CODE_HERE__\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = __YOUR_CODE_HERE__\n    sample_sd = __YOUR_CODE_HERE__\n\n    # Calculate the sample error of the mean \n    sample_standard_error = __YOUR_CODE_HERE__\n\n    # Define comparison value\n    comparison_value = __YOUR_CODE_HERE__\n\n    # Compute t-value\n    t_value[i] = __YOUR_CODE_HERE__\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, main = \"Sampling Distribution of t-values\", xlim=c(-4, 4),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n\n\nAll the answers are in previous sections, pay particular attention to the code defining the t-test in section 5.\nChat with one of your tutors if you get stuck.\n\n\nWell done if you got this correct. The final code should look like this:\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 0, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, main = \"Sampling Distribution of t-values\", xlim=c(-4, 4),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\n    \n\n\n\n\n\n\nAfter this exercise, you can now compute the null model for a one sample t-test yourself. With this code you can run an analysis to explore how likely it is to observe different t-values when we assume the null hypothesis is true.\nThink about the following questions and tweak your code to find the answers.\n\n\n\n\n\n\n Data Skills - how does the sampling distribution change when we have large samples? Use n = 250 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 25:\n    # Number of participants\n    n = 250\n\nRunning this a few times, you should see that the sampling distribution gets narrower compared to what we saw for n = 20. We commonly see values between -2 and 2 but almost never see values of -3 or +3.\n\n\n\n\n\n\n\n\n\n Data Skills - how does the sampling distribution change when we have very small samples? Use n = 4 as an example.\n\n\n\n\n\nUpdate your code to reduce the number of participants to 5:\n    # Number of participants\n    n = 4\n\nRunning this a few times, you should see that the sampling distribution gets broader than for smaller samples. We see more values close to 4 and -4 than we did with a dataset of 20 or 250.\n\n\n\nIn all of these analyses we are absolutely assuming that the null hypothesis of no difference is true. With this assumptions, our sampling distribution shows that we get more extreme t-values (around +/- 4) by chance when working with smaller data samples. By contrast for larger data samples these same values occur extremely rarely.\nWhen data are perfectly normally distributed (parametric assumptions are met), Jamovi and R can compute these null distributions for us using a shortcut without simulating lots of data samples but the result is the same.\nTo conduct a hypothesis test from real data, we would compute the t-statistic as normal and can then compare the observed value to this sampling distribution that assumes no difference and has the same sample size. If the t-value we’ve observed from the real data would be extremely unlikely to have occured if the null is true, then we can reject that hypothesis.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#why-do-we-test-the-null-distribution",
    "href": "tutorials/rmb-week-10_sampling-distributions-tvalues.html#why-do-we-test-the-null-distribution",
    "title": "Week 10 : Simulating Sampling Distributions for t-values",
    "section": "",
    "text": "Finally, let’s consider why we test against the null distribution of no effect. This is fairly straightforward from the code we have written so far.\nWe can absolutely compute a sampling distribution for a case where there is a real difference between the mean and the comparison value. For example, this code simulates data with a mean of 1 and compares that to a comparison value of 0. Note that the rnorm() function inputs have changed.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nt_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Number of participants\n    n = 20\n\n    # Generate a random sample\n    sample = rnorm(n, mean = 1, sd = 1)\n\n    # Calculate the sample mean and standard deviation\n    sample_mean = mean(sample)\n    sample_sd = sd(sample)\n\n    # Calculate the sample error of the mean \n    sample_standard_error = sample_sd / sqrt(n)\n\n    # Define comparison value\n    comparison_value = 0\n\n    # Compute t-value\n    t_values[i] = (sample_mean - comparison_value) / sample_standard_error\n}\n\n# Plot the sampling distribution of t-values\nhist(t_values, breaks=24, main = \"Sampling Distribution of t-values\", xlim=c(-10, 10),\n     xlab = \"Sample t-value\", col = \"skyblue\", border = \"white\")\nThe result is a sampling distribution which is not centred around mean.\n\nThis distribution tells us that with 20 participants, a mean of 1 and a standard deviation of 1 - we are mostly likely to get a -value of 5 but that t-value could be as small as 2 or as large as 9 for extreme data samples.\nThis is very useful, but the issue that is we have had to assume what the real effect is. We get radically different sampling distributions if we change that effect around.\n\n\n\n\n\n\n\n\n\n\n\nThere are an infinite number of possible sampling distributions when there is a real effect! It isn’t possible to know which distribution we should be comparing with without making some massive assumptions about the exact size, variability and direction of effect that we expect to see. This simply isn’t possible in practice.\nIn contrast, the null case is very closely controlled and easier to work with. The interpretation is trickier but rejecting the null hypothesis is a tractable problem that we can do a good job of solving.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 10 - Sampling Distributions of t-values"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#understanding-the-sampling-distribution-of-the-mean",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#understanding-the-sampling-distribution-of-the-mean",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "Once your histogram is customised and ready to go - we can put our code to work and understand the core concepts.\nLet’s review what we have so far.\n\nWe have constructed a loop that draws data samples from a defined population\nWe compute and store the mean of that data sample\nWe use a histogram to visualise the distribution of means estimated from all of the data samples\nThis is the sampling distribution of the mean.\n\n\n\n\n\n\n\nKey step\n\n\n\nMake sure you have an organised script ready to go at this point, try adding comments to understand what each line does. You can delete some of the older parts of the code that aren’t included in the loop if you like. Your code should look something like this.:\n# My loop\nnum_simulations = 10\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(20, mean=1, sd=2)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Means\", xlim=c(-2, 4),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\n\nRecall that the variance of the sampling distribution is one of its key parameters - it tells us how precisely our estimated means map onto the known population mean. (Recall from lecture 3 that the standard error of the mean can be computed from this number).\nSampling distributions with wider variances indicate that we have less precisely estimated the mean, whereas tighter distributions indicate that we have more confidence. Let’s use our code to illustrate this concept.\n\n\n\n\n\n\nKey step\n\n\n\nChange your code to repeat the simulation 1000 times rather than just 10, you should get a much more ‘normal’ distribution that changes less when you rerun the code.\n\n\n\nThis distribution is the sampling distribution of the mean for the simulated data. We can learn the following things:\n\nWe are most likely to estimate a value of the mean that is close to 1\nThe spread of the distribution indicates that values between 0.5 and 1.5 are still fairly common\nWe will very occasionally estimate a mean around 0 or around 2\n\nTry changing your code to answer these questions.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when each data sample has 100 data points rather than just 20?\n\n\nFor this question, you’ll need to update the first input to rnorm() to have 100 data points rather than 20.\n\n\nWith more data in each data sample - we can see that the spread in the sampling distribution has reduced. In other words, each sample has a similar estimate of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nWe will very occasionally estimate a mean around 0.5 or around 1.5, but these are rare\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nHow does the sampling distribution change when we increase the standard deviation to 10?\n\n\nFor this question, you’ll need to update the sd input to rnorm() to be 10 rather than 2.\n\n\nWith more variability in each data sample - we can see that the variance in the sampling distribution has increased. In other words, each sample can have extremely different estimates of the population mean.\nCompare this result to the ‘Key step’ above:\n\n\nThe centre of the distribution is still very close to 1\nIt is still common to get estimates of the mean of 0.5 or around 1.5.\nWith this much variability, some data samples have a mean estimate as far away as -1 or 3!\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nBack in week 4 we computed the following descriptives for participants confidence in a range of face processing tasks.\n\nCan you compute the sampling distribution of the mean for the AIConfidenceBefore variable?\n\n\nFor this question, you’ll need to the number of data samples, the mean and the standard deviation using the values in hte descriptive statistics.\nIts also a good idea to update the xlim of your histogram to something around c(2, 7).\n\n\nThe code and result should look something like this:\n\nBased on this, we can see that the most commonly estimated mean value is very close to our expected mean of 4.57 and that almost every data sample has a mean of between 4 and 5.\nThis is a useful way to visualise how accurate our sample mean estimates are likely to be.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#sampling-distributions-from-non-normal-data",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#sampling-distributions-from-non-normal-data",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "Throughout this course, we have seen many data variables that are not normally distributed. Examples include dice rolls, reaction times and the intrusive memories from last week. These are interesting datasets but don’t follow some of the nice assumptions that we can use with normally distributed data.\nNow that we have our loops to compute sampling distributions, we can explore how much of a problem non normally distributed data is for our estimates.\n\n\n\n\n\n\nKey step\n\n\n\nOpen a new Rj window and copy your code loop from section 5 into it before starting this segment.\nWe will make large changes to the code and you may want to refer back to the previous parts for revision later.\n\n\nLet’s simulate some data for an experiment. Let’s simulate data from an experiment where participants indicate their confidence in performing some action on a scale from 1 to 10. The absolute truth of these data observations is that every participant has a equal probability of selecting any confidence from 1 to 10 - this is known as a uniform distribution and it is very different to a normal distribution.\nWe can simulate this data using runif() - note that this is an abbreviation of ‘random uniform’, not an indication to ‘run if’ something happens…\nTo use runif() we specify an n as usual in addition to the smallest and largest values that might occur.\nrunif(n, min = 1, max = 10)\nAs usual, we can slot this into our data loop to simulate our uniform distribution. Let’s start by drawing a single value during each repetition of the experiment. This will visualise our distribution.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = runif(1, min = 1, max = 10)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(0, 11),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\nYou can run this a few times to verify that the histogram shows a nearly uniform distribution where all outcomes are approximately equally likely to occur.\nNow, something odd happens when we increase the size of each data sample drawn from our population parameters. Rather than visualising the response of a single individual - lets take a look at the average response of 10 different individuals.\nUpdate the code to have an n of 10 rather than 1 in the call to runif().\n\nNow, though the raw data are uniformly distributed - the averages of uniform variables look like they are more normally distributed. Strange.\nThis is a property called the Central Limit Theorem. The Central Limit Theorem states that, given a sufficiently large sample size, the sampling distribution of the sample mean will be approximately normally distributed, regardless of the shape of the population distribution.\nThis is important as it tells us that the properties of data can change when we make computations from it. In our example, each individual response is uniformly distributed but the average response is approximately normally distributed.\nSecondly, this is why the normal distribution is so important. The Central Limit Theorem means that is weirdly likely that we will encounter normally distributed data variables - any variable that is an aggregate of other things becomes more and more normally distributed. We often deal with this sort of composite data in Psychology,\nThe learning statistics with jamovi textbook as an excellent description of central limit theorem if you want more information.",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-8_sampling-distributions-mean.html#sampling-distributions-of-the-maximum",
    "href": "tutorials/rmb-week-8_sampling-distributions-mean.html#sampling-distributions-of-the-maximum",
    "title": "Week 8 : Simulating Sampling Distributions",
    "section": "",
    "text": "Key step\n\n\n\nOpen a new Rj window and copy your code loop into it before starting this segment.\nWe will make large changes to the code and you may want to refer back to the previous parts for revision later.\n\n\nWe can compute sampling distributions for any summary statistic that we want to. This can be useful for quantifying sampling variability properties other than the mean. One straightforward example is the sampling distribution of the maximum.\nLet’s use the heights of human women as an example. According to Our World in Data, the height of women born between 1980 and 1994 is normally distributed with a mean values of 164.7cm and the standard deviation is 7.07cm.\nWe can use these parameters to define a population that we can use to simulate our data samples. This code will draw a data sample of the height of a single person.\nrnorm(1, mean=164.7, sd=7.07)\nWe can plug this into our code to compute the sampling distributions for height. The full loop will look like this - remember to update the xlim input to the histograms as well!\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(10, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n}\n\n# Plot the sampling distribution\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\nThis is the sampling distribution of the mean for samples of 10 data points draw from the population distribution of women’s heights. The sampling distribution shows that the average height of a group of 10 women drawn randomly from the population will nearly always fall between 160cm and 170cm.\n\nWe can adapt this code to ask other questions - for example how tall is the tallest person in a group of 10 women likely to be?\nWe can adapt our code to compute this sampling distribution of maximums as well. Here is an exmaple - look out for sections with NEW CODE indicating a change.\nCritically, we use the max() function to return the maximum value of each data sample, in exactly the same way that we used mean() to compute the average.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nmax_values = numeric(num_simulations)  # &lt;- NEW CODE\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(10, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n    max_values[i] = max(sample)  # &lt;- NEW CODE\n}\n\n# Plot the sampling distribution\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\n# NEW CODE\nhist(max_values, main = \"Sampling Distribution of Maximums\", xlim=c(130, 200),\n     xlab = \"Sample Maximum\", col = \"skyblue\", border = \"white\")\n    \n\nThis shows us that the tallest person in a group of 10 women is likely to be between 170cm and 180cm.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestionHintSolution\n\n\nCan you compute the sampling distribution of the minimum value to compute the height of the shortest person in groups of 10 women?\nUpdate the code that you have already written.\n\n\nTake a look at the changes highlighted in the previous section, can you repeat those changes to include an equivalent to the maximum?\n\n\nThe following code computes and displays the sampling distribution of the minimum value.\n# My loop\nnum_simulations = 1000\n\n# Create a numeric variable to store the means\nmean_values = numeric(num_simulations)\nmax_values = numeric(num_simulations)\nmin_values = numeric(num_simulations)\n\n# Run the loop\nfor (i in 1:num_simulations) {\n    # Draw a data sample\n    sample = rnorm(4, mean=164.7, sd=7.07)\n    # Compute and store the mean of the data sample\n    mean_values[i] = mean(sample)\n    max_values[i] = max(sample)\n    min_values[i] = min(sample)\n    \n}\n\n# Plot the sampling distribution of t-values\nhist(mean_values, main = \"Sampling Distribution of Mean\", xlim=c(130, 200),\n     xlab = \"Sample Mean\", col = \"skyblue\", border = \"white\")\n\nhist(max_values, main = \"Sampling Distribution of Maximums\", xlim=c(130, 200),\n     xlab = \"Sample Maximum\", col = \"skyblue\", border = \"white\")\n    \nhist(min_values, main = \"Sampling Distribution of Minimums\", xlim=c(130, 200),\n     xlab = \"Sample Minimum\", col = \"skyblue\", border = \"white\")",
    "crumbs": [
      "Home",
      "Research Methods B",
      "Week 8 - Sampling Distributions of Means"
    ]
  },
  {
    "objectID": "tutorials/rmb-week-9_one-way-anova.html#control-condition",
    "href": "tutorials/rmb-week-9_one-way-anova.html#control-condition",
    "title": "Week 9 : One Way Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Run yourself with Control variable - do you see a difference?"
  }
]